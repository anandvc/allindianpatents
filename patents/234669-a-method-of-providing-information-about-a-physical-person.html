<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/234669-a-method-of-providing-information-about-a-physical-person by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 14:22:37 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 234669:A METHOD OF PROVIDING INFORMATION ABOUT A PHYSICAL PERSON</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">A METHOD OF PROVIDING INFORMATION ABOUT A PHYSICAL PERSON</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A METHOD OF PROVIDING INFORMATION ABOUT A PHYSICAL PERSON The present invention relates to a method for determining the position and/or orientation of a creature (3) relative to an environment by connecting the creature (3) to a locating member (4) including a transducer (5) so that the relative positions and/or orientations of the creature (3) and the transducer (5) are arranged to be within a limited interval. The transducer determines its position and/or orientation relative to the environment by receiving incident signals from the signal sources (9) in the environment, and the position and/or orientation of the creature (3) is determined by means of the position and/or orientation determined for the transducer (5)</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td> <br><br>
 Method and apparatus of managing information about a person<br>
This invention relates to an apparatus for providing information about a physical person, comprising a recorder for recording audio and/or image data on the physical person.<br>
Moreover, the invention relates to a method of providing information about a physical person, comprising the steps of recording audio/image data of the physical person.<br>
Various technical solutions to process recordings of facial and vocal expressions of people have been proposed in the prior art. Results of the processing have been used to identify persons ~ among others for access control applications. Such applications have for instance been based on recognizing users on the basis of either voice (speaker recognition) or face recognition techniques. Further, various techniques have been proposed for recognizing a mood or an emotional state of a person.<br>
Conference Proceed of ICICS 1997, International Conference on information, Communications and Signal Processing (IEEE, vol.1, pp. 397-401) discloses a computer multimedia method using a combination of video and audio for recording facial and vocal expressions for emotion translation between cultures.<br>
DE 195 02 627 Al discloses a method and a transmitter/receiver apparatus that provide people who desire to establish contact with each other with means for initiating the contact. A contact-seeking person may transmit a signal that can be received by a person willing or ready to establish personal contact, who may, in turn, be notified by an acoustic or a visual signal.<br>
DE 198 01 666 Al discloses a method of detecting received characteristic information and matching the received characteristic information with pre-stored characteristic infonnation to detect and indicate a complete or partial match.<br>
However, the prior art has not proposed any solutions to the problem that most people have difficulties remembering people that they do not meet very often. In some -typically business-related - situations people may have a business card of a person to help<br><br>
them remember details of that particular person. However, they may not be able to call the person to mind even though they have his or her business card in front of them.<br>
In obtaining good legations with people that we do not meet very frequently, it may be of crucial importance to be able to recall impressions in our minds of people we have met before.<br>
The above-mentioned problem is solved when the electronic device mentioned in the opening paragraph further comprises a processor with a memory for processing the audio and/or visual data to recognize the identity of the physical person, and for retrieving stored information about the recognized identity; and a user interface for providing the stored information to a user.<br>
Consequently the electronic device can assist the user in remembering persons the user do not meet very often. This in turn may aid in obtaining improved business relations or an improved personal relationship.<br>
Since it thereby is easier to call people to mind, less effort is spent wondering who is who. Further, a user's efforts in clearing up misunderstandings caused by confusing information on different people may be reduced. This in turn may lead to a more efficient use of time.<br>
In an expedient embodiment the device comprises a microphone for acquiring the audio data; and a camera for acquiring the image data. Since both microphones and cameras are commercially available that have very small physical dimensions they can be built integrally with very small portable appliances.<br>
Further, the processor can be arranged to process the audio data to determine the tone of voice of the person, and to store a representation of the tone of voice. This is convenient in that the tone of voice typically comprises much information about the person and in particular about the emotional state or mood of a speaking person.<br>
Moreover, the processor can be arranged to associate a representation of a tone of voice with a representation of a mood. Thereby a given tone of voice can be associated with a redeemed mood.<br>
Besides, the processor can be arranged to process the image data to determine the facial expressions of the person, and to store a representation of the facial expression. Typically, facial expressions are related closely to the mood or emotional state of a person. This, in turn, provides the device with a good basis for registering or recognizing the mood or emotional state of a person.<br><br>
When the processor is arranged to associate a representation of a facial expression with a representation of a mood, a facial expression can be linked with a predefined type of mood.<br>
The processor can be arranged to combine a representation of a facial expression and a representation of atone of voice with a representation of a mood. Thereby a more detailed or robust registration/recognition of a mood can be achieved.<br>
In a preferred embodiment  the representation of tone of voice comprises values representative of speaking rate, pitch contour, or loudness of voice.<br>
The processing of the audio/image data involves generating data that are characteristic for the person. Thereby a person can be recognized on the basis of recorded data.<br>
In a preferred embodiment the device is arranged to provide a user with the option of storing information about a person and associating that information with the date, that are characteristic for the person. Thereby a user can add his or her own notes or other types of information relating to a given person<br>
This is particularly expedient when the information comprises text, pictures, or image sequences.<br>
When the device is arranged to store a history of representations of facial expressions and/or representations of tones of voice, a user can compile a history of the person. This can, in turn, assist the user in getting to know the person better.<br>
When, the device is arranged to store a history of representations of mood, a more personal knowledge of the person may be obtained.<br>
The device can be arranged to compare a current representation of mood with previously stored representation in the history of representations of mood; and deliver information to the user about the comparison. This may aid the user in obtaining information about the person more quickly than by taking a detailed look of the history of recordings.<br>
The invention also relates to a method.<br>
The invention will be explained more fully below in connection with a preferred embodiment and with reference to the drawing, in which:<br>
Fig, 1 shows a first flowchart for a method of retrieving information about a person;<br><br>
Fig. 2 shows a second flowchart for a method of retrieving information about a person;<br>
Fig. 3 shows a first flowchart for an apparatus for retrieving information about a person;<br>
Fig. 4 shows a second flowchart for an apparatus for retrieving information about a person;<br>
Fig. 5 shows a block diagram of an apparatus for storing and retrieving information about a person;<br>
Fig. 6 shows a second block diagram of an Apparatus for processing information about a person; and<br>
Fig. 7 shows a Portable Digital Assistant (PDA) with a camera unit and a microphone.<br>
Fig. 1 shows a first flowchart for a method of retrieving information about a person. The flowchart starts in step 101 leading to the first step in the method, step 102, wherein a user carrying a Personal Digital Assistant (PDA) meets a person. The user starts to talk to the person while wondering whether he has met the person before. Consequently, while the conversation is going on, the user enables a Persona! Inflammation Retrieval, PIR, function m his PDA in step 103.<br>
While the conversion is ongoing, the PDA records the voice of the person in step 109 to obtain audio data. The audio data are processed in the PDA to identify the person by means of his or her voice. If the person can be identified, the PDA queries a database to find information about the person. This is carried out in step 108. If a result of the query indicates that information about the identified person is found, the user is notified that information about the person the user is cunningly talking to is found. This information is provided to the user in step 107. Consequently, the user may stop wondering whether he has met the person before since he is updated wile information about the person in step 105. This information may comprise the user's own notes about the person. Subsequently, the conversation may go on or terminate before it ends in step 106. Thereby the user is updated with information about the person - for instance by providing an electronic business card with name, (business) address, etc. of the person.<br>
Fig. 2 shows a second flowchart for a method of retrieving information about a person. The flowchart starts in step 201 leading to the first step of the method, step 202,<br><br>
wherein a user meets a person. The user starts to talk to the person while thinking that he must have met the person before, but he is not sure. Once again the user enables a Personal Information Retrieval function in his PDA in step 203. This involves that the user in step 204 is recording audio data of the person while the conversion is ongoing. Correspondingly, audio data is recorded by the PDA in step 210. However, also images/video can be recorded.<br>
During this recording or immediately afterwards the data are processed to recognize the person to identify the person by means of his voice. If image/video data of the person is recorded, the person may be identified by means of his face or overall appearance.<br>
If the person can be identified, the PDA queries a database, DB, to find information about the person in step 208. If a result of the query indicates that information about the identified person is found, the user is notified that information about the person the user has met is found and this information is supplied to the user in step 207. Consequently, the user is updated with this information in step 205. Subsequently, the conversation may continue in step 211.<br>
During the conversation or afterwards, the user can operate the PDA in steps 212 and 213 to update, edit, or add intonation about the person that the user has just acquired during the conversation.<br>
The method ends in step 206. In addition to or as an alternative to recording audio data, the user can record an image or a video sequence of the person. Subsequently, recorded image data can be processed to identify the person.<br>
Fig. 3 shows a first flowchart for an apparatus for retrieving information about a person. This method is executed by a PDA in response to a user's commands to provide the user with means for recalling intonation about physical persons.<br>
Upon receipt of a user's command in step 301, concurrent recordings of audio and image data carried out in step 303 and step 302, respectively. During these recordings or immediately after the recordings are fished, the video and image data are processed to extract features from the audio and video data that can be used to recognize a given person.<br>
In step 307 it is verified whether the extracted features arranged in a feature profile match a previously stored feature profile. If a match is detected (Yes), a person associated with that profile is identified in step 308 and information previously stored and associated witii the person is retrieved. Subsequently, in step 305 this information is provided to the user by means of a user interface.<br>
Alternatively, if a. match is not detected (No), a temporary anonymous person is created in step 309. That is, the feature profile is stored without associating a vserspeci&amp;ed<br><br>
name with the profile. In step 310a reference to the temporary anonymous person is created for subsequent addition of information by the user. This is expedient since the user then is allowed to enter information about a person when a conversation with a person has ended.<br>
Fig. 4 shows a second flowchart for an apparatus for retrieving information      about a person. Likewise, this method is executed by a PDA in response to a user's<br>
commands to provide the user with means for recalling information about physical persons. In this flowchart a user is provided with the option of storing and retrieving information about a person's mood and/or emotional/personal status.<br>
Upon receipt of a user's command in step 401 concurrent recording of audio     and image data are carried out in step 403 and step 402, respectively. During these recordings or immediately after the recordings are discontinued, in step 404 the video and image data are processed to extract features from the audio and video data that can be used to recognize a given person.<br>
In step 407 it is verified whether the extracted features arranged in a feature profile match a previously stored feature profile. If a match is detected (Yes), a person associated with that profile is identified in step 408 and information previously stored and associated with the person is retrieved. In the following step 411 the video and image data are processed to extract features from the audio and video data that can be used to recognize the mood or emotional/personal status of the identified person.<br>
Subsequently, in Step 405 information about the person and his or her mood or emotional/personal status is supplied to the user by means of a user interface.<br>
Alternatively, if a match is not detected (No - in step 407), a temporary anonymous person created in step 409. In step 410 the user creates a reference to the temporary anonymous person for subsequent addition of information.<br>
In the following step 412, the user has access to a user interface that enables him or her to edit and/or update information about a person. If the user selects such option, an editor is opened in step 413.<br>
Likewise, in step 414 the user is provided with options of editing and/or updating information about a person's mood or emotional/personal status. If the user chooses to change this information, an editor is opened in step 415.<br>
The method ends in step 406.<br>
Fig. 5 shows a block diagram of an apparatus for storing and retrieving information about a person.<br><br>
Reference numeral 501 generally designates a Personal Digital Assistant (PDA). The PDA comprises a central processor 509 for executing both basic Operating System (OS) processes and application processes/programs including programs for implementing the method according to the invention.<br>
In order to process image data the processor is operatively connected to an image processor 503 for pre-processing image data from a camera 502.<br>
Likewise, in order to process audio data the processor is also operatively connected to an audio processor 505 for pre-processing audio data from a microphone 504.<br>
In order to enable a user to interact with the PDA it comprises user-interface means with an interactive display 506 and a loudspeaker 507. The interactive display is capable of providing information to a user by displaying text and graphics including images and video sequences.<br>
The FDA is capable of receiving input from the user in that the interactive display is touch-sensitive. Thereby it is possible to make a keyboard appear on the interactive display. Alternatively, or additionally, a user's handwriting on the display can be interpreted.<br>
In order to store information about a person the PDA comprises a database 508 for storing this information in a structured and searchable way.<br>
For the purpose of communicating with a stationary computer, computers on the Internet, other PDAs, or other electronic equipment including mobile phones, the PDA comprises an input/output (I/O) interface 510. The I/O interface may be a wireless interface e.g. a so-called IrDa port or a Blue Tooth interface,<br>
Since the PDA is a portable or mobile device it is primarily powered by a rechargeable battery 511.<br>
Fig. 6 shows a second block diagram of an apparatus for processing information about a person.<br>
The block diagram comprises four main processing means: an image pre¬processor 603, an audio pre-processor 607, a person-recognition processor 604, and a mood-recognition processor 608. Further, the image pre-processor 603 acquires image data from a camera 602 and the audio pre-processor 607 acquires audio data from a microphone 606. Via a user-interface 605 a user is provided with means for registering information about a recognised person's identity or a recognised/registered mood. This information is stored in a database 609.<br>
In a preferred embodiment the image pre-processor 603 is arranged to process image data representing people's faces. This is expedient since people's feces comprise much<br><br>
information that can be used 1) to identify the particular person and 2) to determine the mood of the person.<br>
The audio pre-processor 607 is arranged to process audio data representing people's voices and their tone of voice. This is expedient since people's voices comprise information that can be extracted by signal processing techniques and that is characteristic for an individual person. Likewise, in most situations, the tone of aperson's voice is representative of the person's mood.<br>
Thus, the pre-processors 603 and 607 are arranged to extract features that can be used 1) to identify a particular person and 2) to determine the mood of the person.<br>
Features relating to identifying a particular person are supplied to the person-recognition processor 604; and features relating to determining the mood of a person are supplied to the mood-recognition processor 608. Thereby a given image and/or audio recording can be used both to recognise the identity of a person and to determine the person's raood.<br>
Although the above-mentioned processors are very efficient at processing the image and audio data to discriminate between different moods or tones of voice for a given, person it is convenient that a user can modify registered mood information. Thereby a user can change a relation between a tone of voice and the mood of the associated person. The user mterface 605 provides means for this.<br>
Moreover, the user interface 605 can be used to register, update, or edit information about a person.<br>
The block diagram can be implemented by a PDA, by a mobile phone, by a personal computer, by a laptop computer, by a client-server computer system or by another type of electronic equipment.<br>
Fig. 7 shows a Portable Digital Assistant (PDA) with a camera unit and a microphone. In this embodiment of a PDA generally designated by reference numeral 701, it comprises an interactive display 703, a microphone 708 and buttons 704. The PDA has an interface 705 for connecting to a camera unit 702. The camera unit 702 comprises a camera 709 that can be operated either directly from the camera unit or from the PDA. Images recorded with the camera is transferred tO the PDA via the interface 705.<br>
It should be noted that the information stored about persons can include business cards with name, address, company name, e-mail address, web address, position, expertise, etc. Moreover the information can include notes about the contents of a meeting with the person, etc.<br><br>
Moreover, it should be noted that alternatively or in addition to<br>
storing/processing a person's tone of voice the person's voice print can be stored/processed. A person's voice print can be extracted by processing speech data to identify the person pased on the speech data. A voice print of a person can be considered to by unique to a person; the voice print is among other characteristics determined by the dimensions of the physical voice system of the person, for instance the length and width of his vocal tract.<br><br><br>
CLAIMS:<br>
1.	A method of providing information about a physical person, the method<br>
comprising the steps of:<br>
recording audio and/or image data of the physical person; processing the audio and/or visual data to recognize the identity of the physical person;<br>
retrieving stored information about the recognized identity; and supplying the stored information to a user.<br>
2.	A method according to claim 1 further comprising the steps of: processing the audio data to determine the voice print and/or tone of voice of the person; and storing a representation of the voice print and/or tone of voice.<br>
3.	A method according to claim 2 further comprising the step of associating a representation of a tone of voice with a representation of a mood.<br>
4.	A method according to claim 1 further comprising the steps of: processing the image data to determine facial expressions of the person; and storing a representation of the facial expressions.<br>
5.	A method according to claim 4 further comprising the step of associating a representation of a facial expression with a representation of a mood.<br>
6.	A method according to claim I further comprising the step of providing a user with the option of storing information about a person and associating that information with data that are characteristic for the person.<br>
7.	An apparatus for providing information about a physical person, comprising:<br>
a recorder for recording audio and/or image data of the physical person;<br><br>
a processor with a memory for processing the audio and/or visual data to recognize the identity of the physical person, and for retrieving stored information about the recognized identity; and<br>
a user interface for supplying the stored information to a user.<br>
8.	An apparatus according to claim 7 wherein the processor is arranged to associate a representation of a tone of voice with a representation of a mood.<br>
9.	An apparatus according to claim 7, wherein the processor is arranged to associate a representation of a facial expression with a representation of a mood.<br>
10.	An apparatus according to claim 7, wherein the processor is arranged to combine a representation of a facial expression and a representation of a tone of voice with a representation of a mood.<br>
11.	An apparatus according to claim 7, wherein the apparatus is arranged to provide a user with the option of storing information about a person and associating that information with data that are characteristic for the person.<br>
12.	An apparatus according to claim 7 arranged to compare a current representation of mood with a previously stored representation in a history of representations of mood; and supply information to the user about the result of the comparison.<br><br>
13.	A method of providing information about a physical person<br>
substantially as herein described with reference to the accompanying<br>
drawings.<br>
14.	An apparatus substantially as herein described with reference to the<br>
accompanying drawings.<br><br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgY29ycmVzcG9uZGVuY2Utb3RoZXJzLnBkZg==" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgY29ycmVzcG9uZGVuY2UtcG8ucGRm" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgZGVzY3JpcHRpb24oY29tcGxldGUpLnBkZg==" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgZHJhd2luZ3MucGRm" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgZm9ybS0xLnBkZg==" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgZm9ybS0xOC5wZGY=" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che form-18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUgZm9ybS0zLnBkZg==" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=aW4tcGN0LTIwMDItMjA5OC1jaGUtIGFic3RyYWN0LmpwZw==" target="_blank" style="word-wrap:break-word;">in-pct-2002-2098-che- abstract.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="234668-a-method-for-defining-surface-parameter-for-a-3d-object-model-in-a-computer-system.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="234670-method-device-and-system-for-enabling-access-of-data-related-to-a-communication-device.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>234669</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>IN/PCT/2002/2098/CHE</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>29/2009</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>17-Jul-2009</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>11-Jun-2009</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>17-Dec-2002</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>KONINKLIJKE PHILIPS ELECTRONICS N.V.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>GROENEWOUDSEWEG 1, NL-5621 BA EINDHOVEN</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>CHANG YING-HUI, LIAO, YUAN-FU, YEH LI-FEN &amp; HORNG JYK-KUEN</td>
											<td>C/O PROF. HOLSTLAAN 6, NL-5656 AA EINDHOVEN</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06K9/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/IB02/0133</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2002-04-11</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>01201393.4</td>
									<td>2001-04-17</td>
								    <td>EUROPEAN UNION</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/234669-a-method-of-providing-information-about-a-physical-person by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 14:22:38 GMT -->
</html>
