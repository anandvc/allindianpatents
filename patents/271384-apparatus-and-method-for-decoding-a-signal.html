<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/271384-apparatus-and-method-for-decoding-a-signal by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:45:19 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 271384:APPARATUS AND METHOD FOR DECODING A SIGNAL</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">APPARATUS AND METHOD FOR DECODING A SIGNAL</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>An encoding method and apparatus and a decoding method and apparatus are provided. The decoding method includes extracting a three-dimensional (3D) down-mix signal and spatial information from an input bitstream, removing 3D effects from the 3D down-mix signal by performing a 3D rendering operation on the 3D down-mix signal, and generating a multi-channel signal using the spatial information and a down-mix signal obtained by the removal. Accordingly, it is possible to efficiently encode multi-channel signals with 3D effects and to adaptively restore and reproduce audio signals with optimum sound quality according to the characteristics of a reproduction environment.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>WO 2007/091842	PCT/KR2007/000668<br>
Description<br>
APPARATUS AND METHOD FOR ENCODING/DECODING<br>
SIGNAL<br>
Technical Field<br>
The present invention relates to an encoding/decoding method and an encoding/<br>
decoding apparatus, and more particularly, to an encoding/decoding apparatus which<br>
can process an audio signal so that three dimensional (3D) sound effects can be<br>
created, and an encoding/decoding method using the encoding/decoding apparatus.<br>
Background Art<br>
An encoding apparatus down-mixes a multi-channel signal into a signal with fewer<br>
channels, and transmits the down-mixed signal to a decoding apparatus. Then, the<br>
decoding apparatus restores a multi-channel signal from the down-mixed signal and<br>
reproduces the restored multi-channel signal using three or more speakers, for<br>
example, 5.1-channel	 speakers.<br>
Multi-channel signals may be reproduced by 2-channel speakers such as<br>
headphones. In this case, in order to make a user feel as if sounds output by 2-channel<br>
speakers, were reproduced from three or more sound sources, it is necessary to develop<br>
three-dimensional (3D) processing techniques capable of encoding or decoding multi-<br>
channel signals so that 3D effects can be created.<br>
Disclosure of Invention<br>
Technical Problem<br>
The present invention provides an encoding/decoding apparatus and an encoding/<br>
decoding method which can reproduce multi-channel signals in various reproduction<br>
environments by efficiently processing signals with 3D effects.<br>
Technical Solution<br>
According to an aspect of the present invention, there is provided a decoding<br>
method of restoring a multi-channel signal, the decoding method including extracting a<br>
three-dimensional (3D) down-mix signal and spatial information from an input<br>
bitstream, removing 3D effects from the 3D down-mix signal by performing a 3D<br>
rendering operation on the 3D down-mix signal, and generating a multi-channel signal<br>
using the spatial information and a down-mix signal obtained by the removal.<br>
According to another aspect of the present invention, there is provided a decoding<br>
method of restoring a multi-channel signal, the decoding method including extracting a<br>
3D down-mix signal and spatial information from an input bitstream, generating a<br>
multi-channel signal using the 3D down-mix signal and the spatial information, and<br>
removing 3D effects from the multi-channel signal by performing a 3D rendering<br><br>
WO 2007/091842<br><br>
PCT/KR2007/000668<br><br>
operation on the multi-channel signal.<br>
According to another aspect of the present invention, there is provided an encoding<br>
method of encoding a multi-channel signal with a plurality of channels, the encoding<br>
method including encoding the multi-channel signal into a down-mix signal with fewer<br>
channels, generating spatial information regarding the plurality of channels, generating<br>
a 3D down-mix signal by performing a 3D rendering operation on the down-mix<br>
signal, and generating a bitstream including the 3D down-mix signal and the spatial in-<br>
formation.<br>
According to another aspect of the present invention, there is provided an encoding<br>
method of encoding a multi-channel signal with a plurality of channels, the encoding<br>
method including performing a 3D rendering operation on the multi-channel signal,<br>
encoding a multi-channel signal obtained by the 3D rendering operation into a 3D<br>
down-mix signal with fewer channels, generating spatial information regarding the<br>
plurality of channels, and generating a bitstream including the 3D down-mix signal and<br>
the spatial information.<br>
According to another aspect of the present invention, there is provided a decoding<br>
apparatus for restoring a multi-channel signal, the decoding apparatus including a bit<br>
unpacking unit which extracts an encoded 3D down-mix signal and spatial information<br>
from an input bitstream, a down-mix decoder which decodes the encoded 3D down-<br>
mix signal, a 3D rendering unit which removes 3D effects from the decoded 3D down-<br>
mix signal obtained by the decoding performed by the down-mix decoder by<br>
performing a 3D rendering operation on the decoded 3D down-mix signal, and a multi-<br>
channel decoder which generates a multi-channel signal using the spatial information<br>
and a down-mix signal obtained by the removal performed by the 3D rendering unit.<br>
According to another aspect of the present invention, there is provided a decoding<br>
apparatus for restoring a multi-channel signal, the decoding apparatus including a bit<br>
unpacking unit which extracts an encoded 3D down-mix signal and spatial information<br>
from an input bitstream, a down-mix decoder which decodes the encoded 3D down-<br>
mix signal, a multi-channel decoder which generates a multi-channel signal using the<br>
spatial information and a 3D down-mix signal obtained by the decoding performed by<br>
the down-mix decoder, and a 3D rendering unit which removes 3D effects from the<br>
multi-channel signal by performing a 3D rendering operation on the multi-channel<br>
signal.<br>
According to another aspect of the present invention, there is provided an encoding<br>
apparatus for encoding a multi-channel signal with a plurality of channels, the<br>
encoding apparatus including a multi-channel encoder which encodes the multi-<br>
channel signal into a down-mix signal with fewer channels and generates spatial in-<br>
formation regarding the plurality of channels, a 3D rendering unit which generates a<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
3D down-mix signal by performing a 3D rendering operation on the down-mix signal,<br>
a down-mix encoder which encodes the 3D down-mix signal; and a bit packing unit<br>
which generates a bitstream including the encoded 3D down-mix signal and the spatial<br>
information.<br>
According to another aspect of the present invention, there is provided an encoding<br>
apparatus for encoding a multi-channel signal with a plurality of channels, the<br>
encoding apparatus including a 3D rendering unit which performs a 3D rendering<br>
operation on the multi-channel signal, a multi-channel encoder which encodes a multi-<br>
channel signal obtained by the 3D rendering operation into a 3D down-mix signal with<br>
fewer channels and generates spatial information regarding the plurality of channels, a<br>
down-mix encoder which encodes the 3D down-mix signal, and a bit packing unit<br>
which generates a bitstream including the encoded 3D down-mix signal and the spatial<br>
information.<br>
According to another aspect of the present invention, there is provided a bitstream<br>
including a data field which includes information regarding a 3D down-mix signal, a<br>
filter information field which includes filter information identifying a filter used for<br>
generating the 3D down-mix signal, a first header field which includes information<br>
indicating whether the filter information field includes the filter information, a second<br>
header field which includes information indicating whether the filter information field<br>
includes coefficients of the filter or coefficients of an inverse filter of the filter, and a<br>
spatial information field which includes spatial information regarding a plurality of<br>
channels.<br>
According to another aspect of the present invention, there is provided a computer-<br>
readable recording medium having a computer program for executing any one of the<br>
above-described decoding methods and the above-described encoding methods.<br>
Advantageous Effects<br>
According to the present invention, it is possible to efficiently encode multi-channel<br>
signals with 3D effects and to adaptively restore and reproduce audio signals with<br>
optimum sound quality according to the characteristics of a reproduction environment.<br>
Brief Description of the Drawings<br>
FIG. 1 is a block diagram of an encoding/decoding apparatus according to an<br>
embodiment of the present invention;<br>
FIG. 2 is a block diagram of an encoding apparatus according to an embodiment of<br>
the present invention;<br>
FIG. 3 is a block diagram of a decoding apparatus according to an embodiment of<br>
the present invention;<br>
FIG. 4 is a block diagram of an encoding apparatus according to another<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
embodiment of the present invention;<br>
[20]	FIG. 5 is a block diagram of a decoding apparatus according to another<br>
embodiment of the present invention;<br>
[21]	FIG. 6 is a block diagram of a decoding apparatus according to another<br>
embodiment of the present invention;<br>
[22]	FIG. 7 is a block diagram of a three-dimensional (3D) rendering apparatus<br>
according to an embodiment of the present invention;<br>
[23]	FIGS. 8 through 11 illustrate bitstreams according to embodiments of the present<br>
invention;<br>
[24]	FIG. 12 is a block diagram of an encoding/decoding apparatus for processing an<br>
arbitrary down-mix signal according to an embodiment of the present invention;<br>
[25]	FIG. 13 is a block diagram of an arbitrary down-mix signal compensation/3D<br>
rendering unit according to an embodiment of the present invention;<br>
[26]	FIG. 14 is a block diagram of a decoding apparatus for processing a compatible<br>
down-mix signal according to an embodiment of the present invention;<br>
[27]	FIG. 15 is a block diagram of a down-mix compatibility processing/3D rendering<br>
unit according to an embodiment of the present invention; and<br>
[28]	FIG. 16 is a block diagram of a decoding apparatus for canceling crosstalk<br>
according to an embodiment of the present invention.<br>
Best Mode for Carrying Out the Invention<br>
[29]	The present invention will hereinafter be described more fully with reference to the<br>
accompanying drawings, in which exemplary embodiments of the invention are shown.<br>
[30]	FIG. 1 is a block diagram of an encoding/decoding apparatus according to an<br>
embodiment of the present invention. Referring to FIG. 1, an encoding unit 100<br>
includes a multi-channel encoder 110, a three-dimensional (3D) rendering unit 120, a<br>
down-mix encoder 130, and a bit packing unit 140.<br>
[31]	The multi-channel encoder 110 down-mixes a multi-channel signal with a plurality<br>
of channels into a down-mix signal such as a stereo signal or a mono signal and<br>
generates spatial information regarding the channels of the multi-channel signal. The<br>
spatial information is needed to restore a multi-channel signal from the down-mix<br>
signal.<br>
[32]	Examples of the spatial information include a channel level difference (CLD),<br>
which indicates the difference between the energy levels of a pair of channels, a<br>
channel prediction coefficient (CPC), which is a prediction coefficient used to generate<br>
a 3-channel signal based on a 2-channel signal, inter-channel correlation (ICC), which<br>
indicates the correlation between a pair of channels, and a channel time difference<br>
(CTD), which is the time interval between a pair of channels.<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
The 3D rendering unit 120 generates a 3D down-mix signal based on the down-mix<br>
signal. The 3D down-mix signal may be a 2-channel signal with three or more di-<br>
rectivities and can thus be reproduced by 2-channel speakers such as headphones with<br>
3D effects. In other words, the 3D down-mix signal may be reproduced by 2-channel<br>
speakers so that a user can feel as if the 3D down-mix signal were reproduced from a<br>
sound source with three or more channels. The direction of a sound source may be<br>
determined based on at least one of the difference between the intensities of two<br>
sounds respectively input to both ears, the time interval between the two sounds, and<br>
the difference between the phases of the two sounds. Therefore, the 3D rendering unit<br>
120 can convert the down-mix signal into the 3D down-mix signal based on how the<br>
humans can determine the 3D location of a sound source with their sense of hearing.<br>
The 3D rendering unit 120 may generate the 3D down-mix signal by filtering the<br>
down-mix signal using a filter. In this case, filter-related information, for example, a<br>
coefficient of the filter, may be input to the 3D rendering unit 120 by an external<br>
source. The 3D rendering unit 120 may use the spatial information provided by the<br>
multi-channel encoder 110 to generate the 3D down-mix signal based on the down-mix<br>
signal. More specifically, the 3D rendering unit 120 may convert the down-mix signal<br>
into the 3D down-mix signal by converting the down-mix signal into an imaginary<br>
multi-channel signal using the spatial information and filtering the imaginary multi-<br>
channel signal.<br>
The 3D rendering unit 120 may generate the 3D down-mix signal by filtering the<br>
down-mix signal using a head-related transfer function (HRTF) filter.<br>
A HRTF is a transfer function which describes the transmission of sound waves<br>
between a sound source at an arbitrary location and the eardrum, and returns a value<br>
that varies according to the direction and altitude of a sound source. If a signal with no<br>
directivity is filtered using the HRTF, the signal may be heard as if it were reproduced<br>
from a certain direction.<br>
The 3D rendering unit 120 may perform a 3D rendering operation in a frequency<br>
domain, for example, a discrete Fourier transform (DFT) domain or a fast Fourier<br>
transform (FFT) domain. In this case, the 3D rendering unit 120 may perform DFT or<br>
FFT before the 3D rendering operation or may perform inverse DFT (IDFT) or inverse<br>
FFT (IFFT) after the 3D rendering operation.<br>
The 3D rendering unit 120 may perform the 3D rendering operation in a quadrature<br>
mirror filter (QMF)/hybrid domain. In this case, the 3D rendering unit 120 may<br>
perform QMF/hybrid analysis and synthesis operations before or after the 3D<br>
rendering operation.<br>
The 3D rendering unit 120 may perform the 3D rendering operation in a time<br>
domain. The 3D rendering unit 120 may determine in which domain the 3D rendering<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
operation is to be performed according to required sound quality and the operational<br>
capacity of the encoding/decoding apparatus.<br>
The down-mix encoder 130 encodes the down-mix signal output by the multi-<br>
channel encoder 110 or the 3D down-mix signal output by the 3D rendering unit 120.<br>
The down-mix encoder 130 may encode the down-mix signal output by the multi-<br>
channel encoder 110 or the 3D down-mix signal output by the 3D rendering unit 120<br>
using an audio encoding method such as an advanced audio coding (AAC) method, an<br>
MPEG layer 3 (MP3) method, or a bit sliced arithmetic coding (BSAC) method.<br>
The down-mix encoder 130 may encode a non-3D down-mix signal or a 3D down-<br>
mix signal. In this case, the encoded non-3D down-mix signal and the encoded 3D<br>
down-mix signal may both be included in a bitstream to be transmitted.<br>
The bit packing unit 140 generates a bitstream based on the spatial information and<br>
either the encoded non-3D down-mix signal or the encoded 3D down-mix signal.<br>
The bitstream generated by the bit packing unit 140 may include spatial in-<br>
formation, down-mix identification information indicating whether a down-mix signal<br>
included in the bitstream is a non-3D down-mix signal or a 3D down-mix signal, and<br>
information identifying a filter used by the 3D rendering unit 120 (e.g., HRTF co-<br>
efficient information).<br>
In other words, the bitstream generated by the bit packing unit 140 may include at<br>
least one of a non-3D down-mix signal which has not yet been 3D-processed and an<br>
encoder 3D down-mix signal which is obtained by a 3D processing operation<br>
performed by an encoding apparatus, and down-mix identification information<br>
identifying the type of down-mix signal included in the bitstream.<br>
It may be determined which of the non-3D down-mix signal and the encoder 3D<br>
down-mix signal is to be included in the bitstream generated by the bit packing unit<br>
140 at the user's choice or according to the capabilities of the encoding/decoding<br>
apparatus illustrated in FIG. 1 and the characteristics of a reproduction environment.<br>
The HRTF coefficient information may include coefficients of an inverse function<br>
of a HRTF used by the 3D rendering unit 120. The HRTF coefficient information may<br>
only include brief information of coefficients of the HRTF used by the 3D rendering<br>
unit 120, for example, envelope information of the HRTF coefficients. If a bitstream<br>
including the coefficients of the inverse function of the HRTF is transmitted to a<br>
decoding apparatus, the decoding apparatus does not need to perform an HRTF co-<br>
efficient conversion operation, and thus, the amount of computation of the decoding<br>
apparatus may be reduced.<br>
The bitstream generated by the bit packing unit 140 may also include information<br>
regarding an energy variation in a signal caused by HRTF-based filtering, i.e., in-<br>
formation regarding the difference between the energy of a signal to be filtered and the<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
energy of a signal that has been filtered or the ratio of the energy of the signal to be<br>
filtered and the energy of the signal that has been filtered.<br>
The bitstream generated by the bit packing unit 140 may also include information<br>
indicating whether it includes HRTF coefficients. If HRTF coefficients are included in<br>
the bitstream generated by the bit packing unit 140, the bitstream may also include in-<br>
formation indicating whether it includes either the coefficients of the HRTF used by<br>
the 3D rendering unit 120 or the coefficients of the inverse function of the HRTF.<br>
Referring to FIG. 1, a first decoding unit 200 includes a bit unpacking unit 210, a<br>
down-mix decoder 220, a 3D rendering unit 230, and a multi-channel decoder 240.<br>
The bit unpacking unit 210 receives an input bitstream from the encoding unit 100<br>
and extracts an encoded down-mix signal and spatial information from the input<br>
bitstream. The down-mix decoder 220 decodes the encoded down-mix signal. The<br>
down-mix decoder 220 may decode the encoded down-mix signal using an audio<br>
signal decoding method such as an AAC method, an MP3 method, or a BSAC method.<br>
As described above, the encoded down-mix signal extracted from the input<br>
bitstream may be an encoded non-3D down-mix signal or an encoded, encoder 3D<br>
down-mix signal. Information indicating whether the encoded down-mix signal<br>
extracted from the input bitstream is an encoded non-3D down-mix signal or an<br>
encoded, encoder 3D down-mix signal may be included in the input bitstream.<br>
If the encoded down-mix signal extracted from the input bitstream is an encoder<br>
3D down-mix signal, the encoded down-mix signal may be readily reproduced after<br>
being decoded by the down-mix decoder 220.<br>
On the other hand, if the encoded down-mix signal extracted from the input<br>
bitstream is a non-3D down-mix signal, the encoded down-mix signal may be decoded<br>
by the down-mix decoder 220, and a down-mix signal obtained by the decoding may<br>
be converted into a decoder 3D down-mix signal by a 3D rendering operation<br>
performed by the third rendering unit 233. The decoder 3D down-mix signal can be<br>
readily reproduced.<br>
The 3D rendering unit 230 includes a first renderer 231, a second Tenderer 232, and<br>
a third renderer 233. The first renderer 231 generates a down-mix signal by performing<br>
a 3D rendering operation on an encoder 3D down-mix signal provided by the down-<br>
mix decoder 220. For example, the first renderer 231 may generate a non-3D down-<br>
mix signal by removing 3D effects from the encoder 3D down-mix signal. The 3D<br>
effects of the encoder 3D down-mix signal may not be completely removed by the first<br>
renderer 231. In this case, a down-mix signal output by the first renderer 231 may have<br>
some 3D effects.<br>
The first renderer 231 may convert the 3D down-mix signal provided by the down-<br>
mix decoder 220 into a down-mix signal with 3D effects removed therefrom using an<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
inverse filter of the filter used by the 3D rendering unit 120 of the encoding unit 100.<br>
Information regarding the filter used by the 3D rendering unit 120 or the inverse filter<br>
of the filter used by the 3D rendering unit 120 may be included in the input bitstream.<br>
The filter used by the 3D rendering unit 120 may be an HRTF filter. In this case,<br>
the coefficients of the HRTF used by the encoding unit 100 or the coefficients of the<br>
inverse function of the HRTF may also be included in the input bitstream. If the co-<br>
efficients of the HRTF used by the encoding unit 100 are included in the input<br>
bitstream, the HRTF coefficients may be inversely converted, and the results of the<br>
inverse conversion may be used during the 3D rendering operation performed by the<br>
first Tenderer 231. If the coefficients of the inverse function of the HRTF used by the<br>
encoding unit 100 are included in the input bitstream, they may be readily used during<br>
the 3D rendering operation performed by the first Tenderer 231 without being subjected<br>
to any inverse conversion operation. In this case, the amount of computation of the first<br>
decoding apparatus 100 may be reduced.<br>
The input bitstream may also include filter information (e.g., information<br>
indicating whether the coefficients of the HRTF used by the encoding unit 100 are<br>
included in the input bitstream) and information indicating whether the filter in-<br>
formation has been inversely converted.<br>
The multi-channel decoder 240 generates a 3D multi-channel signal with three or<br>
more channels based on the down-mix signal with 3D effects removed therefrom and<br>
the spatial information extracted from the input bitstream.<br>
The second Tenderer 232 may generate a 3D down-mix signal with 3D effects by<br>
performing a 3D rendering operation on the down-mix signal with 3D effects removed<br>
therefrom. In other words, the first Tenderer 231 removes 3D effects from the encoder<br>
3D down-mix signal provided by the down-mix decoder 220. Thereafter, the second<br>
renderer 232 may generate a combined 3D down-mix signal with 3D effects desired by<br>
the first decoding apparatus 200 by performing a 3D rendering operation on a down-<br>
mix signal obtained by the removal performed by the first Tenderer 231, using a filter<br>
of the first decoding apparatus 200.<br>
The first decoding apparatus 200 may include a renderer in which two or more of<br>
the first, second, and third Tenderers 231, 232, and 233 that perform the same<br>
operations are integrated.<br>
A bitstream generated by the encoding unit 100 may be input to a second decoding<br>
apparatus 300 which has a different structure from the first decoding apparatus 200.<br>
The second decoding apparatus 300 may generate a 3D down-mix signal based on a<br>
down-mix signal included in the bitstream input thereto.<br>
More specifically, the second decoding apparatus 300 includes a bit unpacking unit<br>
310, a down-mix decoder 320, and a 3D rendering unit 330. The bit unpacking unit<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
310 receives an input bitstream from the encoding unit 100 and extracts an encoded<br>
down-mix signal and spatial information from the input bitstream. The down-mix<br>
decoder 320 decodes the encoded down-mix signal. The 3D rendering unit 330<br>
performs a 3D rendering operation on the decoded down-mix signal so that the<br>
decoded down-mix signal can be converted into a 3D down-mix signal.<br>
FIG. 2 is a block diagram of an encoding apparatus according to an embodiment of<br>
the present invention. Referring to FIG. 2, the encoding apparatus includes rendering<br>
units 400 and 420 and a multi-channel encoder 410. Detailed descriptions of the same<br>
encoding processes as those of the embodiment of FIG. 1 will be omitted.<br>
Referring to FIG. 2, the 3D rendering units 400 and 420 may be respectively<br>
disposed in front of and behind the multi-channel encoder 410. Thus, a multi-channel<br>
signal may be 3D-rendered by the 3D rendering unit 400, and then, the 3D-rendered<br>
multi-channel signal may be encoded by the multi-channel encoder 410, thereby<br>
generating a pre-processed, encoder 3D down-mix signal. Alternatively, the multi-<br>
channel signal may be down-mixed by the multi-channel encoder 410, and then, the<br>
down-mixed signal may be 3D-rendered by the 3D rendering unit 420, thereby<br>
generating a post-processed, encoder down-mix signal.<br>
Information indicating whether the multi-channel signal has been 3D-rendered<br>
before or after being down-mixed may be included in a bitstream to be transmitted.<br>
The 3D rendering units 400 and 420 may both be disposed in front of or behind the<br>
multi-channel encoder 410.<br>
FIG. 3 is a block diagram of a decoding apparatus according to an embodiment of<br>
the present invention. Referring to FIG. 3, the decoding apparatus includes 3D<br>
rendering units 430 and 450 and a multi-channel decoder 440. Detailed descriptions of<br>
the same decoding processes as those of the embodiment of FIG. 1 will be omitted.<br>
Referring to FIG. 3, the 3D rendering units 430 and 450 may be respectively<br>
disposed in front of and behind the multi-channel decoder 440. The 3D rendering unit<br>
430 may remove 3D effects from an encoder 3D down-mix signal and input a down-<br>
mix signal obtained by the removal to the multi-channel decoder 430. Then, the multi-<br>
channel decoder 430 may decode the down-mix signal input thereto, thereby<br>
generating a pre-processed 3D multi-channel signal. Alternatively, the multi-channel<br>
decoder 430 may restore a multi-channel signal from an encoded 3D down-mix signal,<br>
and the 3D rendering unit 450 may remove 3D effects from the restored multi-channel<br>
signal, thereby generating a post-processed 3D multi-channel signal.<br>
If an encoder 3D down-mix signal provided by an encoding apparatus has been<br>
generated by performing a 3D rendering operation and then a down-mixing operation,<br>
the encoder 3D down-mix signal may be decoded by performing a multi-channel<br>
decoding operation and then a 3D rendering operation. On the other hand, if the<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
encoder 3D down-mix signal has been generated by performing a down-mixing<br>
operation and then a 3D rendering operation, the encoder 3D down-mix signal may be<br>
decoded by performing a 3D rendering operation and then a multi-channel decoding<br>
operation.<br>
[70]	Information indicating whether an encoded 3D down-mix signal has been obtained<br>
by performing a 3D rendering operation before or after a down-mixing operation may<br>
be extracted from a bitstream transmitted by an encoding apparatus.<br>
[71]	The 3D rendering units 430 and 450 may both be disposed in front of or behind the<br>
multi-channel decoder 440.<br>
[72]	FIG,, 4 is a block diagram of an encoding apparatus according to another<br>
embodiment of the present invention. Referring to FIG. 4, the encoding apparatus<br>
includes a multi-channel encoder 500, a 3D rendering unit 510, a down-mix encoder<br>
520, and a bit packing unit 530. Detailed descriptions of the same encoding processes<br>
as those of the embodiment of FIG. 1 will be omitted.<br>
[73]	Referring to FIG. 4, the multi-channel encoder 500 generates a down-mix signal<br>
and spatial information based on an input multi-channel signal. The 3D rendering unit<br>
510 generates a 3D down-mix signal by performing a 3D rendering operation on the<br>
down-mix signal.<br>
[74]	It may be determined whether to perform a 3D rendering operation on the down-<br>
mix signal at a user's choice or according to the capabilities of the encoding apparatus,<br>
the characteristics of a reproduction environment, or required sound quality.<br>
[75]	The down-mix encoder 520 encodes the down-mix signal generated by the multi-<br>
channel encoder 500 or the 3D down-mix signal generated by the 3D rendering unit<br>
510.<br>
[76]	The bit packing unit 530 generates a bitstream based on the spatial information and<br>
either the encoded down-mix signal or an encoded, encoder 3D down-mix signal. The<br>
bitstream generated by the bit packing unit 530 may include down-mix identification<br>
information indicating whether an encoded down-mix signal included in the bitstream<br>
is a non-3D down-mix signal with no 3D effects or an encoder 3D down-mix signal<br>
with 3D effects. More specifically, the down-mix identification information may<br>
indicate whether the bitstream generated by the bit packing unit 530 includes a non-3D<br>
down-mix signal, an encoder 3D down-mix signal or both.<br>
[77]	FIG. 5 is a block diagram of a decoding apparatus according to another<br>
embodiment of the present invention. Referring to FIG. 5, the decoding apparatus<br>
includes a bit unpacking unit 540, a down-mix decoder 550, and a 3D rendering unit<br>
560. Detailed descriptions of the same decoding processes as those of the embodiment<br>
of FIG. 1 will be omitted.<br>
[78]	Referring to FIG. 5, the bit unpacking unit 540 extracts an encoded down-mix<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
signal, spatial information, and down-mix identification information from an input<br>
bitstream. The down-mix identification information indicates whether the encoded<br>
down-mix signal is an encoded non-3D down-mix signal with no 3D effects or an<br>
encoded 3D down-mix signal with 3D effects.<br>
[79]	If the input bitstream includes both a non-3D down-mix signal and a 3D down-mix<br>
signal, only one of the non-3D down-mix signal and the 3D down-mix signal may be<br>
extracted from the input bitstream at a user's choice or according to the capabilities of<br>
the decoding apparatus, the characteristics of a reproduction environment or required<br>
sound quality.<br>
[80]	The down-mix decoder 550 decodes the encoded down-mix signal. If a down-mix<br>
signal obtained by the decoding performed by the down-mix decoder 550 is an encoder<br>
3D down-mix signal obtained by performing a 3D rendering operation, the down-mix<br>
signal may be readily reproduced.<br>
[81]	On the other hand, if the down-mix signal obtained by the decoding performed by<br>
the down-mix decoder 550 is a down-mix signal with no 3D effects, the 3D rendering<br>
unit 560 may generate a decoder 3D down-mix signal by performing a 3D rendering<br>
operation on the down-mix signal obtained by the decoding performed by the down-<br>
mix decoder 550.<br>
[82]	FIG. 6 is a block diagram of a decoding apparatus according to another<br>
embodiment of the present invention. Referring to FIG. 6, the decoding apparatus<br>
includes a bit unpacking unit 600, a down-mix decoder 610, a first 3D rendering unit<br>
620, a second 3D rendering unit 630, and a filter information storage unit 640. Detailed<br>
descriptions of the same decoding processes as those of the embodiment of FIG. 1 will<br>
be omitted.<br>
[83]	The bit unpacking unit 600 extracts an encoded, encoder 3D down-mix signal and<br>
spatial information from an input bitstream. The down-mix decoder 610 decodes the<br>
encoded, encoder 3D down-mix signal.<br>
[84]	The first 3D rendering unit 620 removes 3D effects from an encoder 3D down-mix<br>
signal obtained by the decoding performed by the down-mix decoder 610, using an<br>
inverse filter of a filter of an encoding apparatus used for performing a 3D rendering<br>
operation. The second rendering unit 630 generates a combined 3D down-mix signal<br>
with 3D effects by performing a 3D rendering operation on a down-mix signal<br>
obtained by the removal performed by the first 3D rendering unit 620, using a filter<br>
stored in the decoding apparatus.<br>
[85]	The second 3D rendering unit 630 may perform a 3D rendering operation using a<br>
filter having different characteristics from the filter of the encoding unit used to<br>
perform a 3D rendering operation. For example, the second 3D rendering unit 630 may<br>
perform a 3D rendering operation using an HRTF having different coefficients from<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
those of an HRTF used by an encoding apparatus.<br>
[86]	The filter information storage unit 640 stores filter information regarding a filter<br>
used to perform a 3D rendering, for example, HRTF coefficient information. The<br>
second 3D rendering unit 630 may generate a combined 3D down-mix using the filter<br>
information stored in the filter information storage unit 640.<br>
[87]	The filter information storage unit 640 may store a plurality of pieces of filter in-<br>
formation respectively corresponding to a plurality of filters. In this case, one of the<br>
plurality of pieces of filter information may be selected at a user's choice or according<br>
to the capabilities of the decoding apparatus or required sound quality.<br>
[88]	People from different races may have different ear structures. Thus, HRTF co-<br>
efficients optimized for different individuals may differ from one another. The<br>
decoding apparatus illustrated in FIG. 6 can generate a 3D down-mix signal optimized<br>
for the user. In addition, the decoding apparatus illustrated in FIG. 6 can generate a 3D<br>
down-mix signal with 3D effects corresponding to an HRTF filter desired by the user,<br>
regardless of the type of HRTF provided by a 3D down-mix signal provider.<br>
[89]	FIG. 7 is a block diagram of a 3D rendering apparatus according to an embodiment<br>
of the present invention. Referring to FIG. 7, the 3D rendering apparatus includes first<br>
and second domain conversion units 700 and 720 and a 3D rendering unit 710. In order<br>
to perform a 3D rendering operation in a predetermined domain, the first and second<br>
domain conversion units 700 and 720 may be respectively disposed in front of and<br>
behind the 3D rendering unit 710.<br>
[90]	Referring to FIG. 7, an input down-mix signal is converted into a frequency-<br>
domain down-mix signal by the first domain conversion unit 700. More specifically,<br>
the first: domain conversion unit 700 may convert the input down-mix signal into a<br>
DFT-domain down-mix signal or a FFT-domain down-mix signal by performing DFT<br>
orFFT.<br>
[91]	The 3D rendering unit 710 generates a multi-channel signal by applying spatial in-<br>
formation to the frequency-domain down-mix signal provided by the first domain<br>
conversion unit 700. Thereafter, the 3D rendering unit 710 generates a 3D down-mix<br>
signal by filtering the multi-channel signal.<br>
[92]	The 3D down-mix signal generated by the 3D rendering unit 710 is converted into<br>
a time-domain 3D down-mix signal by the second domain conversion unit 720. More<br>
specifically, the second domain conversion unit 720 may perform IDFT or IFFT on the<br>
3D down-mix signal generated by the 3D rendering unit 710.<br>
[93]	During the conversion of a frequency-domain 3D down-mix signal into a time-<br>
domain 3D down-mix signal, data loss or data distortion such as aliasing may occur.<br>
[94]	In order to generate a multi-channel signal and a 3D down-mix signal in a<br>
frequency domain, spatial information for each parameter band may be mapped to the<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
frequency domain, and a number of filter coefficients may be converted to the<br>
frequency domain.<br>
[95]	The 3D rendering unit 710 may generate a 3D down-mix signal by multiplying the<br>
frequency-domain down-mix signal provided by the first domain conversion unit 700,<br>
the spatial information, and the filter coefficients.<br>
[96]	A time-domain signal obtained by multiplying a down-mix signal, spatial in-<br>
formation and a plurality of filter coefficients that are all represented in an M-point<br>
frequency domain has M valid signals. In order to represent the down-mix signal, the<br>
spatial information and the filter in the M-point frequency domain, M-point DFT or M-<br>
point FFT may be performed.<br>
[97]	Valid signals are signals that do not necessarily have a value of 0. For example, a<br>
total of x valid signals can be generated by obtaining x signals from an audio signal<br>
through sampling. Of the x valid signals, y valid signals may be zero-padded. Then, the<br>
number of valid signals is reduced to (x-y). Thereafter, a signal with a valid signals and<br>
a signal with b valid signals are convoluted, thereby obtaining a total of (a+b-1) valid<br>
signals,<br>
[98]	The multiplication of the down-mix signal, the spatial information, and the filter<br>
coefficients in the M-point frequency domain can provide the same effect as<br>
convoluting the down-mix signal, the spatial information, and the filter coefficients in<br>
a time-domain. A signal with (3*M-2) valid signals can be generated by converting the<br>
down-mix signal, the spatial information and the filter coefficients in the M-point<br>
frequency domain to a time domain and convoluting the results of the conversion.<br>
[99]	Therefore, the number of valid signals of a signal obtained by multiplying a down-<br>
mix signal, spatial information, and filter coefficients in a frequency domain and<br>
converting the result of the multiplication to a time domain may differ from the<br>
number of valid signals of a signal obtained by convoluting the down-mix signal, the<br>
spatial information, and the filter coefficients in the time domain. As a result, aliasing<br>
may occur during the conversion of a 3D down-mix signal in a frequency domain into<br>
a time-domain signal.<br>
[100]	In order to prevent aliasing, the sum of the number of valid signals of a down-mix<br>
signal in a time domain, the number of valid signals of spatial information mapped to a<br>
frequency domain, and the number of filter coefficients must not be greater than M.<br>
The number of valid signals of spatial information mapped to a frequency domain may<br>
be determined by the number of points of the frequency domain. In other words, if<br>
spatial information represented for each parameter band is mapped to an N-point<br>
frequency domain, the number of valid signals of the spatial information may be N.<br>
[101]	Referring to FIG. 7, the first domain conversion unit 700 includes a first zero-<br>
padding unit 701 and a first frequency-domain conversion unit 702. The third<br><br>
14<br>
WO 2007/091842	PCT/KR2007/000668<br>
rendering unit 710 includes a mapping unit 711, a time-domain conversion unit 712, a<br>
second zero-padding unit 713, a second frequency-domain conversion unit 714, a<br>
multi-channel signal generation unit 715, a third zero-padding unit 716, a third<br>
frequency-domain conversion unit 717, and a 3D down-mix signal generation unit 718.<br>
[102]	The first zero-padding unit 701 performs a zero-padding operation on a down-mix<br>
signal with X samples in a time domain so that the number of samples of the down-mix<br>
signal can be increased from X to M. The first frequency-domain conversion unit 702<br>
converts the zero-padded down-mix signal into an M-point frequency-domain signal.<br>
The zero-padded down-mix signal has M samples. Qf the M samples of the zero-<br>
padded down-mix signal, only X samples are valid signals.<br>
[103]	The mapping unit 711 maps spatial information for each parameter band to an N-<br>
point frequency domain. The time-domain conversion unit 712 converts spatial in-<br>
formation obtained by the mapping performed by the mapping unit 711 to a time<br>
domain. Spatial information obtained by the conversion performed by the time-domain<br>
conversion unit 712 has N samples.<br>
[104]	The second zero-padding unit 713 performs a zero-padding operation on the spatial<br>
information with N samples in the time domain so that the number of samples of the<br>
spatial information can be increased from N to M. The second frequency-domain<br>
conversion unit 714 converts the zero-padded spatial information into an M-point<br>
frequency-domain signal. The zero-padded spatial information has N samples. Of the<br>
N samples of the zero-padded spatial information, only N samples are valid.<br>
[105]	The multi-channel signal generation unit 715 generates a multi-channel signal by<br>
multiplying the down-mix signal provided by the first frequency-domain conversion<br>
unit 712 and spatial information provided by the second frequency-domain conversion<br>
unit 714. The multi-channel signal generated by the multi-channel signal generation<br>
unit 715 has M valid signals. On the other hand, a multi-channel signal obtained by<br>
convoluting, in the time domain, the down-mix signal provided by the first frequency-<br>
domain conversion unit 712 and the spatial information provided by the second<br>
frequency-domain conversion unit 714 has (X+N-l) valid signals.<br>
[106]	The third zero-padding unit 716 may perform a zero-padding operation on Y filter<br>
coefficients that are represented in the time domain so that the number of samples can<br>
be increased to M. The third frequency-domain conversion unit 717 converts the zero-<br>
padded filter coefficients to the M-point frequency domain. The zero-padded filter co-<br>
efficients have M samples. Of the M samples, only Y samples are valid signals.<br>
[107]	The 3D down-mix signal generation unit 718 generates a 3D down-mix signal by<br>
multiplying the multi-channel signal generated by the multi-channel signal generation<br>
unit 715 and a plurality of filter coefficients provided by the third frequency-domain<br>
conversion unit 717. The 3D down-mix signal generated by the 3D down-mix signal<br><br>
15<br>
WO 2007/091842	PCT/KR2007/000668<br>
generation unit 718 has M valid signals. On the other hand, a 3D down-mix signal<br>
obtained by convoluting, in the time domain, the multi-channel signal generated by the<br>
multi-channel signal generation unit 715 and the filter coefficients provided by the<br>
third frequency-domain conversion unit 717 has (X+N+Y-2) valid signals.<br>
[108]	It is possible to prevent aliasing by setting the M-point frequency domain used by<br>
the first, second, and third frequency-domain conversion units 702, 714, and 717 to<br>
satisfy the following equation: M&gt;(X+N+Y-2). In other words, it is possible to prevent<br>
aliasing by enabling the first, second, and third frequency-domain conversion units<br>
702, 714, and 717 to perform M-point DFT or M-point FFT that satisfies the following<br>
equation: M&gt;(X+N+Y-2).<br>
[109]	The conversion to a frequency domain may be performed using a filter bank other<br>
than a DFT filter bank, an FFT filter bank, and QMF bank. The generation of a 3D<br>
down-mix signal may be performed using an HRTF filter.<br>
[110]	The number of valid signals of spatial information may be adjusted using a method<br>
other than the above-mentioned methods or may be adjusted using one of the above-<br>
mentioned methods that is most efficient and requires the least amount of computation.<br>
[Ill]	Aliasing may occur not only during the conversion of a signal, a coefficient or<br>
spatial information from a frequency domain to a time domain or vice versa but also<br>
during the conversion of a signal, a coefficient or spatial information from a QMF<br>
domain to a hybrid domain or vice versa. The above-mentioned methods of preventing<br>
aliasing may also be used to prevent aliasing from occurring during the conversion of a<br>
signal, a coefficient or spatial information from a QMF domain to a hybrid domain or<br>
vice versa.<br>
[112]	Spatial information used to generate a multi-channel signal or a 3D down-mix<br>
signal may vary. As a result of the variation of the spatial information, signal discon-<br>
tinuities may occur as noise in an output signal.<br>
[113]	Noise in an output signal may be reduced using a smoothing method by which<br>
spatial information can be prevented from rapidly varying.<br>
[114]	For example, when first spatial information applied to a first frame differs from<br>
second spatial information applied to a second frame when the first frame and the<br>
second frame are adjacent to each other, a discontinuity is highly likely to occur<br>
between the first and second frames.<br>
[115]	In this case, the second spatial information may be compensated for using the first<br>
spatial information or the first spatial information may be compensated for using the<br>
second spatial information so that the difference between the first spatial information<br>
and the second spatial information can be reduced, and that noise caused by the dis-<br>
continuity between the first and second frames can be reduced. More specifically, at<br>
least one of the first spatial information and the second spatial information may be<br><br>
16<br>
WO 2007/091842	PC17KR2007/000668<br>
replaced with the average of the first spatial information and the second spatial in-<br>
formation, thereby reducing noise.<br>
[116]	Noise is also likely to be generated due to a discontinuity between a pair of<br>
adjacent parameter bands. For example, when third spatial information corresponding<br>
to a first parameter band differs from fourth spatial information corresponding to a<br>
second parameter band when the first and second parameter bands are adjacent to each<br>
other, a discontinuity is likely to occur between the first and second parameter bands.<br>
[117]	In this case, the third spatial information may be compensated for using the fourth<br>
spatial information or the fourth spatial information may be compensated for using the<br>
third spatial information so that the difference between the third spatial information<br>
and the fourth spatial information can be reduced, and that noise caused by the dis-<br>
continuity between the first and second parameter bands can be reduced. More<br>
specifically, at least one of the third spatial information and the fourth spatial in-<br>
formation may be replaced with the average of the third spatial information and the<br>
fourth spatial information, thereby reducing noise.<br>
[118]	Noise caused by a discontinuity between a pair of adjacent frames or a pair of<br>
adjacent parameter bands may be reduced using methods other than the above-<br>
mentioned methods.<br>
[119]	More specifically, each frame may be multiplied by a window such as a Hanning<br>
window, and an "overlap and add" scheme may be applied to the results of the multi-<br>
plication so that the variations between the frames can be reduced. Alternatively, an<br>
output signal to which a plurality of pieces of spatial information are applied may be<br>
smoothed so that variations between a plurality of frames of the output signal can be<br>
prevented.<br>
[120]	The decorrelation between channels in a DFT domain using spatial information, for<br>
example, ICC, may be adjusted as follows.<br>
[121]	The degree of decorrelation may be adjusted by multiplying a coefficient of a<br>
signal input to a one-to-two (OTT) or two-to-three (TTT) box by a predetermined<br>
value. The predetermined value can be defined by the following equation:<br>
(A+(l-A*A)A0.5*i) where A indicates an ICC value applied to a predetermined band<br>
of the OTT or TTT box and i indicates an imaginary part. The imaginary part may be<br>
positive or negative.<br>
[122]	The predetermined value may accompany a weighting factor according to the char-<br>
acteristics of the signal, for example, the energy level of the signal, the energy charac-<br>
teristics of each frequency of the signal, or the type of box to which the ICC value A is<br>
applied. As a result of the introduction of the weighting factor, the degree of<br>
decorrelation may be further adjusted, and interframe smoothing or interpolation may<br>
be applied.<br><br>
17<br>
WO 2007/091842	PCT/KR2007/000668<br>
[123]	As described above with reference to FIG. 7, a 3D down-mix signal may be<br>
generated in a frequency domain by using an HRTF or a head related impulse response<br>
(HRIR), which is converted to the frequency domain..<br>
[124]	Alternatively, a 3D down-mix signal may be generated by convoluting an HRIR<br>
and a down-mix signal in a time domain. A 3D down-mix signal generated in a<br>
frequency domain may be left in the frequency domain without being subjected to<br>
inverse domain transform.<br>
[125]	In order to convolute an HRIR and a down-mix signal in a time domain, a finite<br>
impulse response (FIR) filter or an infinite impulse response (IIR) filter may be used.<br>
[126]	As described above, an encoding apparatus or a decoding apparatus according to an<br>
embodiment of the present invention may generate a 3D down-mix signal using a first<br>
method that involves the use of an HRTF in a frequency domain or an HRIR converted<br>
to the frequency domain, a second method that involves convoluting an HRIR in a time<br>
domain, or the combination of the first and second methods.<br>
[127]	FIGS. 8 through 11 illustrate bitstreams according to embodiments of the present<br>
invention.<br>
[128]	Referring to FIG. 8, a bitstream includes a multi-channel decoding information<br>
field which includes information necessary for generating a multi-channel signal, a 3D<br>
rendering information field which includes information necessary for generating a 3D<br>
down-mix signal, and a header field which includes header information necessary for<br>
using the information included in the multi-channel decoding information field and the<br>
information included in the 3D rendering information field. The bitstream may include<br>
only one or two of the multi-channel decoding information field, the 3D rendering in-<br>
formation field, and the header field.<br>
[129]	Referring to FIG. 9, a bitstream, which contains side information necessary for a<br>
decoding operation, may include a specific configuration header field which includes<br>
header information of a whole encoded signal and a plurality of frame data fields<br>
which includes side information regarding a plurality of frames. More specifically,<br>
each of the frame data fields may include a frame header field which includes header<br>
information of a corresponding frame and a frame parameter data field which includes<br>
spatial information of the corresponding frame. Alternatively, each of the frame data<br>
fields may include a frame parameter data field only.<br>
[130]	Each of the frame parameter data fields may include a plurality of modules, each<br>
module including a flag and parameter data. The modules are data sets including<br>
parameter data such as spatial information and other data such as down-mix gain and<br>
smoothing data which is necessary for improving the sound quality of a signal.<br>
[131]	If module data regarding information specified by the frame header fields is<br>
received without any additional flag, if the information specified by the frame header<br><br>
18<br>
WO 2007/091842	PCT7KR2007/000668<br>
fields is further classified, or if an additional flag and data are received in connection<br>
with information not specified by the frame header, module data may not include any<br>
flag.<br>
[132]	Side information regarding a 3D down-mix signal, for example, HRTF coefficient<br>
information, may be included in at least one of the specific configuration header field,<br>
the frame header fields, and the frame parameter data fields.<br>
[133]	Referring to FIG. 10, a bitstream may include a plurality of multi-channel decoding<br>
information fields which include information necessary for generating multi-channel<br>
signals and a plurality of 3D rendering information fields which include information<br>
necessary for generating 3D down-mix signals.<br>
[134]	When receiving the bitstream, a decoding apparatus may use either the multi-<br>
channel decoding information fields or the 3D rendering information field to perform a<br>
decoding operation and skip whichever of the multi-channel decoding information<br>
fields and the 3D rendering information fields are not used in the decoding operation.<br>
In this case, it may be determined which of the multi-channel decoding information<br>
fields and the 3D rendering information fields are to be used to perform a decoding<br>
operation according to the type of signals to be reproduced.<br>
[135]	In other words, in order to generate multi-channel signals, a decoding apparatus<br>
.   may skip the 3D rendering information fields, and read information included in the<br>
multi-channel decoding information fields. On the other hand, in order to generate 3D<br>
down-mix signals, a decoding apparatus may skip the multi-channel decoding in-<br>
formation fields, and read information included in the 3D rendering information fields.<br>
[136]	Methods of skipping some of a plurality of fields in a bitstream are as follows.<br>
[137]	First, field length information regarding the size in bits of a field may be included<br>
in a bitstream. In this case, the field may be skipped by skipping a number of bits cor-<br>
responding to the size in bits of the field. The field length information may be disposed<br>
at the beginning of the field.<br>
[138]	Second, a syncword may be disposed at the end or the beginning of a field. In this<br>
case, the field may be skipped by locating the field based on the location of the<br>
syncword.<br>
[139]	Third, if the length of a field is determined in advance and fixed, the field may be<br>
skipped by skipping an amount of data corresponding to the length of the field. Fixed<br>
field length information regarding the length of the field may be included in a<br>
bitstream or may be stored in a decoding apparatus.<br>
[140]	Fourth, one of a plurality of fields may be skipped using the combination of two or<br>
more of the above-mentioned field skipping methods.<br>
[141]	Field skip information, which is information necessary for skipping a field such as<br>
field length information, syncwords, or fixed field length information may be included<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
in one of the specific configuration header field, the frame header fields, and the frame<br>
parameter data fields illustrated in FIG. 9 or may be included in a field other than those<br>
illustrated in FIG. 9.<br>
[142]	For example, in order to generate multi-channel signals, a decoding apparatus may<br>
skip the 3D rendering information fields with reference to field length information, a<br>
syncword, or fixed field length information disposed at the beginning of each of the 3D<br>
rendering information fields, and read information included in the multi-channel<br>
decoding information fields.<br>
[143]	On the other hand, in order to generate 3D down-mix signals, a decoding apparatus<br>
may skip the multi-channel decoding information fields with reference to field length<br>
information, a syncword, or fixed field length information disposed at the beginning of<br>
each of the multi-channel decoding information fields, and read information included<br>
in the 3D rendering information fields.<br>
[144]	A bitstream may include information indicating whether data included in the<br>
bitstream is necessary for generating multi-channel signals or for generating 3D down-<br>
mix signals.<br>
[145]	However, even if a bitstream does not include any spatial information such as CLD<br>
but includes only data (e.g., HRTF filter coefficients) necessary for generating a 3D<br>
down-mix signal, a multi-channel signal can be reproduced through decoding using the<br>
data necessary for generating a 3D down-mix signal without a requirement of the<br>
spatial information.<br>
[146]	For example, a stereo parameter, which is spatial information regarding two<br>
channels, is obtained from a down-mix signal. Then, the stereo parameter is converted<br>
into spatial information regarding a plurality of channels to be reproduced, and a multi-<br>
channel signal is generated by applying the spatial information obtained by the<br>
conversion to the down-mix signal.<br>
[147]	On the other hand, even if a bitstream includes only data necessary for generating a<br>
multi-channel signal, a down-mix signal can be reproduced without a requirement of<br>
an additional decoding operation or a 3D down-mix signal can be reproduced by<br>
performing 3D processing on the down-mix signal using an additional HRTF filter.<br>
[148]	If a bitstream includes both data necessary for generating a multi-channel signal<br>
and data necessary for generating a 3D down-mix signal, a user may be allowed to<br>
decide whether to reproduce a multi-channel signal or a 3D down-mix signal.<br>
[ 149]	Methods of skipping data will hereinafter be described in detail with reference to<br>
respective corresponding syntaxes.<br>
[150]	Syntax 1 indicates a method of decoding an audio signal in units of frames.<br>
[151]<br>
[152]	[Syntax 1]<br><br>
WO 2007/091842<br><br>
PCT7KR2007/000668<br><br><br><br>
[153]<br>
[154]<br>
[155]<br>
[156]<br>
[157]<br>
[158]<br>
[159]<br>
[160]<br><br><br>
In Syntax 1, Ottdata() and TttData() are modules which represent parameters (such<br>
as spatial information including a CLD, ICC, and CPC) necessary for restoring a multi-<br>
channel signal from a down-mix signal, and SmgData(), TempShapeData(), Arbitrary-<br>
DownmixData(), and ResidualData() are modules which represent information<br>
necessary for improving the quality of sound by correcting signal distortions that may<br>
have occurred during an encoding operation.<br>
For example, if a parameter such as a CLD, ICC or CPC and information included<br>
in the module ArbitraryDownmixData() are only used during a decoding operation, the<br>
modules SmgData() and TempShapeData(), which are disposed between the modules<br>
TttData() and ArbitraryDownmixData(), may be unnecessary. Thus, it is efficient to<br>
skip the modules SmgData() and TempShapeData().<br>
A method of skipping modules according to an embodiment of the present<br>
invention will hereinafter be described in detail with reference to Syntax 2 below.<br>
[Syntax 2]<br><br>
WO 2007/091842	PCT/KR2007/000668<br><br>
[161]<br>
[162]	Referring to Syntax 2, a module SkipData() may be disposed in front of a module<br>
to be skipped, and the size in bits of the module to be skipped is specified in the<br>
module SkipData() as bsSkipBits.<br>
[163]	In other words, assuming that modules SmgDataQ and TempShapeData() are to be<br>
skipped, and that the size in bits of the modules SmgData() and TempShapeData()<br>
combined is 150, the modules SmgData() and TempShapeData() can be skipped by<br>
setting bsSkipBits to 150.<br>
[164]	A method of skipping modules according to another embodiment of the present<br>
invention will hereinafter be described in detail with reference to Syntax 3.<br>
[165]<br>
[166]	[Syntax 3]<br>
[167]<br>
[168] <br>
[169]	Referring to Syntax 3, an unnecessary module may be skipped by using<br>
bsSkipSyncflag, which is a flag indicating whether to use a syncword, and<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
bsSkipSyncword, which is a syncword that can be disposed at the end of a module to<br>
be skipped.<br>
[170]	More specifically, if the flag bsSkipSyncflag is set such that a syncword can be<br>
used, one or more modules between the flag bsSkipSyncflag and the syncword<br>
bsSkipSyncword, i.e., modules SmgData() and TempShapeData(), may be skipped.<br>
[171]	Referring to FIG. 11, a bitstream may include a multi-channel header field which<br>
includes header information necessary for reproducing a multi-channel signal, a 3D<br>
rendering header field which includes header information necessary for reproducing a<br>
3D down-mix signal, and a plurality of multi-channel decoding information fields,<br>
which include data necessary for reproducing a multi- channel signal.<br>
[172]	In order to reproduce a multi-channel signal, a decoding apparatus may skip the 3D<br>
rendering header field, and read data from the multi-channel header field and the multi-<br>
channel decoding information fields.<br>
[173]	A method of skipping the 3D rendering header field is the same as the field<br>
skipping methods described above with reference to FTC. 10, and thus, a detailed de-<br>
scription thereof will be skipped.<br>
[174]	In order to reproduce a 3D down-mix signal, a decoding apparatus may read data<br>
from the multi-channel decoding information fields and the 3D rendering header field.<br>
For example, a decoding apparatus may generate a 3D down-mix signal using a down-<br>
mix signal included in the multi-channel decoding information field and HRTF co-<br>
efficient information included in the 3D down-mix signal.<br>
[175]	FIG. 12 is a block diagram of an encoding/decoding apparatus for processing an<br>
arbitrary down-mix signal according to an embodiment of the present invention.<br>
Referring to FIG. 12, an arbitrary down-mix signal is a down-mix signal other than a<br>
down-mix signal generated by a multi-channel encoder 801 included in an encoding<br>
apparatus 800. Detailed descriptions of the same processes as those of the embodiment<br>
of FIG. 1 will be omitted.<br>
[176]	Referring to FIG. 12, the encoding apparatus 800 includes the multi-channel<br>
encoder 801, a spatial information synthesization unit 802, and a comparison unit 803.<br>
[177]	The multi-channel encoder 801 down-mixes an input multi-channel signal into a<br>
stereo or mono down-mix signal, and generates basic spatial information necessary for<br>
restoring a multi-channel signal from the down-mix signal.<br>
[178]	The comparison unit 803 compares the down-mix signal with an arbitrary down-<br>
mix signal, and generates compensation information based on the result of the<br>
comparison. The compensation information is necessary for compensating for the<br>
arbitrary down-mix signal so that the arbitrary down-mix signal can be converted to be<br>
approximate to the down-mix signal. A decoding apparatus may compensate for the<br>
arbitrary down-mix signal using the compensation information and restore a multi-<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
channel signal using the compensated arbitrary down-mix signal. The restored multi-<br>
channel signal is more similar than a multi-channel signal restored from the arbitrary<br>
down-mix signal generated by the multi-channel encoder 801 to the original input<br>
multi-channel signal.<br>
[179]	The compensation information may be a difference between the down-mix signal<br>
and the arbitrary down-mix signal. A decoding apparatus may compensate for the<br>
arbitrary down-mix signal by adding, to the arbitrary down-mix signal, the difference<br>
between the down-mix signal and the arbitrary down-mix signal.<br>
[180]	The. difference between the down-mix signal and the arbitrary down-mix signal<br>
may be down-mix gain which indicates the difference; between the energy levels of the<br>
down-mix signal and the arbitrary down-mix signal.<br>
[181]	The down-mix gain may be determined for each frequency band, for each time/<br>
time slot, and/or for each channel. For example, one part of the down-mix gain may be<br>
determined for each frequency band, and another part of the down-mix gain may be<br>
determined for each time slot.<br>
[182]	The down-mix gain may be determined for each parameter band or for each<br>
frequency band optimized for the arbitrary down-mix signal. Parameter bands are<br>
frequency intervals to which parameter-type spatial information is applied.<br>
[183]	The difference between the energy levels of the down-mix signal and the arbitrary<br>
down-mix signal may be quantized. The resolution of quantization levels for<br>
quantizing the difference between the energy levels of the down-mix signal and the<br>
arbitrary down-mix signal may be the same as or different from the resolution of<br>
quantization levels for quantizing a CLD between the down-mix signal and the<br>
arbitrary down-mix signal. In addition, the quantization of the difference between the<br>
energy levels of the down-mix signal and the arbitrary down-mix signal may involve<br>
the use of all or some of the quantization levels for quantizing the CLD between the<br>
down-mix signal and the arbitrary down-mix signal.<br>
[184]	Since the resolution of the difference between the energy levels of the down-mix<br>
signal and the arbitrary down-mix signal is generally lower than the resolution of the<br>
CLD between the down-mix signal and the arbitrary down-mix signal, the resolution of<br>
the quantization levels for quantizing the difference between the energy levels of the<br>
down-mix signal and the arbitrary down-mix signal may have a minute value<br>
compared to the resolution of the quantization levels for quantizing the CLD between<br>
the down-mix signal and the arbitrary down-mix signal.<br>
[185]	The compensation information for compensating for the arbitrary down-mix signal<br>
may be extension information including residual information which specifies<br>
components of the input multi-channel signal that cannot be restored using the<br>
arbitrary down-mix signal or the down-mix gain. A decoding apparatus can restore<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
components of the input multi-channel signal that cannot be restored using the<br>
arbitrary down-mix signal or the down-mix gain using the extension information,<br>
thereby restoring a signal almost indistinguishable from the original input multi-<br>
channel signal.<br>
[186]	Methods of generating the extension information are as follows.<br>
[187]	The multi-channel encoder 801 may generate information regarding components of<br>
the input multi-channel signal that are lacked by the down-mix signal as first extension<br>
information. A decoding apparatus may restore a signal almost indistinguishable from<br>
the original input multi-channel signal by applying the first extension information to<br>
the generation of a multi-channel signal using the down-mix signal and the basic<br>
spatial information.<br>
[188]	Alternatively, the multi-channel encoder 801 may restore a multi-channel signal<br>
using the down-mix signal and the basic spatial information, and generate the<br>
difference between the restored multi-channel signal and the original input multi-<br>
channel signal as the first extension information.<br>
[189]	The comparison unit 803 may generate, as second extension information, in-<br>
formation regarding components of the down-mix signal that are lacked by the<br>
arbitrary down-mix signal, i.e., components of the down-mix signal that cannot be<br>
compensated for using the down-mix gain. A decoding apparatus may restore a signal<br>
almost indistinguishable from the down-mix signal using the arbitrary down-mix signal<br>
and the second extension information.<br>
[190]	The extension information may be generated using various residual coding<br>
methods other than the above-described method.<br>
[191]	The down-mix gain and the extension information may both be used as com-<br>
pensation information. More specifically, the down-mix gain and the extension in-<br>
formation may both be obtained for an entire frequency band of the down-mix signal<br>
and may be used together as compensation information. Alternatively, the down-mix<br>
gain may be used as compensation information for one part of the frequency band of<br>
the down-mix signal, and the extension information may be used as compensation in-<br>
formation for another part of the frequency band of the down-mix signal. For example,<br>
the extension information may be used as compensation information for a low<br>
frequency band of the down-mix signal, and the down-mix gain may be used as comp<br>
ensation information for a high frequency band of the down-mix signal.<br>
[ 192]	Extension information regarding portions of the down-mix signal, other than the<br>
low-frequency band of the down-mix signal, such as peaks or notches that may con-<br>
siderably affect the quality of sound may also be used as compensation information.<br>
[193]	The spatial information synthesization unit 802 synthesizes the basic spatial in-<br>
formation (e.g., a CLD, CPC, ICC, and CTD) and the compensation information,<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
thereby generating spatial information. In other words, the spatial information, which<br>
is transmitted to a decoding apparatus, may include the basic spatial information, the<br>
down-mix gain, and the first and second extension information.<br>
[194]	The spatial information may be included in a bitstream along with the arbitrary<br>
down-mix signal, and the bitstream may be transmitted to a decoding apparatus.<br>
[195]	The extension information and the arbitrary down-mix signal may be encoded<br>
using an audio encoding method such as an AAC method, a MP3 method, or a BSAC<br>
method. The extension information and the arbitrary down-mix signal may be encoded<br>
using the same audio encoding method or different audio encoding methods.<br>
[196]	If the extension information and the arbitrary down-mix signal are encoded using<br>
the same audio encoding method, a decoding apparatus may decode both the extension<br>
information and the arbitrary down-mix signal using a single audio decoding method.<br>
In this case, since the arbitrary down-mix signal can always be decoded, the extension<br>
information can also always be decoded. However, since the arbitrary down-mix signal<br>
is generally input to a decoding apparatus as a pulse code modulation (PCM) signal,<br>
the type of audio codec used to encode the arbitrary down-mix signal may not be<br>
readily identified, and thus, the type of audio codec used to encode the extension in-<br>
formation may not also be readily identified.<br>
[197]	Therefore, audio codec information regarding the type of audio codec used to<br>
encode the arbitrary down-mix signal and the extension information may be inserted<br>
into a bitstream.<br>
[198]	More specifically, the audio codec information may be inserted into a specific con-<br>
figuration header field of a bitstream. In this case, a decoding apparatus may extract<br>
the audio codec information from the specific configuration header field of the<br>
bitstream and use the extracted audio codec information to decode the arbitrary d own-<br>
mix signal and the extension information.<br>
[199]	On the other hand, if the arbitrary down-mix signal and the extension information<br>
are encoded using different audio encoding methods, the extension information may<br>
not be able to be decoded. In this case, since the end of the extension information<br>
cannot be identified, no further decoding operation can be performed.<br>
[200]	In order to address this problem, audio codec information regarding the types of<br>
audio codecs respectively used to encode the arbitrary down-mix signal and the<br>
extension information may be inserted into a specific configuration header field of a<br>
bitstream. Then, a decoding apparatus may read the audio codec information from the<br>
specific configuration header field of the bitstream and use the read information to<br>
decode the extension information. If the decoding apparatus does not include any<br>
decoding unit that can decode the extension information, the decoding of the extension<br>
information may not further proceed, and information next to the extension in-<br><br>
26<br>
WO 2007/091842	PCT/KR2007/000668<br>
formation may be read.<br>
[201]	Audio codec information regarding the type of audio codec used to encode the<br>
extension information may be represented by a syntax element included in a specific<br>
configuration header field of a bitstream. For example, the audio codec information<br>
may be represented by bsResidualCodecType, which is a 4-bit syntax element, as<br>
indicated in Table 1 below.<br>
[202]<br>
[203]	Table 1<br>
[204]<br>
bsResidualCodecType	Codec<br>
0	AAC<br>
1	MP3<br>
2	BSAC<br>
3... 15	Reserved<br>
[205]<br>
[206]	The extension information may include not only the residual information but also<br>
channel expansion information. The channel expansion information is information<br>
necessary for expanding a multi-channel signal obtained through decoding using the<br>
spatial information into a multi-channel signal with more channels. For example, the<br>
channel expansion information may be information necessary for expanding a<br>
5.1-channel signal or a 7.1-channel signal into a 9.1-channel signal.<br>
[207]	The extension information may be included in a bitstream, and the bitstream may<br>
be transmitted to a decoding apparatus. Then, the decoding apparatus may compensate<br>
for the down-mix signal or expand a multi-channel signal using the extension in-<br>
formation. However, the decoding apparatus may skip the extension information,<br>
instead of extracting the extension information from the bitstream. For example, in the<br>
case of generating a multi-channel signal using a 3D down-mix signal included in the<br>
bitstream or generating a 3D down-mix signal using a down-mix signal included in the<br>
bitstream, the decoding apparatus may skip the extension information.<br>
[208]	A method of skipping the extension information included in a bitstream may be the<br>
same as one of the field skipping methods described above with reference to FIG. 10.<br>
[209]	For example, the extension information may be skipped using at least one of bit<br>
size information which is attached to the beginning of a bitstream including the<br>
extension information and indicates the size in bits of the extension information, a<br>
syncword which is attached to the beginning or the end of the field including the<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
extension information, and fixed bit size information which indicates a fixed size in<br>
bits of the extension information. The bit size information, the syncword, and the fixed<br>
bit size information may all be included in a bitstream. The fixed bit size information<br>
may also be stored in a decoding apparatus.<br>
[210]	Referring to FIG. 12, a decoding unit 810 includes a down-mix compensation unit<br>
811, a 3D rendering unit 815, and a multi-channel decoder 816.<br>
[211]	The down-mix compensation unit 811 compensates for an arbitrary down-mix<br>
signal using compensation information included in spatial information, for example,<br>
using down-mix gain or extension information.<br>
[212]	The 3D rendering unit 815 generates a decoder 3D down-mix signal by performing<br>
a 3D rendering operation on the compensated down-mix signal. The multi-channel<br>
decoder 816 generates a 3D multi-channel signal using the compensated down-mix<br>
signal and basic spatial information, which is included in the spatial information.<br>
[213]	The down-mix compensation unit 811 may compensate for the arbitrary down-mix<br>
signal in the following manner.<br>
[214]	If the compensation information is down-mix gain, the down-mix compensation<br>
unit 811 compensates for the energy level of the arbitrary down-mix signal using the<br>
down-mix gain so that the arbitrary down-mix signal can be converted into a signal<br>
similar to a down-mix signal.<br>
[215]	If the compensation information is second extension information, the down-mix<br>
compensation unit 811 may compensate for components that are lacked by the<br>
arbitrary down-mix signal using the second extension information.<br>
[216]	The multi-channel decoder 816 may generate a multi-channel signal by se-<br>
quentially applying pre-matrix Ml, mix-matrix M2 and post-matrix M3 to a down-mix<br>
signal. In this case, the second extension information may be used to compensate for<br>
the down-mix signal during the application of mix-matrix M2 to the down-mix signal.<br>
In other words, the second extension information may be used to compensate for a<br>
down-mix signal to which pre-matrix Ml has already been applied.<br>
[217]	As described above, each of a plurality of channels may be selectively<br>
compensated for by applying the extension information to the generation of a multi-<br>
channel signal. For example, if the extension information is applied to a center channel<br>
of mix-matrix M2, left- and right-channel components of the down-mix signal may be<br>
compensated for by the extension information. If the extension information is applied<br>
to a left channel of mix-matrix M2, the left-channel component of the down-mix signal<br>
may be compensated for by the extension information.<br>
[218]	The down-mix gain and the extension information may both be used as the com-<br>
pensation information. For example, a low frequency band of the arbitrary down-mix<br>
signal may be compensated for using the extension information, and a high frequency<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
band of the arbitrary down-mix signal may be compensated for using the down-mix<br>
gain. In addition, portions of the arbitrary down-mix signal, other than the low<br>
frequency band of the arbitrary down-mix signal, for example, peaks or notches that<br>
may considerably affect the quality of sound, may also be compensated for using the<br>
extension information. Information regarding portion to be compensated for by the<br>
extension information may be included in a bitstream. Information indicating whether<br>
a down-mix signal included in a bitstream is an arbitrary down-mix signal or not and<br>
information indicating whether the bitstream includes compensation information may<br>
be included in the bitstream.<br>
[219]	In order to prevent clipping of a down-mix signal generated by the encoding unit<br>
800, the down-mix signal may be divided by predetermined gain. The predetermined<br>
gain may have a static value or a dynamic value.<br>
[220]	The down-mix compensation unit 811 may restore the original down-mix signal by<br>
compensating for the down-mix signal, which is weakened in order to prevent clipping,<br>
using the predetermined gain.<br>
[221]	An arbitrary down-mix signal compensated for by the down-mix compensation unit<br>
811 can be readily reproduced. Alternatively, an arbitrary down-mix signal yet to be<br>
compensated for may be input to the 3D rendering unit 815, and may be converted into<br>
a decoder 3D down-mix signal by the 3D rendering unit 815.<br>
[222]	Referring to FIG. 12, the down-mix compensation unit 811 includes a first domain<br>
converter 812, a compensation processor 813, and a second domain converter 814.<br>
[223]	The first domain converter 812 converts the domain of an arbitrary down-mix<br>
signal into a predetermined domain. The compensation processor 813 compensates for<br>
the arbitrary down-mix signal in the predetermined domain, using compensation in-<br>
formation, for example, down-mix gain or extension information.<br>
[224]	The compensation of the arbitrary down-mix signal may be performed in a QMF/<br>
hybrid domain. For this, the first domain converter 812 may perform QMF/hybrid<br>
analysis on the arbitrary down-mix signal. The first domain converter 812 may convert<br>
the domain of the arbitrary down-mix signal into a domain, other than a QMF/hybrid<br>
domain, for example, a frequency domain such as a DFT or FFT domain. The com-<br>
pensation of the arbitrary down-mix signal may also be performed in a domain, other<br>
than a QMF/hybrid domain, for example, a frequency domain or a time domain.<br>
[225]	The second domain converter 814 converts the domain of the compensated<br>
arbitrary down-mix signal into the same domain as the original arbitrary down-mix<br>
signal. More specifically, the second domain converter 814 converts the domain of the<br>
compensated arbitrary down-mix signal into the same domain as the original arbitrary<br>
down-mix signal by inversely performing a domain conversion operation performed by<br>
the first domain converter 812.<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
[226]	For example, the second domain converter 814 may convert the compensated<br>
arbitraiy down-mix signal into a time-domain signal by performing QMF/hybrid<br>
synthesis on the compensated arbitrary down-mix signal. Also, the second domain<br>
converter 814 may perform IDFT or IFFT on the compensated arbitrary down-mix<br>
signal.<br>
[227]	The 3D rendering unit 815, like the 3D rendering unit 710 illustrated in FIG. 7,<br>
may perform a 3D rendering operation on the compensated arbitrary down-mix signal<br>
in a frequency domain, a QMF/hybrid domain or a time domain. For this, the 3D<br>
rendering unit 815 may include a domain converter (not shown). The domain converter<br>
converts the domain of the compensated arbitrary down-mix signal into a domain in<br>
which a 3D rendering operation is to be performed or converts the domain of a signal<br>
obtained by the 3D rendering operation.<br>
[228]	The domain in which the compensation processor 813 compensates for the<br>
arbitrary down-mix signal may be the same as or different from the domain in which<br>
the 3D rendering unit 815 performs a 3D rendering operation on the compensated<br>
arbitrary down-mix signal.<br>
[229]	FIG. 13 is a block diagram of a down-mix compensation/3D rendering unit 820<br>
according to an embodiment of the present invention. Referring to FIG. 13, the down-<br>
mix compensation/3D rendering unit 820 includes a first domain converter 821, a<br>
second domain converter 822, a compensation/3D rendering processor 823, and a third<br>
domain converter 824.<br>
[230]	The down-mix compensation/3D rendering unit 820 may perform both a com-<br>
pensation operation and a 3D rendering operation on an arbitrary down-mix signal in a<br>
single domain, thereby reducing the amount of computation of a decoding apparatus.<br>
[231]	More specifically, the first domain converter 821 converts the domain of the<br>
arbitrary down-mix signal into a first domain in which a compensation operation and a<br>
3D rendering operation are to be performed. The second domain converter 822<br>
converts spatial information, including basic spatial information necessary for<br>
generating a multi-channel signal and compensation information necessary for com-<br>
pensating for the arbitrary down-mix signal, so that the spatial information can become<br>
applicable in the first domain. The compensation information may include at least one<br>
of down-mix gain and extension information.<br>
[232]	For example, the second domain converter 822 may map compensation in-<br>
formation corresponding to a parameter band in a QMF/hybrid domain to a frequency<br>
band so that the compensation information can become readily applicable in a<br>
frequency domain.<br>
[233]	The first domain may be a frequency domain such as a DFT or FFT domain, a<br>
QMF/hybrid domain, or a time domain. Alternatively, the first domain may be a<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
domain other than those set forth herein.<br>
[234]	During the conversion of the compensation information, a time delay may occur. In<br>
order to address this problem, the second domain converter 822 may perform a time<br>
delay compensation operation so that a time delay between the domain of the com-<br>
pensation information and the first domain can be compensated for.<br>
[235]	The compensation/3D rendering processor 823 performs a compensation operation<br>
on the arbitrary down-mix signal in the first domain using the converted spatial in-<br>
formation and then performs a 3D rendering operation on a signal obtained by the<br>
compensation operation. The compensation/3D rendering processor 823 may perform a<br>
compensation operation and a 3D rendering operation in a different order from that set<br>
forth herein.<br>
[236]	The compensation/3D rendering processor 823 may perform a compensation<br>
operation and a 3D rendering operation on the arbitrary down-mix signal at the same<br>
time. For example, the compensation/3D rendering processor 823 may generate a<br>
compensated 3D down-mix signal by performing a 3D rendering operation on the<br>
arbitrary down-mix signal in the first domain using a new filter coefficient, which is<br>
the combination of the compensation information and an existing filter coefficient<br>
typically used in a 3D rendering operation.<br>
[237]	The third domain converter 824 converts the domain of the 3D down-mix signal<br>
generated by the compensation/3D rendering processor 823 into a frequency domain.<br>
[238]	FIG. 14 is a block diagram of a decoding apparatus 900 for processing a<br>
compatible down-mix signal according to an embodiment of the present invention.<br>
Referring to FIG. 14, the decoding apparatus 900 includes a first multi-channel<br>
decoder 910, a down-mix compatibility processing unit 920, a second multi-channel<br>
decoder 930, and a 3D rendering unit 940. Detailed descriptions of the same decoding<br>
processes as those of the embodiment of FIG. 1 will be omitted.<br>
[239]	A compatible down-mix signal is a down-mix signal that can be decoded by two or<br>
more multi-channel decoders. In other words, a compatible down-mix signal is a<br>
down-mix signal that is initially optimized for a predetermined multi-channel decoder<br>
and that can be converted afterwards into a signal optimized for a multi-channel<br>
decoder, other than the predetermined multi-channel decoder, through a compatibility<br>
processing operation.<br>
[240]	Referring to FIG. 14, assume that an input compatible down-mix signal is<br>
optimized for the first multi-channel decoder 910. In order for the second multi-<br>
channel decoder 930 to decode the input compatible down-mix signal, the down-mix<br>
compatibility processing unit 920 may perform a compatibility processing operation on<br>
the input compatible down-mix signal so that the input compatible down-mix signal<br>
can be converted into a signal optimized for the second multi-channel decoder 930.<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
The first multi-channel decoder 910 generates a first multi-channel signal by decoding<br>
the input compatible down-mix signal. The first multi-channel decoder 910 can<br>
generate a multi-channel signal through decoding simply using the input compatible<br>
down-mix signal without a requirement of spatial information.<br>
[241]	The second multi-channel decoder 930 generates a second multi-channel signal<br>
using a down-mix signal obtained by the compatibility processing operation performed<br>
by the down-mix compatibility processing unit 920. The 3D rendering unit 940 may<br>
generate a decoder 3D down-mix signal by performing a 3D rendering operation on the<br>
down-mix signal obtained by the compatibility processing operation performed by the<br>
down-mix compatibility processing unit 920.<br>
[242]	A compatible down-mix signal optimized for a predetermined multi-channel<br>
decoder may be converted into a down-mix signal optimized for a multi-channel<br>
decoder, other than the predetermined multi-channel decoder, using compatibility in-<br>
formation such as an inversion matrix. For example, when there are first and second<br>
multi-channel encoders using different encoding methods and first and second multi-<br>
channel decoders using different encoding/decoding methods, an encoding apparatus<br>
may apply a matrix to a down-mix signal generated by the first multi-channel encoder,<br>
thereby generating a compatible down-mix signal which is optimized for the second<br>
multi-channel decoder. Then, a decoding apparatus may apply an inversion matrix to<br>
the compatible down-mix signal generated by the encoding apparatus, thereby<br>
generating a compatible down-mix signal which is optimized for the first multi-<br>
channel decoder.<br>
[243]	Referring to FIG. 14, the down-mix compatibility processing unit 920 may perform<br>
a compatibility processing operation on the input compatible down-mix signal using an<br>
inversion matrix, thereby generating a down-mix signal which is optimized for the<br>
second multi-channel decoder 930.<br>
[244]	Information regarding the inversion matrix used by the down-mix compatibility<br>
processing unit 920 may be stored in the decoding apparatus 900 in advance or may be<br>
included in an input bitstream transmitted by an encoding apparatus. In addition, in-<br>
formation indicating whether a down-mix signal included in the input bitstream is an<br>
arbitrary down-mix signal or a compatible down-mix signal may be included in the<br>
input bitstream.<br>
[245]	Referring to FIG. 14, the down-mix compatibility processing unit 920 includes a<br>
first domain converter 921, a compatibility processor 922, and a second domain<br>
converter 923.<br>
[246]	The first domain converter 921 converts the domain of the input compatible down-<br>
mix signal into a predetermined domain, and the compatibility processor 922 performs<br>
a compatibility processing operation using compatibility information such as an<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
inversion matrix so that the input compatible down-mix signal in the predetermined<br>
domain can be converted into a signal optimized for the second multi-channel decoder<br>
930.<br>
[247]	The compatibility processor 922 may perform a compatibility processing operation<br>
in a QMF/hybrid domain. For this, the first domain converter 921 may perform QMF/<br>
hybrid analysis on the input compatible down-mix signal. Also, the first domain<br>
converter 921 may convert the domain of the input compatible down-mix signal into a<br>
domain, other than a QMF/hybrid domain, for example, a frequency domain such as a<br>
DFT or FFT domain, and the compatibility processor 922 may perform the com-<br>
patibility processing operation in a domain, other than a QMF/hybrid domain, for<br>
example, a frequency domain or a time domain.<br>
[248]	The second domain converter 923 converts the domain of a compatible down-mix<br>
signal obtained by the compatibility processing operation. More specifically, the<br>
second domain converter 923 may convert the domain of the compatibility down-mix<br>
signal obtained by the compatibility processing operation into the same domain as the<br>
original input compatible down-mix signal by inversely performing a domain<br>
conversion operation performed by the first domain converter 921.<br>
[249]	For example, the second domain converter 923 may convert the compatible down-<br>
mix signal obtained by the compatibility processing operation into a time-domain<br>
signal by performing QMF/hybrid synthesis on the compatible down-mix signal<br>
obtained by the compatibility processing operation. Alternatively, the second domain<br>
converter 923 may perform IDFT or IFFT on the compatible down-mix signal obtained<br>
by the compatibility processing operation.<br>
[250]	The 3D rendering unit 940 may perform a 3D rendering operation on the<br>
compatible down-mix signal obtained by the compatibility processing operation in a<br>
frequency domain, a QMF/hybrid domain or a time domain. For this, the 3D rendering<br>
unit 940 may include a domain converter (not shown). The domain converter converts<br>
the domain of the input compatible down-mix signal into a domain in which a 3D<br>
rendering operation is to be performed or converts the domain of a signal obtained by<br>
the 3D rendering operation.<br>
[251]	The domain in which the compatibility processor 922 performs a compatibility<br>
processing operation may be the same as or different from the domain in which the 3D<br>
rendering unit 940 performs a 3D rendering operation.<br>
[252]	FIG. 15 is a block diagram of a down-mix compatibility processing/3D rendering<br>
unit 950 according to an embodiment of the present invention. Referring to FIG. 15,<br>
the down-mix compatibility processing/3D rendering unit 950 includes a first domain<br>
converter 951, a second domain converter 952, a compatibility/3D rendering processor<br>
953, and a third domain converter 954.<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
[253]	The down-mix compatibility processing/3D rendering unit 950 performs a com-<br>
patibility processing operation and a 3D rendering operation in a single domain,<br>
thereby reducing the amount of computation of a decoding apparatus.<br>
[254]	The first domain converter 951 converts an input compatible down-mix signal into<br>
a first domain in which a compatibility processing operation and a 3D rendering<br>
operation are to be performed. The second domain converter 952 converts spatial in-<br>
formation and compatibility information, for example, an inversion matrix, so that the<br>
spatial information and the compatibility information can become applicable in the<br>
first domain.<br>
[255]	For example, the second domain converter 952 maps an inversion matrix cor-<br>
responding to a parameter band in a QMF/hybrid domain to a frequency domain so that<br>
the inversion matrix can become readily applicable in a frequency domain.<br>
[256]	The first domain may be a frequency domain such as a DFT or FFT domain, a<br>
QMF/hybrid domain, or a time domain. Alternatively, the first domain may be a<br>
domain other than those set forth herein.<br>
[257]	During the conversion of the spatial information and the compatibility information,<br>
a time delay may occur. In order to address this problem,<br>
[258]	In order to address this problem, the second domain converter 952 may perform a<br>
time delay compensation operation so that a time delay between the domain of the<br>
spatial information and the compensation information and the first domain can be<br>
compensated for.<br>
[259]	The compatibility/3D rendering processor 953 performs a compatibility processing<br>
operation on the input compatible down-mix signal in the first domain using the<br>
converted compatibility information and then performs a 3D rendering operation on a<br>
compatible down-mix signal obtained by the compatibility processing operation. The<br>
compatibility/3D rendering processor 953 may perform a compatibility processing<br>
operation and a 3D rendering operation in a different order from that set forth herein.<br>
[260]	The compatibility/3D rendering processor 953 may perform a compatibility<br>
processing operation and a 3D rendering operation on the input compatible down-mix<br>
signal at the same time. For example, the compatibility/3D rendering processor 953<br>
may generate a 3D down-mix signal by performing a 3D rendering operation on the<br>
input compatible down-mix signal in the first domain using a new filter coefficient,<br>
which is the combination of the compatibility information and an existing filter co-<br>
efficient typically used in a 3D rendering operation.<br>
[261]	The third domain converter 954 converts the domain of the 3D down-mix signal<br>
generated by the compatibility/3D rendering processor 953 into a frequency domain.<br>
[262]	FIG. 16 is a block diagram of a decoding apparatus for canceling crosstalk<br>
according to an embodiment of the present invention. Referring to FIG. 16, the<br><br>
WO 2007/091842	PC17KR2007/000668<br>
decoding apparatus includes a bit unpacking unit 960, a down-mix decoder 970, a 3D<br>
rendering unit 980, and a crosstalk cancellation unit 990. Detailed descriptions of the<br>
same decoding processes as those of the embodiment of FIG. 1 will be omitted.<br>
[263]	A 3D down-mix signal output by the 3D rendering unit 980 may be reproduced by<br>
a headphone. However, when the 3D down-mix signal is reproduced by speakers that<br>
are distant apart from a user, inter-channel crosstalk is likely to occur.<br>
[264]	Therefore, the decoding apparatus may include the crosstalk cancellation unit 990<br>
which performs a crosstalk cancellation operation on the 3D down-mix signal.<br>
[265]	The decoding apparatus may perform a sound field processing operation.<br>
[266]	Sound field information used in the sound field processing operation, i.e., in-<br>
formation identifying a space in which the 3D down-mix signal is to be reproduced,<br>
may be included in an input bitstream transmitted by an encoding apparatus or may be<br>
selected by the decoding apparatus.<br>
[267]	The input bitstream may include reverberation time information. A filter used in<br>
the sound field processing operation may be controlled according to the reverberation<br>
time information.<br>
[268]	A sound field processing operation may be performed differently for an early part<br>
and a late reverberation part. For example, the early part may be processed using a FIR<br>
filter, and the late reverberation part may be processed using an IIR filter.<br>
[269]	More specifically, a sound field processing operation may be performed on the<br>
early part by performing a convolution operation in a time domain using an FIR filter<br>
or by performing a multiplication operation in a frequency domain and converting the<br>
result of the multiplication operation to a time domain. A sound field processing<br>
operation may be performed on the late reverberation part in a time domain.<br>
[270]	The present invention can be realized as computer-readable code written on a<br>
computer-readable recording medium. The computer-readable recording medium may<br>
be any type of recording device in which data is stored in a computer-readable manner.<br>
Examples of the computer-readable recording medium include a ROM, a RAM, a CD-<br>
ROM, a magnetic tape, a floppy disc, an optical data storage, and a carrier wave (e.g.,<br>
data transmission through the Internet). The computer-readable recording medium can<br>
be distributed over a plurality of computer systems connected to a network so that<br>
computer-readable code is written thereto and executed therefrom in a decentralized<br>
manner. Functional programs, code, and code segments needed for realizing the<br>
present invention can be easily construed by one of ordinary skill in the art.<br>
[271]	As described above, according to the present invention, it is possible to efficiently<br>
encode multi-channel signals with 3D effects and to adaptively restore and reproduce<br>
audio signals with optimum sound quality according to the characteristics of a re-<br>
production environment.<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
Industrial Applicability<br>
[272]	Other implementations are within the scope of the following claims. For example,<br>
grouping, data coding, and entropy coding according to the present invention can be<br>
applied to various application fields and various products. Storage media storing data<br>
to which an aspect of the present invention is applied are within the scope of the<br>
present invention.<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
Claims<br>
[1]	A decoding method of restoring a multi-channel signal, the decoding method<br>
comprising:<br>
extracting a three-dimensional (3D) down-mix signal and spatial information<br>
from an input bitstream;<br>
removing 3D effects from the 3D down-mix signal by performing a 3D rendering<br>
operation on the 3D down-mix signal; and<br>
generating a multi-channel signal using the spatial information and a down-mix<br>
signal obtained by the removal.<br>
[2]	The decoding method of claim 1, wherein the removal comprises using an<br>
inverse filter of a filter used for generating the 3D down-mix signal.<br>
[3]	The decoding method of claim 2, wherein information regarding the filter is<br>
extracted from the input bitstream.<br>
[4]	The decoding method of claim 1, wherein the removal comprises using an<br>
inverse function of a head related transfer function (HRTF) used for generating<br>
the 3D down-mix signal.<br>
[5]	The decoding method of claim 4, wherein information regarding coefficients of<br>
the HRTF or coefficients of the inverse function of the HRTF is extracted from<br>
the input bitstream.<br>
[6]	The decoding method of claim 1, wherein the input bitstream includes at least<br>
one of information indicating whether the input bitstream includes filter in-<br>
formation identifying a filter used to perform the 3D rendering operation and in-<br>
formation indicating whether the filter information specifies an inverse filter of a<br>
filter used for generating the 3D down-mix signal<br>
[7]	The decoding method of claim 1, wherein the removal comprises performing the<br>
3D rendering operation in one of a discrete Fourier transform (DFT) domain, a<br>
fast Fourier transform (FFT) domain, a quadrature mirror filter (QMF)/hybrid<br>
domain, and a time domain.<br>
[8]	The decoding method of claim 1, further comprising decoding the 3D down-mix<br>
signal.<br>
[9]	A decoding method of restoring a multi-channel signal, the decoding method<br>
comprising:<br>
extracting a 3D down-mix signal and spatial information from an input bitstream;<br>
generating a multi-channel signal using the 3D down-mix signal and the spatial<br>
information; and<br>
removing 3D effects from the multi-channel signal by performing a 3D rendering<br>
operation on the multi-channel signal.<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
[10]	An encoding method of encoding a multi-channel signal with a plurality of<br>
channels, the encoding method comprising:<br>
encoding the multi-channel signal into a down-mix signal with fewer channels;<br>
generating spatial information regarding the plurality of channels;<br>
generating a 3D down-mix signal by performing a 3D rendering operation on the<br>
down-mix signal; and<br>
generating a bitstream including the 3D down-mix signal and the spatial in-<br>
formation.<br>
[11]	The encoding method of claim 10, wherein the generation of the 3D down-mix<br>
signal comprises performing the 3D rendering operation using a HRTF.<br>
[12]	The encoding method of claim 11, wherein the bitstream includes at least one of<br>
information regarding coefficients of the HRTF and information regarding co-<br>
efficients of an inverse function of the HRTF.<br>
[13]	The encoding method of claim 10, wherein the generation of the 3D down-mix<br>
signal comprises performing the 3D rendering operation in one of a DFT domain,<br>
an FFT domain, a QMF/hybrid domain, and a time domain.<br>
[14]	An encoding method of encoding a multi-channel signal with a plurality of<br>
channels, the encoding method comprising:<br>
performing a 3D rendering operation on the multi-channel signal;<br>
encoding a multi-channel signal obtained by the 3D rendering operation into a<br>
3D down-mix signal with fewer channels;<br>
generating spatial information regarding the plurality of channels; and<br>
generating a bitstream including the 3D down-mix signal and the spatial in-<br>
formation.<br>
[15]	A decoding apparatus for restoring a multi-channel signal, the decoding<br>
apparatus comprising:<br>
a bit unpacking unit which extracts an encoded 3D down-mix signal and spatial<br>
information from an input bitstream;<br>
a down-mix decoder which decodes the encoded 3D down-mix signal;<br>
a 3D rendering unit which removes 3D effects from the decoded 3D down-mix<br>
signal obtained by the decoding performed by the down-mix decoder by<br>
performing a 3D rendering operation on the decoded 3D down-mix signal; and<br>
a multi-channel decoder which generates a multi-channel signal using the spatial<br>
information and a down-mix signal obtained by the removal performed by the<br>
3D rendering unit.<br>
[16]	A decoding apparatus for restoring a multi-channel signal, the decoding<br>
apparatus comprising:<br>
a bit unpacking unit which extracts an encoded 3D down-mix signal and spatial<br><br>
WO 2007/091842	PCT7KR2007/000668<br>
information from an input bitstream;<br>
a down-mix decoder which decodes the encoded 3D down-mix signal;<br>
a multi-channel decoder which generates a multi-channel signal using the spatial<br>
information and a 3D down-mix signal obtained by the decoding performed by<br>
the down-mix decoder; and<br>
a 3D rendering unit which removes 3D effects from the multi-channel signal by<br>
performing a 3D rendering operation on the multi-channel signal.<br>
[17]	An encoding apparatus for encoding a multi-channel signal with a plurality of<br>
channels, the encoding apparatus comprising:<br>
a multi-channel encoder which encodes the multi-channel signal into a down-mix<br>
signal with fewer channels and generates spatial information regarding the<br>
plurality of channels;<br>
a 3D rendering unit which generates a 3D down-mix signal by performing a 3D<br>
rendering operation on the down-mix signal;<br>
a down-mix encoder which encodes the 3D down-mix signal; and<br>
a bit packing unit which generates a bitstream including the encoded 3D down-<br>
mix signal and the spatial information.<br>
[18]	An encoding apparatus for encoding a multi-channel signal with a plurality of<br>
channels, the encoding apparatus comprising:<br>
a 3D rendering unit which performs a 3D rendering operation on the multi-<br>
channel signal;<br>
a multi-channel encoder which encodes a multi-channel signal obtained by the<br>
3D rendering operation into a 3D down-mix signal with fewer channels and<br>
generates spatial information regarding the plurality of channels;<br>
a down-mix encoder which encodes the 3D down-mix signal; and<br>
a bit packing unit which generates a bitstream including the encoded 3D down-<br>
mix signal and the spatial information.<br>
[19]	A computer-readable recording medium having a computer program for<br>
executing the decoding method of any one of claims 1 through 9 or the encoding<br>
method of any one of claims 10 through 14.<br>
[20]	A bitstream comprising:<br>
a data field which includes information regarding a 3D down-mix signal;<br>
a filter information field which includes filter information identifying a filter<br>
used for generating the 3D down-mix signal;<br>
a first header field which includes information indicating whether the filter in-<br>
formation field includes the filter information;<br>
a second header field which includes information indicating whether the filter in-<br>
formation field includes coefficients of the filter or coefficients of an inverse<br><br>
WO 2007/091842	PCT/KR2007/000668<br>
filter of the filter; and<br>
a spatial information field which includes spatial information regarding a<br>
plurality of channels.<br><br>
An encoding method and apparatus and a decoding method and apparatus are provided. The decoding method includes extracting a three-dimensional (3D) down-mix signal and spatial information from an input bitstream, removing 3D effects from the 3D down-mix signal by performing a 3D rendering operation on the 3D down-mix signal, and generating a multi-channel signal using the spatial information and a down-mix signal obtained by the removal. Accordingly, it is possible to efficiently encode multi-channel signals with 3D effects and to adaptively restore and reproduce audio signals with optimum sound quality according to the characteristics of a reproduction environment.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=ELE0PE/SgPJ/fK5cmqP/BQ==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==" target="_blank" style="word-wrap:break-word;">http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=ELE0PE/SgPJ/fK5cmqP/BQ==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==</a></p>
		<br>
		<div class="pull-left">
			<a href="271383-testing-adapter.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="271385-method-for-producing-thiazole-compound.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>271384</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>2897/KOLNP/2008</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>08/2016</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>19-Feb-2016</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>18-Feb-2016</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>16-Jul-2008</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>LG ELECTRONICS INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>20, YOIDO-DONG, YOUNGDUNGPO-GU, SEOUL</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>JUNG, YANG WON</td>
											<td># 2-803, YEOKSAM HANSIN APT., DOGOK-DONG, GANGNAM-KU, SEOUL 135-720</td>
										</tr>
										<tr>
											<td>2</td>
											<td>OH, HYUN O</td>
											<td>#306-403, HANSIN APT., GANGSUN VILLAGE 3DANJI, JUYEOP1-DONG, IISAN SEO-KU, GOYANG-SI, KYUNGGI-DO, 411-744</td>
										</tr>
										<tr>
											<td>3</td>
											<td>KIM, DONG SOO</td>
											<td>#502, WOOLIM VILLA, 602-265, NAMHYUN-DONG, KWANAK-KU, SEOUL, 151-080</td>
										</tr>
										<tr>
											<td>4</td>
											<td>LIM, JAE HYUN</td>
											<td># 609, PARKVILLE OFFICETEL, 1062-20, NAMHYUN-DONG, KWANAK-KU, SEOUL 151-080</td>
										</tr>
										<tr>
											<td>5</td>
											<td>PANG, HEE SUK</td>
											<td>#101, 4/7, 14-10, YANGJAE-DONG, SEOCHO-KU, SEOUL 137-130</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 19/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/KR2007/000668</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2007-02-07</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/773337</td>
									<td>2006-02-15</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>60/792329</td>
									<td>2006-04-17</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>3</td>
									<td>60/782519</td>
									<td>2006-03-16</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>4</td>
									<td>60/781750</td>
									<td>2006-03-14</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>5</td>
									<td>60/765747</td>
									<td>2006-02-07</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/271384-apparatus-and-method-for-decoding-a-signal by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:45:20 GMT -->
</html>
