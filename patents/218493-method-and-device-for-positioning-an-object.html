<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/218493-method-and-device-for-positioning-an-object by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 12:53:20 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 218493:&quot;METHOD AND DEVICE FOR POSITIONING AN OBJECT&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;METHOD AND DEVICE FOR POSITIONING AN OBJECT&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>The invention relates to image processing and can be used, for example for observing and monitoring moving objects with the aid of TV devices. Said invention makes it possible to position the moving object with the aid of a succession of television pictures of the position thereof. Image retention in the centre of a display filed of a video camera makes it possible to the exclude factors destabilising the accuracy of positioning the object. Analysis of the image parameters makes it possible to classify elements of the object and to form the images thereof.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>METHOD AND DEVICE FOR POSTTIONING AN OBJECT<br>
The Realm of Techniques<br>
The invention relates to video image processing in different applications of computer vision, for example, in tasks of observation and monitoring, and in particular, to methods of object location deteiinination in a sequence of video images. The invention can be used in atomised management systems of street traffic, for observation and documenting of takeoff and landing manoeuvres in airports, to provide the co-ordinated interaction of the device with other devices or production objects in robotics, and at more common approach it can serve as a subsystem for any system of higher interpreting level which is capable of detection, segmentation, and surveillance of stationary and mobile objects as well as automatic determination of their parameters.<br>
Preceding Standard of Technics<br>
The well known methods and devices of object surveillance used as a basis for implementation of elementary Generation I systems are characterised by usage of one of base approaches for image elements classification into object and background elements (object binary image generation) and for positioning an object by means of the binarized image gravity centre coordinates determination (so-called centroid methods and devices) or the object location determination by searching the similarity/dissimilarity measure extremum (differential correlation function, in particular) of the current and reference images (correlation - extreme methods and devices).<br>
Such systems allow automatic positioning of the object with adequate accuracy and t© ensure steady confinement of object image near the centre of field of view only when it is highly contrast in relation to background and/or when the object image does not change a lot during the surveillance time, at that the effect of other image destabilizing factors is null or minimum. These constraints provide a rather narrow range of object surveillance conditions where given systems are applicable.<br>
For many actual applications the object visible images are either low contrast<br>
in relation to background or (at average and high contrast) observed on the contrast tenaIn! background, at that, the object or surveillance system movement can cause considerable changes iri the object image during the surveillance time. The situation is often complicated by the fact that the object can be partially or completely shielded for a time in the scene image by objects of the short-range background, and the video camera is affected by destabilising factors which cause uncontrolled horizontal and vertical displacements and field of view rotation.<br>
There exists method of positioning an object (see inventor's certificate 1562980 USSR„<br>
Int. C1.5 HO4 N° 7/18, The Object Coordinates Determining Device / B.A.Alpatov, P.A.Bakut, I.E.Vornovitski, A.A.Selyaev, A.I.Stepashkin, S.Y.Khludov (USSR), - Jfe 4373608/24-09; Applic. 01.02.88; Publ. 07.05.90, Bullet. Ml 7), which includes the following set of operations.<br>
Simultaneous current frame record and former frame processing.<br>
Forming of the object template h (i, j) with founat MxM from the first received frame by cut of the image central part.<br>
Processing of the previously recorded frame beginning from the third received frame is performed in the following way.<br>
Calculation of the weight differential correlation function F(v,µ)<br>
(Equation Removed) <br>
where g(i+v, j+µ) are elements of the current processed image with dimensions NxN pixels, h(i,j) are the reference image elements with dimensions MxM pixels, Z(i,j) are the weight function elements with dimensions MxM pixels, at that Z(i,j) = 1 if<br>
the image pixel belongs to the object, Z(i,j) = 0 - otherwise,<br>
v,µ are the image shift parameters which vary from 0 to ±(N-M+1).<br>
To abridge the number of computational operations, the differential correlation function<br>
F(v,µ) in the point with coordinates v,u. is calculated till the current value of this function<br>
remains lower than certain threshold function R(k) which depends on the number k of elementary<br>
differences calculated by this moment.<br>
The object coordinates v*, µ* on the image are determined on the basis of minimization<br>
of the differential correlation function F (v,µ).<br>
The current, low-pass (LP) filtering (smoothing) of the image is performed:<br>
. Vn(i,j) = ( 1 - ß ) gn(i+vn*, j+µn*) + ß Vn.,(i,j).	(2 )<br>
where Vn(i,j) is the smoothed image value in n-frame,<br>
p is the smoothing parameter, 0 
Reference image renewal conditions are determined by calculating the duration of the smoothing filter transient process L (2)<br>
(Equation Removed) <br>
and defining the object shift during the last L frames. If the object shift is more than one increment, the template is re-recorded:<br>
h(i,j) = Vn(i,j).	(4)<br>
If the object shift during the last L frames has not exceeded one increment, the template is re-recorded on condition that F(v*,µ*) &gt; Threshold, otherwise the template is saved.<br>
A set of Q points belonging to the object is selected on the smoothed image from the frame L+l. The weight function Z (i,j) in the first L+l frames is accepted equal to 1.<br>
A set of Q pixels belonging to the object is selected by threshold processing of the smoothed image.<br>
A set of Q pixels belonging to the object is used to form a new weight function Z(i,j) of the reference image which is replaced with the template replacement.<br>
The feature of the method which coincides with the feature of the applied engineeriag solution is calculation of image signals weight dissimilarity measure in the current analysis window and object reference image as a differential correlation function F(v,µ) according to formula (1).<br>
Reasons which prevent the achievement of the required technical result when using the given method are in determining the object image coordinates v*, µ* on the basis of minimizing the differential correlation function F(v,µ) with usage in the first L+l frames of the weight function Z(i,j) equal to 1. This results in impossibility of accurate object positioning when the object contrast<br>
is lower than the contrast of its surrounding background elements.<br>
Reasons of the method non-operability in the described situations are illustrated in Fig. la, Ib, Ic, Id.<br>
Fig. 1 represents template forming from the first received frame (Fig la) by cutting its central part (Fig Ib). At this stage the template is not yet protected from interference of extraneous background elements. Typical for this situation intensity disliibution of the formed reference image elements and the type of the differential correlation function F(v,µ) are illustrated in Fig. Ic.<br>
Fig. 1 d represents that interference of background elements with the template results in occurrence of the correlation function minimum, which does not correspond to the objeqt location. At a contrast parti-coloured background the false minimum value appears to be lower than minimum that corresponds to the object location. Thus, the object coordinates estimate according to the given method results in erroneous coordinates determination and failure of tracking.<br>
The set of operations of template renewal and calculation of the current and reference images differential correlation function can also be refered to other reasons which prevent the achievement of the required technical result, which cause failure of tracking at overlapping of even small object areas (10-20%) by high-contrast background elements.<br>
Reasons of the method non-operability in such situations are illustrated in Fig. le. If.<br>
In the given conditions we can proceed from the fact that during the initial tracking stage the background, along which the object was moving, was such that the object reference image was formed correctly, i.e. the template did not contain the background elements (Fig. le).<br>
When the object moves behind contrast objects of a short-range background, as shown in Fig. If, the minimum value of the differential correlation function increases heavily and according to the method operating sequence it causes template re-recording. As a result the contrast background element interferes with the template and causes erroneous object positioning and failure of tracking, as is described above.<br>
Besides, at unprognosticated field of view displacements during the process of template re-recording, the background elements can gel into the object template videodata window<br>
which also cause failure of tracking, as it is described above.<br>
To provide the operability of the object positioning and surveillance systems in such an extended range of conditions the modern methods and devices (conventionally, Generation II methods and devices) are developed in the direction of adaptive and simultaneous use of several intercomplementary channels of image elements classification, pattern generation, and of the positioning of an object, consideration and compensation of destabilising factors.<br>
The known methods and devices of object positioning after the time sequence of images do not feature the indispensable versatility for operation in a wide range of application conditions, namely, at high and low contrast of object images and the enclosing terrain background, at variations in object sizes and visible pattern as a result of its manoeuvres and motion across the terrain, at change or uneven object and terrain intensity, at shielding of the object by other objects or terrain areas, at displacement of the surveillance system carrier and the object positioning system, at absence or partial (poor) video camera field of view stabilisation and some others.<br>
There exists a well-known method for detection and determination of object coordinates in the image (see inventor's certificate M, 1737755 USSR, Int. C1.5 HO4 N 7/18, G06K 9/36, A Device for Detection and Detennination of Object Coordinates in the Image /B.A.Alpatov, E.T.Libiyainen, S.Y.Khludov (USSR) Ryazan Radio Engineering Institute (USSR), -N° 4819727/09; Applic. 27.04.90; Publ. 30.05.92, Bullet. No 20), which implementation requires fulfilment of the following set of operations.<br>
At the first step in the first frame only video image record in memory takes place. During record of the second frame the initial data files are foimed: Go is a background reference image, GO = LO is a current image. (The current image is taken as a background reference image. At that, it is assumed that the object in the image LO is missing).<br>
BO is an array of initial elements of the classification function object/background,<br>
BO: bo,j = -32.<br>
Record of the current frame in memory starts at the third received frame.<br>
At the second step (objects detection) elements bn,j of the classification function are defined for each element lmj of the observed image<br>
(Equation Removed) <br>
where lny - are elements of the current processed image Ln,<br>
gnij - are elements of the prognosticated background image,<br>
(Equation Removed) <br>
Al is a prognosticated minimum distinction of object intensity from background intensity. A decision of an image element with coordinates i, j belonging to the object is made if the obtained value is bnjj &gt; 0.<br>
Coordinates λnx, λny of the binary image centroid are calculated:<br>
(Equation Removed) <br>
where Sn - is the number of image elements which form the object image Hn,<br>
i - is an image element lnij coordinate, X-direction, classified as an object (bni,&gt;0), j - is an image element lnu coordinate, Y-direction, classified as an object (bn,j&gt;0). Limitation of elements values bmj and their record to b(n.i)(j is performed. b(n-i)ij - 32, if bn,j &gt; 32, b(n-i)ij = bnu, if -32
If the number Sn of image elements classified as an object has exceeded the threshold value So, a decision of object detection in a frame is made and the indicated value of the detection counter is increased by 1.<br>
At the third step, if a decision of object detection in a frame (S &gt; So) is made, estimates of object coordinates λ nx and speed V ,u are calculated according to the formula, at X-direction:<br>
(Formula Removed) <br>
where  λ pr.nx - is a prognosticated object location (prior estimate) for the time of n-frame reception, which is calculated according to formula (9),<br>
λ nx - is a posterior estimate of object location; V nx - is a prior estimate of object speed; V nx - is a posterior estimate of object speed; at Y - direction:<br>
(Formula Removed) <br>
	(14)<br>
where  λpmy - is a prognosticated object location for the time of n-frame reception, which is calculated according to foimuia (13),<br>
λ ny - is a posterior estimate of object location; V n&gt; - is a posterior estimate of object speed;<br>
(Formula Removed) <br>
If the object is not detected	in the current frame, its coordinates and speed are<br>
prognosticated according to fomiulas:<br>
(Formula Removed) <br>
At the forth step, the background image is smoothed and for all image pixels classified as background (bnu 
(Formula Removed) <br>
where   g(n+l)IJ = gmj, 0<k3></k3>
At the fifth step the object reference image is smoothed, wherefore for all image pixels classified as an object (bnij &gt; 0), the following is performed:<br>
(Formula Removed) <br>
where   h (n+1 &gt;,j = h nij, 0<k4></k4>
In case, when an image pixel in the former frame belonged to the object and in the current frame is classified as background, smoothing of the object reference image is performed according to the formula:<br>
(Formula Removed) <br>
to zeroize memory elements which corresponded to the object in previous frames.<br>
At the sixth step, if the detection counter value equals to 16, a decision to fasten up the trajectory and continue calculations is made, otherwise there is change to operations of the second step, formula (5) and further.<br>
The prognosticated area S(n+i)v,µ of the image L(n+1) which centre coincides with the object centre is formed.<br>
(Formula Removed) <br>
where   S (n+i)V,n - are elements of the prognosticated image area L(n+1) with dimensions MxM, hniljl - is a smoothed object image according to (24), gm2j2-, - is a smoothed background image according to (23), Z(n+1),v,µ - is a binary object mask generated according to the rule:<br>
(Formula Removed) <br>
where  λ pr.(n+1)x, λ pr.(n+1)y are calculated according to (17) and (21).<br>
At the seventh step, a derivative of the prognosticated image S n,v,µ is formed in the former frame after the parameter λ,nx is calculated:<br>
(Formula Removed) <br>
where   S n,v,µ - is computed according to (26),<br>
(Formula Removed) <br>
At the eighth step, a non-linear estimation of the object coordinate and speed is performed at X-direction:<br>
(Formula Removed) <br>
where  λ pr.nx - is a prognosticated object location (prior estimate) for the time of n-frame reception, which is calculated according to formula (9),<br>
λ nx - is a posterior estimate of object location; V nx - is a prior estimate of object speed; V nx - is a posterior estimate of object speed.<br>
At the ninth step, a derivative of the prognosticated image S n&gt;v,µ is fonned in the former frame after the parameter λny is calculated:<br>
(Equation Removed) <br>
where   S n,v,M - is calculated by (26),<br>
(Equation Removed) <br>
At the tenth step, a non-linear estimation of the object coordinate and speed is performed at Y-direction:<br>
(Equation Removed) <br>
where λ pr.ny - is a prognosticated object location (prior estimate) for the time of n-frame reception, which is calculated according to formula (13),<br>
λ ny - is a posterior estimate of object location, Y-direction, V ny - is a prior estimate of object speed, Y-direction, V ny - is a posterior estimate of object speed, Y-direction.<br>
At the eleventh step, the area (strobe) ψn pixels with dimensions MxM of the current image Ln are classified by the formula check:<br>
(Formula Removed) <br>
where  - are elements of the current processed image Ln,<br>
gnjj - are elements of the prognosticated background image,<br>
hnij - is a smoothed object reference image,<br>
D = 1, the strobe centre ψn has the coordinates λ nx, λ ny.<br>
At the twelfth step, coordinates of the binary image centriod, obtained as a result of (27),<br>
are calculated, and estimates λ nx, λ ny calculated at the eighth and tenth steps are substituted with these coordinates.<br>
The prognosticated prior estimates of object location and speed for (n+l)-frame are foimed after the updated values λ nx, λ ny:<br>
(Equation Removed) <br>
The background image is smoothed for all strobe pixels ψn with dimensions MxM of the current image Ln, classified as background (bnij 
(Equation Removed) <br>
The object reference image is smoothed for all strobe pixels ψn with dimensions MxM of the current image Ln classified as an object (bnij &gt; 0):<br>
(Equation Removed) <br>
where  h(n+i)ij= hny, 0<k4></k4>
In case when image pixel in the former frame belonged to the object and in the current frame is classified as background, smoothing is performed according to the formula:<br>
(Equation Removed) <br>
where 13 = i - (A nx - A (n-i)x),<br>
to zeroize memory elements which corresponded to the object in previous frames.<br>
Value Sn of the object area is checked after classification results at the eleventh step according to formula (27).<br>
If Sn &gt; So, the operations of the sixth step and further are perfoimed.<br>
If Sn 
Feature of the method which coincides with feature of the applied engineering solution is smoothing of the background image according to formula (28).<br>
Reasons which interfere with the achievement of the required technical result when using the described method are the following.<br>
forming of the current object coordinates using processing results of the former frame introduces an extra delay up to 20 or 40 ms in the control loop of the field of view position. It depends on whether processing of video images is performed by interlaced fields or by frames as it restiicts the possibilities of coordinates determination and keeping of dynamic objects in the centre of the field of view.<br>
Defined by the method at the second step classification of image elements into object and background pixels according to formula (5) results in the presence of noise in false determination of object coordinates in a frame or analysis window which dimensions significantly exceed object dimensions (it is characteristic of small-size objects).<br>
Reasons of these errors are the following. Centroid of the binary image, formed by noise bursts, does not depend on coordinates of the captured<br>
object and is located near the frame centre. The method does not allow for elimination of pixels falsely classified through noise bursts as an object at the second step. Therefore, if an object is not centred in a fiame (for example, it is shifted by 2-3 own sizes) and the number of falsely classified image pixels exceeds or equals to the number of object pixels, obtained by the said method estimate of the object coordinate does not lie within the object contour range. Consequently, it can cause false prognostication of position of the object image analysis window in the next frame and impossibility of further determination of its coordinates.<br>
The method does not allow for displacement of the field of view at the time between reception of images of the adjacent frames (interlaced fields), and it does not allow error-free forming of the reference background, averaged object image, prognosticated object and background image at the system operation in a mobile field of view.<br>
It happens so, because a sequence of frames but not one frame is used to form the reference background and reference object, at that, at forming of the said templates, images of different frames must shift (and in more general case they must rotate) in relation to one another so that the identical background pixels (for the background template) or object pixels (for the object template) overlap coordinate by coordinate.<br>
To define the required image shifts (and rotations, if necessary) all field of view displacements must be taken into account.<br>
The method uses detection of all statistically significant deviations of brightness of the current image elements from priori expected brightness of background and object elements irrespective of reasons of its occurrence. It results in non-operability of the method in the following conditions:<br>
-	motion of the observation and object coordinates determination system carrier owing<br>
to extraction of moving elements of close background in the image (at existence of a moving<br>
object in the analysis field it results in false object positioning, and at existence of a stationary<br>
object in the remote background it is not detected);<br>
-	motion of the system carrier owing to rotations of the field of view, since wrong<br>
forming of the reference averaged background takes place, as was mentioned above;<br>
existence of uncontrolled progressive displacement of the field of view (uncompensated stabiliration errors and errors of the field of view drives) and contrast particoloured background.<br>
Under the said conditions the probability to falsely classify background elements as object elements is significantly increased and it causes sharp growth of errors in determination of object coordinates and impossibility to keep it in the centre of the field of view.<br>
There exists a method of detection and tracking of moving objects based on analysis of the sequence of images (see patent 2 680 931 France, Int. C1.5 HO4 No 3/00, Precede de detection et de suivi d'objets en mouvement par analyse de sequences d'images/Pineau Patrick et Robert Philippe (FR), THOMSON-CSF (FR), HO 91 10628; Appiic. 27.08.91; Publ. 05.03.936, Bullet. .No 09) which consists of detection of moving objects, separation of connected moving objects, prognostication and temporal monitoring of object location and manoeuvre.<br>
Images are classified into three categories which represent background, a moving object and a shadow by matching brightness of the reference image and current image, at that the reference image is periodically updated and, according to the method, image pixels are classified into five categories which represent background contours, objects contours in motion, background contours occluded by a moving object, background contours located in a shadow region, and pixels which do not belong to any contour. Each image pixel has a corresponding label with double inforuiation which contains information on pixels classification by matching brightness of the reference image and current image and contour infoanation, taking into account pixels classification by matching contours of the reference image and current image, at tfcaf, image pixels are classified into seven classes, each corresponding to one of seven types of labels with double infounation on area (segment) and contour. For this purpose the relaxation mefihod with simulation of double information Label Markov Field is used to optimiye and classify image pixels into seven classes at the step of objects detection.<br>
Once moving objects have been detected, separation of connected moving objects; is performed, including relaxation with simulation of Markov Movement Field, temporal prognostication of masks to optimize classification of pixels into seven classes at the step of objects detection and temporal prognostication of movement to optimize separation of connected objects.<br>
This method allows to detect and track moving objects in conditions of the fixed field of view.<br>
Features of the method which coincide with features of the applied engineering solution are the following:<br>
-	detection of moving objects by matching brightness of the background reference<br>
image and current image,<br>
-	periodic update of the background reference image.<br>
Reasons which interfere with the achievement of the required technical result consist in image processing to select the object image from the background by operations of detection of the image moving segments, that results in non-operability of the method when observing an object motionless in relation to the remote background, and at motion of the tracking system carrier that leads to occurrence of moving close background, and at a contrast background in presence of uncontioiled displacements of the field of view (uncompensated stabilization errors), as in this case movifig segments of the background are selected as an object and it does not allow to detect the true object.<br>
There exists a method of recursive evaluation of displacement for objects tracking based on analysis of sequences of digital images (see patent 0527791 EP, Int. C1.5 G06F 15/70, VERFAHREN ZUR ZEIl REKURSIVEN BEWEGUNGS-SCHATZUNG UNO ZUR VERVOLGUNG BEWEGTER OBJEKTE BEI DER ANALYSE ZE1ILICHER FOLGEN DIGITALER BILDER/KARMANN, Klaus-Peter, Dr., Strassbergerstrasse, 8, W-8000 Munchen 40 (DE), SIEMENS AKTlfcNGESELLSCHAFT, Wittelsbacherplatz 2, D-80333 Munchen (DE,) No EP 90108666; Applic. 08.05.90; Publ. 20.07.94, Bullet. No 29; international priority PCT/EP91/00769; international publication WO 91/17513 14.11.91, Bullet. No 26).<br>
The method comprises the following set of operations:<br>
-	initial values of predictable variables of the object state (X) are assigned at the start,<br>
-	for each pattern l(k) of sequence of digital images in time 1(1), 1(2), .... l(k):<br>
a) displacement of segments in the pattern l(k) is detected and these displacing segments are defined by the binary segmentation function (S), which is described as<br>
1, if pixel p at the time k belongs to the moving segment j,<br>
SG,k,p) = 
0, otherwise.<br>
b)	displacement parameters (Z) for these moving segments are measured by means of<br>
maximization of the similarity function (C) which is formed using a set of displacement<br>
parameters (Z), transfonnations (T) from segmentation functions (S) and digital images (I)<br>
according to the general fonnula:<br>
C(j,k,T) = C (I(k,p), SG,k,p), I(k+l,T(p)),X S(n,k+l,T(p)))	(31)<br>
n<br>
c)	corrected state variables (Y) are defined from a set of prognosticated state variables (X)<br>
with the help of the measured movement parameters (Z);<br>
d)	prognosticated state variables (X) are defined for the succeeding moment from<br>
corrected state variables (Y) with the help of recursive filter,<br>
where k - is a time index,<br>
p - are pixels coordinates,<br>
j - is a segment index,<br>
S(j,k,p) - is a setynental function of segment] displacement at the time k in pixel p,<br>
s(n,k+l,T(p)) - is a segmental function value of segment n displacement at the time k+1<br>
in pixel T(p),<br>
l(k,p) - is image brightness at the time k in pixel p.<br>
l(k+l,T(p)) - is image brightness at the time k+1 in pixel T(p),<br>
T(p) - are coordinates of pixel p as a result of T transformation.<br>
The method assiimes that the transformations family (T) includes either only affine transformations of a plane pattern or only shifts (translational displacements).<br>
The used state variables (X, Y) describe instantaneous positions, instantaneous speeds and acceleration of displacing segments.<br>
Similarity function (C) is formed according to the formula:<br>
(Formula Removed) <br>
when the transformations set (1) comprises affine  transfonnations of a plane pattern,  or according to the formula:<br>
(Formula Removed) <br>
when the transfoiinations set (T) comprises only shifts, where v is a shift vector in the pattern plane.<br>
Features of the method which coincide with features of the applied engineering solution are:<br>
-	detection of displacing segments in the current image l(k) and their determination by<br>
the binary function (S) which is described as<br>
S(j,k,p) = 1,    if pixel p at the time k belongs to segment j,<br>
S(j,k,p) = 0,    otherwise.<br>
Reasons which interfere with the achievement of the required technical result consist in image processing to select an object image from the background by operations of detection of image moving segments. It causes non-operability of the method when observing an object motionless in relation to the remote background, and at motion of the tracking system carrier that leads to occurrence of a moving close background, as well as at a contrast background in presence of unprognosticated displacements of the field of view (uncompensated stabilization errors), as in this case moving background segments are selected as an object and it does not allow to detect the true object.<br>
There exists a method of object tracking (see The Television Servo-Mechanism with a Bayesian Discriminator /DSc. Bakut P.A., CSc. Labunets V.G. // "Foreign Radioelectronics". -1987. - No 10. - pp. 81-93, - Rus.) which contains the following set of operations:<br>
-	input of the image videosignal,<br>
-	image videosignal analog-to-digital transformation and its record in digital form in<br>
buffer memory,<br>
-	histogram classification of image pixels into object and background pixels on the basis<br>
of bayesian decision rule (object binary image generation).<br>
-	deletion of object image elements,<br>
-	determination of the object areas configuration vector,<br>
-	determination of horizontal and vertical projections of the current binary<br>
image in the analysis window as well as projections of the object reference binary image,<br>
-	determination of disjunctive convolution of the current and reference projections,<br>
-	determination of object coordinates (and speed) using the maximum position of the<br>
found disjunctive convolution values of the current and reference projections,<br>
-	object selection from the scene image by deletion of those object area image elements<br>
which do not belong to the object reference mask in position of maximum of projection<br>
disjunctive convolution,<br>
-	prediction of object location and object spatial strobe in the next frame,<br>
-	object redetection by operations of image pixels histogram classification and operations<br>
of the binary object image centre determination,<br>
-	forming of control actions.<br>
When using this method, the adaptive and, in general case, multi-threshold statistically optimal rule of image pixels classification into object and background pixels is implemented for forming of the the binary object image in the analysis window. Histogram classification of image elements in the analysis window is performed by the following set of operations.<br>
The analysis window with the centre in the point of predictable object position is set in the current image frame. The object spatial strobe (OSS) and background frame are formed inside the analysis window, as shown in Fig. 2.<br>
The histograms Goss(L) and GBF(L) of image intensity L distribution in the object spatial strobe and inside the background frame are calculated in windows of OSS arid background frame respectively.<br>
To decrease the fluctuations of histograms Goss(L) and GBF(L) point estimates, they are averaged with the help of the first order digital recursive filters:<br>
G k oss( L) = b oss * G k., oss( L) + (1 -b oss)* Gk Oss( U	( 34 )<br>
G k BF ( L) = b BF * G k., BF ( L) + (1-b BF)* Gk BF ( L).	( 35 )<br>
where boss- ben are weight factors which control the averaging degree of OSS and background frame histograms.<br>
Using the average histograms G k oss(L) and G k BF(L) the image pixels in the<br>
analysis window are classified according to the following rule.<br>
The image pixel with coordinates (i, j) is considered to belong to the object, that is binary image f(i, j)=l is formed if<br>
GkOSs(L(i,j))&gt;a* A* GkBF(L(i,j))	(36)<br>
where  a is a prior probability of background elements availability in OSS,<br>
A is a cost parameter of wrong classification.<br>
If the formula (36) is not satisfied, the pixel is considered to belong to the background and the binary image f(i, j)=0 is founed.<br>
The pixel classification in the current frame is performed after statistical estimates of the previous frames.<br>
Coordinates of the object binary image centre are determined by calculating horizontal Th(i) and vertical Tv(j) projections of the current binary image f(i,j) as well as horizontal Tmh(i) and vertical Tmv(j) projections of the reference binary object image fmi(i,j) (object mask) generated on the previous frames, and the disjunctive convolutions Sh (k), Sv (1) of corresponding projections of the cm tent and reference binary images are calculated according to formulas:<br>
(Formula  Removed) <br>
Then the maxima positions k0 and l0 of disjunctive convolutions Sh(k) and Sv(l) are defined and the results are taken as object coordinates in the current analysis window.<br>
The object is being selected from the scene image by means of discarding of those image elements of object area which do not enter the object reference mask in position (ko and IQ).<br>
The object image position in the next frame is being prognosticated.<br>
Features of the method which coincide with features of the applied engineering solution are:<br>
-	reception and storage of signals of the current video image field,<br>
-	histogram classification of image pixels into object and background pixels on the basis<br>
of bayesian decision rule (histogram classifier binary image forming),<br>
-	forming of horizontal and vertical projections of the histogram classifier current binary<br>
imatie).<br>
-	object image redetection at its temporary loss,<br>
-	development of signals which control the video camera field of view position.<br>
Reasons which interfere with the achievement of the required technical result when using<br>
this method are the following.<br>
To classify image elements, the method uses differences of averaged histograms of intensity distribution in OOS and inside the background frame. It allows to take into account the infomiation only about relative rate of occurrence of different intensities in the object area and in the surrounding teuain background and does not take into account its spatial distribution ("spatial design"). On account of this reason the method is not used when the histograms of object and background features distribution (their images intensity distribution) at the start of tracking and coordinates determination process appear close. It often takes place when tracking a particoloured object on a fine-dispersed parti-coloured background.<br>
To decrease the estimates fluctuations of the background intensity distribution histogram, it is averaged with the help of the first order digital recursive filter. When the object moves against the heterogeneous background, the averaged histogram does not reflect true background intensity distribution around the object at its passing the background heterogeneity boundary. In its turn, it leads to growth of the number of background image elements falsely classified as an object and then to "creeping" of the object spatial strobe on the background area.<br>
The most close to the applied one is the object positioning method based on complex usage of results of video images sequence processing (see. Optoelectronic Guidance System THASSID / Kolmogorov G.S., Kostromitina E.B., Luchina I.I., Maltsev A.P. // «Foreign Radioelectronics». -1987. No 10. -pp. 57-68. -Rus.) which contains the following set of operations:<br>
-	image videosignal input (reception of the current image field from the video camera),<br>
-	adaptive two-level image videosignal quantization (forming of the object binary<br>
image) with posterior tracking of the object binary image centre inside the adaptive window<br>
(object centre tracking system in the adaptive window - CTSAW),<br>
adaptive four-level image videosignal quantization with posterior:<br>
1)	selection of terrain references,<br>
2)	computation of correlation functions by method of absolute differences for each<br>
reference,<br>
3)	detemiination of references position after minimum of the correlation function,<br>
4)	computation of the correlation function by method of absolute differences for an<br>
object,<br>
5)	determination of object position after minimum of the correlation function (correlation<br>
tracking system - CTS),<br>
6)	re-recording of reference marks and object templates,<br>
7)	stabilization of the dynamic scene image using the results of correlation tracking over<br>
reference marks,<br>
8)	selection of a moving object (forming of the moving object binary image from tiie<br>
scene image),<br>
9)	tracking over the moving object binary image centre (moving object tracking system *<br>
MOTS),<br>
10)	electronic extension of the field of view (scaling of initial images),<br>
-	tracking quality estimate for each of 3 tracking operations groups (object binary image<br>
centre tracking inside the adaptive window, moving object binary image centre tracking,<br>
correlation tracking),<br>
-	selection of one tracking operations group out of 3 (object binary image cenfere<br>
tracking inside the adaptive window, moving object binary image centre tracking, correlation<br>
tracking) to estimate the object coordinates and speed according to fixed priorities:<br>
1-st priority (high) - object binary image centre tracking inside the adaptive window (operation group CTSAW),<br>
2-nd priority - moving object binary image centre tracking (operation group MOTS), 3-rd priority (low) - object correlation tracking (operation group CTS); and assignment of object coordinates  for the tracking operations group  with  low  priority  from  the  tracking operations group with high priority if the object coordinates formed b\ these groups<br>
differ significantly,<br>
-	determination of object coordinates and speed,<br>
-	prediction of object position and spatial strobe in the next frame,<br>
-	object coordinates extrapolation at vanishing or fast change of its image,<br>
-	object redetection by operations of adaptive two-level image videosignal quantization<br>
and by operations of the object binary image centre determination,<br>
-	forming of the video camera field of view rotations management signals.<br>
The known object tracking method allows:<br>
-	to track an object at a pretty high object image contrast in relation to the terrain<br>
background (light object on the dark background or dark object on the light background), while<br>
here the method ensures accurate image pixels classification into object and background pixels<br>
(see Fig. 3);<br>
-	to track an object at its partial screening by contrast barriers in relation to the object<br>
(screening of a light object on the dark background by dark objects or screening of a dark object<br>
on the light background by light objects);<br>
-	to track an object at wide-range change of image scale in the field of view, as well as<br>
at comparatively smooth (low-pass) translational and rotational field of view displacements;<br>
-	to redetect a contrast object at its temporary vanishing by operations of adaptive two-<br>
level image videosignal quantization and by operations of the object binary image centre<br>
deteiinination,<br>
-	to track a fast moving object at lack or lowering of its global contrast in relation to the<br>
background and at presence of local contrast areas in the object image (spatial intensity<br>
"design").<br>
Features of the method which coincide with features of the applied engineering solution are:<br>
—  reception and storage of signals of the current video image field, forming of the image current analysis window.<br>
-	selection of motionless tenain references in the image,<br>
-	determination of references shifts of the current display frame in relation to the<br>
reference marks position in the previous image frame by calculating the correlation functions of<br>
reference marks images,<br>
-	determination of image shift and rotation parameters at the time between reception of<br>
image fields using the defined reference marks shifts,<br>
-	video  image  stabili/ation  (transfomiation  of image  elements  coordinates  with<br>
compensation for image shift and rotation),<br>
-	scaling and storage of analysis window of the current frame image,<br>
-	subtraction of previously stored frame image from the current frame image and<br>
forming of binary image of the moving-objects indicator,<br>
-	storage of the object reference image,<br>
-	detemiination of dissimilarity measure of the object current and reference image as a<br>
differential correlation function,<br>
-	usage of extrapolated object coordinates for forming of video camera field of view<br>
displacement management signals at object image disappearance or fast change,<br>
-	redetection of the lost object using the stored image.<br>
Reasons which interfere with the achievement of the required technical result are the following:<br>
1. The operations of adaptive two-level image videosignal quantization which provide one-threshold image binariztion, accurately classify image pixels into object and background pixels only in a narrow range of requirements: a light object on the dark background or a dark object on the light background.<br>
Hereupon, in other conditions, at a considerable number of falsely classified image pixels (see Fig. 4) OSS displaces gradually on the background object. The operations of videosignal quantization threshold adaptation cause change of the threshold that provides better selection (binarization) of the given background object. It leads to further OSS displacement on the background object, that is to failure of tracking. At that, the described in the method operations group of adaptive two-level image videosignal quantization and object binary image<br>
centre tracking inside the adaptive window has no other means to distinguish between the background and object image elements but the binary image analysis in OSS. In a number of situations it does not allow to work out a criterion of uncertain tracking and, thereby, transmit the object tracking to other operations groups within the comBINed system. This reason considerably increases the probability of OSS "crawl" on the background elements when the object passes the background areas with the intensity close to the object image intensity, especially when the object moves behind the shielding background object owing to contraction of the object visible part area.<br>
2.	The error estimate of object speed owing to retardation of the object visible part centre<br>
from its true centre when the object moves behind the shielding barrier results in forming of false<br>
predictable object coordinates, and consequently, erroneous management signals which control<br>
video camera field of view position and erroneous position and dimensions of images analysis<br>
window. These facts reduce the probability of object redetection after it is no more overlapped by<br>
the shielding background.<br>
3.	The functioning of operations  in  scene  image  stabilization  using the  results  of<br>
correlation tracking of three reference marks becomes impossible at shielding or abrupt change<br>
of reference marks pattern in the process of tracking, for example, by smoke emissions.<br>
Consequently, the probability of keeping object tracking reduces heavily.<br>
4.	The method of image frame stabilization after the reference marks position appears to<br>
be ineffective if there exist broadband random shifts, for example, field of view jitter and<br>
forming of video (or thennal) image frame from several fields. It is stipulated by the fact that the<br>
method implicitly assumes stability of the field of view spatial position at the time of image<br>
frame forming. This assumption is violated at field of view jitter, and consequently, the<br>
interlocation of reference marks in different frame fields (or in different frames), used in the<br>
method to define the parameters of affine image transformations, differs from the true location.<br>
5.	The operations of object position correlation search, described in the method, do not<br>
ensure true object positioning when contrast background elements hit in the area of object<br>
reference image. It is also one of reasons which cause failure of tracking.<br>
6.	Composition of operations set for moving object selection which provides moving<br>
object binary image forming on the basis of interframe subtraction of the current and earlier<br>
stored (reference) images, spatially stabilized with the help of terrain reference marks. Forming<br>
of reference image from the current one (with N frames delay N=l,2...) in the prototype method<br>
in existence of noise or fluctuations of the terrain background image results in false classification<br>
of a great number of scene image elements as belonging to a moving object. It does not allow to<br>
track a moving object of a small size when number of its image pixels is comparable with the<br>
number of falsely binarized terrain background elements.<br>
More over, on account of the third and fourth reasons this operations group is ineffective in situations when the precise image stabilization is not provided with the help of motionless reference marks.<br>
7.	Data of only one tracking operations group is selected at each point of time during the<br>
tracking interval and object coordinates are transmitted from the group with higher priority to the<br>
group with lower priority. Selection of the tracking operations group in the given method is<br>
defined by priori assigned system of their priorities and formed by these groups features of<br>
tracking accuracy. But each separate feature of tracking accuracy, however, is not reliable enough<br>
in complicated viewing conditions, as in this case erroneous object coordinates are enforced to<br>
those tracking operations groups which are able to loan more accurate coordinates. The situation<br>
of object movement across the terrain background with some areas which intensity is close to the<br>
object intensity can serve as an example. In this case the OSS "crawls" on the terrain background<br>
on analogy with the description of the first reason. Moreover, as this process runs slowly and the<br>
binary pattern density remains high because of threshold adaptation, the feature of tracking<br>
reliability CTSAW signals about reliant tracking by this operations group. Owing to high priority<br>
of operations group of adaptive two-level image videosignal quantization and tracking of the<br>
object obtained binary image centre inside the adaptive window at implementation of the given<br>
method, the erroneous object coordinates can be enforced, for example, to a correlation tracking<br>
system, which can provide under these conditions<br>
the accurate coordinates determination.<br>
There, exists an object positioning device (see specification to the inventor's certificate Ml562980 USSR, Int. Cl.5 HO4 N7/18, Object Coordinates Determining Device. B.A. Alpatov, P.A. Bakut, I.E. Vornvitski, A.A. Selayev, A.I. Stepashkin, C.Y. Khludov (USSR), -No 4373608/24-09; Applic. 01.02.88; Publ. 07.05.90, Bullet. .No17), which contains memory unit of the smoothed image signal, memory unit of the reference image signal, memory units of the current image signal, addressing unit, memory unit of the weight function, memory unit of threshold functions, signals switching units, calculation unit of the current and referene image signals difference, multiplier, control unit, calculation unit of object coordinates signals and unit of coordinate registers.<br>
Under certain conditions the given device allows to improve the positioning accuracy, since it is not a current object image but a smoothed one which is used as a reference image in which signal from the object is filtered from distorting additive noise. Usage of the reference image weight function allows to improve the object positioning accuracy on a composite background, since reference image elements which do not belong to the searched object will not be used when calculating the differential correlation function. The smoothed object image formed by a great number of frames is selected in the device as a template. In this case the values of object image sampling errors in the current and smoothed image can be considered independent, if the object displacement during the smoothing period is a few increments. Therefore, at such periodic template renewal the error of object coordinates estimate will not be accumulated. In case of slow object displacement the device uses the template renewal algorithm based on matching the differential correlation function with the threshold, which also does not cause fast errors accumulation in the object coordinates estimate. The object displacement is analysed in the device during the period of L frames defined by the time of settling the template averaging filter, and the template renewal algorithm is selected according to the filter size. It allows to improve the object positioning accuracy.<br>
Reasons which interfere with the achievement of the required technical result when using the given device consist in determining the object image coordinates v*, µ*<br>
on the basis of minimization of the differential correlation function F(v,µ) with usage of the weight function Z (i,j) equal to 1 in the first L+l frames, that causes inability of correct object positioning when the object contrast is lower than the contrast of the surrounding background elements.<br>
Reasons of the device non-operability in the described situations are illustrated in Figs, la, Ib, Ic, Id.<br>
Fig. 1 shows template forming from the first received frame by cutting its central part. On this step template is not yet protected from hitting of outside background elements. Typical for this situation intensity distribution of the formed template image elements and type of the differential correlation function F(v,µ) are illustrated in Figs. Ic. Id.<br>
Fig. Id shows that hitting of background elements in the template causes minimum of the correlation function which does not correspond to the object position. At a contrast particoloured background the false minimum appears to be smaller in size than the minimum which corresponds to the object position. Thus, the object coordinates estimate with the usage of the given device results in erroneous coordinates determination and failure of tracking.<br>
The combination of operations in template renewal and calculation of the current and reference images differential correlation function can be referred to reasons which interfere with the achievement of the required technical result and it leads to failures of tracking at overlapping of rather small object areas (10-20%) by high-contrast background elements.<br>
Reasons of the device non-operability in such situations are illustrated in Fig. le.<br>
In conditions under consideration it is possible to proceed from the fact that during the initial tracking stage the background of the object displacement was such, that the object reference image was formed correctly, that is the template did not contain the background elements (Fig. le).<br>
When the object moves behind the contrast objects of a short-range background, as shown in Fig. If, the minimum value of the differential correlation function increases abruptly and it causes template re-recording. As a result, the contrast background element hits the template and then the erroneous object positioning and failure of tracking takes place, similarly to the described above.<br>
Besides, at non-predictable field of view displacements during the template re-recording, background elements hit in the object template videodata window and cause failure of tracking, as is described above.<br>
Features of the device which coincide with features of the applied engineering solution are: complex of the difference calculation unit of the current and reference image signals, the multiplier by the weight function and microprocessor of the object coordinates calculation unit of the device is equivalent to the former 60 (Fig. 11.1) of the unit 11 of the applied engineering solution (Fig. 8.2).<br>
The most close to the applied one is the device (see Optoelectronic Guidance System THASSID / Kolmogorov F.S., Kostromitina E.V., Luchina I.I.. Maltsev A.P. // «Foreign Radioelectronics». -1987. N° 10. -pp. 57-68. -Rus.) which is the complex tracking system including CTSAW, MOTS, CTS, microprocessor with managing controller functions comBINing all tracking systems (CTSAW, MOTS and CTS) in one complex tracking system. Input videodata comes to tracking systems from the video camera installed on the gyro-stabilized platform, through adaptive quantizers (two-level - in CTSAW and four-level - in MOTS and CTS) and (if necessary) through the unit of field of view electronic extension (scaling).<br>
The microprocessor establishes the mode and defines the operating procedure of all tracking systems. It has a role of the head controller. Its functions are: forming of management and intermediate data (coefficients, coordinates, thresholds etc.) which defines the device operation according to the given algorithm, for example, to control the adaptive image quantization; to select the reference marks for execution of correlation tracking; to re-record the reference marks templates; to stabilize images; to control field of view electronic extension; etc. Using the object coordinates coming from one of observation systems according to established priority and depending on the observation area, the microprocessor controls the position of gyro-stabilized platform with the video camera which tracks the observed object.<br>
Each tracking system works independently till the defined by it object coordinates unessentially differ from coordinates defined by the tracking system with higher priority. Otherwise, the object coordinates are defined for the tracking system with<br>
lower priority from the system with higher priority. Thus, the systems work in quasi-independant mode.<br>
The main functions of the known device are:<br>
-	adaptive quantization for better classification of scene image elements into object and<br>
background elements;<br>
-	selection of scene segments as terrain reference marks for correlation tracking;<br>
-	scene image stabilization at the expense of correlation tracking over the three reference<br>
marks;<br>
-	tracking over the object binary image centre inside the adaptive window.<br>
-	tracking over moving object with the help of a subsystem which processes the<br>
stabilized scene image;<br>
-	object correlation tracking;<br>
-	correlation tracking over four object image segments in the finite object observation<br>
section;<br>
-	object redetection by CTSAW system at loss of object tracking.<br>
Reasons which interfere with the achievement of the required technical result are the following.<br>
1. The adaptive two-level image videosignal quantization which provides one-threshold image binarination, accurately classifies image pixels into object and background pixels only in a narrow range of requirements: a light object on the dark background or a dark object on the light background (see Fig. 3).<br>
Hereupon, in other conditions, at a considerable number of falsely classified image pixels (see Fig. 4) OSS displaces gradually on the background object. The adaptation of videosignal quantization threshold causes change of the threshold that provides better selection (binarization) of the given background object. It leads to further OSS displacement on the background object, that is to failure of tracking. At that, means of adaptive two-level image videosignal quantization and object binary image centre tracking inside the adaptive window, used in the device, have no other possibilities to distinguish between the background and object image elements but the binary image analysis in OSS.<br>
In a series of situations it does not allow to work out a criterion of uncertain tracking and, thereby, transmit the object tracking to other operations groups within the comBINed system. This reason considerably increases the probability of OSS "crawl" on the background elements when the object passes the background areas with the intensity close to the object image intensity, especially when the object moves behind the shielding background object owing to contraction of the object visible part area.<br>
2.	The error estimate of object speed owing to retardation of the object visible part centre<br>
from its true centre when the object moves behind the shielding barrier (see Fig. 5) results in<br>
forming of false predictable object coordinates, and consequently, erroneous management signals<br>
which control the video camera field of view position and erroneous position and dimensions of<br>
images analysis window. These facts reduce the probability of object redetection after it is no<br>
more overlapped by the shielding background.<br>
3.	The scene  image  stabilization  using the results of correlation tracking  of three<br>
reference marks becomes impossible at shielding or abrupt change of reference marks pattern in<br>
the process of tracking, for example, by smoke emissions. Consequently, the probability of<br>
keeping object tracking reduces heavily.<br>
4.	The image frame stabilization after the reference marks position appears to be<br>
ineffective if there exist broadband random shifts, for example, field of view jitter and video<br>
image frame forming from several fields. It is stipulated by the fact that the device implicitly<br>
assumes stability of the field of view spatial position at the time of image frame forming. This<br>
assumption is violated at field of view jitter, and consequently, the interlocation of reference<br>
marks in different frame fields (or in different frames), used in the device to define the<br>
parameters of affine image transformations, differs from the true location.<br>
5.	The correlation search of an object position described in the device does not ensure<br>
true object positioning when contrast background elements hit in the area of object reference<br>
image. It is also one of reasons which causes failure of tracking (see. Fig. Ic, Id, le. 1 f).<br>
6.	The implementation of operations in moving object selection which provide forming of<br>
the moving object binary image on the basis of interframe<br>
subtraction of the current and earlier stored (reference) images, spatially stabilized with the help of terrain reference marks. Forming of referene image from the current one (with N frames delay N=l,2...) in the prototype device at the precence of noise or fluctuations of the terrain background image results in false binarization of a great number of scene image elements as belonging to a moving object. It does not allow to track a moving object of a small size when the number of its image pixels is comparable with the number of falsely binarized terrain background elements.<br>
Moreover, on account of the third and fourth reasons the channel MOTS of the device appears to be ineffective in situations when the precise image stabilization is not provided with the help of terrain reference marks.<br>
7. Data selection of only one tracking operations group at each point of time during the tracking interval and object coordinates transmission from the group with higher priority to the group with lower priority, implemented by the device. Selection of the tracking operations group in the given device is defined by priori assigned system of their priorities and formed by these groups features of tracking accuracy. But each separate feature of tracking accuracy, however, is not reliable enough in complicated viewing conditions, as in this case erroneous object coordinates are enforced to those tracking operations groups which are able to form more accurate coordinates. The situation of object movement across the terrain background with some areas which intensity is close to the object intensity can serve as an example. In this case the OSS "crawls" on the terrain background on analogy with the description of the first reason. Moreover, as this process runs slowly and the binary pattern density remains high because of threshold adaptation, the feature of tracking reliability CTSAW signals about reliant tracking by this operations group. Owing to high priority of operations group of adaptive two-level image videosignal quantization and tracking of the object obtained binary image centre inside the adaptive window at implementation of the given method, the erroneous object coordinates can be enforced, for example, to a correlation tracking system, which can provide under these conditions the accurate coordinates determination.<br>
Features of the device common with features of  the   applied   engineering   solution<br>
are the following:<br>
-	microprocessor of the device and processor of the applied engineering solution in the<br>
part of control and inteimediate data forming (coefficients, coordinates, thresholds etc.), which<br>
defines the device operatability according to given algorithm,<br>
-	the reference marks correlators with buffer memory devices of the reference marks<br>
search area and memory devices of the device reference marks templates and the stabilization<br>
parameters unit of the applied engineering solution in the part of terrain reference marks<br>
selection in the image, determination of the current image frame reference marks displacements<br>
in relation to the reference marks position in the previous image frame by calculating the<br>
correlation functions of reference marks images, determination of images shift and rotation<br>
parameters at the time between reception of image fields using the calculated reference marks<br>
displacements.<br>
Disclosure of the Invention<br>
The implied invention is designed for the extension of functional possibilities of signals processing methods and devices for positioning of an object observed in a sequence of video images, as well as for detention of an object image in the centre of the video camera field of view or in the centre of the tracking window in a wide range of objects observation conditions and at presence of destabilizing factors without prior adjustment for utilization conditions, human -operator aided.<br>
The technical result achieved at implementation of the applied invention is the coordinates determination and steady object image detention in the centre of the video camera field of view or in the centre of the tracking window at reception from the video camera of both contrast and low-contrast images of mobile and stationary objects and terrain background when the object moves across the terrain areas which average intensity coincides with the object average intensity, at change of the object visible pattern stipulated by its rotation in motion, illumination change, object observation in conditions of discontinuous optical communication caused by object overlapping by different barriers, such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc . at controlled<br>
(mobile) and uncontrolled (stationary) video camera field of view, at motion of the surveillance system carrier that causes uncontrolled video camera field of view displacements (jitter, that causes the effect of blurred picture, field of view rotations), as well as occurrence of moving terrain background in the image.<br>
The technical result in the part of assurance of coordinates determination and steady object image detention in the centre of the video camera field of view or in the centre of the tracking window at reception of contrast and low-contrast images of mobile and stationary objects and terrain background from the video camera when the object moves across the terrain areas which average intensity is similar to the object average intensity, at change of the object visible pattern stipulated by its rotation in motion, illumination change, object observation in conditions of discontinuous optical communication caused by object overlapping by different barriers such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc., at controlled (mobile) and uncontrolled (stationary) video camera field of view, at motion of the surveillance system carrier that causes low-pass uncontrolled displacements of the video camera field of view (which cause shifts and rotations of all video image field elements as a whole), as well as at probable absence in the surveillance system of stabilization errors sensors of the video camera field of view, is achieved by the fact that in the signal processing method for determination of object coordinates observed in a sequence of video images, which consists in reception and storage of signals of the current video image field; selection of video images signals of 2N terrain references from signals of the current video image field, where N=3,4,5,..., forming of images dissimilarity measure signals of 2N terrain reference marks from video image signals of 2N terrain reference marks of the current video image field and video images signals of the appropriate 2N terrain reference marks in the previous video image field and determination with their help of 2N terrain reference marks image shifts at the time between reception of signals of the current video images fields; determination of shift and rotation parameters of<br>
the current video image field signals at the time between reception of the signals of the current and preceding video images fields, before the reception of the signals of the current video image n-field, where n=3,4, 5,..., the controlled video camera field of view axis displacement is deteunined at the time between the reception of (n-1) and (n-2) video image fields signals, which is determined by the impact of the displacement of the field of view of the tracking system video camera, speed of the controlled tracking system video camera field of view displacement is determined from the controlled tracking system video camera field of view displacement data at the time between the reception of (n-1) and (n-2) video image fields signals, reception and storage of the tracking system video camera field of view uncontrolled displacement and roll signals are performed simultaneously with the reception of the current video image field signals and they are used for forming of the video image predictable coordinates signals of the 2N terrain reference marks in the current video image field; the shift parameters of the current video image field signals obtained on the basis of 2N terrain reference marks images dissimilarity measure signals at the time between reception of signals of the current video images fields are divided into constituents of controlled and uncontrolled video camera field of view axis displacement; image signals of the current n-frame are formed from image signals of the previous frame with allowance for controlled field of view axis displacement at the time between reception of signals of the (n-1) and (n-2) video image fields and signals of the current video image field with bit transformation of signals coordinates of the current video image field which compensate for the video camera field of view uncontrolled displacements and roll; the video image signals in the current analysis window are formed from the current video image frame and scaled using location and dimensions of the current video image field analysis window signals obtained after processing of the previous (n-1) video image frame, at that the initial values of location and dimensions of the video image field analysis window signals are obtained from the tracking system; signals of the scaled video image in the current analysis window are stored; forming of differential<br>
video image signals of the moving-objects indicator by subtraction the scaled video image signals, which were stored and corrected to the current video image scale in the analysis window and shifted on the value of the tracking system video camera field of view axis shift, from the scaled video image signals in the current analysis window; fotming of the primary binary video image signals of the moving-objects indicator from the differential video image signals of the moving-objects indicator; forming of the secondary binary video image signals of the moving-objects indicator from the primary video image signals of the moving-objects indicator exposed to low-pass filtering; the primary binary video image signals of the background change detector and binary video image signals of the histogram classifier in the current analysis window are formed simultaneously with the storage of the scaled video image signals in the current analysis window, with the forming of the primary and secondary binary video images signals of the moving-objects indicator from the scaled video image signals in the current analysis window signals with allowance to the controlled shift of the tracking system video camera field of view axis; the secondary binary video image signals of the background change detector are formed from the primary binary video image signals of the background change detector exposed to low-pass filtering; horizontal and vertical projections of the secondary binary video images signals of the moving-objects indicator and background change detector, and horizontal and vertical projections of binary video image signals of the histogram classifier are formed; confidence factors of the secondary binary video images signals of the background change detector and moving-objects indicator, as well as of binary video image signals of the histogram classifier are defined; the generalized horizontal and vertical signal projections of the object generalized binary video image are formed from horizontal and vertical signal projections of the secondary binary video images of the moving-objects indicator and background change detector, as well as from horizontal and vertical signal projections of the histogram classifier binary video image<br>
on the basis of their joint processing which uses confidence factors of binary video images signals of the histogram classifier, the secondary binary video images signals of the background change detector and moving-objects indicator; horizontal and vertical boundaries as well as the object image sizes at the cut-off levels on all four sides of the assigned area percent of generalized horizontal and vertical signals projections of the object generalized binary video image are defined; the current and average areas of the object generalized binary video image located inside the formed object image boundaries are defined; the current coordinates of the object generalized binary video image are determined using generalized horizontal and vertical projections of the object generalized binary video image; the current traverse speed of the object generalized binary video image in the inertial coordinates system is defined; the confidence factor of the object generalized binary video image current traverse speed in the inertial coordinates system is determined; simultaneously with forming of signals of the primary and secondary binary video images of the moving-objects indicator and background change detector, binary video image of the histogram classifier, as well as generalized horizontal and vertical signal projections of the object generalized binary video image, object image coordinates are defined in relation to the centre of the current analysis window on the basis of forming of images dissimilarity measure signals as a result of non-linear high-pass filtering of scaled image signals in the current analysis window, fulfilled on condition that the average area of the object generalized binary video image exceeds the threshold value; storage of received signals; forming of static object reference video image signals or static and dynamic object reference video images signals; reduction of object static reference video image signals or object static and dynamic reference video image signals to the current scale; forming and storage of the signals of the dissimilarity measure between the video image signals after the non-linear high-pass filtering of the scaled video image signals<br>
in the current analysis window and object static and dynamic reference video images signals in the two-dimensional search area of object video image shifts; definition of minimum values of video images dissimilarity measure signals along the lines and columns of two-dimensional search area of object video image shifts; forming of minimum values sequences of images dissimilarity measure signals along the lines and columns of two-dimensional search area of object video image shifts; determination of the appropriate object video image coordinate in the analysis window according to the type of minimum values sequence of video images dissimilarity measure signals for this coordinate, namely, by means of analytical approximation of the minimum values sequence of video images dissimilarity measure signals by the fourth degree polynomial and determination of object video image coordinate as the approximation polynomial minimum position on condition that the sequence is related to the type of sequences with two boundaries of values fast growth areas of video images dissimilarity measure signals close to the position of its minimum, or by determination of the boundary shift of values fast growth area of video images dissimilarity measure signals in relation to the analysis window center and object coordinate forming in the analysis window as an amount proportional to the obtained boundary shift of values fast growth area of video images dissimilarity measure signals on condition that the sequence is referred to the type of sequences with plane neighbourhood of the minimum position and existence of one values fast growth area of video images dissimilarity measure signals, or by fbuning of the object coordinate equal to the coordinate of the analysis window center on condition that the sequence is referred to the type of sequences with plane neighbourhood of values minimum position of video images dissimilarity measure signals in the whole search area of object image shifts; the current object video image traverse speed in the inertial coordinates system is defined using object video image coordinates<br>
obtained on the basis of forming of images dissimilarity measure signals; the confidence factor of the current object image traverse speed in the inertial coordinates system obtained on the basis of forming of images dissimilarity measure signals is calculated; the complex estimate of the current object image traverse speed in the inertial coordinates system is formed from estimate data of the current object video image traverse speed obtained on the basis of forming of image dissimilarity measure signals, and estimate of the current object traverse speed across the generalized signals projections of the object generalized binary video image with allowance for confidence factors which form the object video image traverse speeds and prior restrictions of the object manoeuvring speed; the complex estimate signals of the object video image traverse speed in the inertial coordinates system are averaged and stored; the object video image coordinates in the video camera field of view are defined by difference integration of the complex estimate of the current object video image traverse speed in the inertial coordinates system and controlled displacement speed of the field of view axis in the inertial coordinates system, at that the initial coordinates of the object video image in the tracking system video camera field of view are received from the tracking system; with initial conditions formed at the start of the object tracking; M windows of background analysis are formed along the analysis window perimeter and signals projections of the histogram classifier binary video images are determined in M windows of background analysis simultaneously with forming of the primary and secondary binary video images signals of the moving-objects indicator and background change detector, histogram classifier binary video image, generalized horizontal and vertical signals projections of the object generalized binary video image; the areas and coordinates of binary video images boundaries of the histogram classifier in M windows of background analysis are defined after the obtained signals projections of the histogram classifier binary video image in M windows of background analysis; video camera field of view axis displacement management signals are formed using object video image coordinates in the video camera field of view<br>
obtained as the result of video image processing in the current analysis window or using the extrapolated coordinates and object video image traverse speed according to results of analysis of the current and averaged area of the object generalized binary video image, current and averaged object video image traverse speed, area and coordinates of binary video images boundaries of the histogram classifier in M windows of background analysis, at that, the extrapolated object video image traverse speed is regenerated analysing the values history of the average complex estimate of the object video image traverse speed; signals of the position and dimensions of the video image analysis window for the following frame are formed using the signals of the object video image coordinates in the tracking system video camera field of view and object video image dimensions, obtained as a result of the previous frame processing, at that the initial values of the signals of the position and dimensions of the object video image and the signal of the start of the tracking are received from the tracking system.<br>
The technical result is also achieved by the fact that before the reception of the current n-field of the video image, where n=3, 4, 5,..., the controlled displacement dx[n] and dy[n] of the video camera field of view axis at the time between reception of the (n-1) and (n-2) video image fields signals horizontally and vertically is defined by calculating the convolution of management signals XMAN[I], YMAN[I] which control the video camera field of view displacement with pulse characteristics hx[i] and hy[i] of its drives<br>
(Equation Removed) <br>
where XMAN[I], YMAN[I] are the management signals of the video camera field of view horizontal and vertical displacement, formed as a result of processing of the video image i-field;<br>
hx[i] is the pulse characteristic of the horizontal drive of the tracking system video camera field of view, which is the response of the increment of the angular coordinates of the tracking system video camera field of view to the horizontal management signal impact, which is constant at the interval of the first video image field reception and equal to zero at the other periods of time, at the time between the reception of the video image i and (i-1) fields:<br>
hy[i] is the pulse characteristic of the vertical drive of the tracking system video camera field of view, which is the response of the increment of the angular coordinates of the tracking system video camera field of view to the vertical management signal impact, which is constant at the interval of the first video image field reception and equal to zero at the other periods of time, at the time between the reception of the video image i and (i-1) fields;<br>
K is the pulse characteristic length, which is the number of video image fields, at the end of which pulse characteristic module is within the limits of the set level. The technical result is also achieved by the fact that forming of signals LpRAME(ix,jy) of the current frame image at interlacing from signals LH-FRAME(i-p,npk) of the image current half-frame and image L.iFRAME(ixjy) of the former frame is executed by prognostication of signals LFRAME(ix,jy) of the current frame   image  with the help of the    former frame  image  shift L-iFRAME(ix,jy) by the amount of controlled horizontal and vertical displacement dx, dy of the video camera field of view axis at the time between reception of image half-frames<br>
LFRAME(ixuy) = L -iFRAME(ix+dx,jy+dy),<br>
and by substitution of the current frame image pixels with the current half-frame image pixels with compensation for the current uncontrolled shifts rx, ry and roll φ of the tracking system video camera field of view<br>
LFRAME   ix(i,p,npk), jy(i,p,npk)     = L H-FRAME ( up.npk). where   i is the element number in the current half-frame image line, i=l... NK, ix is the element number in the current frame image line, ix=l... NK,<br>
MK<br>
p is the line number in a half-frame, p=l	<br>
jy is the line number in a frame, jy=l, .... MK, NK is the number of image elements in a line, MK is the number of lines in an image frame, npk is the current half-frame index, npk = 1 - in uneven half-frames.<br>
npk = 0 - in even half-frames,<br>
(Equation Removed) <br>
The technical result is also achieved by the fact that signals LI BIN BCD (ixjy) of the primary binary video image of the background change detector are formed by adjustment of reference background video image signals Ln-i mb(ix,iy) obtained in the previous n-1 frame to the current scale, by foniiation of differential video image signals Lp BCD (ixjy) of the background change detector; by subtraction of reference background video image signals Ln_i mb (ixjy) of the former frame from scaled video image signals Ln AW (ixjy) in the current analysis window with a shift which accounts for displacement Vx, Vy of the analysis window center in the inertia! coordinates system at the last frame:<br>
LPBCD (ixjy) = Ln AW (ixjy) - Ln-i mb (ix+Vxjy+Vy),<br>
by determination of the binarization threshold THRESHOLDBCD(ix,jy) of the background change detector as a value proportional to the local values spreading parameter of the differential image Lp BCD (ixjy) of the background change detector in neighborhood of the pixel with coordinates ixjy; by assigning of values to the primary binary video image LI BIN eco(ixjy) of the background change detector<br>
LIBINBCD(ixjy) = 1, if  LpBCD(ixjy)   &gt; THRESHOLDBCD(ixjy),   or LIBINBCD(ixjy) = 0, if  LpBcD(ixjy)   
where ix, jy are the coordinates of the scaled video image signals relatively to the center of the current analysis window,<br>
(Equation Removed) <br>
NX. NY are the dimensions of the current analysis window,<br>
at that, the reference background video image signals Ln mb(ixjy) are formed by sharing the<br>
scaled video image signals in the current analysis window into three types of image signals:<br>
video image signals in the object window	- OW'BCD,<br>
video image signals in the background windov.	- BW'BCD-<br>
video image signals in the window - "New Background"	- NB.<br>
where video image signals in a rectangle which lies in the center of the current analysis window and includes predominantly the object image elements are defined as video image signals in the object window; image elements on outer boundaries of the current analysis window where new background image elements appear on account of object movement and the video camera field of view displacement are defined as image signals of the "New Background" window; all the rest image elements of the analysis window are defined as image signals of the background window; by storage of signals Ln AW (ixjy) from the current analysis window of the current n-frame scaled video image in the "New Background" window:<br>
Ln mb (ixjy) = Ln AW (ix jy).     ixjy ε NB.<br>
where Ln AW (ixjy) are signals values of the scaled video image element intensity in the analysis window with coordinates ixjy; by averaging of scaled image signals Ln AW (ixjy) in the background window from the current analysis window with the constant Wbw and with account for analysis window shift in the inertial coordinates system at the last frame:<br>
Ln mb (ixjy)=0-Wbw)*Ln_i mb(ix+Vxjy+Vy)+Wbw*Ln Aw(ixjy)   ixjyeBWBCD,<br>
where Vx, Vy is the analysis window center displacement at the last frame in the inertial coordinates system horizontally and vertically, by re-recording in the object window of reference background video image signals of the former frame with a shift which accounts for the analysis<br>
window center displacement at the last frame:<br>
Ln mb (ixjy) = Ln_i mb(ix+Vx, jy+Vy)   ix jy e OWBcD<br>
The technical result is also achieved by the fact that the low-pass filtering of the primary binary video image signals LI BINBCD(ixjy) of the background change detector is executed with the help of two-dimensional convolution<br>
(Equation Removed) <br>
where ixjy are the filtered video image signals coordinates S_filBcD(ixjy) relatively to the center of the current analysis window;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current unaUsis window;<br>
NF and MF are the horizontal and vertical parameters of the low-pass filter aperture;<br>
di, dj are the horizontal and vertical internal variables of the low-pass filter aperture,<br>
diε[-NF, NF], djε[-MF, MF];<br>
h1-p [di, dj] is pulse characteristic of the low-pass filter.<br>
The technical result is also achieved by the fact that the low-pass filtering of the primary binary video image signals LI BINMOi(ix, jy) of the moving-objects indicator is executed with the help of two-dimensional convolution<br>
(Equation Removed) <br>
where ix,jy are the coordinates of the filtered video image signals S_filMoi(ix,jy) relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window; NF and MF are horizontal and vertical parameters of the low-pass filter aperture, di, dj are the horizontal and vertical internal variables of the low-pass filter aperture, diε[-NF, NF], djε[-MF, MF];<br>
h|-p [di, dj] is pulse characteristic of the low-pass filter.<br>
The technical result is also achieved by the fact that the secondary binary video image signals L2 BIN BCD(IXjy) of the background change detector are formed from the low-pass filter signals S_filecD(ix,jy) according to the rule:<br>
L2 BINBCD(IXjy)=l,   if S_filBcD(ix,jy) &gt; PorogBCDl and LI BIN BCD(ix,jy) = 1<br>
or S filBcD(ix,jy) &gt; PorogBCDO and LI BIN BCD(ix.jy) = 0. otherwise L2 BIN BCD(ix.jy) = 0,<br>
where ix, jy are the coordinates of the secondary binary video image signals LT BIN ncoUx.jy) °f the background change detector relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions;<br>
PorogBCDL  PorogBCDO are values of the decision thresholds for unit and zero<br>
elements  of the  primary  binary  video   image  of the  background   change  detector, respectively.<br>
The technical result is also achieved by the fact that the secondary binary video image signals L2BINMOi(ix, jy) of the moving-objects indicator are formed from the low-pass filter signals S_filMoi(ixjy) according to the rule:<br>
L2BINMOi(ixjy)=l,   if S_fil Moi(ixjy) &gt; Porog M0il and LI BINMOi(ixjy)=l<br>
or S_fil Moi(ix jy) &gt; Porog Moi 0 and LI BIN MOi(ix jy)=0, otherwise L2 BIN MOi(ixjy) = 0.<br>
where ix, jy are the coordinates of the secondary binary video image signals LT BIN vioi(ix jy) of the background change detector relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions;<br>
PorogMoi 1 &gt; PorogMoI0 are values of the decision thresholds for unit and zero elements of the primary binary video image of the moving-objects indicator.<br>
The technical result is also achieved by the fact that histogram classifier binary video image signals LBIN HC(IX jy) are formed according to the rule:<br>
LBINHc(ixjy)=:l,ifWn.iNcow[LnAw(ixjy)]&gt;an.1*A(ixjy)*Wn.iNHCBw[LnAW(ixjy)], LBIN HC(IX jy)=0 - otherwise.<br>
where ix, jy are the coordinates of the secondary binary video image signals LuiNHc(ixjy) relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions:<br>
Ln Aw(ix.jy) are scaled image signals in the current analysis window,<br>
Wn-1N cow [L,] is a normalized intensity distribution histogram L, of scaled video image<br>
signals in the central object window - COW, obtained in the previous (n-1) frame,<br>
Wn-1 N nnuv [L|] is a normalized intensity distribution histogram of background<br>
video image signals in the histogram classifier background window - HCBW, obtained in<br>
the previous (n-1) frame,<br>
L, is the intensity level of video image signals,<br>
i is the number of the intensity level of video image signals, i - 1,..., NICV.<br>
αan-1  is a parameter which depends on the number of video image elements in the<br>
histogram classifier object window - HCOW, classified as background in the former (n-1)<br>
frame,<br>
A(ix.jy) is a penalty function which depends on video image element coordinates ixjy in<br>
the analysis window, including ixjyeCOW A(ixjy) = A(,;<br>
at that, after the object video image coordinates X0, Y0 and sizes Rxo. Ryo in the current n-frame<br>
are determined, the rectangle with dimensions RxCow=Rxo. Rycow=Ryo which lies inside the<br>
analysis window and which center coincides with the object video image center XQ, YO and<br>
which includes predominantly object video image elements, is defined as the central object<br>
window - HCOW; the area between two rectangles with common center and dimensions Rxcow,<br>
Rycow and RxHcow, RyHcow, where RxHcow&gt;Rxcow and RyHcow&gt;Rycow is defined as the<br>
histogram classifier object window - HCBW; the area between two rectangles with common<br>
center and dimensions RXHCOW, Ryncow and RXHCBW, RYHCBV,- where RXNCBW&gt;RXHCOW and<br>
RyHCBW&gt;Ryncow is defined as the histogram classifier background window - HCBW. The<br>
histogram WHcBW[Lj] of background image intensity distribution L, is defined after image signals<br>
LnAw(ixjy), read from the COW window in the current n-frame; the histogram WCow[Li] of<br>
object image intensity distribution L, is defined after image signals selected from the COW<br>
window in the current n-frame; the histograms WHCBW[L,] and Wcow[L1] are smoothed<br>
(Equation Removed) <br>
where  l\m [1| is pulse characteristic of the smoothing filter:<br>
(2*ns+l) is the number of pixels of the pulse characteristic ol the smoothing filter; the   current   averaging   threshold   Thh,   of  the   smoothed   histogram    W cow [I.,)<br>
of object image intensity distribution is defined according to the expression<br>
(Equation Removed) <br>
where Thno and kthh are the constant values,<br>
1 [x] is a unit function defined by conditions:<br>
1 [x] = 1 at x &gt; 0,<br>
1 [x] = 0 at x 
Th h n = Th h n-l + Yn thresh *( Thh n -  Th h n-I )-<br>
where γn thresh is the constant of the threshold Thh averaging filter which varies according to the frame number from the value yi thresh=1 in the first frame up to the stationary value γthresh; n is the current image frame number;<br>
the values of the average threshold are restricted from above and below, the smoothed histogram<br>
Wcow[L1] of the object video image intensity distribution L, is averaged by the first order<br>
recursive filter<br>
W nCOw[Lj] = W n_i cow[L,J + yn cow*( Wcow[L,] - W n., cowLLi] )• where γncow is the constant of the  averaging  filter of the  object  video  image  intensity<br>
distribution histogram which varies according to the frame number from the value<br>
Yicow=l in the first frame up to the stationary value ycow, at that starting from the frame<br>
number which exceeds Nfc, where Nfc is the number of the frame after which the averaging of the histogram W cow[L,] is<br>
performed according to conditions, Nfc-16	128;<br>
the histogram Wcow[L1] of the object video image intensity distribution L, is averaged only for those intensity levels L, which simultaneously fulfill t\vo conditions:<br>
WCOw[L,] &gt; Th h„   and    WanviL,] &gt; cx,,*A0* WHCBW(L,), the normalized histogram WnNt-cow[L1] of the object video image intensity distribution is formed<br>
from the averaged histogram W n cow[L1] of the object image intensity distribution L,<br>
(Equation Removed) <br>
where Nlev is the number of image intensity levels Ln Aw(ixjy);<br>
the normalized histogram WnNHCBw[Li] of background image intensity distribution is formed from the smoothed histogram W HCBW[L,] of the object image intensity distribution:<br>
(Equation Removed) <br>
The technical result is also achieved by the fact that the horizontal and vertical projections Gpr MOI(IX), Vpr MOI(Jy) of the secondary binary video image signals L2 BIN MOi(ixjy) of the moving-objects indicator are defined as follows<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the secondary binary video image signals L2 BIN Moi(ixjy) of the background change detector relatively to the current analysis window center:<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions;<br>
Nwin and Mwin are horizontal and vertical dimensions of the secondary binary video image of the moving-objects indicator.<br>
The technical result is also achieved by the fact that the horizontal and vertical projections GprBCD(ix), VprBCD(jy) of the secondary binary video image signals L2 BIN BCD of the background change detector are defined as follows<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the of the secondary binary video image signals L2 BIN BCD(ix,jy) of the background change detector relatively to the center of the current analysis window;<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions;<br>
Nwin and Mwin are horizontal and vertical dimensions of the secondary binary video<br>
image of the background change detector.<br>
The technical result is also achieved by the fact that the horizontal and vertical projections GprHc(ix), VprHc(jy) of binary image signals of the histogram classifier LBIN Hc(ixjy) are defined as follows<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the of the secondary binary video image signals LBIN Hc(ixjy) of the histogram classifier relatively to the current analysis window center:<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions;<br>
Nwin and Mwin are horizontal and vertical dimensions of the histogram classifier binary<br>
video image.<br>
The technical result is also achieved by the fact that the confidence factors WBCD- WMOI, WHC are defined as product of functions of initial conditions input and the normalized average densities of binary video images of the background change detector, moving-objects indicator and histogram classifier,<br>
(Equation Removed) <br>
at that, the average densities VBCD(n), VMO,(n), VHC(n) of binary video images of the background change detector, moving-objects indicator, and histogram classifier are obtained as a result of minimum and maximum values limitation and further averaging of the current densities VscoCn), VMoi(n), VHC(H) of the corresponding binary video images by the first order recursive filters<br>
(Equation Removed) <br>
SoBCD(n), SoMOi(n), SOHC(«) are current areas of binary video images of the background change detector, moving-objects indicator and histogram classifier inside the object video image boundaries,<br>
SoR(n) is the current area of a region inside the object video image boundaries, Fic_BCD(n), Fjc_Moi(n), Fic_Hc(n) are the input functions of the initial conditions of the background change detector, moving-objects indicator and histogram classifier, n is the current frame number.<br>
The technical result is also achieved by the fact that the generalized horizontal and vertical signal projections Gpro(ix,n), Vpr Gy(jy,n) of the object generalized binary video image are formed by weighted summation of binary video image projections of the background change detector, moving-objects indicator and histogram classifier:<br>
GprG(ix,n)=WBCD(n)*GprBCD(ix)+WMoi(n)*GprMoi(ix)+WHc(n)*GprHc(ix), VPr G(jy,n)-WBcD(n)*Vpr BCD(jy)+WMoi(n)* Vpr MOi(jy)+WHC(n)*Vpr HC(jy).<br>
where ix, jy are the coordinates of the video image signals relatively to analysis window;center<br>
(Equation Removed) <br>
NX, NY are the current analysis window dimensions;<br>
Wnc(n). WBCD(n), WMOi(n) are the confidence factors of the histogram classifier binary<br>
video image signals, background change detector and moving-objects indicator secondary<br>
binan<br>
video image signals;<br>
GprHc(ix) and VprHcGy)&gt; GPrBCD(ix) and  VprBcD(jy), GprMoi(ix) and VprMOi(jy) are<br>
horizontal and vertical signal projections of the histogram classifier binary video image<br>
signals, background change detector and moving-objects indicator secondary binary video<br>
image signals;<br>
n is the current frame number.<br>
The technical result is also achieved by the fact that the current coordinates XGBIN, YGBIN of the object generalized binary video image are defined as a weighted sum of gravity center coordinates XGGC, YGGC and area median coordinates XGMED- YGMED of the object generalized binary video image<br>
XoBIN(n) = WGc(n) * XGGC + WMHD(n) * XGMED. YoBIN(n) = Woc(n) * YGGC + WMED(n) * YGMED. WMED(n) - 1 - Woc(n), where WMED(n), Woc(n) are the weighting coefficients of the median and the gravity center<br>
coordinates estimates of the object generalized binary video image;<br>
n is the current frame number.<br>
at that, the weighting coefficient WGc(n) is increased at reduction of object coordinates mean deviation from their predictable values.<br>
The technical result is also achieved by the fact that the current horizontal VGOB BIN and vertical WQB BIN estimate components of the object binary video image traverse speed in the inertial coordinates system are defined according to expressions VGoB BIN = (dX + ΔXAW + ΔXoB AW BIN)/!, WOB BIN = (dY + A YAW + A YOB AW BIN)/T, where dX,  dY  is,  respectively,  horizontal  and  vertical  video  camera  field  of view  axis<br>
displacement at the time T between reception of the current and former video image<br>
fields;<br>
AXAW. AYAW is horizontal and vertical repositioning of the analysis window in the<br>
current frame in relation to the former one.<br>
AXOH AW BIN, A YOB AW BIN is. respectively, horizontal and vertical change of object binary<br>
video image coordinates in the analysis window in the current frame in relation to the previous one.<br>
The technical result is also achieved by the fact that the nonlinear high-pass filtering of scaled image signals in the current analysis window is executed according to the expression LnNF(ixjy) = Ln Aw(ixjy) + KNF*FNF[ Ln HpF(ix,jy) ],<br>
where ix, jy are the coordinates of the video image signals relatively to center of the analysis<br>
window;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
FNF [ L ] is the function of two-sided delimitation,<br>
FNF [ L ] = LTHRESH      at L &gt; LTHRESH.<br>
FNF [ L ] = L	at -LTHRESH≤L≤LTHRESH<br>
FNF [ L ] = -LTHRESH     at L 
LTHRESH is the threshold level of the two-sided delimitation function.<br>
(Equation Removed) <br>
KNF, KNPF are the constant coefficients,<br>
di, dj are the internal variables of the high-pass filter;<br>
NF, MF are horizontal and vertical parameters of the filter aperture.<br>
The technical result is also achieved by the fact that signals of the object dynamic reference video image are foimed by means of reading of signals in each frame from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image, and with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XQB FOV - XAWFOV -YC MAS ~ YOB FOV - YAW FOW where XQB FOV- YOB FOV are object image coordinates in the field of view,<br>
XAW FOV. YAW FOV are analysis window coordinates in the field of view.<br>
The technical result is also achieved by the fact that the object static reference video image signals are founed by means of reading and storage of video image signals from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XOB FOV - XAW FOV, YC MAS = YOB FOV - YAW FOV, where XOB FOV, YOB FOV are object video image coordinates in the tracking system video camera<br>
field of view;<br>
XAW FOV, YAW FOV are the analysis window center coordinates in the tracking system video<br>
camera field of view;<br>
meeting the conditions of the object static reference video image change fotmed on the basis of analysis of parameters of dissimilarity measures signals of the video image signals after the nonlinear high-pass filtering in the current analysis window and object static and dynamic reference video image signals, as well as object video image trajectory parameters analysis obtained on the basis of usage of object static and dynamic reference video image signals.<br>
The technical result is also achieved by the fact that the object static reference video image signals are fomied by means of reading and storage of video image signals from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XOB FOV - XAW FOV, YC MAS = YOB FOV - YAW FOV, where XOBFOV, YOB FOV are object video image coordinates in the tracking system video camera<br>
field of view;<br>
XAW FOV, YAW FOV are the analysis window coordinates in the tracking system video<br>
camera field of view;<br>
meeting the conditions of the object static reference video image change formed on the basis of analysis of parameters of dissimilarity measures signals of image signals after the nonlinear<br>
high-pass filtering in the current analysis window and object static and dynamic reference video image signals, as well as object image trajectory parameters analysis obtained on the basis of analysis of dissimilarity measures signals.<br>
The technical result is also achieved by the fact that the current horizontal VGoa DISSIM and vertical VVOB DISSIM estimate components of the object binary video image traverse speed in the inertial coordinates system obtained on the basis of forming of images dissimilarity measure signals are defined according to expressions<br>
VGoe DISSIM = (dX + ΔXAW + ΔXoe AW DISSIM)/T, WOB DISSIM = (dY + ΔYAw + ΔYOB AW DISSIM)/T, where dX, dY is the tracking system video camera field of view axis displacement at the time T<br>
between reception of the current and former video image fields,<br>
AXAW, ΔYAW is horizontal and vertical change of the analysis window position in the<br>
current frame in relation to the former one,<br>
AXOB AW DISSIM, ΔYoB AW DISSIM is horizontal and vertical change of object coordinates in<br>
the analysis window in the current frame in relation to the former one, obtained on the<br>
basis of fotming of images dissimilarity measure signals.<br>
The technical result is also achieved by the fact that the confidence factor WBIN(n) of the current traverse speed of the object generalized binary video image is calculated by defining the<br>
current density VBIN(n)=   GB!N        of the generalized binary video image; by limiting the<br>
SOR(n)<br>
minimum and maximum values of the current density VBIN(n) of the generalized binary video image; further averaging of the generalized binary video image delimited density by the first order recursive filter; nouiiaJizing the average density VBIN (n) of the object generalized binary video image<br>
(Equation Removed) <br>
where SaeiN(n) is the current area of the generalized binary video image inside the object image boundaries;<br>
SoR(n) is the current area of the region inside the object video image boundaries, n is the current frame number, VSIM (n) is the averaged similarity factor obtained after calculating the current similarity<br>
factor VsiM(n)=      Bmin         , after limitation of its maximum and minimum values and EDISSIMmin (n)<br>
averaging by the first order recursive filter;<br>
σBmin(n) is the minimum value of the mean-square value of background video image<br>
signals in M windows of background analysis;<br>
EoissiMmin(n) is the minimum value of video images dissimilarity measure signals in the<br>
current analysis window and object static referense video image in two-dimensional<br>
search area of object video image shifts.<br>
The technical result is also achieved by the fact that the confidence factor WsiM(n) of the object video image current traverse speed, obtained on the basis of fotming of images dissimilarity measure signals, is obtained by calculating the current similarity factor<br>
VSIM(A)-       Bmin          by limiting its maximum and minimum values and averaging by the EoissiMmin(n)<br>
first order recursive filter, normalizing the calculated average similarity factor VSIM (n):<br>
(Equation Removed) <br>
where n is the current frame number;<br>
σBmin(n) is the minimum value of the mean-square value of background video image signals in M windows of background analysis;<br>
EoissiMmin (n) is the minimum value of video images dissimilarity measure signals of the current analysis window and object static reference video image in two-dimensional search area of object video image<br>
shifts;<br>
VBIN(n) is the averaged density of the object generalized binary video image obtained<br>
after calculating the current density VBIN(n) =    G BIN       of the object generalized binary<br>
SOR(n)<br>
video image, after limiting the minimum and maximum values of the current density<br>
VBIN(n) of the object generalized binary video image; and further averaging of the<br>
delimited ciuitnt density VBIN(II) of the object generalized binary video image by the first<br>
order recursive filter;<br>
SGBIN(n) is the current area of the generalized binary video image inside the boundaries of<br>
the object generalized binary video image;<br>
SOR(n) is the cutient area of the region inside the boundaries of the object generalized<br>
binary video image.<br>
The technical result is also achieved by the fact that the complex estimate of horizontal VGOB(n) and veitical VVOB(n) constituents of the object video image current traverse speed in the inertial coordinates system is defined by limitation of the estimates of the object binary video image traverse speed and object video image traverse speed obtained on the basis of forming of images dissimilarity measure signals, by the minimum and maximum values formed with allowance for preceding values of the complex estimate of the object video image current traverse speed, and generating the weighted sum of delimited estimates of the binary object video image uaverse speed and object video image traverse speed obtained on the basis of forming of video images dissimilarity measure signals<br>
VGOB(n) = WBIN(n) * VGOB BIN(n) + WslM(n) * VGOB DISSIM(n), VVOB(n) = WBIN(n) * WOB BIN(n) + WSiM(n) * VVOB DISSIM(n), where WBIN(n), WstMOO are confidence factors of the object generalized binary video image<br>
current traverse speed and the object video image current traverse speed obtained on the<br>
basis of forming of images dissimilarity measure signals,<br>
VGoB BIN(n) and WOB BIN(H) are delimited horizontal and vertical constituents of the<br>
object generalized binary video image current traverse speed;<br>
VGOB DISSIMi(n) and WOB DISSIM(n) are limited horizontal and vertical constituents of the<br>
object video image current traverse speed obtained on the basis of forming of images<br>
dissimilarity measure signals,<br>
n is the current frame number.<br>
The technical result is also achieved by the fact that the analysis of the current SGBIN(B) and average S GBIN(n) area of the object generalized binary video image is executed by testing of the condition satisfaction:<br>
SGBIN(n) 
SGBIN(n) &gt; ks2(n-nEx)* S GBIN(n) for change for forming of tracking system video camera field of view axis displacement management signals along object video image coordinates in the tracking system video camera field of view obtained as a result of image processing in the current analysis window, where ksl is the constant coefficient, ksKl,<br>
ks2(n-nEx) is the coefficient which diminishes with growth of the frame number n,<br>
starting from the frame number HEX, change for forming of the video  camera  field of<br>
view axis displacement management signals along the extrapolated coordinates, ks2(n-<br>
nEx)^ksl.<br>
The technical result is also achieved by the fact that the analysis of the current and averaged object video image traverse speed is executed by testing of satisfaction of conditions: VGcoB(n) - VG coB(n) &gt; kvl* VG CoB(n) + kv2, WcoB(n) - VV coB(n) &gt; kvl * VV COB(n) + kv2,<br>
- for change for the video camera viewing field of view displacement management signals along the extrapolated coordinates, or satisfaction of the conditions:<br>
VGcoB(n) - VG coB(n) 
- for change for foitning of tracking system video camera field of view axis displacement management signals along object video image coordinates in the tracking system video camera field of view obtained as a result of video image processing in the current analysis window, where VGcoe(n), VVcos(n) are horizontal and vertical constituents of the object video image current traverse speed obtained as a result of first order with the filter constant 0<wv1 filtering of the complex estimate constituents vgoe vvoe object image traverse speed:></wv1>
VGCOB(n)= VGcoB(n-l) + WV1*[ VGOB(n) - VGCOB(n-l)], VGCOB(n)= VGcoB(n-l) + WV1*[ VGOB(n) - VGCOB(n-l)],<br>
VG coB(n), VV cos(n) are horizontal and vertical constituents of the object video image averaged traverse speed obtained as a result of first order filtering with the filter constant 0<wv2 of the complex estimate constituents vgoecn vvoscn object image traverse speed:></wv2>
VG coB(n)= VG coB(n-l) + WV2*[ VGOB(n) - VG CoB(n-l)], VGcoB(n)= VG coB(n-l) + WV2*[ VGOB(n) - VG Con(n-l)], atthatWVl&gt;WV2.<br>
The technical result is also achieved by the fact that analysis of the area and coordinates of object video images boundaries of the histogram classifier inside the M windows of background analysis is executed by testing of satisfaction of the conditions of histogram classifier binary video image area changes detection inside the M windows of background analysis:<br>
(Equation Removed) <br>
where Sj(n) are the curient values of the histogram classifier binary video images current area inside the i-window of background analysis, i = 1 ... M;<br>
S ,(n) are the averaged values of the histogram classifier binary video images current area inside the i-window of background analysis, i=l,...,M, obtained at the output of the filter of the first order with the filter constant 0<ws></ws>
S,(n)= Si(n-l) + WS*[S,(n)- S,(n-l)].<br>
Stixcshoid(n) is the threshold of the change detection of the binary video images of the histogram classifier in the M background analysis windows:<br>
(Equation Removed) <br>
where k is the constant coefficient,<br>
S GBIN(n) is the average area of the object generalized binary video image, obtained at the output of the first order recursive filter with the filter constant 0<wsob></wsob>
SGBIN(n) = SGBIN(n-l) + WSOB*[ SGBIN(n) - S GBIN(n-l)]; n is the current frame number,<br>
(Equation Removed) <br>
where   L0 is the average value of video image signals in the object window; LB is the average value of image signals in the background window; crB is the root-mean-square deviation of video image signals in the background window from LB;<br>
FCONSTR(Q) is the constraint function of minimum and maximum values;<br>
when meeting the mentioned above conditions for the current area of the histogram classifier binary video images in one or several background analysis windows with ki numbers, where ki are the numbers of the background analysis windows in which the change of the<br>
histogram classifier binary video image current area was detected, ki=l,.. .,kN;<br>
kN is the background analysis windows number, where the change of the histogram<br>
classifier binary video image current area was detected, kN   M;<br>
the coordinates of two inter-perpendicular boundaries of the histogram classifier binary video image placed on the side of the object window are analyzed in these windows, at that, the horizontal displacement of the binary video image vertical boundary is analyzed in background analysis windows placed on the side of the object window vertical boundaries<br>
and the membership of horizontal boundary coordinates to the object window vertical coordinates is tested; the vertical displacement of the binary video image horizontal boundary is analyzed in background analysis windows placed on the side of the object window horizoatal boundaries and the membership of vertical boundary coordinates to the object window horizontal coordinates is tested, at that the binary video image boundaries displacement in the M background analysis windows analysis is executed by testing the conditions of location of the correspondent histogtam classifier binary video image boundaries inside the boundaries of internal OkJ and exterior Oki2 regions of the ki background analysis window with forming of features Fkil=l and Fki2=l of the belonging of the binary video images boundaries to the to internal Ok,l and exterior Okj2 regions of the ki background analysis window, where Fkil=l if the binary video image boundary of the histogram classifier is within the<br>
boundaries of the Okil region, or Fkil=0 otherwise;<br>
Fki2=l if the binary video image boundary of the histogram classifier is within the<br>
boundaries of the Oki2 region, or Fki2=0 otherwise;<br>
at that, at the detection of the correspondent binary video image boundaries of the histograai classifier transfer from the exterior region to the interior region of the background analysis windows with the numbers ki=kipl,<br>
where kipl are the numbers of the analysis windows in which binary video image boundaries of the histogram classifier transfers from the exterior regions Okipi2 to the interior regions Ok,pi 1 of the background analysis window were detected, kip 1=1,2,...,kN,<br>
at that, at successive forming of features Fkipi2=l at nkjpi-nkipi2, and then Fkipi 1=1 at nkipi=nk,pi I, where nkjpil&gt;nkipi2, rikipil is the frame number, in which in kipl analysis window the feature state is set as Fkipil=l, nkipi2 is the frame number, in which in kipl analysis window the featufp state is set as Fk,pi2=l, and at meeting of the condition of membership of the second tested boundary coordinate of the histogram classifier binary video image in the kipl background analysis window to the range of object window coordinates, the change for forming of the video camera field of view axis displacement management signals along the extrapolated coordinates is executed,<br>
the SP change counter of the histogram classifier image boundaries transfers between the exterior and interior regions of the background analysis window<br>
is bound to SP=1 at the first detection of the histogram classifier image boundaries transfer from the exterior regions Oicjpi2 to the interior regions Okipi 1 or the SP change counter increased by one at the second detection of the histogram classifier image boundaries transfer from the exterior regions Okipi2 to the interior regions Okipi 1 at the time of forming of the tracking system video camera field of view axis displacement management signals along the extrapolated coordinates of the object video image, the features states are set as Fk,pi 1=0, Fkipi2=0, the features states are set as Fj2=0 in the background analysis windows with the numbers j^kipl, j=l,...,kN, and the analysis process of the histogram classifier binary video image boundaries is started from the very beginning;<br>
at the detection of the conespondent image boundaries of the histogram classifier transfer from the exterior regions Owpzl to the interior regions Okipa2 of the background analysis windows, where kip2 are the numbers of the analysis windows in which histogram classifier binary video image boundaries transfers from the exterior regions Okip22 to the interior regions Okipal of the background analysis window were detected, kip2:=l,2,.. .,kN,<br>
at the time of forcing of the tracking system video camera field of view axis displacemeat management signals along the exuapolated coordinates of the object video image, that is at the successive forming of the feature Fkip2l=l at nk,p2-nkip2l, and then Fk,p22=l at nkip2=nkip22 in one or several background analysis windows, where nkip22&gt;nkjp2l, the SP change counter is decreased by one, and if SP=0 the change for forming of video camera field of view axis displacemeat management signals is made according to object video image coordinates in the video camera field of view, obtained as a result of image processing in the current analysis window, the features states are set as Fkipil=0, FkiP22-0, the features states are set as Fj2=0, in those background analysis windows with the numbers j=kip2, j=l,...,kN and the analysis process of the histogram classifier binary video image boundaries is started from the very beginning, the period of forming of video camera field of view axis displacement management signals along the extrapolated coordinates and the time when the<br>
features F,l, Fj2, i=l,...,M, are in the state F,l=l, Fj2=l is tested and when the set time intervals are exceeded the appropriate features are set at zero state, there is a change for forming of video camera field of view axis displacement management signals over the object video image coordinates in the video camera field of view obtained as a result of image processing in the current analysis window.<br>
The technical result in the part of assurance of coordinates determination and steady object image detention in the video camera field of view center or in the tracking window center at reception of contrast and low-contrast images of mobile and stationary objects and terrain background from the video camera when the object moves across the terrain areas which average intensity is similar to the object average intensity, at change of the object visible pattern stipulated by its rotation in motion, illuminance change, object observation in conditions of discontinuous optical communication caused by object overlapping by different barriers such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc., at controlled (mobile) and uncontiolled (stationary) video camera field of view, at motion of the surveillance system carrier that causes low-pass uncontrolled displacements of the video camera field of view (which cause shifts and rotations of all video image field elements as a whole), as well as at probable absence of stabilization errors sensors of the video camera field of view, is achieved by the fact that to the device containing unit (4) of the forming of the signals of video image dissimilarity measure of the 2N terrain references and deteitnination of the rotation and shift parameters of the current video image field at the time between the reception of the current and previous video image fields and the processor (24) for the computation processing of the local data time sharing over the bidirectional bus, the following units are embeded: the unit (1) of the reception and storage of the current video image field of the tracking system video camera and of the forming of the device functioning synchronization signals, unit (2) of the reception and storage of computation of the of the controlled rotation of the tracking system video camera, unit (3) of the computation of the controlled rotation of the tracking system video camera field of view axis at the time between the reception of preceding and current video image field signals<br>
n=3, 4, 5,..., is the cuirent video image field number, unit (5) of the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis controlled shift and of the hacking system video camera field of view axis uncontrolled displacement at the time between the reception of the current and the previous video image fields signals; switching unit (6); former (7) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise cuaent video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling; unit (8) of the video image signals fanning and scaling in the current analysis window using the video image current analysis window location and dimensions signals; unit (9) of the M analysis windows forming about tibe analysis window perimeter and the histogram classifier binary video image signals projection detemiination in the M background analysis windows; &gt;mit (10) of the deteiminatjon of the current coordinates of the object generalized binary video image using the generalized horizontal ami veitical projections of the object generalized binary video image; unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; unit (12) of detei urination of the current speed of the object video image displacement in the ineilial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming; commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view; "nit (14) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; unit (15) of the current speed of the object video image displacement confidence factors determination in the inertial coordinate system obtained on the basis of<br>
unit (5) of the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis controlled shift and of the tracking system video camera field of view axis uncontrolled displacement at the time between the reception of the current and the previous video image fields signals; switching unit (6); former (7) of the current video image signals from the former frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfoiination which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling; former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; former (9) of the background analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the background analysis windows; unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image; unit (11) of the object video image coordinates detemiintion relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; unit (12) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming; commutator (13) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view; analyzer (14) of the current speed of the object generalized binary video image displacement in the inertial coordinate system; analyzer (15) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of<br>
video images dissimilarity measure signals; unit (16) of the binary video images boundaries area and coordinates detemiination in M background analysis windows; unit (17) of detennination of the cinrent speed of the object generalized binary video image displacement in the inertial coordinate system; "nit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view; analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows; unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values, unit (22) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement; former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the backing system video camera field of view or coordinates and speed of the object video image displacement, at that the first and second inputs of the device are connected to the first and second inputs of the unit (1) of reception and storage of the current video image field signals from the hacking system video camera and of the forming of the device functioning synchroni/ation signals; the third input of the device is connected to the input of the unit (2) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view; the fourth and fifth inputs of the device are connected to the first and second inputs of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view; the sixth input of the device is connected to the second input of the unit (3) of computation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2)<br>
video images dissimilarity measure signals; unit (16) of the binary video images boundaries area and coordinates determination in background analysis windows; unit (17) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view; analyzer (20) of the conditions of the usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows; unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values, extrapolator (22) of the coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the values of the averaged complex estimate of the speed of the object video image displacement; former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, at that the first and second inputs of the device are connected to the first and second inputs of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchroni/ation signals; the third input of the device is connected to the input of the unit (2) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view; the fourth and fifth inputs of the device are connected to the first and second inputs of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view; the sixth input of the device is connected to tihe second input of the unit (3) of computation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current<br>
video image fields signals, where n=3,4, 5,... is the current video image field number; the seventh input of the device is connected to the fifth input of the switching unit (6); the first output of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchronization signals is connected to the first input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the fomring of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and previous video image fields and to the first input of the former (7) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling; the second output of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the the forming of the device functioning synchroni ration signals is connected to the third input of the unit (3) of the computation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3, 4, 5, ... is the current video image field number, to the fifth input of the unit (8) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions siavls, to the fourth input of the former (7) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling, to the second input of the unit (5) of the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis controlled displacement and of the tracking system video camera field of view axis uncontrolled displacement at the time<br>
video image fields signals, where seventh input of the device is connected to the fifth input of the switching unit (6); the first output of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchronisation signals is connected to the first input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the determination of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and previous video image fields and to the first input of the former (7) of the current video image signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled displacement at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling; the second output of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the the forming of the device functioning synchronization signals is connected to the third input of the unit (3) of the computation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals, where n=3, 4, 5, ... is the current video image field number, to the fifth input of the former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals, to the fourth input of the former (7) of the current video image signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled displacement at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfouiiation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling, to the second input of the unit (5) of the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis coiiholkxi displacement and of the tracking system video caemra of view axis uncomroiied displacement at the time<br>
between the reception of the current and the previous video image fields signals, to the fourth input of the unit (4) of the fonuing of the 2N terrain reference marks video image dissimilarly measure signals and of the forming of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields, to the second inputs of the unit (9) of the M analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection deteuninRtion in the M background analysis windows and of the unit (16) of the binary video images boundaries area and coordinates deteuiiination in M background analysis windows, to the third input of the "nit (10) of the detemiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the eighth input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the fourth input of the unit (12) of deteiirnnation of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the fifth input of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the fourth inputs of the unit (14) of detemiination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors and of the unit (15) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, to the fourth input unit (17) of deteuiiination of the current speed of the object generalised binary video image displacement hi the inertial coordinate system, to the fifth input of the unit (18) of the forming of the cuiftnt speed of the object video image displacement in the inertial coordinate system complex estimate, to the third input of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view,<br>
between the reception of the current and the previous video image fields signals, to the fourth input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the determination of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields, to the second inputs of the former (9) of the background analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection detemiination in the background analysis windows and of the unit (16) of the binary video images boundaries area and coordinates deteunination in background analysis windows, to the third input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the eighth input of the unit (11) of the object video image coordinates deteanination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the fourth input of the unit (12) of deteunination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the fifth input of the commutator (13) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view, to the fourth inputs of the analyzer (14) of determination of the ciuient speed of the object generalized binary video image displacement in the inertial coordinate system and of the analyzer (15) of determination of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarly measure signals, to the fourth input unit (17) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system, to the fifth input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate, to the third input of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view,<br>
to the sixth input of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows, to the third inputs of unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values and to the unit (22) of detennination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement and to the sixth input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the output of the unit (2) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view is connected to the-third input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the forming of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields and to the third input switching unit (6); the first output unit (3) of the computing of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3, 4, 5, ... is the current video image field number is connected to the second input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the forming of the parameters of the rotation and shift of the signals of the current video image fieW at the time between the reception of the current and former video image fields and to the fouifli input switching unit (6); the first output of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the forming of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former<br>
to the sixth input of the analyzer (20) of the conditions of the usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows, to the third inputs of nnit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values and to the extrapolator (22) of the coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of values of the averaged complex estimate of the speed of the object video image displacement and to the sixth input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the output of the unit (2) of me reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view is connected to the third input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the determination of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields and to the third inpift switching unit (6); the first output unit (3) of the computing of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals is connected to the second input of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the deteunination of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields and to the fourth input switching unit (6); the first output of the unit (4) of the forming of the 2N temiiti reference marks video image dissimilarity measure signals and of the determination of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former<br>
video image fields is connected to the first input of the unit (5) of the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis controlled displacement and of the tracking system video camera field of view axis uncontrolled displacement at the time between the reception of the ciment and the former video image fields signals and its first and second outputs are connected to the first and second inputs of the switching unit (6); the first output of the switching unit (6) is connected to the second input of the former (7) of the current n-frame video image signals from the previous frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the trackiag system video camera field of view uncontrolled displacements and rolling, and the second output is connected to the third input of the former (7) of the current n-frame video image signals from, the fourter frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates tpmsfotmation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling, to the fifth input of the unit (10) of the deteanination of the current coordinates of the object generalized binary video image using the generalized horizontal and veidcal projections of the object generalized binary video image, to the first input unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the first input of the unit (12) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the second input of the unit (17) of detemiination of the current speed of the object generalised binary video image displacement in the inertial coordinate<br>
video image fields is connected to the first input of the unit (5) of the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis controlled displacement and of the tracking system video camera field of view axis uncontrolled displacement at the time between the reception of the current and the former video image fields signals and its first and second outputs are connected to the first and second inputs of the switching unit (6); the first output of the switching unit (6) is connected to the second input of the former (7) of the current video image signals from the previous frame video image signals with regard to the tracking system video camera field of view axis controlled displacement at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling, and the second output is connected to the third input of the former (7) of the current video image signals from the former frame video image signals with regard to tiie tracking system video Camera field of view axis controlled displacement at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling, to the fifth input of the unit (10) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the first input unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming. to the first input of the unit (12) of determination of the current speed of the object video image displacement in the inertia! coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the second input of the unit (17) of detemiination of the current speed of the object generalised binary video image displacement in the inertia! coordinate<br>
system, and to the first input of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view; the output of the former (7) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled displacement at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise cuirent video image field signals coordinates transfoimation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and-rolling is connected to the first input of the unit (8) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the output of the unit (8) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals is connected to the seventh input of the unit (10) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the generalized binary video image of the object and to the third input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the output of the unit (9) of the M analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the M background analysis windows is connected to the first input of the unit (16) of the binary video images boundaries area and coordinates deteunination in M background analysis windows, which first output is connected to the third input of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogtam classifier binary video images in the M background analysis windows; the first output of the unit (10) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the first input of the unit (9)<br>
system, and to the first input of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view; the output of the former (7) of the current video image signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled displacement at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfoimation which supports the compensation of the tracking system video camera field of view uncontrolled displacements and rolling is connected to the first input of the former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the output of the former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals is connected to the seventh input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the geneialized binary video image of the object and to the third input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the output of the former (9) of the analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection deteiiiiination in the background analysis windows is connected to the first input of the unit (16) of the binary video images boundaries area and coordinates determination in background analysis windows, which first output is connected to the third input of the analyzer (20) of the conditions of the usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows; the first output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the first input<br>
of the M analysis windows fbmiing about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the M background analysis windows; the second output of the unit (10) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the third input of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the analyzer (14) of deteunination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors and to the first input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the third output of the unit (10) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the first input of the unit (17) of deteunination of the current speed of the object generalised binary video image displacement in the inertial coordinate system; the fourth output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the second input of the analyzer (14) of deteunination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors and to the second input of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows; the fifth output of the first image processing unit is connected to the first input of the conditions analyzer; the sixth output of the unit (10) of the determination of the current coordinates of the object generalized binar\ video image using the generalized<br>
of the foutier (9) of background analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the background analysis windows; the second output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the third input of the commutator (13) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the analyzer (14) of the current speed of the object generalized binary video image displacement in the inertial coordinate system and to the first input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the third output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the first input of the unit (17) of determination of the current speed of the object generalised binary video image displacement in the inertial coordinate system; the fourth output of the unit (10) of the deteimination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the second input of the analyzer (14) of the current speed of the object generalized binary video image displacement in the inertial coordinate system and to the second input of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows; the fifth output of the first image processing unit is connected to the first input of the conditions analyzer; the sixth output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized<br>
horizontal and vertical projections of the object generalized binary video image is connected to the first input of the Analyzer (20) of the conditions of the transition to the use of the object video image exUapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows; the sixth output of the unit (10) of the detennination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the thicd input of the unit (15) of detemiination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, the seventh output of the unit (10) of the detemiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the fourth input of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchronization signals; the first output of the unit (11) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video irrv»ge similarity and dissimilarity measure signals forming is connected to the second input of the unit (12) of deteunination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, which first output is connected to the third input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the second output of the unit (11) of the object video image coordinates detennination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first input of the unit (15) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure<br>
horizontal and vertical projections of the object generalized binary video image is connected to the first input of the analyzer (20) of conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows; the sixth output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the third input of the analyzer (15) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, the seventh output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the fourth input of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchronization signals; the first output of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the unit (12) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, which first output is connected to the third input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the second output of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first input of the analyzer (15) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure<br>
signals, which first output is connected to the second input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system compleK estimate; the first output of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view is connected to the first input of the unit (10) of the determination of the cunent coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the seventh input of the nnif (11) of the object video image coordinates detennination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the third input of the unit (8) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the second output of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view is connected to the second input of the unit (10) of the detennination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the sixth input of the unit (11) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and to the second input of the unit (8) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (14) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors is connected to the first input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate complex estimate, and the; second output is connected to the second input of the unit (15) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, which second output is connected to the third input of the analyzer (14) of determination of the current speed of the object generalized binary video image<br>
signals, which first output is connected to the second input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the first output of the commutator (13) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view is connected to the first input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the seventh input of the unit (11) of the object video image coordinates detetmination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the third input of the former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the second output of the commutator (13) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view is connected to the second input of the unit (10) of the deteiinination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the sixth input of the unit (11) of the object video image coordinates deteiinination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and to the second input of the former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (14) of the current speed of the object generalized binary video image displacement in the inertial coordinate system is connected to the first input of the unit (18) of the fotming of the current speed of the object video image displacement in the inertial coordinate complex estimate, and the second output is connected to the second input of the analyzer (15) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, which second output is connected to the third input of the analyzer (14) of the current speed of the object generalized binary video image<br>
displacement in the inertial coordinate system confidence factors; the first output of the unit (17) of deteiUiination of the current speed of the object generalized binary video image displacement in the inertial coordinate system is connected to the fourth input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate systera complex estimate; tihe first output of the unit (18) of the forming of the current speed of the object video image Displacement in the inertial coordinate system complex estimate is connected to the fourth input of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows, to the fifth input of the unit (11) of the object video image coordinates detei urination relatively to the current analysis window center on the basis of the video image simiiaiily and dissimilarity measure signals forming, to the first input of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values and to the second input of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view; the first output of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view is connected to the fourth input of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the unit (22) of deteituiwtion of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement and to the third input of the fouaer (23) of the tracking system video camera field of view axis displacemeat management signals and of the signals of location and dimensions of the analysis window in the next video image frafne with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, which second output is connected to the third inputs of the unit (12) of determination<br>
displacement in the inertial coordinate system; the first output of the unit (17) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system is connected to the fourth input of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the first output of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate is connected to the fourth input of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows, to the fifth input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the first input of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values and to the second input of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view; the first output of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view is connected to the fourth input of the commutator (13) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the extrapolator (22) of the coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of values of the averaged complex estimate of the speed of the object video image displacement and to the third input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, which second output is connected to the third inputs of the unit (12) of detennination<br>
of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming and of the unit (17) of deteunination of the current speed of the object generalised binary video image displacement in the inertial coordinate system, to the sixth input of the unit (10) of the deteiinination of the current coordinates of the object generalized binary video image using the generalised horizontal and vertical projections of the object generalized binary video image and to the fourth inputs of the unit (11) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and of the unit (8) of the video image signals fotming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (20) of the conditions of the sition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalised binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogian) classifier binary video images in the M background analysis windows is connected to the second input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image ftame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, to the second input of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values and to the fourth input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalised binary video image; the second output of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of<br>
of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming and of the unit (17) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system, to the sixth input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image and to the fourth inputs of the unit (11) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and of the former (8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows is connected to the second input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, to the second input of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values and to the fourth input of the unit (10) of the deteution of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image; the second output of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of<br>
the histogram classifier binary video images in the M background analysis windows is connected to the second-input of the unit (11) of the object video image coordinates deteiinination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fotwing; the fourth output of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows is connected to the third input of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchronisation signals; the first output of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values is connected to the fifth input of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows and to the second input of the unit (22) of deteimination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement; the second output of the unit (22) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement is connected to the fifth input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the unit (22) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the<br>
the histogram classifier binary video images in the background analysis windows is connected to the second input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the fourth output of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows is connected to the third input of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchronization signals; the first output of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values is connected to the fifth input of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows and to the second input of the extrapolator (22) of the coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the values of the averaged complex estimate of the speed of the object video image displacement; the second output of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values is connected to the fifth input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the extrapolator (22) of the coordinates and of the displacement speed of the object video image in the next frame on the basis of the<br>
analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement is connected to the fourth input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement is the device output and is connected to the first input of the unit (3) of the computation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3, 4, 5,... is the current video image field number; the output of the processor (24) of the local data computation processing transmitted over the bidirectional bus is connected to the second outputs of the unit (3) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3, 4, 5,,.. is the current video image field number, of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the forming of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields, of the object unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, of the unit (12) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, of the unit (16) of the binary video images boundaries area and coordinates determination in M background analysis windows, which first output, and of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate.<br>
analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement is connected to the fourth input of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement is the device output and is connected to the first input of the unit (3) of the computation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals; the output of the processor (24) for local data computation processing transmitted in time sharing over the bidirectional bus is connected to the second outputs of the unit (3) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals, of the unit (4) of the forming of the 2N terrain reference marks video image dissimilarity measure signals and of the determination of the parameters of the rotation and shift of the signals of the current video image field at the time between the reception of the current and former video image fields, of the unit (12) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images- dissimilarity measure signals forming, of the unit (16) of the binary video images boundaries area and coordinates determination in background analysis windows, which first output, of the unit (17) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system and of the unit (18) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate.<br>
of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view, of the unit (22) of deteimination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement, to the third outputs of the analyzer (14) of deteimination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors, of the "nit (15) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, of the analyzer (20) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows, of the unit (21) of averaging the complex estimate of the cuucut speed of the object video image displacement and of the storage of the averaged speed values and of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, to the eighth output of the unit (1) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the device functioning synchroruzation signals.<br>
The technical result is also achieved by the fact that the unit (10) of the detei urination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image contains the first videodata commutator (31), the first buffer random access memory (30), the second buffer random access memory (41) and the third buffer random access memory (43), the former (32) of the histogram classifier binary video image signals in the current analysis window, former<br>
of the unit (19) of the complex estimate of the video image coordinates in the tracking system video camera field of view, of the extrapolator (22) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement, to the third outputs of the unit (1) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video images dissimilarity measure signals forming, of the analizer (15) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, of the analyzer (20) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows, of the unit (21) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values and of the former (23) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, to the eighth output of the unit (10) of detennination of the coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image.<br>
The technical result is also achieved by the fact that the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image contains the videodata commutator (31), the first buffer random access memory (30), the second buffer random access memory (41) and the third buffer random access<br>
(36) of the background change detector primary video image signals, fanner (38) of the background change detector secondary video image signals, former (45) of the moving-objects indicator primary video image signals, former (47) of the moving-objects indicator secondary video image signals, the first node (42) of the video image signals scaling and shift, the node (44) of the forming of the moving-objects indicator difference video image signals, former (33) of the histogram classifier binary video image signals horizontal and vertical projections, former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, the unit (35) of the object/background ratio and backgrouad video image signals mean square value minimum determination in M background analisys windows, spatial low-pass filter (37) of the background change detector primary binary video image signals, spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, unit (34) of the histogram classifier binary video image signals confidence factor deteniiination, unit (40) of the background change detector binary secondary video image signals confidence factor detemiination, unit (49) of the moving-objects indicator binary secondary video image signals confidence factor determination, and the former (50) of the generalised vertical and horizontal projections of the object generalized binary video image, unit (51) of the deteiimn»tion of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window, analyzer (52) of the averaged binary video image coordinates automatic deteiinination break, at that, the first and second inputs of the unit (10) of the detenrtination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image are connected to the first and second inputs of the former (32) of the histogiam classifier binary video image signals in the current<br>
 (36) of the background change detector primary video image signals, former (38) of the background change detector secondary video image signals, former (45) of the moving-objects indicator primary video image signals, former (47) of the moving-objects indicator secondary video image signals, the first node (42) of the video image signals scaling and shift, the node (44) of the forming of the moving-objects indicator difference video image signals, former (33) of the histogram classifier binary video image signals horizontal and vertical projections, former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, the unit (35) of the object/background ratio and background video image signals minimum mean square value detennination in the background analysis windows, spatial low-pass filter (37) of the background change detector primary binary video image signals, spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, analizer (34) of the histogram classifier binary video image signals, unit (40) of the background change detector binary secondary video image signals confidence factor determination, anaiizer (49) of the moving-objects indicator binary secondary video image signals, and the former (50) of the generalized vertical and horizontal projections of the object generalized binary video image, calculator (51) of the object coordinates and sizes, of the cuirerjt and averaged area of the object binary video image in the analysis window, analyzer (52) of the averaged binary video image coordinates automatic detennination break, at that, the first and second inputs of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image are connected to the first and second inputs of the former (32) of the histogram classifier binary video image signals in the current<br>
analysis window, and of the unit (35) of the object/background ratio and background video image signals mean-square value minimum determination in the M background analisys windows, and to the fourth and third inputs of the former (36) of the background change detector primary respectively; the first input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generali/ed binary video image is also connected to the first input of the first node (42) of video image signals scaling and shift; the third input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vralical projections of the object generalized binary video image is connected to the third input of the first videodata commutator (31), to the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window, to the second input of the former (33) of the histogram classifier binary video image signals horizontal and vertical projections, to the second input of the unit (34) of the histogram classifier binary video image signals confidence factor deteimination, to the fifth input of the unit (35) of the object/background ratio and background video image signals mean square value minimum detei miration hi the M background analysis windows, to the sixth input of the former (36) of the background change detector primary video image signals, to the second input of the unit (40) of the background change detector binary secondary video image signals confidence factor deteinrination, to the fourth input of the first node (42) of video image signals scaling and shift, to the second input of the unit (40) of the background change detector binary secondary video image signals confidence factor deteimination, to the seventh input of the former (50) of the generated vertical and horizontal projections of the generalized object binary video image, and to the second input of the unit (51) of the determination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window; the fourth input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image<br>
analysis window, and of the unit (35) of the object/background ratio and background video image signals minimum mean square value determination in the background analysis windows and to the fourth and third inputs of the former (36) of the background change detector primary respectively; the first input of the unit (10) of the deteimination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is also connected to the first input of the first node (42) of video image signals scaling and shift; the third input of the unit (10) of the deteimination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the third input of the first videodata commutator (31), to the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window, to the second input of the former (33) of the histogram classifier binary video image signals horizontal and vertical projections, to the second input of the analyzer (34) of the histogram classifier binary video image signals, to the fifth input of the unit (35) of the object/background ratio and background video image signals mean minimum square value determination in the background analysis windows, to the sixth input of the former (36) of the background change detector primary video image signals, to the second input of the analyzer (40) of the moving objects indicator binary secondary video image signals, to the fourth input of the first node (42) of video image signals scaling and shift, to the second input of the analyzer (40) of the background change detector binary secondary video image signals confidence factor deteimination, to the seventh input of the former (50) of the generalized vertical and horizontal projections of the generated object binary video image, and to the second input of the calculator (51) of the the objects coordinates and sizes, of the current and averaged area of the object binary video image; th0 fourth input of the unit (10) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image<br>
is connected to the fourth input of the former (32) of the histogram classifier binary video image signals in the current analysis window; the fifth input of the unit (10) of the detei urination of the current coordinates of the object generalized binary video image using the generalized horizon^ and veitical projections of the object generalized binary video image is connected to the first input of the former (36) of the background change detector primary video image signals and to the second input of the first node (42) of video image signals scaling and shift; the sixth input of the unit (10) of the detemiination of the current coordinates of the object generalized binaiy video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the second input of the former (36) of the background change detector primary video image signals, to the fifth input of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the third input of the "nit (35) of the object/background ratio and background video image signals mean square value minimum determination in M background analysis windows; the seventh input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to inputs of the first buffer random access memory (30), and of the second buffer random access memory (41), to the first input of the node (44) of the forming of the moving-objects indicator difference video image signals, to the fifth input of the former (36) of the background change detector primary video image signals and to the first input of the first videodata commutator (31); the output of the first buffer random access memory (30) is connected to the second input of the first input of the first videodata commutator (31), which first and second outputs are connected to the third and sixth inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the fourth input of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination in M background analysis windows, the first output of which is the fifth output of the unit (10) of the detei mi nation of the current coordinates of the object generalized binary video image using<br>
is connected to the fourth input of the former (32) of the histogram classifier binary video image signals in the current analysis window; the fifth input of the unit (10) of the detennination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the first input of the former (36) of the background change detector primary video image signals and to the second input of the first node (42) of video image signals scaling and shift; the sixth input of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the second input of the former (36) of the background change detector primary video image signals, to the fifth input of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the third input of the unit (35) of the object/background ratio and background video image signals minimum mean square value detennination in background analysis windows; the seventh input of the unit (10) of the deteiiiiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image i$ connected to inputs of the first buffer random access memory (30), and of the second buffer random access memory (41), to the first input of the node (44) of the forming of the moving-objects indicator difference video image signals, to the fifth input of the former (36) of the background change detector primary video image signals and to the first input of the videodata commutator (31); the output of the first buffer random access memory (30) is connected to the second input of the first input of the first videodata commutator (31), which first and second outputs are connected to the third and sixth inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the fourth input of the unit (35) of the object/background ratio and background video image signals minimum mean square value detennination in background analysis windows, the first output of which is the fifth output of the unit (10) of the determination of the current coordinates of the object generalized binary video image using<br>
the generalized horizontal and vertical projections of the object generalized binary video image; the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the first output of the unit (10) of the detemiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image and to the first input of the former (33) of the histogram classifier binary video image signals horizontal and vertical projections, which output is connected to the first input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image and to the first input of the unit (34) of the histogram classifier binary video image signals confidence factor determination, which first output is connected to the second input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the output of the former (36) of the background change detector primary video image signals is connected to the input of the spatial low-pass filter (37) of the background change detector primary binary video image signals, which output is connected to the input of the former (38) of the background change detector secondary video image signals, which output is connected to the input of the former (39) of the background change detector secondary binary video image signals horizontal and veitical projections, which output is connected to the third input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image and to the first input of the unit (40) of the background change detector binary secondary video image signals confidence factor determination, which first output is connected to the fourth input of the former (50) of the generalized vertical and horizontal projections of the generalised object binary video image; the output of the second buffer random access memory (41) is connected to the third input of the first node (42) of video image signals scaling and shift, which output is connected to the input of the third buffer random access memory (43), which output is connected to the second input of the node (44) of the forming of the moving-objects indicator<br>
the generalized horizontal and vertical projections of the object generalized binary video im^gs; the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the first output of the unit (10) of the detemiination of the cuncnt coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image and to the first input of the former (33) of the histogram classifier binary video image signals horizontal 3nd vertical projections, which output is connected to the first input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image and to the first input of the analyzer (34) of the histogram classifier binary video image signals, which first output is connected to the second input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the output of the fbuiier (36) of the background change detector primary video image signals is connected to the input of the spatial low-pass filter (37) of the background change detector primary binary video image signals, which output is connected to the input of the former (38) of the background change detector secondary video image signals, which output is connected to the input of the former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, which output is connected to the third input of the former (50) of the generalised vertical and horizontal projections of the generalized object binary video image and to the fit'sl input of the analyzer (40) of the background change detector binary secondary video image signals, which first output is connected to the fourth input of the former (50) of the generalised vertical and horizontal projections of the generalized object binary video image; the output of the second buffer random access memory (41) is connected to the third input of the first node (42) of video image signals scaling and shift, which output is connected to the input of the third buffer random access memory (43), which output is connected to the second input of the node (44) of the forming of the moving-objects indicator<br>
difference video inwge signals, which output is connected to the input of the former (45) of the moving-objects indicator piimary video image signals, which output is connected to the input of the spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, which output is connected to the input of the former (47) of the moving-objects indicator secondary video image signals, which output is connected to the input of the former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, and which output is connected to the first input of the unit (49) of the moving-objects indicator binary secondary video image signals confidence factor deteunination and to the sixth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the first output of the unit (49) of the moving-objects indicator binary secondary video image signals confidence factor determination is connected to the fifth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, which first output is connected to the first input of the unit (51) of the determination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window and to the fourth output of the "nit (10) of the deteuiiination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the first and the second outputs of the unit (51) of the deteunination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window are the second and third outputs of the unit (10) of the determination of the current coordinates of the object generalized binary video image using the generalised horizontal and vertical projections of the object generalized binary video image; the third output of the unit (51) of the detei mi nation of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window is connected to the input of the analyzer (52) of the averaged binary video image coordinates automatic determination break, which first output is the seventh output of the unit (10) of the determination of the current coordinates of the object generalized binary video image<br>
difference video image signals, which output is connected to the input of the former (45) of the moving-objects indicator piimary video image signals, which output is connected to the input of the spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, which output is connected to the input of the former (47) of the moving-objects indicator secondary video image signals, which output is connected to the input of the former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, and which output is connected to the first input of the analizer (49) of the moving-objects indicator binary secondary video image signals and to the sixth input of the former (50) of the generalized vcilical and horizontal projections of the generalized object binary video image; the first output of the analizer (49) of the moving-objects indicator binary secondary video image signals is connected to the fifth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, which first output is connected to the fust input of the unit (51) of the determination of the object coordinates and sizes, of the current and averaged area of the object binary video image and to the fourth output of the unit (10) of the detennination of the current coordinates of the generated binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the first and the second outputs of the calculator (51) of the object coordinates and sizes, of the current and averaged area of the object binary video image are the second and third outputs of the unit (10) of the detei urination of the current coordinates of the object generaliTed binary video image using the generalized horizontal and vertical projections of the object generalized binary video image; the third output of the calculator (51) of the object coordinates and sizes, of the current and averaged area of the object binary video image is connected to the input of the analyzer (52) of the averaged binary video image coordinates automatic detemiination break, which first output is the seventh output of the unit (10) of the deteimination of the current coordinates of the object generalized binary video image<br>
using the generalized horizontal and vertical projections of the object generalized binary video image; the third output of the unit (35) of the objectftackground ratio and background video image signals mean square value minimum determination in M background analysis windows is connected to the sixth output of the unit (10) of the detemiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, which eighth output is connected to the second outputs of the former (32) of the histogram classifier binary video inwge signals in the current analysis window, of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination in M background analysis windows, of the unit (34) of the histogram classifier binary video image signals confidence factor detemiination, of the "nit (40) of the background change detector binary secondary video image signals confidence factor detemiination, of the unit (49) of the moving-objects indicator binary secondary video imige signals confidence factor detemiination, of the analyzer (52) of the averaged binary video image coordinates automatic deteunination break, and of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, and to the fourth output of the unit (51) of the detemiination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window.<br>
The technical result is also achieved by the fact That the former (32) of the histogram classifier binary video image signals in the current analysis window contains former (67) of the normed histograms of the intensity distribution of the signals of the object video images and node (68) of the histogiam classifier binary video image, at that, the first, the second, the third, the fourth, and the fifth inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window are connected to the first, the second, the third, the fourth, and the fifth inputs of the former (67) of the normed histograms of the intensity distribution of the signals of the object video images; the sixth input is connected to the second input of the node (68) of the histogram classifier binary video image;<br>
using the generated horizontal and vertical projections of the object generated binary video image; the third output of the unit (35) of the object/background ratio and background video image signals minimum mean square value detemiination in background analysis windows is connected to the sixth output of the unit (10) of the determination of the current coordinates of the object generated binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, which eighth output is connected to the second outputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, of the unit (35) of the object/background ratio and background video image signals minimum mean square value determination in background analysis windows, of the analyzer (34) of the histogiam classifier binary video image signals, of the analyzer (40) of the background change detector binary secondary video image signals confidence factor detemiination, of the analizer (49) of the moving-objects indicator binary secondary video image signals, of the analyzer (52) of the averaged binary video imagg coordinates automatic detennination break, and of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, and to the fourth output of the calculator (51) of the object coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window.<br>
The technical result is also achieved by the fact that the former (32) of the histogram classifier binary video image signals in the current analysis window contains calculator (67) of the normed histograms of the intensity distribution of the signals of the object video images and node (68) of the histogram classifier binary video image, at that, the first, the second, the third, the fourth, and the fifth inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window are connected to the first, the second, the third, the fourth, and the fifth inputs of the calculator (67) of the noimed histograms of the intensity distribution of the signals of the object video images; the sixth input is connected to the second input of the node (68) of the histogram classifier binary video image;<br><br>
the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the sixth input of the former (67) of the nonned histograms of the intensity dislubution of the signals of the object video images and to the third input of the node (68) of the histogram classifier binary video image; the first output of the former (67) of the nonued histograms of the intensity distribution of the signals of the object video images is connected to the first input of the node (68) of the histogram classifier binary video image, which output is the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window; the second output of the former (67) of ths normed histograms of the intensity distribution of the signals of the object video images is connected to the second output of the former (32) of the histogram classifier binary video image signals in the current analysis window.<br>
The technical result is also achieved by the fact that the former (36) of the background change detector primary video image signals contains former (70) of the reference background video image signals, the fifth (69) and the sixth (72) buffer random access memory, the third node (71) of video linage signals scaling and shift, node (73) of the background change detector difference video image signals forming, and node (74) of the background change detector primary binary video image fotming, at that, the first and second inputs of the former (36) of the background change detector primary video image signals are connected to the first and second inputs of the former (70) of the reference background video image signals and of the third node (71) of video image signals scaling and shift; the third and fourth inputs are connected to the third and fourth inputs of the former (70) of the reference background video image signals; and the fifth input is connected to the first input of the node (73) of the background change detector difference video image signals forming and to the input of the fifth (69) buffer random access memory which output is connected to the fifth input of the former (70) of the reference background video image signals; and the sixth input of the former (36) of the background change detector primary video image signals is<br>
the seventh input of .the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the sixth input of the calculator (67) of the nonned histograms of the intensity distribution of the signals of the object video images and to the third input of the node (68) of the histogram classifier binary video image; the first output of the calculator (67) of the noimed histograms of the intensity distribution of the signals of the object video images is connected to the first input of the node (68) of the histogram classifier binary video image, which output is the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window; the second output of the calculator (67) of the nouiied histograms of the intensity distribution of the signals of the object video images is connected to the second output of the former (32) of the histogram classifier binary video image signals in the current analysis window.<br>
The technical result is also achieved by the fact that the former (36) of the background change detector primary video image signals contains former (70) of the reference background video image signals, the first (69) and the second (72) buffer random access memory, the node (71) of reference background video image signals scaling and shift, node (73) of the background change detector difference video image signals forming, and node (74) of the background change detector primary biiwri/ation, at that, the first and second inputs of the former (36) of the background change detector primary video image signals are connected to the first and second inputs of the former (70) of the reference background video image signals and of the node (71) of reference background video image signals scaling and shift; the third and fourth inputs are connected to the third and fourth inputs of the former (70) of the reference background video image signals; and the fifth input is connected to the first input of the node (73) of the background change detector difference video image signals forming and to the input of the first (69) buffer random access memory which output is connected to the fifth input of the former (70) of the reference background video image signals; and the sixth input of the former (36) of the background change detector primary video image signals is<br>
connected to the third input of the node (73) of the background change detector difference video image signals forming Mid to the seventh input of the former (70) of the reference background' video image signals, which output is connected to the third input of the third node (71) of reference background video image signals scaling and shift, which output is connected to the input of the sixth (72) buffer random access memory, which output is connected to the sixth input of the former (70) of the reference background video image signals and to the second input of the node (73) of the background change detector difference video image signals forming, which output is connected to the input of the node (74) of the background change detector primary binary video image fbuning, which output is connected to the output of the ftnmer (36) of the background change detector primary video image signals.<br>
The technical result is also achieved by the fact that the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming contains the second videodata commutator (53), high-pass nonlinear filter (54), analyzer (55) of the static video reference image renewal conditions, node (61) of the video image dissimilarity measure signals minimum value sequence type forming and analysis along the lines and columns of the binary search area of the object video image shifts, the third (56) and the fourth (59) buffer random access memory, former (57) of the static and dynamic video images signals, Conner (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, the second node (58) of the object video image signals scaling and shift, commutator (62) of the video image dissimilarity measure signals minimum value sequence data, approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth power polynomial, node (64) of the object video image coordinates determination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data, node (65) of the object video image coordinates detemiination in the analysis window by the boundaries<br>
connected to the thir.d input of the node (73) of the background change detector difference video image signals forming and to the seventh input of the former (70) of the reference background video image signals, which output is connected to the third input of the node (71) of reference background video image signals scaling and shift, which output is connected to the input of the sixth (72) buffer random access memory, which output is connected to the sixth input of the former (70) of the reference background video image signals and to the second input of the node (73) of the background change detector difference video image signals forming, which output is connected to the input of the node (74) of the background change detector primary binarization, which output is connected to the output of the former (36) of the background change detector primary video image signals.<br>
The technical result is also achieved by the fact that the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming contains the commutator (53) of the scaled image signals, high-pass nonlinear filter (54), analyzer (55) of the static video reference image renewal conditions, analizer (61) of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, the first (56) and the second (59) buffer random access memory, former (57) of the static and dynamic video images signals, former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, the node (58) of the object video image signals scaling, selector (62) of the video image dissimilarity measure signals minimum value sequence, approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth power polynomial, coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial, coordinator (65) of the object video image in the analysis window by the boundary<br>
shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center, node (66) of the object video image coordinates detemiination in the analysis window after the analysis window center coordinates, at that, the first input of the unit (11) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first inputs of the analyzer (55) of the static video reference image renewal conditions and of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the second input of the mit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals foutiing is connected to the first input of the second videodata commutator (53); the third input of the "nit (11) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the second videodata commutator (53) and to the first input of the high-pass nonlinear filter (54), which output is connected to the third input of the second videodata commutator (53); the fourth input of the unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals foiling is connected to the third input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the second inputs of the former (57) of the static and dynamic video images signals and of the analyzer (55) of the static video reference image renewal conditions; the fifth input of the unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the sixth input of the unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the third input of the former (57) of the static<br>
shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center, coordinator (66) of the object video image in the analysis window after the analysis window center coordinates, at that, the first input of the unit (11) of tfie object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fomung is connected to the first inputs of the analyzer (55) of the static video reference image renewal conditions and of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the second input of the unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first input of the commutator (53) of the scaled image signals; the third input of the unit (11) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the commutator (53) of the scaled image signals and to the first input of the high-pass nonlinear filter (54), which output is connected to the third input of the commutator (53) of the scaled image; the fourth input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the third input of the foiuier (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the second inputs of the former (57) of the static and dynamic video images signals and of the analyzer (55) of the static video reference image renewal conditions; the fifth input of the unit (11) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the sixth input of the unit (11) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the third input of the former (57) of the static<br>
and dynamic video images signals; the seventh input of the unit (11) of the object video image coordinates detemiinftion relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (57) of the static and dynamic video images signals and to the fifth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the eighth input of the unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the high-pass nonlinear filter (54), to the fifth input of the analyzer (55) of the static video reference image renewal conditions, to the fourth input of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data, and to the seventh input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the output of the second videodata commutator (53) is connected to the second input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the input of the third buffer random access memory (56), which output is connected to the first input of the former (57) of the static and dynamic video images signals; the first output of the analyzer (55) of the static video reference image renewal conditions is connected to the fifth input of the former (57) of the static and dynamic video images signals, which output is connected to the input of the node (58) of the object video image signals scaling and shift, which output is connected to the input of the fourth buffer random access memory (59), which output is connected to the sixth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, which output is connected to the input of the node (61) of the forming and analysis of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, which first<br>
and dynamic video images signals; the seventh input of the unit (11) of the object video image coordinates detei mi nation relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (57) of the static and dynamic video images signals and to the fifth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the eighth input of the unit (11) of the object video image coordinates deteuiiinatton relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fbiming is connected to the second input of the high-pass nonlinear filter (54), to the fifth input of the analyzer (55) of the static video reference imflge renewal conditions, to the second input of the selector (62) of the video image dissimilarity measure signals minimum value sequence, and to the seventh input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the output of the second videodata commutator (53) is connected to the second input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the input of the first buffer random access memory (56), which output is connected to the first input of the former (57) of the static and dynamic video images signals; the first output of the analyzer (55) of the static video reference image renewal conditions is connected to the fifth input of the former (57) of the static and dynamic video images signals, which output is connected to the input of the node (58) of the object video image signals scaling and shift, which output is connected to the input of the second buffer random access memory (59), which output is connected to the sixth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, which output is connected to the input of the analizer (61) of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, which first<br>
output is connected to the second output of the unit (11) of the object video image coordinates deteun motion relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the third input of the analyzer (55) of the static video reference image renewal conditions, and to the input of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data, which first output is connected to the input of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth degree polynomial, which first output is connected to the input of the node (64) of the object video image coordinates deteimination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data; the second and third outputs of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data are connected to the inputs of the node (65) of the object video image coordinates deteimination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the node (66) of the object video image coordinates determination in the analysis window after the analysis window center coordinates; the first outputs of the node (64) of the object video image coordinates detei urination in the analysis window by the status of the approximating forth degiee polynomial of the video image dissimilarity measure signals minimum value sequence data, of the node (65) of the object video image coordinates determination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the node (66) of the object video image coordinates deteimination in the analysis window after the analysis window center coordinates are connected to the first output of the unit (11) of the object video image coordinates detennination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the fourth input of the analyzer (55) of the static video reference image renewal conditions; the third output of the unit (11) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second outputs of the node (61) of the forming and analysis of the<br>
output is connected to the second output of the unit (11) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fotming, to the third input of the analyzer (55) of the static video reference image renewal conditions, and to the first input of the selector (62) of the video image dissimilarity measure signals minimum value sequence, which first output is connected to the input of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth degree polynomial, which first output is connected to the input of the coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial; the second and third outputs of the selector (62) of the video image dissimilarity measure signals minimum value sequence are connected to the inputs of the node (65) of the object video image coordinates deteunination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the coordinator (66) of the object video image in the analysis window after the analysis window center coordinates; the first outputs of the coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial, of the coordinator (65) of the object video image in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the coordinator (66) of the object video image in the analysis window after the analysis window center coordinates are connected to the first output of the unit (11) of the object video image coordinates determination relatively to the ciurent analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the fourth input of the analyzer (55) of the static video reference image renewal conditions; the third output of the unit (11) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second outputs of the anali (61) of the<br>
video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, of the analyzer (55) of the static video reference image renewal conditions, of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth degree polynomial, of the node (64) of the object video image coordinates deteimination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data, of the node (65) of the object video image coordinates detei urination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the node (66) of the object video image coordinates deteimination in the analysis window aftec the analysis window center coordinates.<br>
The technical result in the part of assurance of coordinates deteunination and steady object image detention hi the centre of the video camera field of view or in the centre of the tracking window at reception of contrast and low-contrast images of mobile and stationary objects and terrain background from the video camera when the object moves across the terrain areas which average intensity is similar to the object average intensity, at change of the object visible pattern stipulated by its rotation in motion, illumination change, object observation in conditions of discontinuous optical communication caused by object overlapping by different barriers such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc., at controlled (mobile) and uncontrolled (stationary) video camera field of view, at motion of the surveillance system carrier that causes wideband uncontrolled displacements of the video camera field of view (image "shaking", which could cause the change in mutual position of video image field elements not as a whole), with higher dynamics of object movement is achieved by the fact that in the signal processing method for deteunination of object coordinates observed in a sequence of video images, which consists in reception and storage of signals of the current video image field, before the reception of the signals of the current video image n-field, where n=3,4,5,..., the controlled video camera field of view axis displacement is deteimined at the time between the reception<br>
video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, of the analyzer (55) of the static video reference image renewal conditions, of the approximator (63) of the video image dissimilaiity measure signals minimum value sequence by the forth degree polynomial, of the coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial, of the coordinator (65) of the object video image in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the coordinator (66) of the object video image in the analysis window after the analysis window center coordinates.<br>
The technical result in the part of assurance of coordinates deteunination and steady object image detention in the centre of the video camera field of view or in the centre of the tracking window at reception of contrast and low-contrast images of mobile and stationary objects and terrain background from the video camera when the object moves across the terrain areas which average intensity is similar to the object average intensity, at change of the object visible pattern stipulated by its rotation in motion, illumination change, object observation in conditions of discontinuous optical communication caused by object overlapping by different barriers such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc., at controlled (mobile) and uncontrolled (stationary) video camera field of view, at motion of the surveillance system carrier that causes wideband uncontrolled displacements of the video camera field of view (image "shaking", which could cause the change in mutual position of video image field elements not as a whole), with higher dynamics of object movement is achieved by the fact that in the signal processing method for determination of object coordinates observed in a sequence of video images, which consists in reception and storage of signals of the current video image field, before the reception of the signals of the current video image n-field, where n=3,4,5,..., the controlled video camera field of view axis displacement is determined at the time between the reception<br>
of (n-1) and (n-2) video image fields signals, which is detei mined by the impact of the displacement of the field of view of the tracking system video camera, speed of the controlled tracking system video camera field of view displacement is detei mined Irom the controlled tracking system video camera field of view displacement data at the time between the reception of (n-1) and (n-2) video image fields signals, reception and storage of the signals of the uncontrolled displacement and rolling of the field of view of the tracking system video camera are performed sinwlaaneously with the reception of the current video image field signals and they are used for current video image frame forming, forming of the video image signals of the current n-frame from image signals of the previous frame with allowance for controlled field of view axis displacement at the time between reception of signals of the (n-1) and (n-2) video image fields and signals of the current video image field with bit transfbuiiation of signals coordinates of the current video image field which compensate for the video camera field of view uncontrolled displacements and roll; the video image signals in the current analysis window are founed from the current video image frame and scaled using location and dimensions of the current video image field analysis window signals obtained after processing of the former (n-1) video image frame, at that the initial values of location and dimensions of the video image field analysis window signals are formed with initial values of location and dimensions of object video image, received from surveillance system with the tracking initialition, signals of the scaled video image in the current analysis window are stored; forming of differential video image signals of the moving-objects indicator by subtraction the scaled video image signals, which were stored and coutcted to the current video image scale in the analysis window and shifted on the value of the tracking system video camera field of view axis displacement, from the scaled video image signals in the current analysis window; forming of the primary binary image signals of the moving-objects indicator from the differential video image signals of the moving-objects indicator; forming of the secondary binary image signals of the moving-objects indicator from the primary<br>
primary video imnge signals of the moving-objects indicator exposed to low-pass filtering; the primary binary imnge signals of the background change detector and binary image signals of the histogiam classifier in the current analysis window are fomied simultaneously with the storage of the scaled video image signals in the current analysis window, with the forming of the primary and secondary binaiy image signals of the moving-objects indicator from the scaled video image signals in the cuuetit analysis window signals with allowance to the tracking system video camera field of view axis controlled displacement; the secondary binary image signals of the background change detector are fomied from the primary binary image signals of the background change detector exposed to low-pass filtering; horizontal and vertical projections of the secondary binary images signals of the moving-objects indicator and background change detector, and horiyontal and vertical projections of binary image signals of the histogiami classifier are founed; confidence factors of the secondary binary images signals of the background change detector and moving-objects indicator, as well as of binary image signals of the histogtam classifier are defined; the generalized horizontal and vertical signal projections of the object generalised binary image are formed from horizontal and vertical signal projections of the secondary binary images of the moving-objects indicator and background change detector, as well as from horizontal and vertical signal projections of the histogram classifier binary image on the basis of their joint processing which uses confidence factors of binary images signals of the histogiam classifier, the secondary binary images signals of the background change detector and moving-objects indicator; horizontal and vertical boundaries as well as the object image sizes at the cut-off levels on all four sides of the assigned area percent of generalized horizontal and vertical signals projections of the object generalized binary image are defined; the current and average areas of the object generalized binary image<br>
located inside the fcaned object image boundaries are defined; the current coordinates of the object generalized binary image are defined using generalized horizontal and vertical projections of the object generalized binary image; the current traverse speed of the object generalized binary image in the inertia! coordinates system is defined; the confidence factor of the object generaliyed binary irnage current traverse speed in the inertial coordinates system is defined; simultaneously with forming of signals of the primary and secondary binary images of the moving-objects indicator and background change detector, binary image of the histogram classifier, as well as generalized horizontal and vertical signal projections of the object generalised binary image, object image coordinates are defined in relation to the centre of the current analysis window on the basis of forming of images dissimilarity measure signals as a result of non-linear high-pass filtering of scaled image signals in the current analysis window, fulfilled on condition that the average area of the object generalized binary image exceeds the threshold value; storage of received signals; generation of static object reference image signals or static and dynamic object reference images signals; reduction of static object reference image signals or static and dynamic object refetrence image signals to the current scale; forming and storage of the signals of the dissimilarity measure between the video image signals after the nonlinear high-pass filtering of the scaled video image signals in the current analysis window and static and dynamic object reference images signals in the two-dimensional search area of object video image shifts; definition of minimum values of video images dissimilarity measure signajs along the lines and columns of two-dimensional search area of object video image shifts; forming of minimum values sequences of images dissimilarity measure signals along the lines and columns of two-dimensional search area of object image shifts; determination of the appropriate object image coordinate<br>
in the analysis window according to the type of minimum values sequence of images dissimilarity measure signals for this coordinate, namely, by means of analytical approximation of the minimum values sequence of images dissimilarity measure signals by the fourth degiee polynomial and detemunation of object image coordinate as the approximation polynomial minimum position on condition that the sequence is related to the type of sequences with two boundaries of values fast growth areas of images dissimilarity measure signals close to the position of its minimum, or by deteunination of the boundary shift of values fast growth area of images dissimilarity measure signals in relation to the analysis window centre and object coordinate forming in the analysis window as an amount proportional to the obtained boundary shift of values fast growth area of images dissimilarity measure signals on condition that tke sequence is referred to the type of sequences with plane neighbourhood of the minimum position and existence of one values fast growth area of images dissimilarity measure signals, or by forming of the object coordinate equal to the coordinate of the analysis window centre on condition that the sequence is referred to the type of sequences with plane neighbourhood of values minimum position of images dissimilarity measure signals in the whole search area of object image shifts; the current object image traverse speed in the inertial coordinates system is defined using object image coordinates, obtained on the basis of forming of images dissimilarity measure signals; the confidence factor of the current object image traverse speed in the ineilifll coordinates system obtained on the basis of forming of images dissimilarity measure signals is calculated; the complex estimate of the current object image traverse speed in the inertial coordinates system is fouued from estimate data of the current object image traverse speed got on the basis of forming of image dissimilarity measure signals, and estimate of the current object traverse speed across the generalized signals projections<br>
of the generalized binary object image with allowance for confidence factors which fouii the object image traverse speeds and prior restrictions of the object manoeuvring speed; the complex estimate signals of the object image current traverse speed in the inertial coordinates system are averaged and stored; the object image coordinates in the video camera field of view are defined by difference integration of the complex estimate of the current object image traverse speed in the inertial coordinates system and controlled displacement speed of the field of view axis in the inertial coordinates system, at that the initial coordinates of the object video image in the tracking system video camera field of view are received from the tracking system; with initial conditions formed at start of the object tracking; M windows of background analysis, at that M= 4,5,6,..., are formed along the analysis window perimeter and signals projections of the histogram classifier binary images are deteimined in M windows of background analysis simultaneously with forming of the primary fnl secondary binary images signals of the moving-objects indicator and background change detector, binary image of the histogram classifier, generated horizontal and vertical signals projections of the generalized binary object image; the areas and coordinates of binary images bonndaries of the histogiam classifier in M windows of background analysis are defined after the obtained signals projections of the histogram classifier binary image in M windows of backgjound analysis; management signals to control the video camera field of view axis displacement are foiuied using object image coordinates in the video camera field of view obtained in the result of image processing in the current analysis window or using the extrapolated coordinates and object image traverse speed according to results of analysis of the current and average area of the generalized binary object image, current and average object image traverse speed, area and coordinates of binary images boundaries of the histograjn classifier in M windows of background analysis, at that, the extrapolated object image travcase speed is fomied on the basis of analysis of the stored values of the average complex estimate of the object image current traverse speed;<br>
signals of the position and dimensions of the video image analysis window for the following frame are fomied using the signals of the object video image coordinates in the field of view of the tracking systeai video camera and object video image dimensions, obtained as a result of the former frame processing, at that the initial values of the signals of the position and dimensions of the object video irmge and the signal of the start of the tracking are received from the tracking system.<br>
The technical result is also achieved by the fact that, before the reception of the current n-field of the video image, where n=3, 4, 5,..., the controlled displacement dx[n] and dy[n] of the video camera field of view axis at the time between reception of the (n-1) and (n-2) video image fields signals horizontally and vertically is defined by calculating the convolution of management signals XMANW* YMAN[I] which control the video camera field of view displacement with pufee characteristics hx[i] and hy[i] of its drives<br>
(Equation Removed) <br>
where XMAN[I], YMAN[I] are the management signals of the video camera field of view displacement horizontally and vertically, formed as a result of processing of the video image i-field;<br>
hx[i] is the pulse characteristic of the horizontal drive of the tracking system video camera field of view, which is the response of the increment of the angular coordinates of the tracking system video camera field of view to the horizontal management signal impact, which is constant at the interval of the first video image field reception and equal to zero at the other periods of time, at the time between the reception of the video image i and (i-1) fields;<br>
hy[i] is the pulse characteristic of the vertical drive of the tracking system video camera field of view, which is the response of the increment of the angular coordinates of the tracking system video camera field of view to the vertical management signal impact, which is constant at the interval of the first video image field reception and equal to zero at the other periods of time, at the time between the reception of the video image i and (i-1) fields;<br>
K is the pulse characteristic length, which is the number of video image fields, at the en4 of which pulse characteristic module is within the limits of the set level. The technical result is also achieved by the fact that foinring of video image signals LFRAME(JX jy) of the current video image frame at interlacing from signals LH-FRAME(i,P,npk) of the video image current half-frame and video image L -iFRAMe(ixjy) of the former frame is executed by prognostication of signals LFRAME(IX* jy) of the current video image frame with the help of the foioier video image frame shift L -iFRAME(ixjy) by the amount of controlled displacement dx, dy of the video camera field of view axis horizontally and vertically at the time between reception of video image half-frames<br>
LpRAME(ixjy) = L -iFRAME(ix+dx,jy+dy),<br>
and by substitution of the current frame video image pixels with the current half-frame video image pixels with compensation for the current uncontrolled shifts rx(p), ry(p) and roll cp(p) of thd tracking system video camera field of view<br>
LFRAME   ix(i,p,npk), jy(i,p,npk)     = L H-FRAME<i></i>
where i is the element number in the current half-frame video image line, i=l,..., NK, ix is the element number in the current frame video image line, ix=l,..., NK,<br>
p is the line number in a half-frame. p= 1,...,<br>
jy is the line number in a frame, jy=l,.., MK, NK is the number of video image elements in a line, MK is the number of lines in an video image frame, npk is the current half-frame index,<br>
npk = 1 - in uneven half-frames,<br>
npk = 0 - in even half-frames,<br>
(Equation Removed) <br>
The technical result is also achieved by the fact that signals LI BIN BCD (ixjy) of the primary binary image of the background change detector are fonned by adjustment of reference background image signals Ln-i mb(ix,iy) obtained in the former n-1 frame to the current scale, by fonnating of differential image signals Lp BCD (ixjy) of the background change detector; by subtraction of reference background image signals Ln-i mb (ix jy) of the former frame from scaled image signals LnAw(ixjy) in the current analysis window with a shift which accounts for displacement Vx, Vy of the analysis window center in the inertial coordinates system at the last frame:<br>
Lp BCD (ixjy) = Ln AW (ixjy) - Ln., mb (ix+Vx,jy+Vy),<br>
by deteiiuination of the binari/ation threshold THRESHOLDBCD(ix,jy) of the background change detector as a value proportional to the local values spreading parameter of the differential image Lp BCD (ixjy) of the background change detector in neighborhood of the pixel with coordinates ixjy; by assigning of values to the primary binary image LI BINBCD(ixjy) of the background change detector<br>
LIBINBCD(ixjy)= 1, if  LpBCD(ixjy)   &gt; THRESHOLDBcD(ix jy),  or LIBINBCD(ixjy) = 0, if  LpBCD(ixjy)   
where ix, jy are the coordinates of the scaled video image signals relatively to the center of the current analysis window,<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window,<br>
at that, the reference background video image signals Ln mb (ixjy) are formed by sharing the<br>
scaled video image signals in the current analysis window into three types of image signals:<br>
video image signals in the object window	- OWBCD,<br>
video image signals in the background window	- BWBCD,<br>
video image signals in the window - "New Background"	- NB,<br>
where video image signals in a rectangle which lies in the center of the current analysis window and includes predominantly the object image elements are defined as video image signals in the object window: image elements on outer boundaries of the current analysis window where new background video image elements appear on account of object movement and the video camera field of view displacement are defined as video image signals of the window "New Background";<br>
all the rest image elements of the analysis window are defined as video image signals of the background window; by storage of signals Ln AW (ix jy) from the current analysis window of the current n-frame scaled video image in the window "New Background":<br>
Ln mb (ixjy) = Ln AW (ix jy),    ix jy e NB,<br>
where Ln AW (ixjy) are signals values of the scaled video image element intensity in the analysis window with coordinates ixjy; by averaging of scaled video image signals Ln AW (ixjy) in the background window from the current analysis window with the constant Wbw and with account for analysis window shift in the inertial coordinates system at the last frame:<br>
Ln mb (ixjy)=(l-Wbw)*Ln.i mb(ix+Vxjy+Vy)+Wbw*Ln AW(ixjy)   ixjyeBWBCD, where Vx, Vy is the analysis window center displacement at the last frame in the inertial coordinates system horizontally and vertically by re-recording in the object window of reference background video image signals of the former frame with a shift which accounts for the analysis window center displacement at the last frame:<br>
Ln mb (ixjy) = Ln.i mb(ix+Vx, jy+Vy)   ix jyεOWBCD<br>
The technical result is also achieved by the fact that the low-pass filtering of the primary binary video image signals LI BIN BCD(ix jy) of the background change detector is executed with the help of two-dimensional convolution<br>
(Equation Removed) <br>
where ixjy are the coordinates filtered video image signals S_filBcD(ixjy) relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
NF and MF are the horizontal and vertical parameters of the low-pass filter aperture;<br>
di, dj are the horizontal and vertical internal variables of the low-pass filter aperture,<br>
respectively, diε[-NF, NF], djε[-MF, MF];<br>
h|.p [di, dj] is pulse characteristic of the low-pass filter.<br>
The technical result is also achieved by the fact that the low-pass filtering of the primary binary video image signals LI BIN MOi(ix, jy) of the moving-objects indicator is executed with the help of two-dimensional convolution<br>
(Equation Removed) <br>
where ixjy are the coordinates filtered video image signals S_filMoi(ixjy) relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window; NF and MF are horizontal and vertical parameters of the low-pass filter aperture, di, dj are the horizontal and vertical internal variables of the low-pass filter aperture, diε[-NF, NF], djε[-MF, MF]; h1-p [di, dj] is pulse characteristic of the low-pass filter.<br>
The technical result is also achieved by the fact that the secondary binary video image signals La BIN BCD(ix jy) of the background change detector are fonned from the low-pass filter signals S_filBCD(ixjy) according to the rule:<br>
L2 BIN BCD(IXjy)=l,    if S_filBcD(ix jy) &gt; PorogBCDl and LI BIN BCD(ix jy) = 1 or S_filBcD(ix jy) &gt; PorogBCDO and LI BIN BCD(IXjy) = 0, otherwise L2 BIN BCD(ix jy) = 0,<br>
where ix, jy are the coordinates of the secondary binary video image signals L2 BIN BCD(ix jy) of the background change detector relatively to the center of the current analysis window;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
PorogBCDl, PorogBCDO are values of the decision thresholds for unit and zero elements of the primary binary video image of the background change detector.<br>
The technical result is also achieved by the fact that the secondary binary' video image signals L2BINMOi(ix, jy) of the moving-objects indicator are formed from the low-pass filter signals S_filMoi(ixjy) according to the rule:<br>
La BIN M0i(ix jy)=l,   if S_fil MOi(ix jy) &gt; Porog Moil and LI BIN Moi(ix jy)=l or S_fil M0i(ix jy) &gt; Porog Moi 0 and Lt BIN MOi(ix jy)=0, otherwise L2 BIN MOi(ixJy) = 0.<br>
where ix, jy are the coordinates of the secondary binary video image signals L-i BIN MOI(IX jy) of the background change detector relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
PorogMoil, PorogMOI0 are values of the decision thresholds for unit and zero elements of the primary binary video image of the moving-objects indicator.<br>
The technical result is also achieved by the fact that binary video image signals LBIN Hc(ixjy) of the histogram classifier are formed according to the rule:<br>
LBINHc(ixjy)=l, if Wn-iNcow[Ln Aw(ix jy)]&gt;an.i*A(ix jy)* Wn.iNHCBw[Ln Aw(ixjy)], LBIN Hc(ix jy)=0 - otherwise,<br>
where ix, jy are the coordinates of the secondary binary video image signals LBIN nc(ix jy) relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
Ln Aw(ix jy) are scaled video image signals in the current analysis window,<br>
Wn-iNcow [Li] is a normalized intensity distribution histogram L, of scaled video image<br>
signals in the central object window - COW, obtained in the former (n-1) frame,<br>
Wn-iNHCBW [Li] is a normalized intensity distribution histogram of background video<br>
image signals in the histogram classifier background window - HCBW, obtained in the<br>
former (n-1) frame,<br>
Li is the intensity level of video image signals,<br>
i is the number of the intensity level of image signals, i = 1, ..., N!ev,<br>
an.i is a parameter which depends on the number of video image elements in the<br>
histogram classifier object window - HCOW, classified as background in the former<br>
(n-1) frame,<br>
A(ix jy) is a penalty function which depends on video image element coordinates ix jy in<br>
the analysis window, including ixjyεCOW A(ix,jy) = A0;<br>
at that, after the object video image coordinates X0, YO and sizes Rxo, Ryo in the current n-frame are determined, the rectangle with dimensions Rxcow=Rxo, Rycow=Ryo which lies inside the analysis window and which center coincides with the object video image center X0, YO and which includes predominantly object video image elements, is defined as the central object window - HCOW; the area between two rectangles with common center and dimensions Rxcow, Rycow and RXHCOW, RyHCOw, where RxHcow&gt;Rxcow and RyHcow&gt;Rycow is defined as the histogram classifier object window - HCBW; the area between two rectangles with common center and dimensions RXHCOW, Ryncow and RXRCBW, RyHcew, where RXNCBW&gt;RXHCOW and RyHCBW&gt;RyHCOw is defined as the histogram classifier background window - HCBW. The histogiam WncBw[Li] of background image intensity distribution L, is defined after video image signals LI, AW(IXjy), read from the COW window in the current n-frame; the histogram Wcow[Lj] of object image intensity distribution L, is defined after image signals selected from the COW window in the cuucnt n-frame; the histograms WHCBW[LI] and Wcow[Lj] are being smoothed<br>
(Equation Removed) <br>
where hsm [j] is pulse characteristic of the smoothing filter;<br>
(2*ns+l) is the number of points of the pulse characteristic of the smoothing filter; the current averaging threshold Thh of the smoothed histogram WcowfL,] of object image intensity distribution is defined according to the expression<br>
(Equation Removed) <br>
where Thho and kthh are the constant values,<br>
1 [x] is a unit function defined by conditions:<br>
1 [x] = 1 at x &gt; 0,<br>
1 [x] = 0 at x 
Th h n ~ Th h n-l + Yn thresh *( Thh n - Th h n-l),<br>
where yn thresh is the constant of the threshold Thh averaging filter which varies according to the frame number from the value j\ thresh=l in the first frame up to the stationary value ythresh; n is the current image frame number;<br>
the values of the aveiage threshold are restiicted from above and below, the smoothed histogram<br>
Wcow[Li] of the object video image intensity distribution L, is averaged by the first order<br>
recursive filter<br>
W ncow[Li] = W „., cow[Li] + yn cow*( WcowfL,] - W „., Cow[Li]), where Yncow is the constant of the averaging filter of the object video image intensity<br>
disuibution histogram which varies according to the frame number from the value<br>
Yicow-1 in the first frame up to the stationary value ycow, at that starting from the frame<br>
number which exceeds NfC, where Nfc is the number of the frame after which the averaging of the histogram W cow[Lj] is<br>
performed according to conditions, Nfc=16,.. .,128;<br>
the histogram W cow[LiJ of the object image intensity distribution L, is averaged only for those intensity levels L, which simultaneously fulfill two conditions:<br>
W cow[Lj] &gt; Th h „   and   W cow[Lj] &gt; an*A0* W HcBw(L,), the normalized histogram WnNcow[Lj] of the object image intensity distribution is formed from<br>
the average histogram W ncow[Li] of the object image intensity distribution L,<br>
(Equation Removed) <br>
where N|CN is the number of image intensity levels Ln Aw(ixjy):<br>
the normalized histogram Wn HCBW[LI] of background image intensih<br>
distribution is fouiied from the smoothed histogram W HCBW[L,] of the object image intensity distribution:<br>
(Equation Removed) <br>
The technical result is also achieved by the fact that the horizontal and vertical projections Gpr MOI(IX), Vpr MOi(jy) of the secondary binary video image signals L2 BIN MOI(IX jy) of the moving-objects indicator are defined as follows<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the of the secondary binary video image signals L2 BIN MOI(ix jy) of the background change detector relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
Nwin and Mwin are horizontal and vertical dimensions of the secondary binary image of<br>
the moving-objects indicator.<br>
The technical result is also achieved by the fact that the horizontal and veitical projections GprBCD(ix), VprBcrX(jy) °f the secondary binary video image signals L2 BIN BCD(IX,jy) of the background change detector are defined in the following way<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the oi the secondary binary video image signals L2 BIN BCD(IXjy) of the background change detector relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
Nwin and Mwin are horizontal and vertical dimensions of the secondary binary video<br>
image of the background change detector.<br>
The technical result is also achieved by the fact that the horizontal and vertical projections GprHc(ix), VprHc(jy) of binary image signals of the histogram classifier LBINHc(ixjy) are defined in the following way<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the of the secondary binary video image signals LBIN Hc(ixjy) of the histogram classifier relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
Nwin and Mwin are respectively horizontal and vertical dimensions of the histogram<br>
classifier binary image.<br>
The technical result is also achieved by the fact that the confidence factors WBCD, WMQI, WHC are defined as composition of functions of initial conditions input and the normalized average densities of binary video images of the background change detector, moving-objects indicator and histogram classifier,<br>
(Equation Removed) <br>
at that, the average densities   VBCD(n).   VMOI(n),   VHC(n)  of binary video images of the<br>
background change detector, moving-objects indicator, and histogram classifier are got as a result of minimum and maximum values limitation and further averaging of the current densities VBCD(n), VMOI(n), Vnc(n) of the appropriate hinan<br>
video images by the first order recursive filters<br>
(Equation Removed) <br>
SoBCD(n), SoMoiCn), S0Hc(n) are current areas of binary video images of the background<br>
change detector, moving-objects indicator and histogram classifier inside the object<br>
image boundaries,<br>
SoR(n) is the current area of a region inside the object video image boundaries,<br>
Fjc_BCD(n),  Fic_MOi(n),  Fic_nc(n)  are  the  initial  conditions  input  functions  of the<br>
background change detector, moving-objects indicator and histogram classifier.<br>
n is the cuu ent frame number.<br>
The technical result is also achieved by the fact that the generalized horizontal and<br>
vertical signal projections Gpro(ix,n), Vpro(jy,n) of the generalized binary object video image are<br>
formed by weighted summation of binary video image projections of the background change<br>
detector, moving-objects indicator and histogram classifier:<br>
GprG(ix,n)=WBCD(n)*Gpr BCD(ix)+WMoi(n)*Gpr MOi(ix)+WHC(n)*Gpr Hc(ix), Vpr o(jy,n)=WBCD(n)* Vpf BCD(jy)+WMor(n)*Vpr MOi(jy)+WHC(n)* Vpr HC(jy).<br>
where ix, jy are the coordinates of the video image signals relatively to analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
WHc(n), WBCD(n), WMOi(n) are the confidence factors of the histogram classifier binary<br>
video image signals, background change detector and moving-objects indicator secondary<br>
binary video image signals;<br>
GprHc(ix) and VprHc(jy), GprBcD(ix) and  VprBcD(JyX GprMoi(ix) and VprMOi(jy) are<br>
horizontal and vertical signal projections of the histogram classifier binary video image<br>
signals, background change detector and moving-objects indicator secondary binary video<br>
image signals;<br>
n is the current frame number.<br>
The technical result is also achieved by the fact that the current coordinates XGBIN, YGBIN of the generalized binary object video image are defined as a weighted sum of gravity center coordinates XGGO YGGC and area median coordinates XGMED, YGMED of the generalized binary object video image<br>
XGBIN(n) = WGC(n) * XGGC + WMED(n) * XGMED, YoBIN(n) = WGC(n) * YGQC + WMED(n) * YGMED, WMED(n)=l-WGc(n),<br>
where WMEo(n), WGC(n) are the weighting coefficients of the median and the gravity center coordinates estimates of the generalized binary object video image; n is the current frame number,<br>
at that, the weighting coefficient Woc(n) is increased at reduction of object coordinates video image mean deviation from their predictable values.<br>
The technical result is also achieved by the fact that the current horizontal VGoe BIN and vertical WOB BIN estimate components of the binary object video image traverse speed in the inertial coordinates system are defined according to expressions VGoB BIN - (dX + ΔXAW + ΔXoB AW BIN)/T, VVOB BIN = (dY + ΔYAW + ΔYOB AW BIN)/T,<br>
where dX, dY is horizontal and vertical axis displacement of the video camera field of view at the time T between reception of the current and previous video image fields; ΔXAw, ΔYAw is horizontal and vertical repositioning of the analysis window in the current frame in relation to the former one,<br>
AXoB AW BIN, ΔYoB AW BIN is horizontal and vertical change of binary object image coordinates in the analysis window in the current frame in relation to the former one.<br>
The technical result is also achieved by the fact that the nonlinear high-pass filtering of scaled video image signals in the current analysis window is executed according to the expression<br>
(Equation Removed) <br>
where ix, jy are the coordinates of the video image signals relatively to analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
FNF [ L ] is the function of two-sided delimitation,<br>
FNF [ L ] = LTHRESH     at L &gt; LTHRESH,<br>
FNF [ L ] = L	at -LTHRESH<i thresh></i>
FNF [ L ] = -LTHRESH     at L 
LTHRESH is the threshold level of the two-sided delimitation function.<br>
(Equation Removed) <br>
KNF, KNPF are the constant coefficients,<br>
di, dj are the internal variables of the high-pass filter;<br>
NF, MF are horizontal and vertical parameters of the filter aperture.<br>
The technical result is also achieved by the fact that signals of the object dynaraie reference video image are foimed by means of reading signals in each frame from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image, and with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XOB FOV - XAW FOV, YC MAS = YOB FOV - YAW FOV, where XOB FOV, YOB FOV are object video image coordinates in the field of view,<br>
XAW FOV, YAW FOV are analysis window coordinates in the field of view.<br>
The technical result is also achieved by the fact that signals of the object static reference video image are fouiied by means of reading and storage of video image signals from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS - XOB FOV - XAW FOV, YC MAS = YOB FOV - YAW FOV, where XOB FOV, YOB FOV are object video image coordinates in the tracking system video cameit<br>
field of view;<br>
XAW FOV, YAW FOV are the coordinates of the center of the analysis window in the tracking<br>
system video camera field of view;<br>
meeting the conditions of the object static reference video image change formed on the basis of analysis of parameters of dissimilarity measures signals of image signals after the nonlinear high* pass filtering in the current analysis window and static and dynamic object reference video image signals, as well as object video image trajectory parameters analysis obtained on the basis of analysis of static and dynamic object reference video image signals being used.<br>
The technical result is also achieved by the fact that signals of the object static reference video image are formed by means of reading and storage of video image signals from the current analysis window in a rectangular window with dimensions equal to dimensions of the generalized object binary video image with the center which coordinates XCMAS, YCMAS are defined by the difference<br>
Xc MAS = XOB FOV - XAW FOV, YC MAS = YOB FOV - YAW FOV, where XOB FOV, YOB FOV are object video image coordinates in the tracking system video camera<br>
field of view;<br>
XAW FOV, YAW FOV are analysis window coordinates in the tracking system video camera<br>
field of view;<br>
meeting the conditions of the object static reference video image change formed on the basis of analysis of parameters of dissimilarity measures signals of video image signals after the nonlinear high-pass filtering in the current analysis window and static and object dynamic reference image signals, as well as object video image trajectory parameters analysis obtained on the basis of analysis of dissimilarity measures signals.<br>
The technical result is also achieved by the fact that the current horizontal VGos DISSIM and vertical WOB DISSIM estimate components of the binary object video image traverse speed<br>
in the inertial coordinates system obtained on the basis of forming of video images dissimilarity measure signals are defined according to expressions<br>
VGOB DISSIM = (dX + ΔXAW + ΔXOB AW DISSIM)/T. VVOB DISSIM = (dY + A YAW + ΔYOB AW DISSIM)/T, where dX, dY is a displacement of the axis of the tracking system video camera field of view at<br>
the time T between reception of the current and former video image fields,<br>
AXAW, ΔY AW is horizontal and vertical change of the analysis window position in the<br>
current frame in relation to the former one,<br>
AXoB AW DISSIM, ΔY0B AW DISSIM is horizontal and vertical change of object coordinates in<br>
the analysis window in the current frame in relation to the former one, obtained on the<br>
basis of forming of video images dissimilarity measure signals.<br>
The technical result is also achieved by the fact that the confidence factor WBIN(n) of the object generalized binary video image current traverse speed is calculated by defining the current<br>
density VBIN(n)=  SGBIN       of the generalized binary video image; by limiting the minimum and S0R(n)<br>
maximum values of the current density VBIN(n) of the generalized binary video image; further averaging of the generali/ed binary video image delimited density by the first order recursive filter; nounalizing the average density VBIN (n) of the object generalized binary video image<br>
(Equation Removed) <br>
where SGBIN(n) is the current area of the generalized binary video image inside the object video image boundaries;<br>
Soa(n) is the current area of the region inside the object video image boundaries, n is the current frame number, VSIM (n) is the average similarity factor obtained after calculating<br>
(Equation Removed) <br>
minimum values and averaging by the first order recursive filter;<br>
ofBmin(n) is the minimum value of the mean-square value of background video image<br>
signals in M windows of background analysis;<br>
EDISSIMmin(n) is the minimum value of video images dissimilarity measure signals of the<br>
current analysis window and static template in two-dimensional search area of object<br>
video image shifts.<br>
The technical result is also achieved by the fact that the confidence factor WsiM(n) of the object video image current traverse speed, obtained on the basis of forming of video images dissimilarity measure signals, is obtained by calculating the current similarity factor<br>
(Equation Removed) <br>
where n is the current frame number;<br>
cfBmin(n) is the minimum value of the mean-square value of background video image signals in M windows of background analysis;<br>
EoissiMmin (n) is the minimum value of video images dissimilarity measure signals of the current analysis window and static video reference image in two-dimensional search area of object video image shifts; VBIN(n) is the average density of the generalized binary object video image obtained<br>
(Equation Removed) <br>
video image, after limiting the minimum and maximum values of the current density VB|N(n)<br>
of the object generalized binary image; and further averaging of the delimited current<br>
density VBIN(n) of the object generalized binary video image by the first order recursive<br>
filter;<br>
SGBIN(H) is the current area of the generalized binary video image inside the boundaries of<br>
the object generated binary video image;<br>
SoR(n) is the current area of the region inside the boundaries of the generalized binary<br>
video image of the object.<br>
The technical result is also achieved by the fact that the complex estimate of horizontal VGOB(n) and vertical VVOB(n) constituents of the object video image current traverse speed in the inertial coordinates system is defined limiting the estimates of the binary object video image traverse speed and object image traverse speed obtained on the basis of forming of video images dissimilarity measure signals, by the minimum and maximum values foimed with allowance for preceding values of the complex estimate of the object video image current traverse speed, and forming the weighted sum of delimited estimates of the binary object video image traverse speed and object video image traverse speed obtained on the basis of forming of video images dissimilarity measure signals<br>
VGoe(n) = WBIN(n) * VGOB BIN(n) + WSiM(n) * VGOB DISSIM(n), VVOB(n) = WBIN(n) * VVOB BIN(n) + WS!M(n) * WOB Diss.M(n), where WBIN(n), WSJM(H) are confidence factors of the generalized binary object video image<br>
current traverse speed and the object video image current traverse speed obtained on the<br>
basis of fomung of video images dissimilarity measure signals,<br>
VGoB BIN(n) and WOB BIN(n) are delimited horizontal and vertical constituents of the<br>
generalized binary video object image current traverse speed;<br>
VGoB DISSIM(H) and WOB DISSIM(n) are limited horizontal and vertical constituents of the<br>
object video image current traverse speed obtained on the basis of forming of video<br>
images dissimilarity measure signals,<br>
n is the current frame number.<br>
The technical result is also achieved by the fact that the analysis of the current SGBIN(a) and average S GBIN(n) area of the object generalized binary video image is executed by testing of the condition satisfaction:<br>
SGBIN<x ksl for change forming of management signals to control the video camera field view axis displacement after extrapolated coordinates or condition></x>
SGBIN(n) &gt; ks?(n-nex)* S GBIN(n) for change for forming of tracking system video camera field of view axis displacement management signals after object video image coordinates in the tracking system video camera field of view obtained as a result of video image processing in the current analysis window, where ksl is the constant coefficient, ksl<l></l>
ks2(n-nnx) is the coefficient which diminishes with growth of the frame number n,<br>
starting from the flame number ngx, change for forming of video camera field of view<br>
axis   displacement management signals after   the   extrapolated   coordinates, ks2(n*<br>
HEX) 
The technical result is also achieved by the fact that the analysis of the current and average object video image traverse speed is executed by test of the condition satisfaction: VGCOB<n vg cob> kvl * VG COB(n) + kv2, VVcou(n) - VV coB(n) &gt; kvl * VV COB(n) + kv2,<br>
-	for change for forming of video camera field of view axis displacement management signals<br>
after the extrapolated coordinates, or the conditions satisfaction:<br>
VGcoB(n) - VG coB(n) 
-	for change for forming of tracking system video camera field of view axis displacement<br>
management signals after object video image coordinates in the tracking system video camera<br>
field of view obtained as a result of video image processing in the current analysis window,<br>
where VGcoB(n), VVCoB(n) are horizontal and vertical constituents of the object video image<br>
current traverse speed obtained as a result of filtering of the of the first order of the complex<br>
estimate constituents VGoB(n), VVoB(n) of the object video image traverse speed with the filter<br>
constant 0<wv1></wv1>
VGcoB(n) = VGcoB(n-l) + WV1*[ VGOB(n) - VGCOB(n-l)], VGcoB(n) = VGcoe(n-l) + WV1*[ VGOB(n) - VGCoB(n-l)],<br>
VG coB(n), VVcoB(n) are horizontal and vertical constituents of the object video imgge average traverse speed obtained as a result of filtering of the first order of the complex estirofrtp constituents VGoB(n), VVoe(n) of the object video image traverse speed with the filter constant 0<wv2></wv2>
VGcoa(n) = VG coe(n-l) + WV2*[ VGOB(n) - VG cos(n-l)], VG coB(n) = VG coB(n-l) + WV2*[ VGOB(n) - VG coB(n-l)], atthatWVl&gt;WV2.<br>
The technical result is also achieved by the fact that analysis of the area and coordinates of object video images boundaries of the histogram classifier inside the M windows of background analysis is executed by testing of the satisfaction of the conditions of histogianfi classifier binary video image area changes detection inside the M windows of background analysis:<br>
|Si(n)-Si(n)|&gt;S,hreshoid(n),<br>
where Sj(n) are the current values of the histogram classifier binary video images current area inside the i-window of background analysis, i = 1 ... M;<br>
S j(n) are the averaged values of the histogram classifier binary video images current area inside the i-window of background analysis, i=l,...,M, obtained at the output of the filter of the first order with the filter constant 0<ws></ws>
Sj(n)= S j(n-l) + WS*[Sj(n) - S ,(n-l)],<br>
Sthreshoid(n) is the threshold of the change detection of the binary video images of the histogram classifier in the M background analysis windows:<br>
(Equation Removed) <br>
where k is the constant coefficient,<br>
S GBIN(II) is the average area of the object generalized binary video image, obtained at the output of the first order recursive filter<br>
with the filter constant 0<wsob></wsob>
SGBIN(n) = SGBIN(n-l) + WSOB*[ SGBIN(n) - S GBIN(n-l)]; n is the current frame number,<br>
(Equation Removed) <br>
where   L0 is the average value of video image signals in the object window;<br>
LB is the average value of video image signals in the background window;<br>
ΣB is the root-mean-square deviation of video image signals in the background window<br>
from LB;<br>
FCONSTR(Q) is the constraint function of minimum and maximum values;<br>
when fulfilling the mentioned above conditions for the current area of the histogram classifier binary video images in one or several background analysis windows with ki numbers, where ki are the numbers of the background analysis windows in which the change of the<br>
histogram classifier binary video image current area was detected, ki=l,.. .,kN;<br>
kN is the number of the background analysis windows, in which the change of the<br>
histogram classifier binary video image current area was detected, kN M; the coordinates of two inter-peipendicular boundaries of the histogram classifier binary video image placed on the side of the object window are analyzed in these windows, at that, the horizontal displacement of the binary video image vertical boundary is analyzed in background analysis windows placed on the side of the object window vertical boundaries and the membership of horizontal boundary coordinates to the object window vertical coordinates is tested; the vertical displacement of the binary video image horizontal boundary is analyzed ia background analysis windows placed on the side of the object window horizontal boundaries and the membership of vertical boundary coordinates to the object window horizontal coordinates is tested, at that the binary video image boundaries displacement in the M background analysis windows analysis is executed by checking the conditions of location of the correspondent histogram classifier binary video image boundaries inside the boundaries of internal Okjl and exterior Ok,2 regions of the ki background analysis window with forming of features Fk,l=l and F|<i2 of the belonging></i2>
of the binary video images boundaries to the to internal Okil and exterior Oki2 regions of the ki<br>
background analysis window,<br>
where Fkil=l if the binary video image boundary of the histogram classifier is inside the<br>
boundaries of the Oki 1 region, or Fkil=0 otherwise;<br>
Fkj2=l if the binary video image boundary of the histogram classifier is inside the<br>
boundaries of the Oki2 region, or Fki2=0 otherwise;<br>
at that, at the detection of the correspondent binary video image boundaries of the histogram classifier transfer from the exterior region to the interior one of the background analysis windows with the numbers ki=kipl,<br>
where kipl are the numbers of the analysis windows in which binary video image boundaries of the histogram classifier transfers from the exterior regions Okipi2 to the interior regions Okipil of the background analysis window were detected, kipl=l,2,...,kN,<br>
at that, at successive fouiticng of features Fk,pi2=l at nkipi=nkipi2, and then Fkipil=l at where nkipil&gt;nkipi2, nidpil is the frame number, in which in kipl analysis window the feature state is set as Fkipil=l, nkipi2 is the frame number, in which in kipl analysis window the feature state is set as Fkipi2=l, and at satisfaction of the condition of membership of the second tested boundary coordinate of the histogram classifier binary video image in the kipl background analysis window to the range of object window coordinates, the change for forming of video camera field of view axis displacement management signals after the extrapolated coordinates is executed,<br>
the SP change counter of the image boundaries of the histogram classifier transfers between the exterior and interior regions of the background analysis window is bound to SP=1 at the first detection of the image boundaries of the histogram classifier transfer from the exterior regions Okipi2 to the interior regions Okipil or the SP change counter increased by one at the second detection of the video image boundaries of the histogram classifier transfer from the exterior regions Okipi2 to the interior regions Ok,pil at the time of forming of the tracking system video camera field of view axis displacement management signals over the extrapolated coordinates of the object video image, the features states are set as Fkpi 1=0, Fk,pi2=0,<br><br>
the features states are set as Fj2=0 in the background analysis windows with the numbers j*kipl, j=l,...,kN, arid the analysis process of the histogram classifier binary video image boundaries is started from the very beginning;<br>
-,-^t the detection of the correspondent binary video image boundaries of the histogram classifier transfer from the exterior regions Ok,p21 to the interior regions Ok,p22 of the background analysis windows,<br>
where kip2 are the numbers of the analysis windows in which binary video image boundaries of the histogram classifier transfers from the exterior regions Okip22 to the interior regions 0^21 of the background analysis window were detected, kip2=:l ,2,.. .,kN,<br>
at the time of forming of the tracking system video camera field of view axis displacement management signals that is at the successive forming of the feature Fkip2l=l at nic,p2=ntc,p21, and then Fkip22=\ at nkip2=niop22 in one or several background analysis windows, where nklp22&gt;nk,p2l, the SP change counter is decreased by one, and if SP=0 the change for forming of video camera field of view axis displacement management signals is made after object video image coordinates in the video camera field of view, obtained as a result of video image processing in the current analysis window, the features states are set as Fkip2l=0, FkiP22=0, the features states are set as Fj2=0, in those background analysis windows with the numbers j*kip2, j=l,...,kN and the analysis process of the histogram classifier binary video image boundaries is started from the very beginning, the period of forming of the video camera field of view axis displacement management signals after the extrapolated coordinates and the time when the features F,l, Fj2, i=l,...,M, are in the state Fjl=l, Fj2=l is tested and when the set time intervals are exceeded the appropriate features are set at zero state, there is a change for forming of video camera field of view axis displacement management signals after object video image coordinates in the video camera field of view obtained as a result of video image processing in the current analysis window.<br>
The technical result in the part of assurance of coordinates determination and steady<br>
object image detention in the center of the video camera field of view or in the center of the tracking window at reception of contrast and low-contrast images of mobile and stationary; objects and tenain background from the video camera when the object moves across the terrain areas which average intensity is similar to the object average intensity, at change of the object visible pattern stipulated by its rotation in motion, illuminance change, object observation in conditions of discontinuous optical communication caused by object overlapping by different barriers such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc., at controlled (mobile) and uncontrolled (stationary) tracking system video camera field of view^ at motion of the surveillance system carrier that causes wide-band uncontrolkd displacements of the tracking system video camera field of view (image "shaking", which could cause the change in mutual position of video image field elements not as a whole), with higher dynamics of object movement, is achieved by the fact that into the device for object coordinates detenninatiom, observed in the sequence of video images, containing the computing processor (95) of the local data, being transmitted over the bidirerctional bus, are in the unit (75) of the reception anyl storage of the current video image field of the tracking system video camera and of the forming of the device functioning synchronisation signals, unit (76) of the reception and storage of computation of the operated rotation of the tracking system video camera, unit (77) of the computation of the controlled rotation of the tracking system video camera field of view axis at the time between the reception of the (n-1) and (n-2) video image fields axis, where n-3,4,5,... is the current video image field number, former (78) of the current n-frame video image signal from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the cunent video image field signals with the elementwise current video image field signals coordinates transfbimation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; "nit (79) of the video image signals forming and scaling is the current analysis window using the video image current analysis window location and dimensions signals;<br>
object image detention in the center of the video camera field of view or in the center of the tracking window at reception of contrast and low-contrast images of mobile and stationary objects and terrain background from the video camera when the object moves across the tenaip<br>
areas which average intensity is similar to the object average intensity, at change of the objeci<br>
visible pattern stipulated by its rotation in motion, illuminance change, object observation in<br>
conditions of discontinuous optical communication caused by object overlapping by different barriers such as ground configuration accidents, buildings, vegetation, dust, smoke, spatter etc., at controlled (mobile) and uncontrolled (stationary) tracking system video camera field of viewa at motion of the surveillance system carrier that causes wide-band uncontrolled displacements of the tracking system video camera field of view (image "shaking", which could cause the change in mutufi] position of video image field elements not as a whole), with higher dynamics of object movement, is achieved by the fact that into the device for object coordinates determination, observed in the sequence of video images, containing the processor (95) for the local data computing in time sharing over the bidirectional bus, are in the unit (75) of the reception and storage of the current video image field of the tracking system video camera and of the fbuuing of the device functioning synchronisation signals, unit (76) of the reception and storage of computation of the operated rotation of the tracking system video camera, unit (77) of the computation of the controlled rotation of the tracking system video camera field of view axis at the time between the reception of the former and current video image fields axis, foxmer (78) of the current video irwge signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled shift at the time between the reception of the video image former and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals;<br>
unit (80) of the M analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection detennination in the M background analysis windows; unit (81) of the detennination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the<br>
'generalized binary video image of the object; unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; unit (83) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming; commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view; unit (85) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; unit (86) of detennination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals; unit (87) of the binary video images boundaries area and coordinates detennination in M background analysis windows; unit (88) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view; analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows; unit (92) of averaging the integrated assessment of the current speed of the object video image displacement and of the storage of the averaged speed values, unit (93) of determination of the prognosticated coordinates and of the displacement speed of the<br>
former (80) of the 'analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the background analysis windows; unit (81) of the deteunination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; unit (82) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; unit (83) of deteunination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming; commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view; analyzer (85) of the current speed of the object generalized binary video image displacement in the inertial coordinate system; analyzer (86) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals; unit (87) of the binary video images boundaries area and coordinates determination in background analysis windows; unit (88) of deteunination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; "nit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view; analyzer (91) of the conditions of the usage of the object video image extrupolated coordinates on the basis of the current and averaged area of the object generalized bipary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows; unit (92) of averaging the integrated assessment of the current speed of the object video image displacement and of the storage of the filtered speed values, extrapolator (93) of the coordinates and of the displacement speed of the<br>
object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement; former (94) of the tracking system video camera field of view axis displacement management signals and of the ,|ignals of location and dimensions of the analysis window in the next video image frame with the  of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, at that the first and second inputs of the device are connected to the first and second inputs of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the signals of device functioning synchronization; the third input of the device is connected to an input of the unit (76) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view; the fourth and fifth inputs of the device are connected to the first and second inputs of the commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video cameia field of view; the sixth input of the device is connected to the second input of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3,4,5,... is the current video image field number; the first output of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the signals of device functioning synchronization is connected to the first input unit of the former (78) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field sigwls with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; the second output of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the generation of the signals of device functioning synchronization is connected to the third input of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception, of the (n-1) and (n-2) video image<br>
object video image in the next frame on the basis of the analysis of the values of the averaged speed of the object video image displacement; former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and "'fmensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, at that the first and second inputs of the device are connected to the first and second inputs of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the signals of device functioning synchronization; the third input of the device is connected to an input of the unit (76) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view; the fourth and fifth inputs of the device are connected to the first and second inputs of the commutator (84) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view; the sixth input of the device is connected to the second input of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the former and current video image fields signals, the first output of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the signals of device functioning synchronization is connected to the first input unit of the former (78) of the current video image signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled shift at the time between the reception of the video image former and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports thr compensation of the tracking system video camera field of view uncontrolled shifts and rolling; the second output of the unit (75) of reception and storage of the current video image fieiti signals from the tracking system video camera and of the generation of the signals of device functioning synchronization is connected to the third input of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception et the termer <br><br>
fields signals, where n=3,4,5,... is the current video image field number, to the fifth input of the unit (79) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals, to the fourth input of the former (78) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling for the second inputs of the unit (80) of the M analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the M background analysis windows and of the unit (87) of the binary video images boundaries area and coordinates determination in M background analysis windows, to the third input of the unit (81) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the eighth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the fourth input of the unit (83) of deteunination of the current speed of the object video image displacement in the inertia! coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the fifth input of the commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the fourth inputs of the unit (85) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors and of the unit (86) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, to the fourth input unit (88) of determination of the current speed of the object generalised binary video imaue<br>
fields signals, to the fifth input of the former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals, to the fourth input of the former (78) of the current video image signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled shift at the time between the reception of the video image former and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfbunation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling for the second inputs of the former (80) of the analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection detemiination in the background analysis windows and of the unit (87) of the binary video images boundaries area and coordinates determination ia background analysis windows, to the third input of the unit (81) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the eighth input of the unit (82) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the fourth input of the unit (83) of detemiination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the fifth input of the commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the fourth inputs of the analyzer (85) of the current speed of the object generated binary video image displacement in the inertial coordinate system and of the analyzer (86) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, to the fourth input unit (88) of detennination of the current speed of the object generalized binary video image<br>
displacement in the ineitial coordinate system, to the fifth input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integiated assessment, to the third input of the unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view, to the sixth input of tae analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and bondaries of the histogram classifier binary video images in the M background analysis windows, to the third inputs of unit (92) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values and to the unit (93) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement and to the sixth input of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signajs of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3,4,5,... is the current video image field number is connected to the second input unit of the foiuier (78) of the current n-frame video image signals from the foiuier frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; the output of the unit (76) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view is connected to the third input of the former (78) of the current n-frame video image signals<br>
displacement in the inertial coordinate system, to the fifth input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment, to the third input of the unit (90) of the integrated assessment of the video image coordinates in the tracking system video camera field of view, to the sixth input of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows, to the third inputs of unit (92) of averaging the integrated assessment of the current speed of the object video image displacement and of the storage of the filtered speed values and to the extrapolator (93) of coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the averaged speed values of the object video image displacement and to the sixth input of the generator (94) of the tracking system video camera field of view axis displacement manageaient signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals, is connected to the second input unit of the generator (78) of the current video image signals from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the eiementwise current video image field signals coordinates transfoimation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; the output of the "nit (76) of the reception and storage of uncontrolled displacement and rolling of the tracking system video camera field of view is connected to the third input of the generator (78) of the current video image signals<br>
from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; to the fifth input of the unit (81) of the deteuuination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object, to the first input unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the first input of the unit (83) of deteimination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the second input of the unit (88) of deteimination of the current speed of the object generalised binary video image displaceuient in the inertial coordinate system, and to the first input of the unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view; the output of the former (78) of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfoi illation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling is connected to the first input of the unit (79) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the output of the unit (79) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals is connected to the seventh input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections<br>
from the former frame video image signals with regard to the tracking system video camera field of view axis controlled shift at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfomiation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling; to the fifth input of the unit (81) of the deteuination of the current coordinates of the generated binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object, to the first input unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the first input of the unit (83) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, to the second input of the unit (88) of detemiination of the current speed of the object generalised binary video image displacement in the inertial coordinate system and to the first input of the unit (90) of the integrated assessment of the video image coordinates in the tracking system video camera field of view; the output of the generator (78) of the current video image signals from the former frame video image signals with regard to the hacking system video camera field of view axis controlled shift at the time between the reception of the video image preceding and current fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfonnation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and rolling is connected to the first input of the former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the output of the former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals is connected to the seventh input of the unit (81) of the detennination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections<br>
of the object generalised binary video image and to the third input of the unit (82) of the object video image coordfftes dctcimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the output of the foinier (80) of the M analysis windows fanning about the analysis window perimeter and the histogram classifier binary video image signals projection detennination in the M background analysis windows is connected to the first input of the unit (87) of the binary video images boundaries area and coordinates detennination in M background analysis windows, which first output is connected to the third input of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalised binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M backgiound analysis windows; the first output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and verkaJ projections of the generalized binary video image of the object is connected to the first input of the former (80) of the M analysis windows forming about the analysis window peilmeter and the histogram classifier binary video image signals projection detennination in the M background analysis windows; the second output of the unit (81) of the deteimination of the current coordinates of the object generalized binary video image using the generali/ed horizontal and vertical projections of the object generalized binary video image is connected to the third input of the commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the analyzer (85) of determination of the current speed of the object generalised binary video image displacement in the inertial coordinate system confidence factors and to the first input of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the third output of the unit (81) of the detennination of the current coordinates of the generalized binary<br>
of the generalized binary video image of the object and to the third input of the unit (82) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the output of the foinier (80) of the analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the background analysis windows is connected to the first input of the unit (87) of the binary video images borders area and coordinates detennination in background analysis windows, which first output is connected to the third input of the analyzer (91) of the conditions of the usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows; the first output of the unit (81) of the deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the geneialized binary video image of the object is connected to the first input of the foirtier (80) of the analysis windows forming about the analysis window perimeter and the histogram classifier binary video image signals projection deteimination in the background analysis windows; the second output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the third input of the multiplexer (84) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the analyzer (85) of the current speed of the object generalized binary video image displacement in the inertial coordinate system and to the first input of the generator (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the third output of the unit (81) of the detennination of the current coordinates of the generalized binary<br>
video image of the object using the generalized horizontal and vertical projections of the generalised binary video image of the object is connected to the first input of the unit (88) of deteimination of the current speed of the object generalised binary video image displacement in the inertial coordinate system; the fourth output of the unit (81) of the deteimination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the second input of the analyzer (85) of deteimination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors and to the second input of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows; the fifth output of the unit (81) of the determination of the current coordinates of the object generalised bmwy video image using the generalized horizontal and vertical projections of the object generalised binary video image is connected to the first input of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalised binary video image, current and averaged speed of the object video image displacement, area and boundaries of me histogiam classifier binary video images in the M background analysis windows; the sixth output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the object generalized binary video image is connected to the third output of the analyzer (86) of determi.iation of the confidence factors of the current speed of the object video image displacement :n the inertiat coordinate system obtained on the basis of video images dissimilarity measurr signals, the seventh output of the unit (81) of the deteimination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of object the generalized binary video image is<br>
video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the first input of the unit (88) of deteimination of the current speed of the object generalised binary video image displacement in the inertial coordinate system; the fourth output of the unit (81) of the deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the second input of the analyzer (85) of the current speed of the object generalized binary video image displacement in the inertial coordinate system and to the second input of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows; the fifth output of the unit (81) of the deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the first input of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generated binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows; the sixth output of the unit (81) of the deteunination of the current coordinates of the generalized binary video image of the object using the generalised horizontal and vertical projections of the generalized binary video image of the object i$ connected to the third output of the analyzer (86) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, the seventh output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is<br>
connected to the fourth input unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the signals of synchronbation of the work of the device; the first output of the unit (82) of the object video image coordinates detaiurortion relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the unit (83) of deteunination of the current speed of the object video image displacement in the ineitial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, which first output is connected to the third input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the second output of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first input of the analyzer (86) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, which first output is connected to the second input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the first output of the commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view is connected to the first input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object, to the seventh input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the third input of the unit (79) of the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the second output of the commutator (84) of the codes of the ingoing 0r current object video image dimensions and of the object coordinates in the tracking system video camera field of view is connected to the second input of the unit (81) of the determination of the current coordinates of the object generalized binary<br>
connected to the fourth input unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the generation of the signals of synchronization of the work of the device; the first output of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the unit (83) of detemiination of the current speed of the object video image displacement in the ineilial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, which first output is connected to the third input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment; the second output of the unit (82) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first input of the analyzer (86) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, which first output is connected to the second input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment; the first output of the multiplexer (84) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view is connected to the first input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object, to the seventh input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the third input of the former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the second output of the multiplexer (84) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view is connected to the second input of the unit (81) of the deteunination of the current coordinates of the generalized binary<br><br>
video image using the generalized horizontal and vertical projections of the object generalized binary video image, to the sixth input of the unit (82) of the object video image coordinates detennination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and to the second input of the unit (79) of the video image signals forming and scaling in the current analysis window using the video image cuaent analysis window location and dimensions signals; the first output of the analyzer (85) of deteimina.tion of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors is connected to the first input of the "nit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment, and the second output is connected to the second input of the analyzer (86) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, which second output is connected to the third input of the analyzer (85) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; the first output of the unit (88) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system is connected to the fourth input of the unit (89) of the fouling of the current speed of the object video image displacement in the inertial coordinate system complex estimate; the first output of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate is connected to the fourth input of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generaliyed binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows, to the fifth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the first input of the unit (92) of averaging the complex estimate of the current speed of the object video image displacement and of the storage<br>
video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object, to the sixth input of the unit (82) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and to the second input of the former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (85) of the current speed of the object generalized binary video image displacement in the inertial coordinate system is connected to the first input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment, and the second output is connected to the second input of the analyzer (86) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, which second output is connected to the third input of the analyzer (85) of the current speed of the object generalized binary video image displacement in the inertial coordinate system; the first output of the unit (88) of deteunination of the cuifcnt speed of the object generalized binary video image displacement in the inertial coordinate system is connected to the fourth input of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment; the first output of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system integrated assessment is connected to the fourth input of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generated binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows, to the fifth input of the unit (82) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the first input of the unit (92) of averaging the integrated assessment of the current speed of the object video image displacement and of the storage<br>
of the averaged speed values and to the second input of the unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view; the first output of the unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view is connected to the fourth input of the commutator (84) of the codes of the ingoing or current object video image dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the unit (93) of detei urination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement and to the third input of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, which second output is connected to the third inputs of the unit (83) of determination of the current speed of the object video image displacement in the inertia! coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming and of the unit (87) of detei initiation of the current speed of the object generalized binary video image displacement in the inertial coordinate system, to the sixth input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the object generalized binary video image and to the fourth inputs of the unit (82) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and of the unit (79) of the video image signals feinting and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of<br>
of the averaged speed values and to the second input of the unit (90) of the integrated assessment of the video image coordinates in the tracking system video camera field of view; the first output of the unit (90) of the integrated assessment of the video image coordinates in the tracking system video camera field of view is connected to the fourth input of the multiplexer (84) of the codes of the ingoing or current object dimensions and of the object coordinates in the tracking system video camera field of view, to the first input of the extrapolator (93) of coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the averaged values of the speed of the object video image displacement and to the third input of the generator (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, which second output is connected to the third inputs of the unit (83) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming and of the unit (88) of determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system, to the sixth input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object and to the fourth inputs of the unit (82) of the object video image coordinates deteaaination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, and of the former (79) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals; the first output of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and borders of<br>
the histogram classifier binary video images in the M background analysis windows is connected to the second input of the fouaer (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, to the second input of the unit (92) of averaging the complex estimate of ihp current speed of the object video image displacement and of the storage of the averaged speed values and to the fourth input of the unit (81) of the determination of the current coordinates of the object generalised binary video image using the generalized horizontal and vertical projections of the object generalized binary video image; the second output of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogiam classifier binary video images in the M background analysis windows is connected to the second input of the unit (82) of the object video image coordinates deteiinjnation relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fomring; the fourth output of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows is connected to the third input of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the forming of the signals of device functioning synchronization; the first output of the unit (92) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values is connected to the fifth input of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the<br>
the histogiam classifier binary video images in the background analysis windows is connected to the second input of the generator (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement, to the second input of the unit (92) of averaging the integrated assessment of the current speed of the object video image displacement and of the storage of the filtered speed values and to the fourth input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the second output of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows is connected to the second input of the unit (82) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the fourth output of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows is connected to the third input of the unit (75) of reception and storage of the current video image field signals from the tracking system video camera and of the generation of the signals of synchronization of the work of the device; the first output of the unit (92) of averaging the integtated assessment of the current speed of the object video image displacement and of the storage of the filtered speed values is connected to the fifth input of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the<br>
object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows and to the second input of the unit (93) of deteimination of the prognosticated coordinates and of the displacement speed of the object video image in the next ftame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement; the second output of the unit (93) of deteuflination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement is connected to the fifth input of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the unit (93) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement is connected to the fourth input of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates hi the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement is the device output and is connected to the first input of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3,4,5,... is the current video image field number; the output of the processor (95) of the local data computing processing, being transmitted over the bidirectional bus is connected to the second outputs of the unit (77) of the calculation of the tracking system<br>
object video image displacement, area and borders of the histogram classifier binary video images in the background analysis windows and to the second input of the extrapolator (93) of the coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the averaged values of the speed of the object video image displacement; the second output of the unit (92) of averaging of the current speed complex estimate of the object video image storage of the filtered speed values is connected to the fifth input of the generator (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the extrapolator (93) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the averaged values of the speed of the object video image displacement is connected to the fourth input of the geneiator (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement; the first output of the generator (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement is the device output and is connected to the first input of the unit (77) of the calculation of the tracking system video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals, the output of the processor (95) for the local data computing processing in time sharing over the bidirectional bus is connected to the second outputs of the unit (77) of the calculation of the tracking system<br>
video camera field of view axis controlled displacement at the time between the reception of the (n-1) and (n-2) video image fields signals, where n=3,4,5,... is the cmrent video image field number, of the object "nit (82) of the object video image coordinates detennipation relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fonwng, of the unit (83) of determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, of the unit (87) of the binary video images boundaries area and coordinates detennination in M background analysis windows, which first output, and of the unit (89) of the foiuiing of the current speed of the object video image displacement in the inertial coordinate system complex estimate, of the unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view, of the unit (93) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement, to the third outputs of the analyzer (85) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors, of the speed unit (86) of determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, of the analyzer (91) of the conditions of the transition to the use of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows, of the unit (92) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the averaged speed values and of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking system video camera field of view or extrapolated coordinates and speed of the object video image displacement.<br>
video camera field of view axis controlled displacement at the time between the reception of the preceding and current video image fields signals, of the unit (83) of deteimination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming, of the unit (87) of the binary video images boundaries area and coordinates deteimination in background analysis windows, which first output, and of the unit (88) of detennination of the current speed of the object generalized binary video image displacement in the inertial coordinate system of the unit (89) of the forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate, of the unit (90) of the complex estimate of the video image coordinates in the tracking system video camera field of view, of the extrapolator (93) of determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame on the basis of the analysis of the averaged values of the speed of the object video image displacement, to the third outputs of the object unit (82) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming of the analyzer (85) of the current speed of the object generalized binary video image displacement in the inertial coordinate system, of the analyzer (86) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals, of the analyzer (91) of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows, of the unit (92) of averaging the complex estimate of the current speed of the object video image displacement and of the storage of the filtered speed values and of the former (94) of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video image coordinates in the tracking Systran video camera field of view or extrapolated coordinates and speed of the object video image displacement.<br>
to the eighth output of the unit (75) of reception and storage of the current video image field signals from .the tracking system video camera and of the forming of the device functioning synchronisation signals.<br>
The technical result is also achieved by the fact that the unit (81) of the determination of the cuuetit coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image contains the first videodata commutator (31), the first buffer random access memory (30), the second buffer random access memory (41) and the third buffer random access memory (43), the former (32) of the histogram classifier binary video image signals in the current analysis window, former (36) of the background change detector primary video image signals, former (38) of the background change detector secondary video image signals, former (45) of the moving-objects indicator primary video image signals, former (47) of the moving-objects indicator secondary video image signals, the first node (42) video image signals scaling and shift, the node (44) of forming of the moving-objects indicator difference video image signals, former (33) of the histogram classifier binary video image signals horizontal and vertical projections, former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, the unit (35) of the object/background ratio and background video image signals mean square value minimum determination, spatial low-pass filter (37) of the background change detector primary binary video image signals, spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, unit (34) of the histogram classifier binary video image signals confidence factor deteuni nation, unit (40) of the background change detector binary secondary video image signals confidence factor deteuuination, unit (49) moving-objects indicator<br>
to the eighth output of the unit (81) of the detei mi nation of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object.<br>
The technical result is also achieved by the fact that the unit (81) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image contains the first videodata commutator (31), the first buffer random access memory (30), the second buffer random access memory (41) and the third buffer random access memory (43), the former (32) of the histogiam classifier binary video image signals in the current analysis window, former (36) of the background change detector primary video image signals, former (38) of the background change detector secondary video image signals, former (45) of the moving-objects indicator primary video image signals, foiuier (47) of the moving-objects indicator secondary video image signals, the first node (42) video image signals scaling and shift, the node (44) of forming of the moving-objects indicator difference video image signals, former (33) of the histogram classifier binary video image signals horizontal and vertical projections, former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, the unit (35) of the object/background ratio and background video image signals mean square value minimum determination, spatial low-pass filter (37) of the background change detector primary binary video image signals, spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, analyzer (34) of the histogram classifier binary video image signals, analyzer (40) of the background change detector binary secondary video image signals, analyzer (49) moving-objects indicator<br>
binary secondary video image signals confidence factor deteimination, and the former (50) of the generalised vertical and horizontal projections of the generalized object binary video image, unit (51) of the detemiination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window, analyzer (52) of the video image coordinates automatic detemiination break, at that, the first and second inputs of the unit (81) of the detennination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image are connected to the first and second inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, and of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination and to the fourth and third inputs of the former (36) of the background change detector primaryvideo image signals; the first input of the unit (81) of the deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is also connected to the first input of the first node (42) of video image signals scaling and shift; the third input of the unit (81) of the deteijnination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the third input of the first videodata commutator (31), to the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window, to the second input of the former (33) of the histogram classifier binary video image signals horizontal and veitical projections, to the second input of the unit (34) of the histogram classifier binary video image signals confidence factor determination, to the fifth input of the unit (35) of the object/background ratio and background video image signals mean square value minimum detemiination in the M background analysis windows, to the sixth input of the former (36) of the background change detector primary video image signals, to the second input of the unit (40) of the<br>
indicator binary secondary video image signals, and the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, calculator (51) of the object coordinates and sizes, of the current and averaged area of the object binary video image, analyzer (52) of the object video image coordinates automatic determination break, at that, the first and second inputs of the unit (81) of the detemiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image are connected to the first and second inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, and of the unit (35) of the object/background ratio and background video image signals mean square value minimum detemiination and to the fourth and third inputs of the former (36) of the background change detector primary video image signals; the first input of the unit (81) of the detenmnation of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is also connected to the first input of the first node (42) of video image signals scaling and shift; the third input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generali7.ed binary video image of the object is connected to the third input of the video data commutator (31), to the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window, to the second input of the former (33) of the histogram classifier binary video image signals horizontal and vertical projections, to the second input of the analyzer (34) of the histogram classifier binary video image signals, to the fifth input of the unit (35) of the object/background ratio and background video image signals mean square value minimum detemiination in the background analysis windows, to the sixth input of the former (36) of the background change detector primary video image signals, to the second input of the analyzer (40) of the<br>
background change detector binary secondary video image signals confidence factor deteniiination, to the fourth input of the first node (42) of video image signals scaling and shift, to the second input of the unit (40) of the background change detector binary secondary video image signals confidence factor deteimination, to the seventh input of the former (50) of the generalised vertical and horizontal projections of the generalized object binary video image, an/1 to the second input of the unit (51) of the determination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window; the fouilh input of the unit (81) of the deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalised binary video image of the object is connected to the fourth input of the former (32) of the histogram classifier binary video image signals in the current analysis window; the fifth input of the unit (81) of the deteuiri nation of the current coordinates of the generalised binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the first input of the former (36) of the background change detector primary video image signals and to the second input of the first node (42) of video image signals scaling and shift; the sixth input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the second input of the former (36) of the background change detector primary video image signals, to the fifth input of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the third input of the unit (35) of the object/background ratio and background video image signals mean square value minimum deteimination in M background analysis windows; the seventh input of the "nit (81) of the detennination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to inputs of the first buffer operating memory device (30), and of the second buffer random access memory (41), to the first input of the node (44) of the forming of the<br>
background change detector binary secondary video image signals, to the fourth input of the first node (42) of video inwge signals scaling and shift, to the second input of the analyzer (40) of the background change detector binary secondary video image signals, of the second inputs of the analyzer (49) moving-objects indicator binary secondary video image signals, to the seventh input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, and to the second input of the calculator (51) of the determination of object coordinates and sizes, of the current and averaged area of the object binary video image; the fourth input of the unit (81) of the deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the fourth input of the former (32) of the histogram classifier binary video image signals in the current analysis window; the fifth input of the unit (81) of the detenaination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to the first input of the former (36) of the background change detector primary video image signals and to the second input of the first node (42) of video image signals scaling and shift; the sixth input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalised horizontal and vertical projections of the generalized binary video image of the object is connected to the second input of the former (36) of the background change detector primary video image signals, to the fifth input of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the third input of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination in background analysis windows; the seventh input of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object is connected to inputs of the first buffer operating memory device (30), and of the second buffer random access memory (41), to the first input of the node (44) of the forming of the<br>
moving-objects indicator difference video image signals, to the fifth input of the former (36) of the background change detector primary video image signals and to the first input of the fitst videodata commutator (31); the output of the first buffer random access memory (30) is connected to the second input of the first videodata commutator (31), which first and secoad outputs are connected to the third and sixth inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the fourth input of the unit (35) of the object/background ratio and background video image signals mean square value minimum deteifnination in M background analysis windows, the first output of which is the fifth output of the unit (81) of the deteuiiination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the first output of the unit (81) of the deteuiiination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object and to the first input of the former (33) of the histogram classifier binary video image signals horizontal and vertical projections, which output is connected to the first input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image and to the first input of the unit (34) of the histogram classifier binary video image signals confidence factor determination, which first output is connected to the second input of the former (50) of the generalized vertical and horizontal projections of the gencialized object binary video image; the output of the former (36) of the background change detector primary video image signals is connected to the input of the spatial low-pass filter (37) of the background change detector primary binary video image signals, which output is connected to the input of the former (38) of the background change detector secondary video image signals, which output is connected to the input of the former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, which<br>
moving-objects indicator difference video image signals, to the fifth input of the former (36) of the background change detector primary video image signals and to the first input of the videodata commutator (31); the output of the first buffer random access memory (30) is connected to the second input of the videodata commutator (31), which first and second outputs are connected to the third and sixth inputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, and to the fourth input of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination in background analysis windows, the first output of which is the fifth output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the first output of the unit (81) of the deteum'nation of the current coordinates of the generalized binary video image of the object using the generaliyed horizontal and vertical projections of the generalized binary video image of the object and to the first input of the former (33) of the histogram classifier binary video image signals horizontal and vertical projections, which output is connected to the first input of the former (50) of the generalized vertical anu horizontal projections of the generalized object binary video image and to the first input of the analyzer (34) of the histogram classifier binary video image signals, which first output is connected to the second input of the former (50) of the generalized veitical and horizontal projections of the generalized object binary video image; the output of the former (36) of the background change detector piunary video image signals is connected to the input of the spatial low-pass filter (37) of the background change detector primary binary video image signals, which output is connected to the input of the former (38) of the backgiound change detector secondary video image signals, which output is connected to the input of the former (39) of the background change detector secondary binary video image signals horizontal and vertical projections, which<br>
output is connected to the third input of the former (50) of the generalized vertical and horizontal projections of the ge,ierali7ed object binary video image and to the first input of the unit (40) of the background change detector binary secondary video image signals confidence factor detemiination, which first output is connected to the fourth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the output of the second buffer operating memory device (41) is connected to the third input of the first node (42) of video image signals scaling and shift, which output is connected to the input of the third buffer random access (43), which output is connected to the second input of the node (44) of the forming of the moving-objects indicator difference video image signals, which output is connected to the input of the former (45) of the moving-objects indicator primary video image signals, which output is connected to the input of the spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, which output is connected to the input of the former (47) of the moving-objects indicator secondary video image signals, which output is connected to the input of the former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, and which output is connected to the first input of the unit (49) of the moving-objects indicator binary secondary video image signals confidence factor detei i ni nation and to the sixth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the first output of the unit (49) of the moving-objects indicator binary secondary video image signals confidence factor deteunination is connected to the fifth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, which first output is connected to the first input of the unit (51) of the detemiination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window and to the fourth output of the unit (81) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image<br>
output is connected to the third input of the former (50) of the generalized vertical and horizontal projections of the geneialized object binary video image and to the first input of the analyzer (40) of the background change detector binary secondary video image signals, which first output is connected to the fourth input of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the output of the second buffer operating memory device (41) is connected to the third input of the first node (42) of video image signals scaling and shift, which output is connected to the input of the third buffer random access (43), which output is connected to the second input of the node (44) of the forming of the moving-objects indicator difference video image signals, which output is connected to the input of the former (45) of the moving-objects indicator primary video image signals, which output is connected to the input of the spatial low-pass filter (46) of the moving-objects indicator primary binary video image signals, which output is connected to the input of the former (47) of the moving-objects indicator secondary video image signals, which output is connected to the input of the former (48) of the moving-objects indicator secondary binary video image signals horizontal and vertical projections, and which output is connected to the first input of the analyzer (49) of the moving-objects indicator binary secondary video image signals and to the sixth input of the fbuner (50) of the generalized vertical and horizontal projections of the generalized object binary video image; the first output of the analyzer (49) of the moving-objects indicator binary secondary video image signals is connected to the fifth input of the former (50) of the generalized veitical and horizontal projections of the generalized object binary video image, which first output is connected to the first input of the calculator (51) of the object coordinates and sizes, of the current and averaged area of the object binary video image and to the fourth output of the "nit (81) of the deteunination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image<br>
the first and the second outputs of the unit (51) of the detemiination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window are Ae second and third outputs of the unit (81) of the determination of the current coordinates of the object generated binary video image using the generalized horizontal and vertical projections of the object generated binary video image; the third output of the unit (51) of the determination of the coordinates and sizes, of the current and averaged area of the object BINpry video image in the analysis window is connected to the input of the analyzer (52) of the averaged binary video image coordinates automatic determination break, which first output is the seventh output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary, video image of the object; the third output of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination in M background analysis windows is connected to the sixth output of the unit (81) of the detemiination of the current coordinates of the object generalised binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, which eighth output is connected to the second outputs of the former (32) of the histogram classifier binary video image signals in the current analysis window, of the unit (35) of the object/background ratio and background video image signals mean square value minimum determination in M background analysis windows, of the "nit (34) of the histogram classifier binary video image signals confidence factor determination, of the unit (40) of the background change detector binary secondary video image signals confidence factor deteiinination, of the unit (49) of the moving-objects indicator birwy secondary video image signals confidence factor determination, of the analyzer (52) of the averaged binary video image coordinates automatic deteiinination break, and of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, and to the fourth output of the unit (51) of the deteimination of the coordinates and sizes, of the current and averaged area of the object binary video image in the analysis window.<br>
the first and the second outputs of the calculator (51) ot the object coordinates and sizes, ot the current and averaged area of the object binary video image are the second and third outputs of the unit (81) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image; the third output of the calculator (51) of the object coordinates and sizes, of the current and avciaged area of the object binary video image is connected to the input of the analyzer (52) of the object video image coordinates automatic deteniiination break, which first output is the seventh output of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the third output of the unit (35) of the object/background ratio and background video image signals mean square value minimum deteuuination in background analysis windows is connected to the sixth output of the unit (81) of the detemiination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image, which eighth output is connected to the second outputs of the former (32) of the histogiam classifier binary video inwge signals in the current analysis window, of the unit (35) of the object/background ratio and background video image signals mean square value minimum deteniiination in background analysis windows, of the analyzer (34) of the histogram classifier binary video image signals, of the analyzer (40) of the background change detector binary secondary video image signals, of the analyzer (49) of the moving-objects indicator binary secondary video image signals, of the analyzer (52) of the object video image coordinates automatic determination break, and of the former (50) of the generalized vertical and horizontal projections of the generalized object binary video image, and to the fourth output of the calculator (51) of the object coordinates and sizes, of the current and averaged area of the object binary video image.<br>
The technical result is also achieved by the fact that the former (32) of the histogiam classifier binary video image signals in the current analysis window of the unit (81) of the deteiiuination of the ciment coordinates of the object generalized binary video image using the generated horizontal and vertical projections of the object generalized binary video image contains the former (67) of the nomied histograms of the intensity distribution of the signals of the object video images; the sixth input is connected to the second input and the node (68) of the histogram classifier binary video image, at that the first, the second, the third, the fourth and the fifth inputs of the former (32) of the histogram classifier binary video image signals are connected to the first, to the second, to the third, to the fourth and to the fifth inputs of the former(67) of the normed histograms of the intensity distribution of the signals of the object video images, the sixth input is connected to the second input of the node (68) of the histogram classifier binary video image, the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the sixth input of the former (67) of the nomied histograms of the intensity distribution of the signals of the object video images and to the third input of the node (68) of the histogram classifier binary video image; the first output of the former (67) of the normed histograms of the intensity distribution of the signals of the object video images is connected to the first input of the node (68) of the histogram classifier binary video image, which output is the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window; the second output of the former (67) of the normed histograms of the intensity distribution of the signals of the object video images is connected to the second output of the former (32) of the histogram classifier binary video image signals in the current analysis window.<br>
The technical result is also achieved by the fact that the former (36) of the background change detector primary video image signals contains former (70) of the reference background video image signals of the unit (81) of the determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object<br>
The technical result is also achieved by the fact that the former (32) of the histogram classifier binary video image signals in the current analysis window of the unit (81) of the determination of the current coordinates of the object generalized binary video image using the generalized horizontal and vertical projections of the object generalized binary video image contains the calculator (67) of the normed histograms of the intensity distribution of the signals of the object video images; the sixth input is connected to the second input and the node (68) of the histogram classifier binary video image, at that the first, the second, the third, the fourth and the fifth inputs of the former (32) of the histogram classifier binary video image signals are connected to the first, to the second, to the third, to the fourth and to the fifth inputs of the calculator (67) of the noiiiied histograms of the intensity distribution of the signals of the object video images, the sixth input is connected to the second input of the node (68) of the histogiam classifier binary video image, the seventh input of the former (32) of the histogram classifier binary video image signals in the current analysis window is connected to the sixth input of the calculator (67) of the nouned histograms of the intensity distribution of the signals of the object video images and to the third input of the node (68) of the histogram classifier binary video image; the first output of the calculator (67) of the normed histograms of the intensity distribution of the signals of the object video images is connected to the first input of the node (68) of the histogram classifier binary video image, which output is the first output of the former (32) of the histogram classifier binary video image signals in the current analysis window; the second output of the calculator (67) of the nouned histograms of the intensity distribution of the signals of the object video images is connected to the second output of the former (32) of the histogram classifier binary video image signals in the current analysis window.<br>
The technical result is also achieved by the fact that the former (36) of the background change detector primary video image signals contains former (70) of the reference background video image signals of the unit (81) of the detemiination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object<br>
contains former (70) of the reference background video image signals, the fifth (69) and the sixth<br>
(72)	buffer random access memory, the third node (71) of video image signals scaling and shift,<br>
node (73) of the backgmund change detector difference video image signals forming, and node<br>
(74) of the background change detector primary binary video image forming, at that, the first and<br>
second inputs of the former (36) of the background change detector primary video image signals<br>
are connected to the first and second inputs of the former (70) of the reference background video<br>
image signals and of the third node (71) of video image signals scaling and shift; the third and<br>
fourth inputs are connected to the third and fourth inputs of the former (70) of the reference<br>
background video image signals; and the fifth input is connected to the first input of the node<br>
(73)	of the background change detector difference video image signals forming and to the input<br>
of the fifth (69) buffer random access memory which output is connected to the fifth input of the<br>
former (70) of the reference background video image signals; and the sixth input of the former<br>
(36) of the background change detector primary video image signals is connected to the third<br>
input of the node (73) of the background change detector difference video image signals forming<br>
and to the seventh input of the former (70) of the reference background video image signals,<br>
which output is connected to the third input of the node (71) of video image signals scaling and<br>
shift, which output is connected to the input of the sixth (72) buffer random access memory,<br>
which output is connected to the sixth input of the former (70) of the reference background video<br>
image signals and to the second input of the node (73) of the background change detector<br>
difference video image signals forming, which output is connected to the input of the node (74)<br>
of the background change detector primary binary video image forming, which output is<br>
connected to the output of the former (36) of the background change detector primary video<br>
image signals.<br>
The technical result is also achieved by the fact that the unit (82) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming contains the second videodata commutator (53), high-pass nonlinear filter (54).<br>
contains foiuier (70) of the reference background video image signals, the first (69) and the second (72) buffer random access memory, the node (71) of video image signals scaling and shift, node (73) of the background change detector difference video image signals forming, and node (74) of the background change detector primary binarization, at that, the first and second inputs of the former (36) of the background change detector primary video image signals are connected to the first and second inputs of the former (70) of the reference background video image signals and of the node (71) of video image signals scaling and shift; the third and fourth inputs are connected to the third and fourth inputs of the former (70) of the reference background video image signals; and the fifth input is connected to the first input of the node (73) of the background change detector difference video image signals forming and to the input of the first<br>
(69)	buffer random access memory which output is connected to the fifth input of the former<br>
(70)	of the reference background video image signals; and the sixth input of the former (36) of<br>
the background change detector primary video image signals is connected to the third input of<br>
the node (73) of the background change detector difference video image signals forming and to<br>
the seventh input of the former (70) of the reference background video image signals, which<br>
output is connected to the input of the node (71) of video image signals scaling and shift, which<br>
output is connected to the input of the second (72) buffer random access memory, which output<br>
is connected to the sixth input of the former (70) of the reference background video image<br>
signals and to the second input of the node (73) of the background change detector difference<br>
video image signals foniiing, which output is connected to the input of the node (74) of the<br>
background change detector primary binarization, which output is connected to the output of the<br>
former (36) of the background change detector primary video image signals.<br>
The technical result is also achieved by the fact that the unit (82) of the object video image coordinates detetniination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming contains the commutator (53) of scaled image signals, high-pass nonlinear filter (54),<br>
analyzer (55) of the static video reference image renewal conditions, node (61) of forming and analysis of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, the third (56) and the forth (59) buffer random access memory, former (57) of the static and dynwiic video images signals, former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, the second node (58) of the object video image signals scaling and shift, commutator (62) of the video image dissimilarity measuie signals minimum value sequence data, approximator (63) of the video image dissimilarity measure signals minitnum value sequence by the forth degree polynomial, node (64) of the objeet video image coordinates determination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data, node (65) of the object video image coordinates determination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center, node (66) of the object video image coordinates detei urination in the analysis window after the analysis window center coordinates, at that, the first input of the unit (82) of the object video image coordinates determination: relatively to the cuueat analysis window center on the basis of the video image similarity and dissimilarity measure signals fbuning is connected to the first inputs of the analyzer (55) of the static video reference image renewal conditions and of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the second input of the unit (82) of the object video image coordinates deteiniination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first input of the second videodata commutator (53); the third input of the unit (82) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the second videodata commutator (53) and to the first input of the high-pass nonlinear filter (54),<br>
analyzer (55) of the static video reference image renewal conditions, node (61) of forming and analysis of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, the first (56) and the second (59) buffer random access memory, former (57) of the static and dynamic video images signals, former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, the node (58) of the object video image signals scaling and shift, selector (62) of the video image dissimilarity measure signals minimnnj value sequence, approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth degree polynomial, coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial minimum, coordinator (65) of the object video image in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center, coordinator (66) of the object video image in the analysis window after the analysis window center coordinates, at that, the first input of the unit (82) of the object video image coordinates deteiniination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the first inputs of the analyzer (55) of the static video reference image renewal conditions and of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the second input of the unit (82) of the object video image coordinates deteiminntion relatively to the ciuient analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to me first input of the commutator (53) of scaled image signals; the third input of the unit (82) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the commutator (53) of scaled image signals and to the first input of the high-pass nonlinear filter (54),<br>
which output is connected to the third input of the second videodata commutator (53); the fourth input of the unit (82) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals foum'ng is connected to the third input of the foauer (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the second inputs of the former (57) of the static and dynamic video wages signals and of the analyzer (55) of the static video reference image renewal conditions; the fifth input of the unit (82) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure sigiiais forming is connected to the fourth input of the former (60) of the dissimilarity measure sigiafe; between the video image signals in the binary search area of the object video image shifts; the sixth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the third input of the former (57) of the static and dynamic video images signals; the seventh input of the unit (82) of the object video image coordinates detemiination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (57) of the static and dynamic video images signals and to the fifth input of me former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the eighth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the high-pass nonlinear filter (54), to the fifth input of the analyzer (55) of the static video reference image renewal conditions, to the fourth input of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data, and to the seventh input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the output of the second videodata<br>
which output is connected to the third input of the commutator (53) of scaled image signals; the fourth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilaiity measure signals forming is connected to the third input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the second inputs of the former (57) of the static and dynamic video images signals and of the analyzer (55) of the static video reference image renewal conditions; the fifth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the sixth input of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the third input of the former (57) of the static and dynamic video images signals; the seventh input of the unit (82) of the object video image coordinates detenuination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the fourth input of the former (57) of the static and dynamic video images signals and to the fifth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts: the eighth input of the unit (82) of the object video image coordinates deteunination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second input of the high-pass nonlinear filter (54). to the fifth input of the analyzer (55) of the static video reference image renewal conditions, to the fourth input of the selector (62) of the video image dissimilarity measure signals minimum value sequence, and to the seventh input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts; the output of the<br>
commutator (53) is connected to the second input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the input of the third buffer random access memory (56), which output is connected to the first input of the former (57) of the static and dynamic video images signals; the first output of the analyzer (55) of the static video reference image renewal conditions is connected to the fifth input of the former (57) of the static and dynamic video images signals, which output is connected to the second node (58) of the object video image signals scaling and shift, which output is connected to the input of the fourth buffer random access memory (59), which output is connected to the sixth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, which output is connected to the input of the node (61) of the forming and analysis of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, which first output is connected to the second output of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the third input of the analyzer (55) of the static video reference image renewal conditions, and to the input of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data, which first output is connected to the input of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth power polynomial, which first output is connected to the input of the node (64) of the object video image coordinates detennination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data; the second and third outputs of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data are connected to the inputs of the node (65) of the object video image coordinates determination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values<br>
commutator (53) of scaled image signals is connected to the second input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts and to the input of the first buffer random access memory (56), which output is connected to the first input of the former (57) of the static and dynamic video images signals; the first output of the analyzer (55) of the static video reference image renewal conditions is connected to the fifth input of the former (57) of the static and dynamic video images signals, which output is connected to the input of the node (58) of the object video image signals scaling, which output is connected to the input of the second buffer random access memory (59), which output is connected to the sixth input of the former (60) of the dissimilarity measure signals between the video image signals in the binary search area of the object video image shifts, which output is connected to the input of the analyzer (61) of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, which first output is connected to the second output of the unit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming, to the third input of the analyzer (55) of the static video reference image renewal conditions, and to the input of the selector (62) of the video image dissimilarity measure signals minimum value sequence, which first output is connected to the input of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth power polynomial, which first output is connected to the input of the coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial minimum; the second and third outputs of the commutator (62) of the video image dissimilarity measure signals minimum value sequence data are connected to the inputs of the coordinator (65) of the object video image in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values<br>
relatively to analysis window center and of the node (66) of the object video image coordinates deteimination in the analysis window after the analysis window center coordinates; the fiist outputs of the node (64) of the object video image coordinates deteimination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data, of the node (65) of the object video image coordinates determination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the node (66) of the object video image coordinates deteimination in the analysis window after the analysis window center coordinates are connected to the first output of the unit (82) of the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming and to the fourth input of the analyzer (55) of the static video reference image renewal conditions; the third output of the unit (82) of the object video image coordinates deteimination relatively to the cintcitt analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second outputs of the node (61) of the forming and analysis of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, of the analyzer (55) of the static video reference image renewal conditions, of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth degree polynomial, of the node (64) of the object video image coordinates deteanination in the analysis window by the status of the approximating forth degree polynomial of the video image dissimilarity measure signals minimum value sequence data, of the node (65) of the object video image coordinates determination in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the node (66) of the object video image coordinates determination in the analysis window after the analysis window center coordinates.<br>
Summary of the drawings<br>
The invention is illustrated by the drawings, where forming of the template actual for the first L+l frames is on the Fig. la, Ib, the intensity distribution of the image elements of the<br>
relatively to analysis window center and of the coordinator (66) of the object video image in the analysis window after the analysis window center coordinates; the first outputs of the coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial minimum, of the coordinator (65) of the object video image in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the coordinator (66) of the object video image in the Analysis window after the analysis window center coordinates are connected to the first output of the "nit (82) of the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fouling and to the fourth input of the analyzer (55) of the static video reference image renewal conditions; the third output of the unit (82) of the object video image coordinates detennination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming is connected to the second outputs of me analyzer (61) of the video image dissimilarity measure signals minimum value sequence type along the lines and columns of the binary search area of the object video image shifts, of the analyzer (55) of the static video reference image renewal conditions, of the approximator (63) of the video image dissimilarity measure signals minimum value sequence by the forth degree polynomial, of the coordinator (64) of the object video image in the analysis window by the status of the approximating polynomial minimum, of the coordinator (65) of the object video image in the analysis window by the boundaries shift of the fast escalation area of the video images dissimilarity measures signals values relatively to analysis window center and of the coordinator (66) of the object video image in the analysis window after the analysis window center coordinates.<br>
Summary of the drawings<br>
The invention is illustrated by the drawings, where forming of the template actual for the first L+l frames is on the Fig. la, Ib, the intensity distribution of the image elements of the<br>
template formed at the first frames is on the Fig. Ic, the characteristic type of the difference correlation function F(v,µ) calculated as provided by the expression (1) is on the Fig. Id, the intensity distribution of the elements of the reference image and of the current frame image which results in tracking failure is on the Fig. le, If, the OSS data windows location and background frame in the frame is on the Fig. 2, Fig. 3 is the illustiation of the intensity distribution along the image scene line with the light object on the dark background or with the dark object on the light background, Fig. 4 is the illustration of the intensity distribution along the image scene line, in which misclassification takes place, Fig. 5 is the illustration of the intensity distribution along the image scene line at the object movement behind the opaque contrasting background object, the characteristic type of the impulse response hx[i] of the tracking system video camera field of view drive is on the Fig. 6, Fig. 7 is the time BINding of the received controlled displacement signals of the tracking system video camera field of view dx and dy axis to the tjming diagram of the video image, Fig. 8 is the block diagram of the first variant of the object coordinates deteunination device, Fig. 9 is the block diagiam of the unit 4, Fig. 10 is the block diagram of the unit 10, Fig. 11 is the block diagram of the unit 11, Fig. 12 is the block diagram of the fbaiier 32, Fig. 13 is the block diagram of the former 36, Fig. 14 is the block diagram of the second variant of the object positioning device, Fig. 15 is the block diagram of the unit 6, Fig. 16 is the diagram of the commutator 13 and 84, Fig. 17 is the diagram of the commutator 31, unit 10 and unit 81, Fig. 18 is the diagram of the commutator 53, unit 11 and unit 82, Fig. 19 is the block diagram of the work algorithm of the claimed object coordinates deteunination device according to the first variant, Fig. 20 is the block diagtam of the work algorithm of the claimed object coordinates determination device according to the second variant.<br>
Claimed method of signals processing for object positioning being observed in a succession of video images, provides for the possibility of the set problem solution in the presence and in the absence of the video camera field of view uncontrolled displacement and rolling sensors, as well as in the presence but with the poor accuracy perfoimance of the said sensors.<br>
On application of the method on the assumption of the absence or presence with a poor accuracy performance of the video camera field of view uncontrolled displacement and rolling sensors, the claimed method is as follows.<br>
Before the start of the reception of the signals of the current video image n-field. where<br>
n=3, 4. 5	the controlled video camera field of view dx[n| and dy[n] axis displacement is<br>
template fonned at the first frames is on the Fig. 1 c, the characteristic type of the difference correlation Function F(v,µ) calculated as provided by the expression (1) is on the Fig. Id, the intensity distiibution of the elements of the reference image and of the current frame image, which results in tracking failure is on the Fig. le, If, the OSS data windows location and background frame in the frame is on the Fig. 2, Fig. 3 is the illustration of the intensity distribution along the image scene line with the light object on the dark background or with the dark object on the light background, Fig. 4 is the illustration of the intensity distribution along the image scene line, in which misclassification takes place, Fig. 5 is the illustration of the intensity distribution along the image scene line at the object movement behind the opaque contrasting background object, the characteristic type of the impulse response hx[i] of the tracking system video camera field of view drive is on the Fig. 6, Fig. 7 is the time BINding of the received controlled displacement signals of the tracking system video camera field of view dx and dy axis to the timing diagram of the video image, Fig. 8 is the block diagram of the first variant of the object coordinates deteunination device, Fig. 9 is the block diagram of the unit 4, Fig. 10 is the block diagram of the unit 10, Fig. 11 is the block diagram of the unit 11, Fig. 12 is the block diagram of the former 32, Fig. 13 is the block diagram of the former 36, Fig. 14 is tiie block diagram of the second variant of the object positioning device. Fig. 15 is the block diagiara of the unit 6, Fig. 16 is the diagram of the commutator 13 and 84, Fig. 17 is the diagram of the commutator 31, unit 10 and "nit 81, Fig. 18 is the diagram of the commutator 53, unit 11 and unit 82, Fig. 19 is the block diagram of the work algorithm of the claimed object coordinates determination device according to the first variant, Fig. 20 is the block diagram of the work algorithm of the claimed object coordinates determination device according to the second variant.<br>
Invention implementation variants.<br>
Claimed method of signals processing for object positioning being observed in a succession of video images, provides for the possibility of the set problem solution in the presence and in the absence of the video camera field of view uncontrolled displacement and rolling sensors, as well as in the presence but with the poor accuracy performance of the said sensors.<br>
On application of the method on the assumption of the absence or presence with a poor accuracy performance of the video camera field of view uncontrolled displacement and rolling sensors, the claimed method is as follows.<br>
Before the start of the reception of the signals of the current video image n-field, where<br>
n=3. 4, 5	the controlled video camera field of view dx|n| and dy(n| axis displacement is<br>
defined at the time between the reception of (n-1) and (n-2) video image fields signals horizontally and veiu'caily, which is detennined by the impact of the field of view displacement management signals on the video camera.<br>
The controlled displacement dx[n] and dy[n] of the video camera field of view axis is defined, for example, by calculation of the convolution of management signals XMAN[I], YMAN[i] which control the video camera field of view displacement with pulse characteristics hx[i] and hy[i] of its drives<br>
(Equation Removed) <br>
where XMANp]&gt; YMAN[JJ are the management signals of the video camera field of view displacement horizontally and vertically, formed as a result of the video image i-field processing;<br>
hx[i] is the pulse characteristic of the horizontal drive of the tracking system video camera field of view, which is the response of the increment of the angular coordinates of the tracking system video camera field of view to the horizontal management signal impact, which is constant at the interval of the first video image field reception and equal to zero at the other periods of time, at the time between the reception of the video image i and (i-1) fields;<br>
hy[i] is the pulse characteristic of the vertical drive of the tracking system video camera field of view, which is the response of the increment of the angular coordinates of the tracking system video camera field of view to the vertical management signal impact, which is constant at the interval of the first video image field reception and equal to zero at the other periods of time, at the time between the reception of the video image i and (i-1) fields;<br>
K is the pulse characteristic length, which is the number of video image fields, at the end of which pulse characteristic module is within the limits of the set level. The aim of the video camera field of view axis controlled displacement determination is in its consideration at the analysis of image change of objects, observed in the moving video camera field of view. At the little angular displacements of the field of view axis horizontally and vertically at the time of the frames (fields) change the observed object video image change is brought to its shift on some values dx[n] and dyfn] dependent of the previously given control, actions on the drivers of the<br>
video camera field of view. As the control actions are formed after the results of the processing of the video images changing discretely with the field rate, dx[n] and dy[n] displacements are dependent of the n number of the current video image field. In many spheres of the video tracking systems application, drivers of the video camera field of view relatively to the management signals may be considered as linear, that is like those, for which connection of the output dx[n] and dy[n] and input XMAN[I] and YMAN[I] signals is set by the expressions like convolution (39).<br>
Here driveis management signals XMAN[I] and YMAN[I] have the meaning of the set field of view axis displacement angular speed, and the pulse characteristics hx[i] and hy[i] are the responses of the field of view axis angle location increment to the impact of the management signals in the form of the discrete delta function, taken in the discrete periods of time i*T, divisible to the T period of the video images fields change (see Fig. 6).<br>
The characteristic type of the pulse characteristic hx[i] of the video camera field of view drive horizontally is on the Fig. 6.<br>
Practically, video camera field of view drives characteristics may be defined in the following way, for example.<br>
The small contrast test object is placed in the video camera field of view at that its coordinates may be easily defined. The test management signal XMAN[J] is given to the video camera field of view drive input input horizontally in the form of the discrete delta function, as shown on the Fig. 6a. The test object video image coordinates, changed as a result of the processing by the management signal drive, are determined and stored at the set number of half-frames. The test object video image coordinates increment is calculated dependency of the video image field number, counted from the moment when the test management signal is given. Measurements are executed several times, the obtained results are averaged. After the said test object video image coordinates increments amplitude normalization the sought drive pulse characteristic hx[i] horizontally is obtained.<br>
The drive pulse characteristic hy[i] vertically is obtained analogously.<br>
The time BINding of the received tracking system video camera field of view dx[n] axis controlled displacement signals horizontally to the video image fields reception and processing time diagram is shown on the Fig. 7.<br>
The time periods of video image fields reception and fields quenching pulses are shown schematically on the Fig. 7a.<br>
The time periods of the analysis windows video signals processing, resulting in forming of the management signals XMAN by the horizontal drive of the tracking system video camera field of view are shown on the Fig. 7b.<br>
The moments of the change of the XMAN[I] signal relatively to the reception intervals from the video image fields video camera are shown on the Fig. 7c.<br>
The moments of the change of the dx[t] signal relatively to the reception intervals from the video image fields video camera are shown on the Fig. 7d.<br>
Fig. 7 shows that the horizontal consistuent dx[n] of the video camera field of view axis displacement at the time T between the reception of the (n-1) and (n-2) video image fields signals is calculated at the time of the quenching pulse, which precedes the current n-field video signals reception, and thus its availability for use at the current n-field video signals processing is provided.<br>
Time diagrams of the video camera field of view axis controlled displacement vertical consistuent dy[n] change are analogous to the mentioned above.<br>
Field of view axis controlled displacement speed is detei mined from the field of view axis coirtrolled displacement data at the time between the reception of the previous and of the current video image fields signals.<br>
Current video image field and video camera field of view uncontrolled displacement and rolling signals are received and stored. These signals as well as the signals of the video camera field of view axis controlled displacement signals are used for forming of the signals of the prognosticated coordinates of the video images of the 2N terrain reference marks in the current video image field.<br>
For that purpose signals of the video images of the 2N terrain reference marks are chosen from the former video image field signals, at that N=3, 4, 5,...<br>
Terrain reference marks are chosen in the following way, for example.<br>
Allowable area of the terrain references search with the minimum dimensions (Nd-2dx[n])x(Md-dy[n]), taking into consideration horizontal and vertical dx[n] and dy[n] field of view displacement at the half frame, is determined in the video image field (at the interlacing - in the half-frame) of the NdxM<j elements format.></j>
The area of the terrain reference marks search is devided into the NOP= 100-150 windows belonging each other, in which SXY criterion value is determined:<br>
(Equation Removed) <br>
where iok and  pok  are  the  coordinates  of the  window  center  with  the  number  nok,<br>
nok=l,...,NOP,<br>
(2No+l), (2Mo+l) - are the windows dimensions.<br>
SXY(nok) criterion values are compared with the threshold and those, which exceeded the threshold are ranked in descending, after that the list of the windows NK numbers - which are the candidates to become reference marks, having the maximum SXY(nok) criterion values, is foimed. N pairs of reference marks are selected from the list of NK candidates. Reference marks included in the pair are chosen by the criterion of the maximum space betweeriothe reference marks horizontally and vertically, and by the exceeding of these spaces of the threshold value. The corresponding window center coordinates iok and pok are accepted as the reference marks coordinates.<br>
Forming of images dissimilarity measure signals of 2N terrain reference marks from video image signals of 2N terrain reference marks of the current video image field and video images signals of the appropriate 2N terrain reference marks in the former video image field and deteunination with their help of 2N terrain reference marks image shifts at the time between reception of signals of the current video images fields.<br>
Current video image field signal shift and rotation parameters at the time between the reception of the current video image fields are deteimined for each of the N pair of the reference marks using the 2N terrain reference marks video images signals shifts in the following way, for example.<br>
For each pair of the reference marks (with numbers k and k+1) system of the 4 linear equations is made at first<br>
(Equation Removed) <br>
where Xk(n), Yk(n) and Xk+i(n), Yk+i(n) are the coordinates of the k and k+1 reference marks in the current n-half-frame,<br>
Δφ is the field of view rolling angle change at the time between the reception of the video image fields,<br>
ΔX and ΔY is the horizontal and vertical field of view axis displacement at the time between the reception of the video image fields. Coefficients ay in the linear equations system (40) are as following:<br>
an=Xic(n-l)	ai2 = Yk(n-l)	an = 1	aJ4 = 0,<br>
321 = Yk(n-l)	a22 = -Xk(n-l)	a23 = 0	a24 = 1,<br>
aai = Xk+i(n-l)	a32 = Yk+i(n-l)	a33 = 1	a34 = 0,	(43)<br>
341 = Yk-(-i(n-l)	a42 = Xk+i(n-l)	343 = 0	aii = l,<br>
where Xk(n-l), Yk(n-l) and Xk+i(n-l), Yk+i(n-l) are the coordinates of the k and k+l reference marks in the foiuier n-half-frame.<br>
The linear equations system (40) with the 4 unknown quantities tl t4 is calculated using the Gauss exclusion method [Glovatskaya A. P. Methods and Algorythms of the Computational Mathematics. - M.: Rpdio and Connection, 1999.- p. 408: pict., pp 4-10] after that field of view roll angle Δφ change is detemiined.<br>
(Equation Removed) <br>
and then system of 2 linear equations relatively to the 2 unknown quantities ΔX and ΔY is made using the obtained values tl   t4:<br>
-tl*AX-t2*AY = t3<br>
t2*AX-tl*AY = t4	(45)<br>
The system (44) of two linear equations with the 2 unknown quantities ΔX and ΔY may be calculated using the Gauss exclusion method [Glovatskaya A. P. Methods and Algorythms of the Computational Mathematics. - M.. Radio and Connection. 1999.-p. 408: pict., pp 4-10] as well. Determination of the ΔX. ΔY parameters of the displacement and Δφ roll of the video camera field of view is finished b\ the calculation of the system (44) of linear equations.<br>
The selection of the N pairs of the reference marks is determined by the following observations.<br>
Video image portions chosen as reference marks must belong to the stationary terrain background to obtaip robust estimator of the field of view roll and displacement parameters. Cases of their enoneous choice is possible at the automatic choice of the reference marks, when extraneous moving objects may be chosen as the reference marks, or at the move of the vehicle of the tracking system of the short-range terrain background moving video image. At that values of the field of view roll and displacement parameters, obtained after the data of pairs containing erroneous reference marks, differ from others radically.<br>
Obtained N values of each of the roll and displacement parameters are ranked in ascending and descending powers. After the k of the maximum and minimum values of the ranked parameters (k-0,1,2,...) are rejected, and residuary values are averaged.<br>
Increase of N  and k allows reference marks choice error tolerance enhancement but also increases costs of reaHyMion of the field of view shift and roll parameters determination. Practically acceptable results are achieved at N=5 and k=l or at N=7 and k=2.<br>
By the intention of the of the current change of the Δφ angle of roll of field of view the current angle of roll of φ field of view is determined.<br>
The ΔX, ΔY shift parameters of the current video image field signals relatively to the former video image field obtained on the basis of 2N terrain reference marks images dissimilarity measure signals are devided into constituents dx, dy of controlled video camera field of view axis displacement and into constituents rx, ry of uncontrolled video camera field of view axis displacement.<br>
Under the conditions of method use when there is no possibility for field of view uncontrolled displacement and roll signals receiving, sequence of operations differs from the described above by the fact that the prognostication of the 2N tenain reference marks video images coordinates in the current video image field is not perfoimed. It entails necessity of upsi7.ing of the dimensions of the binary search area of each of the reference marks, which results in increase either of time of processing when the search of the reference marks location takes place in the current video image field, or of hardware costs for the realization of the search of the reference marks location procedures.<br>
Image signals of the current frame are formed from image signals of the former frame with allowance for controlled field of view axis displacement at the time between reception of the former and current video image fields signals with elementwise transfotmation of signals coorHinntes of the cwrcnt video image field which compensate for the video camera field of view uncontrolled displacements and roll.<br>
At that foiuiing of signals LpRAME(ixjy) of the current frame video image at interlacing from signals LH-FRAMEOjpjipk) of the image current half-frame and video image L.i FRAME(IX, jy) of the former frame is executed, for example, by prognostication of signals LpRAME(ix,jy) of the current frame video image with the help of the former frame video image shift L-iFRAME(ixjy) by the amount of controlled displacement dx, dy of the video camera field of view axis horizontally and vertically at the time between reception of video image half-frames<br>
LFRAME(ixjy) = L -iFRAME(ix+dxjy+dy),<br>
and by substitution of the current frame prognosticated video image points with the current half-frame image points with compensation for the current uncontrolled shifts rx, ry and roll φ of the tracking system video camera field of view<br>
LFRAME   ix(i,p,npk), jy(i,p,npk)    - L H-FRAME (i,p,npk),<br>
where i is the element number in the current half-frame video image line, i=l,..., NK, ix is the element number in the current frame video image line, ix=l,..., NK,<br>
MK p is the line number in a half-frame, p=l,...,<br>
jy is the line number in a frame, jy=l,..., MK, NK is the number of video image elements in a line, MK is the number of lines in an video image frame, npk is the current half-frame index,<br>
npk = 1 - in uneven half-frames.<br>
npk = 0 - in even half-frames.<br>
(Equation Removed) <br>
As it follows from the aforesaid, in the concerned situation (of the presence or poor accuracy performance of the video camera field of view uncontrolled displacement and rolling sensors) video image stabilization parameters rx, ry and φ are determined and used once at the half-frame (are constant in the boundaries of the current half-frame) and that is why in such conditions the method is advisable to be used in the presence of the video camera field of view destabilizing slow factors.<br>
The video image signals in the current analysis window are fouiied and scaled at the, availability of the video image frame format signals using current video image field analysis window location and dimensions signals and current frame video image signals.<br>
Scaled video image signals in the current analysis window are stored.<br>
Differential video image signals of the moving-objects indicator are foiuied by subtraction of the scaled video image signals in the current analysis window, which were stored and couected to the current video image scale in the analysis window and shifted on the value of the tracking system video camera field of view axis shift, from the scaled video image signals in the current analysis window.<br>
Under the conditions of method use when there is necessity for high-speed objects coordinates deteuiiiaation, at documenting of take-off and landing maneuvers of the aircrafts in the airports, for example, at the forming of the difference video image of the moving-objects indicator, difference of the current and former frames is used, and at the low-speed objects coordinates determination, difference of the and of the stored K frames before images in me analysis window.<br>
Moving-objects indicator primary binary video image signals L1 BIN MOI(IXxj}') are formed from the differential video image signals of the moving-objects indicator, by the comparison of the differential video image signals module with the adaptive formed threshold, signals LI BIN MOI(ix,jy) are exposed to low-pass filtering, for example with the bivariate<br>
convolution<br>
(Equation Removed) <br>
where ix,jy are the coordinates filtered video image signals S_filMoi(ixjy) relatively to the current analysis window center;<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window;<br>
NF and MF are horizontal and vertical parameters of the low-pass filter aperture,<br>
di, dj are the horizontal and vertical internal variables of the low-pass filter aperture,<br>
diε[-NF, NF], djε[-MF, MF];<br>
hj.p [di, dj] is pulse characteristic of the low-pass filter,<br>
and the secondary binary image signals L2 BIN MOI(IX jy) of the moving-objects indicator are generated from them<br>
La BIN Moi(ix jy)=l,   if S_fil Moi(ixjy) &gt; Porog Moi 1 and LI BIN MOI(IX jy)=l or S_fil Moi(ixjy) &gt; Porog MOI 0 and LI BIN Moi(ix jy)=0, otherwise L2 BIN Moi(ixjy) = 0,<br>
where PorogMoI1 PorogMOI0 are values of the decision thresholds for unit and zero elements of the primary binary image of the moving-objects indicator.<br>
The primary binary image signals of the background change detector and binary image signals of the histogram classifier window are founed simultaneously with the storage of the scaled video image signals in the current analysis window, with the fomting of the primary binary image signals of the moving-objects indicator from the scaled video image signals in the current analysis window signals with allowance to the controlled shift of the field of view axis of the tracking system video camera.<br>
At that signals LI BIN BCD (ix jy) of the primary binary image of the background change detector are  formed,  for example,  by  adjustment  of reference background  image  signals. L,,-i mb(ix.iy) obtained in the former n-1 frame to the current scale, by formation of differential image signals<br>
Lp BCD (ixjy) of the backgiound change detector by subtraction of reference background image signals Ln-i mb(ixjy) of the former frame from scaled image signals Ln AW (ixjy) in the cuiiont analysis window with a shift which accounts for displacement Vx, Vy of the analysis window center in the inertial coordinates system at the last frame:<br>
Lp BCD (ixjy) = Ln AW (ixjy) - Ln.i mb (ix+Vxjy+Vy),<br>
by deteimination of the binarization threshold THRESHOLDBCD(ixjy) of the background change detector as a value proportional to the local values spreading parameter of the differential image Lp BCD (ixjy) of the background change detector in neighborhood of the point with coordinates<br>
ixjy,<br>
where ix, jy are the coordinates of the scaled video image signals relatively to the current analysis window center,<br>
(Equation Removed) <br>
NX, NY are the dimensions of the current analysis window.<br>
by assigning of values to the primary binary image LI BIN BCD(ixjy) of the background change detector according to the rule:<br>
LIBINBCD(ixjy) = 1, if  LpBCD(ixjy)   &gt; THRESHOLDBCD(ix jy),  or LIBINBCDOxjy^O, if  LpBCD(ixjy)   
at that, the reference background image signals Lnmb(ixjy) are formed by sharing the scaled<br>
image signals in the current analysis window into three types of image signals:<br>
image signals in the object window	- OWBCD,<br>
image signals in the background window	- BWBCD'<br>
image signals in the window - "New Background"	- NB.<br>
where image signals in a rectangle which lies in the center of the current analysis window and includes predominantly the object video image elements are defined as image signals in the object window; video image elements on outer boundaries of the current analysis window where new background image elements appear on account of object movement and the video camera field of view displacement are defined as image signals of the window "New Background"; all the rest image elements of the analysis window are defined as image signals of the background<br>
window; by storage of signals Ln AW (ix jy) from the current analysis window of the current lift ame scaled image in the window "New Background":<br>
Ln mb (ixjy) = Ln Aw (ix jy),    ixjy E NB, where Ln AW (ixjy) are signals values of the scaled image element intensity in the analysis<br>
window with coordinates ixjy;<br>
by averaging of scaled image signals Ln AW (ixjy) in the background window from the current analysis window by the recursive filter of the first order with the constant Wbw and with account for analysis window shift in the inertial coordinates system at the last frame:<br>
Lnmb(ixjyHl-Wbw)*Ln-i mb(ix+Vxjy+Vy)+Wbw*LnAW(ixjy)   ixjyeBWBCD, where Vx, Vy is the analysis window center displacement at the last frame in the inertial<br>
coordinates system horizontally and vertically;<br>
by re-recording in the object window of reference background image signals of the former frame with a shift which accounts for the analysis window center displacement at the last frame:<br>
Ln mb (ixjy) = U-i mb(ix+Vx, jy+Vy)   ix jy e O WBCD-<br>
The low-pass filtering of the primary binary image signals LI BW BCD(IX jy) of the background change detector may be executed with the help of two-dimensiowal convolution, for example<br>
(Equation Removed) <br>
where NF and MF are the horizontal and vertical parameters of the low-pass filter aperture; h|.p [di, dj] is pulse characteristic of the low-pass filter.<br>
The secondary binary image signals L2 BIN BCD(ix jy) of the background change detector are formed from the low-pass filter signals S_filBCD(ix jy) according to the rule:<br>
L2BINBCD(ixjy)=l, if S_filBcD(ixjy) &gt; PorogBCDl and Li BIN BCD(ix jy) = 1 or S_filBcD(ixjy) &gt; PorogBCDO and L, BIN BCD(ixjy) = 0, otherwise L2 BIN BCD(ix jy) = 0, where PorogBCDl, Porogeco0 are values of the decision thresholds for unit and<br>
zero elements of the primary binary image of the background change detector.<br>
binary image signals LaiNHc(ixjy) of the histogram classifier are formed according to the<br>
rule:<br>
LBINHc(ixjy)=l, if Wn.1NCow[LAw(ixjy)]&gt;an.i*A(ixjy)*Wn.iNHCBw[LnAw(ix1jy)], LBINHc(ixjy)=0 - otherwise,<br>
where Ln Aw(ixjy) are scaled image signals in the current analysis window,<br>
Wn.iNcow [T-i] is a noiinaliyed intensity distiibution histogram Lj of scaled image signals<br>
in the central object window - COW, obtained in the former (n-1) frame,<br>
Wn.iNHCBW [Li] is a noiiiialized intensity distribution histogram of background image<br>
signals in the histogram classifier background window - HCBW, obtained in the former<br>
(n-1) frame,<br>
Lj is the intensity level of image signals,<br>
i is the number of the intensity level of image signals, i = 1,..., Niev,<br>
N|ev is the video image signals intensity level number in the current analysis window,<br>
αn-i is a parameter which depends on the number of image elements in the histogram<br>
classifier object window - HCOW, classified as background in the former (n-1) frame,<br>
αn-i has the meaning of prior probability of availability of the background image element<br>
in the object window and may be defined according to the formula:<br>
(Formula Removed) <br>
where NNbackground(n-l) is the number of the background image elements detected in the central object window in the (n-1) frame,<br>
A(ixjy) is a penalty function which depends on image element coordinates ixjy in the analysis window, including ixjy ε COW A(ixjy)=Ao; A(ixjy) has the meaning of the erroneous classification of the video image elements into the object and background elements, values A(ixjy) are set as minimum in the central object window (Ao) and as crescent to the analysis window boundaries.<br>
value AO may be chosen in the values range 1,5,..., 2,0.<br>
After the object image coordinates X0, Y0 and sizes Rxo, Ryo in the current n-frame are detemiined, the rectangle with dimensions Rxcow=Rxo, Rycow=Ryo which lies inside the analysis window and which center coincides with the object image center Xo, YO and which includes predominantly object image elements, is defined as the central object window - HCOW; the area between two rectangles with common center and dimensions<br>
Rxcow, Rycow and RxHcow, RyHcow, where RxHcow&gt;Rxcow and RyHcow&gt;Rycow, is defined as the histogiam classifier object window - HCBW; the area between two rectangles with common center and dimensions<br>
RXHCOW,    RyHcow    and    RXHCBW,    RyHcsw,    where    RXNCBW&gt;RXHCOW    and RyHCBw&gt;RyHcow-<br>
The histogiaw WHCBW[LJ] of background image intensity distribution Lj is defined after image signals read from the COW window in the current n-firame; the histogram Wcow[L,] of object image intensity distribution Lj is defined after image signals selected from the COW window in the current n-frame; the histograms WHCBW[LJ] and WCow[L,] are smoothed<br>
(Equation Removed) <br>
where hsm [j] is pulse characteristic of the smoothing filter;<br>
(2*ns+l) is the number of points of the pulse characteristic of the smoothing filter; the current averaging threshold Thh of the smoothed histogram  Wcow[Li] of object image intensity distribution is defined according to the expression<br>
(Equation Removed) <br>
where Thho and kthh are the constant values.<br>
l[x] is a unit function defined by conditions: l[x] = latx&gt;0, l[x] = 0atx
Th h n = Th h n-l + Yn thresh *( Thh n " Th h n-l),<br>
where yn thresh is the constant of the threshold Thh averaging filter which varies according to the frame number from the value yi thresh=l in the first frame up to the stationary value Ythresh; n is the current image frame number;<br>
the values of the average threshold are restricted from above and below, the smoothed histogram<br>
Wcow[Lj] of the object image intensity distribution L, is averaged by the first order recursive<br>
filter<br>
W ncow[L,] - W „., COw[Li] + yn Cow*( WCOw[Li] - W „., COw[L,]),<br>
where yn cow is the constant of the averaging filter of the object image intensity distribution histogram which varies according to the frame number from the value yicow=l in the first frame up to the stationary value ycow, at that starting from the frame number which exceeds Nfc, where Nfc is the number of the frame after which the averaging of the histogram W cowDU] is perfoinied according to conditions, NfC=16,.. .,128;<br>
the histogram Wcow[Lj] of the object image intensity distribution Lj is averaged only for those intensity levels Lj which simultaneously fulfill the two conditions:<br>
W cow[Lj] &gt; Th h „   and   W COw[Li] &gt; an*AO* W HcuwOU).<br>
In the claimed method for statistical stability increase the object intensity distribution histogram is averaged only, as the object image in a sequence of frames is constantly in the COW and is changed relatively slowly. The use of the said conditions for the histogram averaging<br>
W nCow[Lj] allows to reduce expectancy of hitting of the background video image elements into the object histogram. Background intensity distribution smoothed histogram W HCBW[L,] is not averaged for the histogram W HCBW[L,] change and to avoid the erroneous classification of the background video image elements as belonging to the object, at the object<br>
move in the direction of the background area, having the same intensity as the object.<br>
The nouafllized histogram WnNcow[Lj] of the object video image intensity distribution is<br>
founed from the average histogram W „ cow[Lj] of the object video image intensity distribution Ls<br>
(Equation Removed) <br>
where NICV is the number of video image intensity levels Ln Aw(ix,jy);<br>
The nomiali/ed histogiam WnNHCBw[Lj] of background video image intensity distribution is foimed from the smoothed histogram W HCBW[LJ] of the object video image intensity distribution:<br>
(Equation Removed) <br>
Horizontal and vertical projections of the moving objects indicator and background change detector secondary binary images signals and of the histogram classifier binary video image signals are generated after the reception of the binary images.<br>
At that the horizontal and vertical projections GprMoi(ix), VprMoi(jy) °f the secondary binary image signals L2 BIN MOI(IX jy) of the moving-objects indicator are defined, for example, as follows<br>
(Equation Removed) <br>
where Nwin and Mwin are respectively horizontal and vertical dimensions of the secondary<br>
binary image of the moving-objects indicator.<br>
At that the horizontal and vertical projections GprBCD(ix), VprBCD(jy) of the secondary binary image signals L2 BIN BCD(ix,jy) of the background change detector may be defined as follows<br>
(Equation Removed) <br>
where Nwin and Mwin are horizontal and vertical dimensions of the secondary binary image of<br>
the background change detector.<br>
The horizontal and vertical projections GprHc(ix), VprHc(jy) of binary image signals of the histogram classifier LBIN HC(IX jy)are defined, for example, as follows<br>
(Equation Removed) <br>
where Nwin and Mwin are horizontal and vertical dimensions of the histogram classifier binary<br>
image.<br>
The confidence factors WBCD, WMOI, WHC of the moving objects indicator and background change detector secondary binary images signals and of the histogram classifier binary video image signals are defined, for example, as composition of functions of initial conditions input and the normalized average densities of binary images of the background change detector, moving-objects indicator and histogram classifier, respectively<br>
(Equation Removed) <br>
at that, the average densities VBCD(n), VMOI(n), VHC(n) of binary images of the background<br>
change detector, moving-objects indicator, and histogram classifier are obtained as a result of minimum and maximum values limitation and further averaging of the current densities VBCD(A), V\toi(n), Vnc(n) of the appropriate binary images by the first order recursive filters<br>
(Equation Removed) <br>
formed object image boundaries are defined.<br>
The current coordinates XGBIN, YGBIN of the generalized binary object image are defined, for example, as a weighted sum of gravity center coordinates XGGC, YGGC and area median coordinates XGMED, YGMED of the generalized binary object image XOBIN(n) = WGC(n) * XGGC + WMED(n) * XGMED, YGBIN(n) = WGC(n) * YGGC + WMED(n) * YGMED, WMED<n where wmed wgc are the weighting coefficients of median and gravity center></n>
coordinates estimates of the generalized binary object image;<br>
n is the current frame number,<br>
at that, the weighting coefficient WGC(n) of the centre of gravity coordinates value of the object generalized binary video image is increased at reduction of object coordinates mean deviation from their predictable values.<br>
Current speed of the object generalized binary video image in the inertial coordinate system is determined. At that the current horizontal VGQBBIN and vertical WQBBIN estimate components of the generalized object binary image traverse speed in the inertial coordinates system may be defined according to expressions<br>
VGoB BIN - (dx + ΔXAw + ΔXoB AW BIN)/T, WOB BIN = (dy + Δ YAW + Δ YOB AW BIN)/T, where dx, dy is, respectively, horizontal and vertical axis displacement of the video camera field<br>
of view at the time T between reception of the current and former video image fields;<br>
ΔXAW, ΔYAW is horizontal and vertical repositioning of the analysis window in the<br>
current frame in relation to the previous one,<br>
ΔXoB AW BIN, ΔYoB AW BIN is horizontal and vertical change of binary object image<br>
coordinates in the analysis window in the current frame in relation to the previous one.<br>
Simultaneously with processing of signals of the channels of the histogram classification, of the background change detector and of the moving objects indicator and with determination<br>
on their basis of the current coordinates and speed of the object generalized binary video image displacement in the images dissimilarity measure evaluation channel, object image coordinate are defined in relation to the center of the current analysis window on the basis of forming of images dissimilarity measure signals as a result of non-linear high-pass filtering of scaled video image signals in the ciment analysis window, fulfilled on condition that the average area of the generalized binary object video image exceeds the threshold value, of storage of received after the non-linear filtering signals, of generation of static master object image signals or static and dynamic master object images signals, of reduction of object static reference image signals or object static and dynamic reference image signals to the current scale, of forming and storage of dissimilarity measure signals between the image signals of the current analysis window and object static or static and dynamic reference images signals in the two-dimensional search area of object video image shifts, of definition of minimum values of video images dissimilarity measure signals along the lines and columns of two-dimensional search area of object video image shifts. At that, the nonlinear high-pass filtering of scaled video image signals in the current analysis window is executed, for example according to the expression<br>
(Equation Removed) <br>
FNF [ L ] is the function of two-sided delimitation,<br>
FNF [ L ] = LTHRESH     at L &gt; LTHRESH,<br>
FNF [ L ] = L	at -LTHRESH<i></i>
FNF [ L ] = -LTHRESH    at L 
NF, MF are horizontal and vertical parameters of the filter aperture,<br>
KNF, KNPF are the constant coefficients,<br>
LTHRESH is the threshold level of the two-sided delimitation function.<br>
Depending on the limitations of the allowed time of the reaction at the forming of the tracking system video camera field of view axis location management signals or on the limitations of the hardware overhead, the method provides for the possibility of the object coordinates detennination in the analysis window on the basis of the image dissimilarity measure signals using either object static or static and dynamic reference images only.<br>
Processing with the use of the object static reference video image only requires less time and hardware overheads, but it is less reliable than processing with the use of the object static and dynamic reference images in deteimining the static template change conditions.<br>
In case of the usage of object static reference video image only, the object static reference video image signals axe generated, for example, by means of reading and storage of video image signals from the cita'ent analysis window in a rectangular window with dimensions equal to dimensions of the generalised binary object image with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XOB FOV - XAW FOV, YC MAS = YOB FOV - YAW FOV, where XOB FOV, YOB FOV are object video image coordinates in the field of view,<br>
XAWFOV, YAW FOV are analysis window coordinates in the field of view,<br>
meeting the conditions of the object static reference image fomied on the basis of analysis of parameters of dissimilarity measures signals of image signals after the nonlinear high-pass filtering in the current analysis window and object static and dynamic reference image signals, as well as object image trajectory parameters analysis obtained on the basis of analysis of dissimilarity measures signals.<br>
In case of the usage of object static and dynamic reference images signals of the object dynamic reference image are foiined, for example, by means of reading signals in each frame from the current analysis window in a rectangular window with dimensions equal to dimensions of the generalized binary object image, and with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XOB FOV - XAW i ov-<br>
YC MAS - YOB FOV - YAW FOV, where XOB FOV, YOB FOV are object image coordinates in the field of view,<br>
XAW FOV, YAW FOV are analysis window coordinates in the field of view, and the signals of the object static reforence image are formed by means of reading and storage of image signals from the current analysis window in a rectangular window with dimensions equal to dimensioas of the generalized binary object image with the center which coordinates Xc MAS, YC MAS are defined by the difference<br>
Xc MAS = XOB FOV • XAW FOV, YC MAS = YOB FOV " YAW FOV, where XOB FOV, YOB FOV are object video image coordinates in the field of view,<br>
XAW FOV, YAW FOV are the coordinates of analysis window in the field of view, meeting the conditions of the object static reference image change foiuied on the basis of analysis of parameteis of dissimilarity measures signals of image signals after the nonlinear high-pass filtering in the ctuient analysis window and object static and dynamic reference image signals, as well as object image trajectory parameters analysis obtained on the basis of analysis of object static and dynamic reference image signals being used.<br>
Object video image coordinates in the analysis window in the images dissimilarly measure value chawei are deteonined depending on the minimum images dissimilarity measure signals values succession type.<br>
Object video images coordinates in the analysis window are deteru-ined by means of analytical approximation of the minimum values sequence of images dissimilarity measure signals by the fomth degree polynomial and detemiination of object image coordinates as the approximation polynomial minimum position on condition that the sequence is related to the type of sequences with two boundaries of values fast growth areas of images dissimilarity measure signals close to the position of its minimum.<br>
Analytic approximation of the minimum images dissimilarity measure signals values succession is executed, for example, by the method of the least squares (Glovatskaya A. P. Methods and Algorythms of the Computational Mathematics. - M.: Radio and Connection, 1999.- p. 408: pict., pp 99-109), resulting in the solution of the 5 linear equations relatively to the 5 unknown factors b, (i = 0,...,4)<br>
of the F(x) type approximating polynomial: F(x) = bo+b|X+b2x2+b3x3+b4x4<br>
Minimum of the approximation function is determined in explicit type by the cubic equation solution<br>
bi+b2x+b3x2+b4x3=0,<br>
for example, trigonometiic method (see G. Korn and T. Korn - Mathematics handbook -Translation from the second American revised edition. Edited by Abramovich I. G. M., "Nauka" 1973, 832 pages with drawings, pp 43 11).<br>
If the succession may be referred to the type of sequences with plane neighbourhood of the minimum position and existence of one values fast growth area of images dissimilaiky measure signals, then object video image coordinates in the analysis window determine as the value proportional to the shift of values fast growth area boundary of images dissimilaiky measure signals relatively to the analysis window center. This allows to increase stability of tracking over the bovnndaries of the extensive and intensity homogeneous objects.<br>
Analysis window center coordinates are taken as the object video image coordinates in the analysis window on condition that the sequence is referred to the type of sequences with plane neighbourhood of values minimum position of images dissimilarity measure signals in the whole search area of object video image shift. This allows to eliminate random displacements ("wandering") of the tracking system video camera field of view axis, within the limits of the object dimensions when tracking over the extensive and intensity homogeneous objects is taking place.<br>
Current object video image traverse speed in the inertial coordinates system and confidence factor of this speed are defined using object video image coordinates obtained on the basis of images dissimilarity measure signals forming.<br>
At that the current horizontal VGoe DISSIM and vertical WQB DISSIM estimate components of the binary object video image traverse speed in the inertial coordinates system obtained on the basis of images dissimilarity measure signals forming are defined, for example, according to expressions<br>
VGoB DISSIM = (dX + ΔXAW + ΔXoB AW DISSIM)T • VVoB DISSIM = (dY + ΔYAW + ΔYos AW DISSIM)/ I -<br>
where dX, dY is respectively horizontal and vertical displacement of the tracking system videq<br>
camera field of view axis at the time T between reception of the current and fanner video<br>
image fields,<br>
ΔXAW, Δ YAW is, respectively horizontal and vertical change of the analysis window<br>
position in the current frame in relation to the former one,<br>
ΔXOB AW DISSIM, Δ YOB AW DISSIM is horizontal and vertical change of object coordinates in<br>
the analysis window in the current frame in relation to the former one, obtained on the<br>
basis of images dissimilarity measure signals forming.<br>
The confidence factor WsiM(n) of the object image current traverse speed, obtained on the basis of images dissimilarity measure signals forming, is obtained by calculating the current<br>
similarity factor <br>
(Equation Removed) <br>
averaging by the first order recursive filter, normalizing the calculated average similarity factor<br>
VSIM(n):<br>
(Equation Removed) <br>
where n is the current frame number;<br>
σBmin(n) is the minimum value of the mean-square value of background image signals in<br>
M windows of background analysis;<br>
EoissiMmin (n) is the minimum value of images dissimilarity measure signals of the current<br>
analysis window and static reference video image in two-dimensional search area of<br>
object image,<br>
VBIN(n) is the average density of the object generalized binary image obtained after<br>
calculating the current density VBIN(H) =   of the object generalized binary<br><br>
image, after limiting the minimum and maximum values of the current density VBIN(n) of the generalized binary object image; and further averaging of the delimited current density VBIN(n) of the object generalized binary image by the first order recursive filter; SGBIN(n) is the current area of the generalized binary image inside the boundaries of the<br>
generalized binary image of the object;<br>
SoR(n) is the cunent area of the region inside the boundaries of the object generalized<br>
binary image.<br>
The confidence factor of the object generalized binary image current traverse speed in the inertial coordinates system is detemiined. At that the confidence factor WBIN(n) of the object generated binary image current traverse speed may be calculated, for example, by defining the<br>
current density VBIN(n)=  GBINl- ' of the generalized binary image; by limiting the minimum and<br>
SOR(n)<br>
maximum values of the current density VBIN(n) of the generalized binary image; further averaging of the generalized binary image delimited density by the first order recursive filter; normalizing the average density VBIN(n) of the generalized binary object image<br>
(Equation Removed) <br>
where SGBIN(n) is tine cuaent area of the generalized binary image inside the object image boundaries;<br>
SoR(n) is the current area of the region inside the object image boundaries, n is the current frame number, VSIM (n) is the average similarity factor obtained after calculating the current similarity<br>
factor VSIM(*I)= after limitation of its maximum and minimum values and<br><br>
averaging by the first order recursive filter;<br>
σBmin(n) is the minimum value of the mean-square value of background video imagf-<br>
signals in M windows of background analysis;<br>
EDissiMmin(n) is the minimum value of images dissimilarity measure signals of the cvuicnt<br>
analysis window and object static reference video image in two-dimensional search area<br>
of object image shifts.<br>
The complex estimate of the current object image traverse speed in the inerttal coordinates system is fomied from estimate data of the current object image traverse speed' obtained on the basis of image dissimilarity measure signals forming and estimate data of the current object traverse speed across the generalized signals projections of the generalized binaty object image with allowance for confidence factors which fonn the object image traverse speeds and prior restrictions of the object manoeuvring speed, for example, the complex estimate of horizontal VGoe(n) and vertical VVoB(n) constituents of the object image current traverse speed in the inertial coordinates system is defined limiting the estimates of the binary object imgge traverse speed and object image traverse speed obtained on the basis of images dissimilarity measure signals fottnifig, by the minimum and maximum values foimed with allowance for preceding values of the complex estimate of the object image current traverse speed, and forming the weighted sum of delimited estimates of the object binary image traverse speed and object image traverse speed obtained on the basis of images dissimilarity measure signals forming VGon(n) = WBIN(n) * VGOB BIN(n) + WSiM(n) * VGOB DISSIM(n), VVOB(n) = WsnoCn) * VVOB BIN(n) + WSiM(n) * VVOB DISSIM(n), where WBIN(II), WSIM(n) are, respectively, confidence factors of the object generalized binary<br>
image current traverse speed and the object image current traverse speed obtained on the<br>
basis of fouling of images dissimilarity measure signals,<br>
VGoe BIN(n) and VVOB BIN(H) are delimited horizontal and vertical constituents of the<br>
object generalized binary image current traverse speed;<br>
VGoB DI.SSIM(H) and VVOB DISSIM(n) are limited horizontal and vertical constituents of the<br>
object image current traverse speed obtained on the basis of images dissimilarity measure<br>
signals forming,<br>
n is the cuuettt frame number.<br>
The complex estimate signals of the current video image traverse speed using, for example,<br>
recursive low-pass filter of the first order and averaged speed values are stored.<br>
The object inwge coordinates in the video camera field of view are defined by difference integration of the complex estimate of the current object video image traverse speed in the inertial coordinates system and controlled displacement speed of the video camera field of view axis in the inertial coordinates system, at that the initial coordinates of the object video image in the video camera field of view and the signal of the beginning of tracking are received from the tracking system. Object video image coordinates are extrapolated.<br>
Tracking system video camera field of view axis displacement management signals are fonned using object video image coordinates in the video camera field of view obtained as the result of video image processing in the current analysis window or using the extrapolated coordinates and object video image traverse speed according to results of analysis of the current and average area of the object generalized binary image, current and average object image traverse speed, area and coordinates of binary images boundaries of the histogram classifier in M windows of background analysis, at that, the extrapolated object image traverse speed is regenerated on the basis of the analysis of the object video image traverse speed averaged complex estimate stored values.<br>
Signals of the position and dimensions of the video image analysis window for the following frame are formed using the signals of the object video image coordinates in the field of view of the tracking system video camera and object video image dimensions, obtained as the result of the former frame processing, at that the initial values of the signals of the location and dimensions of the object video image and the signal of the start of the tracking are received from the tracking system.<br>
Simultaneously with the signals processing in the histogram classifier, background change detector, moving objects indicator and image dissimilarity measure value channels M windows of background analysis are foimed along the analysis window perimeter and signals projections of the histogram classifier binary video images are deteimined in M windows from which histogram classifier binary images boundaries areas and coordinates are determined in the M background analysis windows.<br>
Current SGBIN(n) and averaged S GBfN(n) areas of the object generalized binary image and analyzed, for example, by testing of the condition satisfaction:<br>
SGBIN(n) 
SGBIN(N)&gt; ks?(n-nEx)* S GBIN(n) for change for fanning of the tracking system video camera field of view axis displacement management signals after object image coordinates in the tracking system video camera field of view obtained as a result of image processing in the current analysis window, where ksl is the constant coefficient, ksl<l></l>
ks2(n-nnx) is the coefficient which diminishes with growth of the frame number n,<br>
starting Horn the frame number REX, change for forming of the video camera field of view<br>
axis  displacement  management  signals  after  the  extrapolated  coordinates,   ks2(n-<br>
BEX) 
The technical result is also achieved by the fact that the analysis of the current and average object image traverse speed is executed by testing of the condition satisfaction: VGcoB(n) - VG coB(n) &gt; kvl * VG CoB(n) + kv2, VVCoB(n) - VV coB(n) &gt; kvl * VV COB(n) + kv2,<br>
-	for change for forming of the video camera viewing field axis displacement management<br>
signals after the extrapolated coordinates, or the conditions satisfaction:<br>
VGCOB(n) - VG coB(n) 
-	for change for forming of the tracking system video camera field of view axis displacement<br>
management signals after object image coordinates in the tracking system video camera field of<br>
view obtained as a result of image processing in the current analysis window,<br>
where VGcoB(n), VVcoe(n) are horizontal and vertical constituents of the object video image current traverse speed obtained as a result of recursive filtering of the of the first order with the filter constant 0<wv1 of the constituents vgob vvoe complex estimate object video image traverse speed></wv1>
VGcoB(n&gt;= VGcoB(n-l) + WV1 *[ VGOB(n) - VGCOB(n-l)], VGCOB(n)= VGcoB(n-l) + WV1 *[ VGOB(n) - VGCOB(n-l)],<br>
VGcoe(n), VVcoB(n) are horizontal and vertical constituents of the object image average traverse speed obtained as a result of filtering of the first order of the complex estimate constituents VGosfcX VVoaCn) of the object image traverse speed with the filter constant 0<wv2></wv2>
VG coB(n)= VG coB(n-l) + WV2*[ VGOB(n) - VG coe(n-l)], VG coB(n)= VG coB(n-l) + WV2*[ VGOB(n) - VG coB(n-l)j, atthatWVl&gt;WV2.<br>
The area and coordinates of object images boundaries of the histogiam classifier ai§ analyzed in the M background analysis windows by testing the satisfaction of the conditions of histogram classifier binary image area changes detection in the M background analysis windows:<br>
|Si(n)- Si(n)|&gt;Sthreshoid(n),<br>
where Si(n) are the current values of the histogram classifier video binary images current area inside the i-window of background analysis, i = I,..., M;<br>
S i(n) are the averaged values of the histogram classifier binary video mages current area inside the i-window of background analysis, i=l,...,M, obtained at the output of the filter of the first order with the filter constant 0<ws></ws>
Sj(n) = Si(n-l) + WS*[S,(n)- S.(n-l)],<br>
Sthreshoid(n) is the threshold of the change detection of the binary video images of the histogram classifier in the M background analysis windows:<br>
(Equation Removed) <br>
where k is the constant coefficient,<br>
S GBIN(n) is the average area of the generalized binary object video image, obtained at the output of the first order recursive filter with the filter constant 0<wsob></wsob>
SGBIN(n) = SGBIN(n-l) + WS0u*[ SGBIN(n) - SGBIN(n-l)]; n is the current frame number.<br>
(Equation Removed) <br>
where  L0 is the average value of video image signals in the object window;<br>
LB is the aveiage value of video image signals in the background window;<br>
crB is the root-mean-square deviation of video image signals in the background window<br>
from LB;<br>
FCONSTR(Q) is the constraint function of minimum and maximum values;<br>
when fulfilling the mentioned above conditions for the current area of the histogram classifier binary video images in one or several background analysis windows with ki numbers, where ki are the nnmbers of the background analysis windows in which the change of the<br>
histogram classifier binary video image current area was detected, ki=l,.. .,kN;<br>
kN is the number of the background analysis windows, in which the change of the<br>
histogram classifier binary video image current area was detected, kN M; the coordinates of two inter-perpendicular boundaries of the histogram classifier binary image placed on the side of the object window are analyzed in these windows, at that, the horizontal displacement of the binary image vertical boundary is analyzed in background analysis windows placed on the side of the object window vertical boundaries and the membership of horizontal boundary coordinates to the object window vertical coordinates is tested; the vertical displacement of the binary image horizontal boundary is analyzed in background analysis windows placed on the side of the object window horizontal boundaries and the membership of vertical boundary coordinates to the object window horizontal coordinates is tested, at that the binary video image boundaries displacement in the M background analysis windows analysis is executed by checking the conditions of location of the correspondent histogram classifier binary image boundaries inside the boundaries of internal Okil and exterior Oki2 regions of the ki background analysis window with forming of features Fkil-1 and Fkj2=l of the belonging to the binary video images boundaries to the to internal Oki1 and exterior Oki,2 regions of the ki background analysis window,<br>
where Fkil = l   if the binary  video  image  border of the histogram  classifier  is  inside  the boundaries of<br>
the Okil region, or Fkil=0 otherwise;<br>
Fkj2=l if the binary video image boundary of the histogram classifier is inside the<br>
boundaries of the Okj2 region, or F|cj2=0 otherwise;<br>
at that, at the detection of the correspondent binary video image boundaries of the histogram classifier transfer from the exterior region to the interior region of the background analysis windows with the numbers ki=kipl, where kipl are the numbers of the analysis windows in which binary video image boundaries of<br>
the histogram classifier transfers from the exterior regions Oidpi2 to the interior regions<br>
Oicipil of the background analysis window were detected, kip 1=1,2,..., kN, at that, at successive forming of features Fkipi2=l at nk,pi=nk,pi2, and then Fkjpil=l at nkipi=nkipil, where nkipil&gt;nkjpi2, niopil is the frame number, in which in kipl analysis window the feature state is set as Fkipil=l, nkipi2 is the frame number, in which in kipl analysis window the feature state is set as Fkipi2=l, and at satisfaction of the condition of membership of the second tested boundary coordinate of the histogram classifier binary video image in the kipl background analysis window to the range of object window coordinates, the change for forming of the video camera field of view axis displacement management signals after the extrapolated coordinates is made,<br>
the SP chgnge counter of the image boundaries of the histogram classifier transfers between the exterior and interior regions of the background analysis window is bound to SP=1 at the first detection of the image boundaries of the histogram classifier transfer from the exterior regioas OkiPi2 to the interior regions Okipil or the SP change counter increased by one at the second detection of the image boundaries of the histogram classifier transfer from the exterior regions Okipi2 to the interior regions Okipil at the time of forming of the tracking system video camera field of view axis displacement management signals over the extrapolated coordinates of the object video image, the features states are set as Fkipi 1=0, Fkipi2=0,<br>
the features states are set as Fj2=0 in the background analysis windows with the numbers j*kipl, j=l,...,kN, and the analysis process of the histogram classifier binary image boundaries is started from the very beginning,<br>
at the detection of the correspondent image boundaries of the histogram classifier transfer from the exterior regions Okip2 1 to the interior regions Okip22 of the background analysis windows, where kip2 are the numbers of the analysis windows in which binary video image boundaries of the histogram classifier transfers from the exterior regions OkiP22 to the interior regions O|dp2l of the background analysis window were detected, kip2=l ,2,.. .,kN, at the time of forming of the tracking system video camera field of view axis displacement management signals that is at the successive forming of the feature Fkjp2l=l at , and then Fki22 =l at ninpj=nicjp22 in one or several background analysis windows, where nk,p22&gt;nkjp2l» the SP change counter is decreased by one, and if SP=0 the change for forming of the video camera field of view axis displacement management signals is made after object image coordinates in the video camera field of view, obtained as a result of image processing in the current analysis window, the features states are set as Fk,p2l=0, Fkip22=0, the features states are; set as Fj2=0, in those background analysis windows with the numbers j*kip2, j=l,...,kN and the analysis process of the histogiam classifier binary image boundaries is started from the very beginning, the period of foiniing of the video camera field of view axis displacement management signals after the extrapolated coordinates and the time when the features Fjl, Fj2, i=l,...,M, are in the state Fjl=l, Fj2=l is tested and when the set time intervals are exceeded the, appropriate features are set at zero state, there is a change for forming of the video camera field of view axis displacement management signals after object image coordinates in the video camera field of view obtained as a result of image processing in the current analysis window.<br>
On application of the said method, in the presence and adequate accuracy of the tracking system video camera field of view uncontrolled displacement and shift sensors, the structure and sequence of operations differ from the mentioned above only in the part, preceding to the generation and scaling of the video image signals in the analysis window, which consists in the following.<br>
Before the start of the reception of the signals of the current video image n-field, where n=3,4, 5,..., the controlled video camera field of view dx[n] and dy[n] axis displacement is defined at the time between the reception of (n-1) and (n-2) video image fields signals horizontally and veitieaHy, respectively, which is determined by the impact of the field of view displacement management signals on the video camera.<br>
The controlled displacement dx[n] and dy[n] of the video camera field of view axis is defined, for example, by calculation of the convolution of management signals XMAN[I], YMAN[I] which control the video camera field of view displacement with pulse characteristics hx[i] and hy[i] of its drives according to the expression (39).<br>
Tracking system video camera field of view axis controlled displacement speed is deteimined from the video camera field of view axis controlled displacement data at the time between the reception of the foaiier and of the current video image fields signals.<br>
Current video image field and tracking system video camera field of view uncontrolled displacement and rolling signals are received and stored, and then used during the forming of the current video image frame in the following way.<br>
Video image signals of the current frame are formed from image signals of the former frame with allowance for controlled field of view axis displacement at the time between reception of the footer and current video image fields signals with elementwise transfoimation of signals coordinates of the current video image field which compensate for the tracking system video camera field of view uncontrolled displacements and roll.<br>
At that fanning of signals LpRAME(ix,jy) of the current frame video image at interlacing from signals LH-FRAME(i,P,npk) of the image current half-frame and video image L -iFRAME(ix jy) of the former frame is executed, for example, by prognostication of signals LpRAME(ix jy) of the current frame video image with the help of the former frame video image shift L -iFRAME(ixjy) by the amount of controlled displacement dx, dy of the video camera field of view axis horizontally and vertically, respectively, at the time between reception of video image half-frames<br>
LFRAME(ixjy) = L -iFRAME(ix+dx,jy+dy),<br>
and by substitution of the current frame prognosticated video image pixels with the current half-frame image pixels with compensation for the current uncontrolled shifts rx(p), ry(p) and roll cp(p) of the tracking system video camera field of view<br>
LFRAME   ix(i,p,npk), jy(i,p,npk)    = L H-FRAME (i,p,npk),<br>
where i is the element number in the current half-frame video image line, i=l,..., NK, ix is the element number in the current frame video image line, ix=l,..., NK,<br>
p is the line number in a half-frame, p=l,...,<br>
jy is the line number in a frame, jy=l,..., MK, NK is the number of video image elements in a line, MK is the number of lines in an video image frame, npk is the current half-frame index,<br>
npk = 1 - in uneven half-frames,<br>
npk = 0 - in even half-frames,<br>
(Equation Removed) <br>
As it follows from the aforesaid, in the concerned situation (of the presence or poor accuracy performance of the video camera field of view uncontrolled displacement and rolling sensors) video image stabilization parameters rx(p), ry(p) and φ(p) are determined and used once at the half-frame (are constant in the boundaries of the current half-frame) and that is why in such conditions the method is advisable to be used in the presence of the video camera field of view destabili/jng slow factors.<br>
The method is universal relatively to the information array of the destabilising factors and provides the adaptive operation factors setting based on the analysis of the available data of these factors, at that operability remains the same in the partial absence of the factors.<br>
In the absence of the information of any of the signals sensors on the basis of which video image stabilization parameters are generated the method keeps the possibility of<br>
coordinates determination and of keeping objects in the field of view center, but with the correspoading degtadation of parameters. So in the absence of the information of the roll sensor the video image stabion is performed in the compensation part of the horizontal and vertical displacements of field of view, and field of view angle of roll value is set as (p(p)=0, in most cases this is enough for the automatic coordinates detemiination and to keep object video image in the field of view center when the dimensions of the object video image are not more than % of the field of view width.<br>
The block of actions development in the foim of the 4 groups of the concurrent run actions, which results are shared and are complementary in relation to each other, provides the continuity of the automatic coordinates determination and allows to keep object video image in the field of view center even at the abrupt change of the conditions of functioning.<br>
When the fault of the uncontrolled displacements and (or) of the field of view roll sensors takes place, for example, data of the function channels (of the moving objects indicator and of the background change detector), which require video image stabilization, become practically uncertain in the presence of the said field of view disturbance in hard teuain background conditions. According to the method, these function channels parameters are adapted in such a way that the density of the binary images they foun is reduced, which results in their confidence factors reduction which is equivalent to the "self-switching-off' of the uncertain channels. Remaining functional channels (histogram classifier channel and images dissimilarity measure analysis channel) can provide uninterrupted coordinates detetmination and allow to keep object video image in the field of view center in a number of conditions.<br>
Functioning of the whole system under the unfavourable change of tracking conditions is provided similarly.<br>
Industrial application<br>
For industrial utiliyation of the invention, the device according to the first variant may be assembled in the following way, for example.<br>
The first input l.l of the unit I of the object coordinates determination device, (see Fig. 8), is connected to the first input of the device, and the second input 1.2 is connected to the second input of the device.<br>
memory (43), the former (32) of the histogram classifier binary video image signals in the current analysis window, former<br>
Input of the unit 2 is connected to the third input of the device. The second output 1-2 of the unit 1 is connected to the third input 3.3 of the unit 3 which first output 3-1 is connected to the second input 4.2 of the unit 4, which first output 4-1 is connected to the first input 5.1 of the unit 5, which first 5-1 and the second 5-2 outputs are connected to the first 6.1 and the second 6.2 inputs of the unit 6. The second input 5.2 of the unit 5 is connected to the second output 1-2 of the unit 1. The first output 1-1 of the unit 1 is connected to the first input 4.1 of the unit 4 and to the first input 7.1 of the former 7, which second 7.2 and the third 7.3 inputs are connected to the first 6.1 and the second 6-2 outputs of the unit 6, respectively. Output of the unit 2 is connected to the third inputs 4.3 and 6.3 of the unit 4 and of the unit 6 respectively, which fourth input 6.4 is connected to the first output 3-1 of the unit 3. The fourth input 4.4 of the unit 4 is connected to the second 1-2 output of the unit 1. The output of the former 7 is connected to the first input 8.1 of the unit 8, and the fourth input 7.4 of the former 7 is connected to the second output 1-2 of the unit 1, to the fifth input 8.5 of the unit 8, to the second input 9.2 of the unit 9, to the third input 10.3 of the unit 10, to the eighth input 11.8 of the unit 11, to the fourth input 12.4 of the unit 12, to the fifth input 13.5 of the commuator 13, to the fourth input 14.4 of the unit 14, to the fourth input 15.4 of the unit 15, to the second input 16.2 of the unit 16, to the fourth input 17.4 of the unit 17, to the fifth input 18.5 of the unit 18, to the third input 19.3 of the unit 19, to the sixth input 20.6 of the analyzer 20, to the third input 21.3 of the unit 21, to the third input 22.3 of the unit 22 and to the sixth input 23.6 of the unit 23. The output of the unit 8 is connected to the seventh input 10.7 of the unit 10 and to the third input 11.3 of the unit 11. The first output 11-1 of the unit 11 is connected to the second input 12.2 of the unit 12, The fourth and the fifth inputs of the device are connected to the first 13.1 and to the second 13.2 inputs of the commutator 13 respectively, which first 13-1 and the second 13-2 outputs are connected to the third 8.3 and to the second 8.2 inputs of the unit 8 respectively, and with the first 10.1 and the second 10.2 and to the seventh 11.7 and to the 11.6 inputs of the unit 10 and of the unit 11 respectively. The sixth input of the device is connected to the second input 3.2 of the unit 3, and the seventh input is connected to the fifth input 6.5 of the unit 6. The first input 9.1 of the unit 9 is connected to the first output 10-1 of the unit 10, which fifth input 10.5 is connected to the second output of the 6-2 of the unit 6, to the first input 11.1 of the unit 11 and to the first input 12.1 of the unit 12. The second output 10-2 of the unit 10 is connected third input 13.3 of the multiplexer 13 and to the first input 14.1 of the unit 14, which second input 14.2 is connected to the fourth output 10-4 of the unit 10, and the third input 14.3 is connected to the second output 15-2 of the unit 15. which first input 15.1 is connected to the second output 1 1-2 of the unit 11 and the second input 15.2 is connected to the second output 14-2 of the unit 14. The output of the unit 9 is connected to the first input 16.1 of the unit 16. The third output 10-3 of the unit 10 is connected to the first input 17.1 of the unit 1 7,<br>
Input of the unit 2 is connected to the third input of the device. The second output 1-2 of the unit 1 is connected to the third input 3.3 of the unit 3 which first output 3-1 is connected to the second input 4.2 of the unit 4, which first output 4-1 is connected to the first input 5.1 of the unit 5, which first 5-1 and the second 5-2 outputs are connected to the first 6.1 and the second 6.2 inputs of the unit 6. The second input 5.2 of the unit 5 is connected to the second output 1-2 of the unit 1. The first output 1-1 of the unit 1 is connected to the first input 4.1 of the unit 4 and to the first input 7.1 of the former 7, which second 7.2 and the third 7.3 inputs are connected to the first 6.1 and the second 6-2 outputs of the unit 6, respectively. Output of the unit 2 is connected to the third inputs 4.3 and 6.3 of the unit 4 and of the unit 6 respectively, which fourth input 6.4 is connected to the first output 3-1 of the unit 3. The fourth input 4.4 of the unit 4 is connected to the second 1-2 output of the unit 1. The output of the former 7 is connected to the first input 8.1 of the unit 8, and the fourth input 7.4 of the former 7 is connected to the second output 1-2 of the unit 1, to the fifth input 8.5 of the unit 8, to the second input 9.2 of the unit 9, to the third input 10.3 of the unit 10, to the eighth input 11.8 of the unit 11, to the fourth input 12.4 of the unit 12, to the fifth input 13.5 of the commnator 13, to the fourth input 14.4 of the unit 14, to the fourth input 15.4 of the unit 15, to the second input 16.2 of tine unit 16, to the fourth input 17.4 of the unit 17, to the fifth input 18.5 of the unit 18, to the third input 19.3 of the unit 19, to the sixth input 20.6 of the analyzer 20, to the third input 21.3 of the unit 21, to the third input 22.3 of the extrapolator 22 and to the sixth input 23.6 of the unit 23. The output of the former 8 is connected to the seventh input 10.7 of the unit 10 and to the third input 11.3 of the unit 11. The first oMput 11-1 of the unit 11 is connected to the second input 12.2 of the unit 12. The fourth and the fifth inputs of the device are connected to the first 13.1 and to the second 13.2 inputs of the commutator 13 respectively, which first 13-1 and the second 13-2 outputs are connected to the third 8.3 and to the second 8.2 inputs of the former 8 respectively, and with the first 10.1 and the second 10.2 and to the seventh 11.7 and to the 11.6 inputs of the unit 10 and of the unit 11 respectively. The sixth input of the device is connected to the second input 3.2 of the unit 3, and the seventh input is connected to the fifth input 6.5 of the unit 6. The first input 9.1 of the unit 9 is connected to the first output 10-1 of the unit 10, which fifth input 10.5 is connected to the second output of the 6-2 of the unit 6, to the first input 11.1 of the unit 11 and to the first input 12.1 of the unit 12. The second output 10-2 of the unit 10 is connected third input 13.3 of the multiplexer 13 and to the first input 14.1 of the analyzer 14. which second input 14.2 is connected to the fourth output 10-4 of the unit 10, and the third input 14.3 is connected to the second output 15-2 of the analyzer 15, which first input 15.1 is connected to the second output 11 -2 of the unit 11 and the second input 15.2 is connected to the second output 14-2 of the unit 14. The output of the former 9 is connected to the first input 16.1 of the unit 16. The third output 10-3 of the unit 10 is connected to the first input 17.1 of the unit 17,<br>
which first output 17-1 is connected to the fourth input 18.4 of the unit 18, which first 18.1 and the second 18.2 inputs are connected to the first outputs 14-1 and 15-1 of the units 14 and 15 respectively, and the third input 18.3 is connected to the first output 12-1 of the unit 12. The second input 17.2 of the "nit 17 and the first input 19.1 of the unit 19 are comBINed and connected to the second output 6-2 of the unit 6. The first output 16-1 of the unit 16 is connected to the third input 20.3 of the analyzer 20, which second input 20-2 is connected to the first output 10-4 of die unit 10, and the first input 20.1 is connected to the fifth output 10-5 of the unit 10, which sixth output 10-6 is connected to the third input 15.3 of the unit 15. The second output 20-2 of the analyzer 20 is connected to the second input 11.2 of the unit 11. The first output 18-1 of the unit 18 is connected to the fifth input 11.5 of the unit 11, to the second input 19.2 of the unit 19, to the fourth input 20.4 of the analyzer 20 and to the first input 21.1 of the unit 21, which first output 21-1 is connected to the second input 22.2 of the unit 22 and to the fifth input 20.5 of the analyzer 20, which first output 20-1 is connected to the fourth input 10.4 of the unit 10, to the second input 21.2 of the "nit 21 and to the second input 23.2 of the former 23, which first input 23.1 is connected to the second output 10-2 of the unit 10. The first output 19-1 of the unit 19 is connected to the forth input 13.4 of the commutator 13, to the first input 22.1 of the unit 22 and to the third input 23.3 of the former 23, which fourth input 23.4 is connected to the first output 22-1 of the "nit 22, and the fifth input 23.5 is connected to the second output 21-2 of the unit 21. The first output 23-1 of the former 23 is connected to the device output and to the first input 3.1 of the unit 3, and the second output 23-2 is connected to the forth input 8.4 of the unit 8, to the sixth input 10.6 of the wiit 10, to the fourth input 11.4 of the unit 11 and to the third inputs 12.3 and 17.3 of the units 12 and 17 respectively. The third input 1.3 of the unit 1 is connected to the fourth output 20-4 of the analyzer 20, and the fourth input 1.4 of the unit 1 is connected to the seventh output 10-7 of the unit 10. The output of the processor 24 is connected to the second outputs 3-2 of the unit 3,4-2 of the unit 4, 12-2 and 17-2 of the unit 12 and 17, 16-2 of the unit 16, 18-2, 19-2 and 22-2 of the units 18, 19 and 22, to the third outputs 14-3 and 15-3 of the units 14 and 15, 20-3 of the analyzer 20, 21-3 of the unit 21 and 23-3 of the former 23 and to the eighth output 10-8 of the unit 10 using the bidirectional bus.<br>
The first input 4.1 of the unit 4 is connected to the first input 25.1 of the selector 25 (see Fig. 9), which first 25-1, second 25-2 and third 25-3 outputs are connected to the first inputs 26.1 of the first, 27.1 of the second and 28.1 of the third calculators 26, 27 and 28 respectively, which second 26.2, 27.2 and 28.2, third 26.3, 213 and 28.3 and forth 26.4. 21.4 and 28.4 inputs are connected to the first -1.1, second 4.2 and third 4.3 inputs of the unit 4 respectively. The first<br>
which first output 17-1 is connected to the fourth input 18.4 of the unit 18, which first 18.1 and the second 18.2 inputs are connected to the first outputs 14-1 and 15-1 of the analyzers 14 and 15 respectively, and the third input 18.3 is connected to the first output 12-1 of the unit 12. The second input 17.2 of the unit 17 and the first input 19.1 of the unit 19 are comBINed and connected to the second output 6-2 of the unit 6. The first oulput 16-1 of the unit 16 is connected to the third input 20.3 of the analyzer 20, which second input 20-2 is connected to the first output 10-4 of the unit 10, and the first input 20.1 is connected to the fifth output 10-5 of the unit 10, which sixth output 10-6 is connected to the third input 15.3 of the unit 15. The second output 20-2 of the analyzer 20 is connected to the second input 11.2 of the unit 11. The first output 18-1 of the unit 18 is connected to the fifth input 11.5 of the unit 11, to the second input 19.2 of the unit 19, to the fourth input 20.4 of the analyzer 20 and to the first input 21.1 of the unit 21, which first output 21-1 is connected to the second input 22.2 of the extrapolator 22 and to the fifth input 20.5 of the analyzer 20, which first oulput 20-1 is connected to the fourth input 10.4 of the unit 10, to the second input 21.2 of the unit 21 and to the second input 23.2 of the former 23, which first input 23.1 is connected to the second output 10-2 of the unit 10. The first output 19-1 of the unit 19 is connected to the forth input 13.4 of the commutator 13, to the first input 22.1 of the exliapolator 22 and to the third input 23.3 of the former 23, which fourth input 23.4 is connected to the first output 22-1 of the exUapolator 22, and the fifth input 23.5 is connected to the second output 21-2 of the unit 21. The first output 23-1 of the former 23 is connected to the device output and to the first input 3.1 of the unit 3, and the second output 23-2 is connected to the forth input 8.4 of the former 8, to the sixth input 10.6 of the unit 10, to the fourth input 11.4 of the unit 11 and to the third inputs 12.3 and 17.3 of the units 12 and 17 respectively. The third input 1.3 of the unit 1 is connected to the fourth output 20-4 of the analyzer 20, and the fouilh input 1.4 of the unit 1 is connected to the seventh output 10-7 of the unit 10. The output of the processor 24 is connected to the second outputs 3-2 of the unit 3,4-2 of the unit 4, 12-2 and 17-2 of the unit 12 and 17, 16-2 of the unit 16,18-2,19-2 and 22-2 of the units 18,19 and extrapolator 22, to the third outputs 11-3 of the unit 11 and 14-3 and 15-3 of the analy7ers 14 and 15, 20-3 of the analyzer 20,21-3 of the unit 21 and 23-3 of the former 23 and to the eighth output 10-8 of the unit 10 using the bidirectional bus.<br>
The first input 4.1 of the unit 4 is connected to the first input 25.1 of the selector 25 (see Fig. 9), which first 25-1, second 25-2 and third 25-3 outputs are connected to the first inputs 26.1 of the first, 27.1 of the second and 28.1 of the third calculators 26, 27 and 28 respectively, which second 26.2. 27.2 and 28.2, third 26.3, 27.3 and 28.3 and forth 26.4, 27.4 and 28.4 inputs are connected to the first 4.1. second 4.2 and third 4.3 inputs of the unit 4 respectively. The first outputs 26-1 of the first 26, 27-1 of the second 27 and 28-1 of the third 28 calculators are<br>
outputs 26-1 of the first 26, 27-2 of the second 27 and 28-2 of the third 28 calculators are connected to the first 29.1, second 29.2 and to the third 29.3 inputs of the calculator 29 respectively, which output 29-1 is the first output 4-1 of the unit 4. The fourth input 4.4 of the unit 4 is connected to the second input 25.2 of the selector 25. The fourth output 25-4 of the selector 25 and the second outputs 26-2, 27-2, 28-2 and 29-2 of the calculators 26, 27, 28 and 29 are comBINed and connected to the second output 4-2 of the unit 4.<br>
The output of the first buffer random-access memory (BRAM) 30 of the unit 10 is connected to the second input 31.2 of the commutator 31 (see Fig. 10), which first 31-1 and second 31-2 outputs are connected to the third 32.3 and to the sixth 32.6 inputs of the first former 32, which forth input 32.4 is connected to the forth input 10.4 of the unit 10. The first output 32-1 of the former 32 is connected to the first input 33.1 of the former 33, which output is connected to the first input 34.1 of the unit 34. The fourth input 35.4 of the unit 35 is connected to the second input 31.2 of the commutator 31, and the second input 35.2 is connected to the third 36.3 and toe the second 32.2 inputs of the formers 36 and 32 respectively and with he second input 10.2 of the unit 10. The second input 36.2 of the former 36 is connected to the third input 35.3 of the unit 35, to the fifth input 32.5 of the former 32 and to the sixth input 10.6 of the unit 10, and output is connected to the input of the filter 37, which output is connected to the input of the former 38, which output is connected to the input of the former 39, which output is connected to the first input 40.1 of the unit 40. The output of the BRAM 41 is connected to the third input 42.3 of the node 42, which second input 42.2 is connected to the first input 36.1 of the former 36 and to the fifth input 10.5 of the unit 10, first input 42.1 is connected to the first input 35.1 of the unit 35, to the fourth 36.4 and to the first 32.1 inputs of the formers 36 and 32 respectively and to the first input 10.1 of the unit 10, and the output of the node 42 is connected to the input of the BRAM 43, which output is connected to the second input 44.2 of the node 44. which first input 44.1 is connected to the seventh input 10.7 of the unit 10, with the inputs of the BRAM 41 and 30 respectively, to the fifth input 36.5 of the former 36 and to the first input 31.1 of the commutator 31, and the output is connected to the input of the former 45, which output is connected to the input of the filter 46, which output is connected to the input of the former 47, which is connected to the input of the former 48, which output is connected to the first input 49.1 of the unit 49, which first output 49-1 is connected to the fifth input 50.5 of the analyzer 50, which fourth input 50.4 is connected to the first output 40-1 of the unit 40, the third input 50 3 is connected to the output of the former 39, the second input 50.2 is connected to the first output 34-I of the unit 34, and the first one 50.1 is connected to its first input 34.1. The sixth input 50.6 of the former 50 is connected to thhe output of the former 48. and the first output 50-1 is connected to the first input 51.1 of the unit<br>
connected to the first 29.1, second 29.2 and to the third 29.3 inputs of the calculator 29 respectively, which output 29-1 is the first output 4-1 of the unit 4. The fourth input 4.4 of the unit 4 is connected to the second input 25.2 of the selector 25. The fourth output 25-4 of the selector 25 and the second outputs 26-2,27-2,28-2 and 29-2 of the calculators 26,27,28 and 29 are comBINed and connected to the second output 4-2 of the unit 4.<br>
The output of the fust buffer random-access memory (BRAM) 30 of the unit 10 is connected to the second input 31.2 of the commutator 31 (see Fig. 10), which first 31-1 and second 31-2 outputs are connected to the third 32.3 and to the sixth 32.6 inputs of the former 32, which forth input 32.4 is connected to the forth input 10.4 of the unit 10. The first output 32-1 of the former 32 is connected to the first input 33.1 of the former 33, which output is connected to the first input 34.1 of the analyzer 34. The fourth input 35.4 of the unit 35 is connected to the second input 31.2 of the commutator 31, and the second input 35.2 is connected to the third 36.3 and toe the second 32.2 inputs of the formers 36 and 32 respectively and with he second input 10.2 of the unit 10. The second input 36.2 of the former 36 is connected to the third input 35.3 of the unit 35, to the fifth input 32.5 of the fbmier 32 and to the sixth input 10.6 of the unit 10, and oijtput is connected to the input of the filter 37, which output is connected to. the input of the former 38, which output is connected to the input of the former 39, which output is connected to the first i^fHit 40.1 of the unit 40. The output of the BRAM 41 is connected to the third input 42.3 of the node 42, which second input 42.2 is connected to the first input 36.1 of the former 36 and to the fifth input 10.5 of the unit 10, first input 42.1 is connected to the first input 35.1 of the unit 35, to the fouilh 36.4 and to the first 32.1 inputs of the formers 36 and 32 respectively and to the first input 10.1 of the nnit 10, and the owtjnit of the node 42 is connected to the input of the BRAM 43, which output is connected to the second input 44.2 of the node 44, which first input 44.1 is connected to the seventh input 10.7 of the unit 10, with the inputs of the BRAM 41 and 30 respectively, to the fifth input 36.5 of the former 36 and to the fusl input 31.1 of the commutator 31, and the output is connected to the input of the former 45, which ouijiut is connected to the input of the filter 46, which output is connected to the input of the former 47, which is connected to the input of the former 48, which output is connected to the fust input 49.1 of the analyzer 49, which first output 49-1 is connected to the fifth input 50.5 of the former 50, which fourth input 50.4 is connected to the first output 40-1 of the analyzer 40, the third input 50.3 is connected to the output of the former 39, the second input 50.2 is connected to the first output 34-1 of the unit 34, and the first one 50.1 is connected to its first input 34.1. The sixth input 50.6 of the former 50 is connected to the output of the former 48. and the first output 50-1 is connected to the first input 51.1 of die calculator<br>
51 and to the fourth output 10-4 of the processing unit 10, which fifth output 10-5 is connected to &amp;e first output 35:1 of the unit 35, the first output 10-1 is connected to the first input 33.1 of the former 33, the second output 10-2 is connected to the first output 51-1 of the unit 51, the third output 10-3 is connected to the second output 51-2 unit 51, the sixth output 10-6 is connected to the third output 35-3 of the unit 35, and the seventh 10-7 is connected to the first output 52-1 of the unit 52 which input is connected to the third output 51-3 unit 51. The third input 10.3 of the unit 10 is connected to the third input 31.3 of the commutator 31, to the seventh input 32.7 of the former 32, to the second input 33.2 of the former 33, to the second input 34.2 of the unit 34, to the fifth input 35.5 of the unit 35, to the sixth input 36.6 of the former 36, to the second input 40.2 of the unit 40, to the fourth input 42.4 of the unit 42, to the second input 49.2 of the unit 49, to the seventh input 50.7 of the former 50 and to the second input<br>
51.2	of the unit 51. The eighth output 10-8 of the unit 10 is connected to the second outputs 32-2 of the<br>
former 32, 35-2 of the unit 35, 34-2 of the unit 34, 50-2 of the former 50, 40-2 of the unit 40, 49-2 of the<br>
unit 49 and 52-2 of the unit 52 and to the fourth output 51-4 of the unit 51 using the bidirectional bus.<br>
The first input 53.1 of the commutator 53 of the unit 11 (see Fig. 11) is connected to the second input 11.2 of the unit 11, which third input 11.3 is connected to the second input 53.2 of the commutator 53 and to the first input 54.1 of the filter 54 which output is connected to the third input 53.3 of tfie commutator 53. The first input 11.1 of the unit 11 is connected to the first input 55.1 of the analyzer 55. The output of the commutator 53 is connected to the input of the BRAM 56 which output is connected to the first input 57.1 of the former 57, which fifth input 57.5 is connected to the first output 55-1 of the analyzer 55, and its output is connected to the input of the node 58 which output is connected to the input of the BRAM 59 which output is connected to the sixth input 60.6 of the former 60 which output is connected to the input of the node 61, which first output 61-1 is connected to the first input 62.1 of the commutator 62, to the third input 55.3 of the analyzer 55 and to the second output 11-2 of the unit 11, which fourth input 11.4 is connected to the third input 60.3 of the former 60, to the second input 57.2 of the former 57 and to the second input 55.2 of the analyzer 55. The fifth input 11.5 of the unit 11 is connected to the fourth input 60.4 of the former 60, the sixth input 11.6 is connected to the third input<br>
57.3	of the former 57, and the seventh input 11.7 is connected to the fifth input 60.5 of the former 60 and<br>
to the fourth input 57.4 of the former 57. The first input 60.1 of the former 60 is connected to the first<br>
input 55.1 of the analyzer 55, and the second 60.2 is connected to the input of the BRAM 56. The first<br>
output 62-1 of the commutator 62 is connected to the input of the approximator 63, which first output 63-<br>
1 is connected to the input of the node 64. The second 62-2 and the third 62-3 outputs of the commutator<br>
62 are connected to the inputs of the nodes<br>
51 and to the fourth output 10-4 of the unit 10, which fifth output 10-5 is connected to the first output 35-1 of the unit 35, the first otflput 10-1 is connected to the first input 33.1 of the former 33, the second output 10-2 is connected to the first O'Jtput 51-1 of the calculator 51, the third output 10-3 is connected to the second output 51-2 calculator 51, the sixth output 10-6 is connected to the third output 35-3 of the mil 35, and the seventh 10-7 is comiected to the first output 52-1 of the analyzer 52 which input is connected to the third output 51-3 calculator 51. The third input 10.3 of the unit 10 is connected to the third input 31.3 of the commutator 31, to the seventh input 32.7 of the former 32, to the second input 33.2 of the former 33, to the second input 34.2 of fee anajyyer 34, to the fifth input 35.5 of the unit 35, to the sixth input 36.6 of the former 36, to the second input 40.2 of the analyzer 40, to the fourth input 42.4 of the unit 42, to the second input 49.2 of the analyzer 49, to the seventh input 50.7 of the former 50 and to the second input 51.2 of the calculator 51. The eighth o\*put 10-8 of the unit 10 is connected to the second outputs 32-2 of the former 32, 35-2 of the unit 35, 34-2 of the analyzer 34, 50-2 of the former 50, 40-2 of the analyzer 40, 49-2 of the analyzer 49 and 52-2 of the analyzer 52 and to the fourth output 51-4 of the analyzer 51 using the bidirectional bus.<br>
The first input 53.1 of the commutator 53 of the unit 11 (see Fig. 11) is connected to the second input 11.2 of the unit 11, which third input 11.3 is connected to the second input 53.2 of the commutator 53 and to the first input 54.1 of the filter 54 which output is connected to the third input 53.3 of the commutator 53. The first input 11.1 of the "nit 11 is connected to the first input 55.1 of the analyzer 55. The output of the commutator 53 is connected to the input of the BRAM 56 which output is connected to the first input 57.1 of the former 57, which fifth input 57.5 is connected to the first output 55-1 of the analyzer 55, and its ouput is connected to the input of the node 58 which output is connected to the input of the BRAM 59 which output is connected to the sixth input 60.6 of the former 60 which output is connected to the input of the analyzer 61, which firs* output 61-1 is connected to the first input 62.1 of the selector 62, to the third input 55.3 of the analyzer 55 and to the second output 11-2 of the unit 11, which fourth input 11.4 is connected to the third input 60.3 of the former 60, to the second input 57.2 of the former 57 and to the second input 55.2 of the analyzer 55. The fifth input 11.5 of the unit 11 is connected to the fourth input 60.4 of the former 60, the sixth input 11.6 is connected to the third input 57.3 of the former 57. and the seventh input 11.7 is connected to the fifth input 60.5 of the former 60 and to the fourth input 57.4 of the former 57. The first input 60.1 of the former 60 is connected to the first input 55.1 of the analyzer 55. and the second 60.2 is connected to the input of the BRAM 56. The first output 62-1 of the selector 62 is connected to die input of the approximutor 63, which first output 63-1 is connected to the input of the coordinator 64. The second 62-2 and the third 62-3 outputs of the selector 62 are connected to the inputs of the coordinators<br>
65 and 66 respectively. The first outputs of the nodes 64, 65 and 66 are comBINed with the fourth input 55.4 of the analyzer 55 and are connected to the first output 11-1 of the unit 11. The eighth input 11.8 of the unit 11 is connected to the second input 54.2 of the filter 54, to the fifth input 55.5 of the analyzer 55, to the seventh input 60.7 of the former 60 and to the second input 62.2 of the commutator 62. The third output 11-3 of the unit 11 is connected to the second outputs 55-2 of the analyzer 55, 61-2 of the unit 61, 63-2 of the approximator 63, 64-2, 65-2 and 66-2 of the units 64, 65 and 66 using the bidirectional bus.<br>
The first 32.1, the second 32.2, the third 32.3, the fourth 32.4 and the fifth 32.5 inputs of the former 32 of the unit 10 are connected to the first 67.1, to the second 67.2, to the third 67.3, to the fourth 67.4 and to the fifth 67.5 inputs of the former 67 (see Fig. 12), which first output 67-1 is connected to the first input 68.1 of the unit 68 which output is the first output 32-1 of the former 32 of the unit 10, which sixth input 32.6 is connected to the second input 68.2 of the unit 68, and the seventh input 32.7 is connected to the sixth input 67.6 of the foaner 67 and to the third input 68.3 of the unit 68. The second output 32-2 of the former 32 of the unit 10 is connected to the second output 67-2 of the former 67 using the bidirectional bus.<br>
The fifth input 36.5 of the former 36 of the unit 10 is connected to the input of the BRAM 69 (see Fig. 13) which output is connected to the fifth input 70.5 of the former 70, which first<br>
70.1,	second 70.2, third 70.3 and fourth 70.4 inputs are connected to the first 36.1, to the second<br>
36.1,	to the third 36.3 and to the fourth 36.4 inputs of the former 36 of the unit 10 respectively.<br>
The output of the former 70 is connected to the third input of 71.3 of the node 71 which output is<br>
connected to the input of the BRAM 72 which output is connected to the sixth 70.6 and to the<br>
second 73.2 inputs of the former 70 and of the unit 73 respectively. The first 71.1 and the second<br>
71.2 inputs of the node 71 are connected to the first 70.1 and to the second 70.2 inputs of the<br>
former 70. The output of the unit 73 is connected to the input of the unit 74 which output is the<br>
output of the former 36 of the unit 10. which sixth input 36.6 is connected to the third input 73.3<br>
of the unit 73 and to the seventh input 70.7 of the former 70.<br>
For use of the invention in the industry the device on the second variant may be executed, for example, as follows.<br>
The first input 75.1 of the unit 75 of the object coordinates determination device (see Fig. 14) is connected to the first input of the device, and the second input 75.2 is connected to the second input of the device. The input of the unit 76 is connected to the third input of the device. The first output 77-1 of the unit 77 is connected to the second input 78 2 of the former 78, which first input 78.1 is<br>
65 and 66 respectively. The first outputs 64-1, 65-1 and 66-1 of the coordinators 64, 65 and 
The first 32.1, the second 32.2, the third 32.3, the fourth 32.4 and the fifth 32.5 inputs of the former 32 of the unit 10 are connected to the first 67.1, to the second 67.2, to the third 67.3, to the foiufli 67.4 and to the fifth 67.5 inputs of the calculator 67 (see Fig. 12), which first output 67-1 is connected lo the first input 68.1 of the unit 68 which output is the first output 32-1 of the former 32 of the unit 10, which sixth input 32.6 is connected to the second input 68.2 of the unit 68, and the seventh input 32.7 is connected to the sixth input 67.6 of the calculator 67 and to the third input 68.3 of the unit 68. The second output 32-2 of the former 32 of the unit 10 is connected to the second output 67-2 of the calculator 67 using the bidirectional bus.<br>
The fifth input 36.5 of the former 36 of the unit 10 is connected to the input of the BRAM 69 (see Fig. 13) which output is connected to the fifth input 70.5 of the former 70, which fiisl 70.1, second 70.2, third 70.3 and fourth 70.4 inputs are connected to the first 36.1, to the second 36.2, to the third 36.3 and to the fourth 36.4 inputs of the former 36 of the unit 10 respectively. The output of the former 70 fe connected to the third input of 71.3 of the node 71 which output is connected to the input of the BRAM 72 which output is connected to the sixth 70.6 and to the second 73.2 inputs of the former 70 and of the unit 73 respectively. The first 71.1 and the second 71.2 inputs of the node 71 are connected to the first 70.1 and to the second 70.2 inputs of the former 70. The output of the unit 73 is connected to the input of the unit 74 which output is the output of the former 36 of the unit 10, which sixth input 36.6 is connected to the third input 73.3 of the "nit 73 and to the seventh input 70.7 of the former 70.<br>
For use of the invention in the industry the device on the second variant may be executed, for example, as follows.<br>
The first input 75.1 of the unit 75 of the object coordinates determination device (see Fig. 14) is connected to the fust input of the device, and the second input 75.2 is connected to the second input of the device. The input of the unit 76 is connected to the third input of the device. The first output 77-1 of the unit 77 is connected to the second input 78.2 of the former 78. which first input 78.1 is<br>
connected to the first output 75-1 of the unit 75. The second input 77.2 of the unit 77 is connected to the sixth input of the device. The output of the former 78 is connected to the first input 79.1 of the unit 79, The second output 75-2 of the unit 75 is connected to the third input 77.3 of the unit 77, to the fourth input 78.4 of the frame former 78, to the fifth input 79.5 of the unit 79, to the second input 80.2 of the unit 80, to the third input 81.3 of the unit 81, to the eighth input 82.8 of the unit 82, to the fourth input 83.4 of the unit 83, to the fifth input 84.5 of the commutator 84, to the fourth input 85.4 of the unit 85, to the fourth input 86.4 of the unit 86, to the second input 87.2 of the unit 87, to the fourth input 88.4 of the unit 88, to the fifth input 89.5 of the unit 89, to the third input 90.3 of the unit 90, to the sixth input 91.6 of the analyzer 91, to the third input 92.3 of the unit 92, to the third input 93.3 of the unit 93 and to the sixth input 94.6 of the former 94. The output of the unit 76 is connected to the third input 78.3 of the former 78, to the fifth input 81.5 of the unit 81, to the first input 82.1 of the unit 82, to the first 83.1 and to the second 88.2 inputs of the units 83 and 88 respectively and to the first input 90.1 of the unit 90. The fourth input of the device is connected to the first input 84.1 of the commutator 84, and the fifth input is connected to the second input 84.2 of the multiplexer 84, which first 84-1 and second 84-2 outputs are connected to the third 79.3 and to the second 79.2 inputs of the unit 79 respectively, and also to the first 81.1 and to the second 81.2 and to the seventh 82.7 and to the sixth 82.6 inputs of the units 81 and 82 respectively. The output of the unit 79 is connected to the seventh input 81.7 of the unit 81 and to the third input 82.3 of the unit 82. The output of the unit 80 is connected to the first input 87.1 of the unit 87. The first output 82-1 of the unit 82 is connected to the second input 83.2 of the unit 83. The first output 81-1 of the unit 81 is connected to the first input 80.1 of the unit 80. The second output 81-2 of the unit 81 is connected to the third input 84.3 of the commutator 84, to the first input 85.1 of the unit 85 and to the first input 94.1 of the former 94. The third output 81-3 of the unit 81 is connected to the first input<br>
88.1	of the unit 88, which first output 88-1 is connected to the fourth input 89.4 of the unit 89. The fourth<br>
output 81-4 of the unit 81 is connected to the second input 85.2 of the unit 85 and to the second input<br>
91.1	of the analyzer 91. The fifth output 81-5 of the unit 81 is connected to the first input 91.1 of the<br>
analyzer 91. The sixth output 81-6 of the unit 81 is connected to the third input 86.3 of the unit 86. The<br>
seventh output 81-7 of the unit 81 is connected to the fourth input 75.4 of the unit 75. The first output 83-<br>
1 of the unit 83 is connected to the third input 89.3 of the unit 89. The second output 82-2 of the unit 82<br>
is connected to the first input 86.1 of the unit 86. The first outputs of 85-1 and 86-1 of the units 85 and 86<br>
are connected to the first and to the second inputs of the unit 89 respectively. The second output 85-2 of<br>
the unit 85 is connected to the second input 86.2 of the unit 86, which second output 86-2 is connected to<br>
the third input 85.3 of the unit 85. The first output 87-1 of the unit 87 is connected to the third input 91.3<br>
of the analyzer 91. The first output 89-1 of the unit 89 is connected to the fifth input 82.5 of the unit 82<br>
second input 90.2 of the unit 90, to the fourth input 91 4 of the analyzer 91 and to the first input 92.1 of<br>
the unit 92, which first output 92-1 is connected to the second input 93.2 of the unit 93 and to the fifth<br>
input 91.5 of the analyzer 91, which first output 91-1 is connected to the fourth input 81.4 of the unit<br>
connected to the first output 75-1 of the unit 75. The second input 77.2 of the unit 77 is connected to the sixth input of the device. The output of the former 78 is connected to the first input 79.1 of the former 79. The second oiitput 75-2 of the unit 75 is connected to the third input 77.3 of the unit 77, to the fourth input 78.4 of the former 78, to the fifth input 79.5 of the former 79, to the second input 80.2 of the former 80, to the third input 81.3 of the "nit 81, to the eighth input 82.8 of the unit 82, to the foiath input 83.4 of the unit 83, to the fifth input 84.5 of the commutator 84, to the fourth input 85.4 of the anajyzer 85, to the fourth input 86.4 of the analyzer 86, to the second input 87.2 of the unit 87, to the fourth input 88.4 of the unit 88, to the fifth input 89.5 of the unit 89, to the third input 90.3 of the unit 90, to the sixth input 91.6 of the analyzer 91, to the third input 92.3 of the unit 92, to the third input 93.3 of the exUapolator 93 and to the sixth input 94.6 of the former 94. The output of the unit 76 is connected to the third input 78.3 of the former 78, to the fifth input 81.5 of the unit 81, to the first input 82.1 of the unit 82, to the first 83.1 and to the second 88.2 inputs of the units 83 and 88 respectively and to the first input 90.1 of the unit 90. The fourth input of the device is connected to the first input 84.1 of the commutator 84, and the fifth input is connected to the second input 84.2 of the multiplexer 84, which first 84-1 and second 84-2 outputs ate connected to the third 79.3 and to the second 79.2 inputs of the former 79 respectively, and also to the fust 81.1 and to the second 81.2 and to the seventh 82.7 and to the sixth 82.6 inputs of the units 81 and 82 respectively. The oirtput of the former 79 is connected to the seventh input 81.7 of the unit 81 and to the third input 82.3 of the unit 82. The output of the former 80 is connected to the first input 87.1 of the unit 87. The first output 82-1 of the unit 82 is connected to the second input 83.2 of the unit 83. The fmA output 81.1 of the unit 81 is connected to the first input 80.1 of the former 80. The second output 81-2 of the unit 81 is connected to the third input 84.3 of the commutator 84, to the first input 85.1 of the analyzer 85 and to the first input 94.1 of the former 94. The third output 81-3 of the unit 81 is connected to the fits* input 88.1 of the unit 88, which first output 88-1 is connected to the fourth input 89.4 of the unit 89. The fourth output 81-4 of the unit 81 is connected to the second input 85.2 of the analyzer 85 and to the second injHit 91.2 of the analyzer 91. The fifth output 81-5 of the unit 81 is connected to the first input 91.1 of the analyzer 91. The sixth output 81-6 of the unit 81 is connected to the third input 86.3 of the analyzer 86. The seventh output 81-7 of the unit 81 is connected to the fourth input 75.4 of the unit 75. The first output 83-1 of the unit 83 is connected to the third input 89.3 of the unit 89. The second output 82-2 of the unit 82 is connected to the first input 86.1 of the analyzer 86. The first outputs of 85-1 and 86-1 of the analyzers 85 and 86 are connected to the first and to the second inputs of the unit 89 respectively. The second output 85-2 of the analyzer 85 is connected to the second input 86.2 oi the analyzer 86, which second output 86-2 is connected to the third input 85.3 of the analyzer 85. The first output 87-1 of the unit 87 is connected to the third input 91.3 of the analyzer 91. The first output<br>
81, to the second input 92.2 of the unit 92 and to the second input 94.2 of the former 94. The first output 90-1 of the unit 90 is connected to the fourth input 84.4 of the commutator 84, to the first input 93.1 of the "nit 93 and to the third input 94.3 of the former 94. The second output 91-2 of the analyzer 91 is connected to the second input 82.2 of the unit 82. The first output 93-1 of the unit 93 is connected to the fourth input 94.4 of the former 94. The second output 92-2 of the unit 92 is connected to the fifth input 94.5 of the former 94. The first output 94-1 of the former 94 is connected to the output of the object positioning device and to the first input 77.1 of the unit 77, and the second output 94-2 is connected to the fourth input 79.4 of the unit 79, to the sixth input 81.6 of the unit 81, to the fourth input 82.4 of the unit 82 and to the third inputs 83.3 and 88.3 of the units 83 and 88. The fourth output 91-4 of the analyzer 91 is connected to the third input 75.3 of the unit 75. The processor 95 is connected to the second outputs 77-2 of the unit 77, 83-2 and 88-2 of the units 83 and 88, 87-2 of the unit 87, 89-2 and 90-2 of the units 89 and 90 and 93-2 of the "nit 93, to the third outputs 85-3 and 86-3 of the units 85 and 86, 91-3 of the analyzer 91, 92-3 of the unit 92 and to the 94-3 of the former 94 and to the eighth output 81-8 of the unit 81 using the bidirectional bus.<br>
The first 6.1 and the second 6.2 inputs of the unit 6 of the object positioning device are connected to the first and to the second inputs of the first multiplexer (see Fig. 15). The third 6.3 and the fourth 6.4 inputs of the unit 6 are connected to the first inputs of the first and second multiplexers. The fifth input 6.5 of the unit 6 is connected to the third inputs of both multiplexers which outputs are the outputs 6-1 and 6-2 of the unit 6.<br>
The first 13.1 and the second 13.2 inputs of the commutator 13 (in the first variant), (and also the first 84.1 awl the second 84.2 inputs of the commutator 84 in the second variant), are connected to the first inputs of the first and second multiplexers (see Fig. 16), and the third 13.3 and the fourth 13.4 inputs of the commutator (both the third 84.3 and the fourth 84.4 inputs in the second variant) are connected to the second inputs of both multiplexers. The fifth input 13.5 of the multiplexer 13 (or the fifth input 84.5 of the multiplexer 84) is connected to the third inputs of both multiplexers which outputs are the first and second outputs 13-1 and 13-2 of the commutator 13 (or the first and second outputs 84-1 and 84-2 of the commutator 84).<br>
The first 31.1, the second 31.2 and the third 31.3 inputs of the commutator 31 (see Fig. 17) ofthe<br>
89-1 of the unit 89 is connected to the fifth input 82.5 of the unit 82 second input 90.2 of the unit 90, to the fourth input 91.4 of tihe analyzer 91 and to the fust input 92.1 of the unit 92, which fust output 92-1 is connected to the second input 93.2 of the exliapolator 93 and to the fifth input 91.5 of the analyzer 91, which first output 91-1 is connected to the fourth input 81.4 of the unit 81, to the second input 92.2 of the unit 92 and to the second input 94.2 of the former 94. The first output 90-1 of the unit 90 is connected to the fouilh input 84.4 of the commutator 84, to the first input 93.1 of the extrapolator 93 and to the third input 94.3 of the former 94. The second output 91-2 of the analyzer 91 is connected to the second input 82.2 of the unit 82. The fust output 93-1 of the extiapolator 93 is connected to the fourth input 94.4 of the former 94. The second output 92-2 of the unit 92 is connected to the fifth input 94.5 of the former 94. The first output 91 1 of the former 94 is connected to the output of the object positioning device and to the first input 77.1 of the unit 77, and the second output 91 2 is connected to the fourth input 79.4 of the former 79, to the sixth input 81.6 of the unit 81, to the fourth input 82.4 of the unit 82 and to the third inputs 83.3 and 88.3 of the units 83 and 88. The fourth output 91-4 of the analyzer 91 is connected to the third input 75.3 of the unit 75. The processor 95 is connected to the second outputs 77-2 of the unit 77, 83-2 and 88-2 of the units 83 and 88,87-2 of the unit 87,89-2 and 90-2 of the units 89 and 90 and 93-2 of the extrapotetor 93, to the third outputs 85-3 and 86-3 of the analyzers 85 and 86,91-3 of the analyzer 91, 92-3 of the unit 92 and to the 94 3 of the former 94 and to the eighth output 81-8 of the unit 81 using the bidirectional bus.<br>
The first 6.1 and the second 6.2 inputs of the unit 6 of the object positioning device are connected to the first and to the second inputs of the first multiplexer (see Fig. 15). The third 6.3 and the fouilh 6.4 inputs of the unit 6 are connected to the first inputs of the first and second multiplexers. The fifth input 6.5 of the unit 6 is connected to the third inputs of both multiplexers which outputs are the outputs 6-1 and 6-2 of the unit 6.<br>
The first 13.1 and the second 13.2 inputs of the commutator 13 (in the first variant), (and also the first 84.1 and the second 84.2 inputs of the commutator 84 in the second variant), are connected to the first inputs of the fust and second multiplexers (see Fig. 16), and the third 13.3 and the fourth 13.4 inputs of the commutator (both the third 84.3 and the fourth 84.4 inputs in the second variant) are connected to the second inputs of both multiplexers. The fifth input 13.5 of the multiplexer 13 (or the fifth input 84.5 of the multiplexer 84) is connected to the third inputs of both multiplexers which outputs are the first and second outputs 13-1 and 13-2 of the commutator 13 (or the first and second outputs 84 1 and 84-2 of the commutator 84).<br>
The first 31.1, the second 31.2 and the third 31.3 inputs of the commutator 31 (see Fig. 17) of the<br>
unit 10 (and also the first 81.1, the second 81.2 and the third 81.3 inputs of the commutator of the unit 81 in the second variant of the said device) are connected to the first, to the second and to the third inputs of the multiplexer. The first and second outputs of the multplexer are the first 31-1 and the second 31-2 outputs of the commutator 31 (or the first 81-1 and the second 81-2 outputs of the commutator 81).<br>
The first 53.1, the second 53.2 and the third 53.3 inputs of the commutator 53 (see Fig. 18) of the "nit 11 (and also the first 82.1, the second 82.2 and the third 82.3 inputs of the commutator 82 in the second variant of the said device) are connected to the first, to the second and to the third inputs of the multiplexer. The output of the multiplexer is the output of the commutator 53 (or of the commutator 82).<br>
For reaction of the invention on the first variant, the object positioning device may contain, for example, the unit 1 (see Figs. 8), intended for reception and storage of signals of the current field of the video image from the tracking system video camera, received to the first input 8.1, of reception of external signal START/STOP received to the second input 8.2, of reception of the automatic coordinates determination (ACD) failure signal, received to the third input 8.3 from the unit 20, and reception of the signal to the input 8.4 of the unit 10 and of the development of signals of device operability synchronisation, the unit 2 intended for reception and storage of codes of tracking system video camera field of view axis uncontrolled displacement and roll angular speed signals (for example, digital), received to the input of the unit from the third input of the device and their translation from outputs of buffer registers to the output of the unit, the unit 3 intended for field of view axis controlled displacement detennination at the time between reception of signals of the former and current video image fields, caused by impact of the video camera field of view displacement management signals on tracking system video camera, at that values of codes of management signals XMAN[N], VMAN[n] are accepted to the first input 3.1 of the unit from the former 23, values of codes of angular position of control panel are received to the second input 3.2 of the unit from the sixth input of the device, ACD signals, of the frame sync pulses (FSP) (determining the beginning of the calculation in the current video images fields (VIF), of the clock pulses of word-by-word video data synchronization TSP (synchronizing work of the unit) are received to the input 3.3 of the unit from the unit 1 using the synchronization bus (SB), the unit 4 intended for forming of the 2N terrain reference marks video images dissimilarity measure signals in the current and former video<br>
unit 10 (and also the first 81.1, the second 81.2 and the third 81.3 inputs of the commutator 31 of the unit 81 in the second variant of the said device) are connected to the first, to the second and to the third inputs of the multiplexer. The first and second outputs of the multiplexer are the first 31-1 and the second 31-2 outputs of the commutator 31 (or the first 81-1 and the second 81-2 outputs of the commutator 31 of the unit 81).<br>
The first 53.1, the second 53.2 and the third 53.3 inputs of the commutator 53 (see Fig. 18) of the unit 11 (and also the first 82.1, the second 82.2 and the third 82.3 inputs of the commutator 53 of the unit 82 in the second variant of the said device) are connected to the first, to the second and to the third inputs of the multiplexer. The output of the multiplexer is the output of the commutator 53 (or of the commutator 82).<br>
For realization of the invention on the first variant, the object positioning device may contain, for example, the unit 1 (see Figs. 8), intended for reception and storage of signals of the current field of the video image from the tracking system video camera, received to the first input 1.1, of reception of external signal START/STOP received to the second input 1.2, of reception of the automatic coordinates detennination (ACD) failure signal, received to the third input 1.3 from the unit 20, and reception of the signal to the input 1.4 of the unit 10 and of the development of signals of device operability synchronization, the unit 2 intended for reception and storage of codes of tracking system video camera field of view axis uncontrolled displacement and roll angular speed signals (for example, digital), received to the input of the unit from the third input of the device and their translation from outputs of buffer registers to the output of the unit, the unit 3 intended for field of view axis controlled displacement determination at the time between reception of signals of the former and cuucnt video image fields, caused by impact of the video camera field of view displacement management signals on tracking system video camera, at that values of codes of management signals XMAN[n], YMAN[n] are accepted to the first input 3.1 of the unit from the former 23, values of codes of angular position of control panel are received to the second input 3.2 of the unit from the sixth input of the device, ACD signals, of the frame sync pulses (FSP) (determining the beginning of the calculation in the current video images fields (VIF), of the clock pulses of word-by-word video data synchronization TSP (synchronizing work of the unit) are received to the input 3.3 of the unit from the unit 1 using the synchronization bus (SB), the unit 4 intended for forming of the 2M terrain reference marks video images dissimilarity measure signals in the current and former video<br>
images fields, 2N tifain reference marks video images signals displacements deteimination anc calculation using them the current video image n-field signals shift and turn parameters relatively to the video image (n-l) field, the unit 5 intended for the division of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N tenain reference marks into the constituents of the tracking system video camera field of view axis controlled shift and of the tracking system video camera field of view axis uncontrolled shift at the time between the reception of the current and the former video image fields signals, the unit 6, intended for signal injection of tracking system video camera field of view axis controlled displacement signals and of the tracking system video camera field of view controlled displacement and roll signals to the units of the device directly from the »nit 3 and from the unit 2 or from the unit 5; former 7 of the current n-frame video image signals from, the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-l) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfomiation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and roll; the unit 8, intended for the video image signals forming and scaling in the current analysis window using the video image current analysis window location and dimensions signals; the nnit 9 intended for of the M analysis windows forming about the analysis window peiimeter and the histogram classifier binary video image signals projection deteiiiiination in the M background analysis windows; the unit 10 intended for determination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the unit 11 intended for the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the unit 12, intended for deteimination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates<br>
images fields, 2N teiiatn reference marks video images signals displacements detemiination and calculation using them the current video image n-field signals shift and turn parameters relatively to the video image (n-1) field, the unit 5 intended for the division of the current video image fielH signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N teuain reference marks into the constituents of the tracking system video camera field of view axis controlled shift and of the tracking system video camera field of view axis uncontrolled shift at the time between the reception of the current and the former video image fields signals, the unit 6, intended for signal injection of tracking system video camera field of view axis couuolled displacement signals and of the tracking system video camera field of view controlled displacement and roll signals to the units of the device directly from the unit 3 and from the unit 2 or from the unit 5; former 7 of the current n-frame video image signals from the former frame video image signals with regard to the field of view axis controlled shift at the time between the reception of the video image (n-1) and (n-2) fields signals and of the current video image field signals with the elementwise current video image field signals coordinates transfoiaiation which supports the compensation of the tracking system video camera field of view uncontrolled shifts and roll; the former 8, intended for the video image signals forming and scaling in the current walysis window using the video image current analysis window location and dimensions sigwta; the foaiier 9 intended for of the M analysis windows foiniing about the analysis window perimeter and the histogram classifier binary video image signals projection detemiination in the background analysis windows; the unit 10 intended for deteiniination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and veitieal projections of the generalized binary video image of the object; the unit 11 intended for the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the unit 12, intended for determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates<br>
obtained on the basis of video images dissimilarity measure signals forming; commutator 13 intended for electronic switching of the codes of the prior (received from the tracking system) or of the current (received as a result of the video image processing in the current analysis window) object dimensions and coordinates in tracking system video camera field of view to the units of the device; the unit 14 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; the unit 15 intended for the determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals; the unit 16 intended for the determination of the histogram classifier binary video images boundaries area and coordinates of binary video images in M background analysis windows after the received of the histogram classifier binary video images signals in M background analysis windows; the unit 17 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; the unit 18 intended for forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate from the object video image displacement current speed estimate data, obtained on the basis of the video images dissimilarity measure signals fouling and on the basis of the object video image displacement current speed estimate after the object generalized binary video image signals generalized projections, taking into consideration confidence factors of the object video image displacement speed components and of the object maneuvering speed prior limitations, the unit 19 intended for the forming of the complex estimate of the object video image coordinates in the tracking system video camera field of view by the integration of the difference of the object image displacement current speed complex estimate and of the field of view axis controlled displacement speed in the inertial coordinates system using the initial coordinates of the object video image in the video camera field of view, received from the tracking system video at the moment of the start of the tracking the object, the analyzer 20, intended for the analysis of the of the current and averaged area<br>
obtained on the basis of video images dissimilarity measure signals forming; commutator 13 intended for electronic switching of the codes of the prior (received from the tracking system) or of the current (received as a result of the video image processing in the current analysis window) object dimensions and coordinates in tracking system video camera field of view to the units of the device; the analyzer 14 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; the analyzer 15 intended for the determination of the confidence factors of the cunent speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals; the unit 16 intended for the detetmination of the histogram classifier binary video images boundaries area and coordinates of binary video images in background analysis windows after the received of the histogram classifier binary video images signals in background analysis windows; the unit 17 intended for the detemiination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; the unit 18 intended for forming of the current speed of the object video image displacement in the inertial coordinate system complex estimate from the object video image displacement current speed estimate data, obtained on the basis of the video images dissimilarity measure signals forming and on the basis of the object video image displacement current speed estimate after the object generalized binary video image signals generalized projections, taking into consideration confidence factors of the object video image displacement speed components and of the object maneuvering speed prior limitations, the unit 19 intended for the forming of the complex estimate of the object video image coordinates in the tracking system video camera field of view by the integration of the difference of the object image displacement current speed complex estimate and of the field of view axis controlled displacement speed in the inertial coordinates system using the initial coordinates of the object video image in the video camera field of view, received from the tracking system video at the moment of the start of the tracking the object, the analyzer 20, intended for the analysis of the of the current and averaged area<br><br>
of the object generalised binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows and for fanning on the basis of this analysis of the signal of tke change for forming of the tracking system video camera field of view axis displacement management signals using the object video image extrapolated coordinates; the unit 21 intended for the averaging of (low-pass filtering) the object video image displacement current speed complex estimate and of the storage of the averaged (filtered) speed values; the unit 22, intended for the deteunination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame and for the restoring of the true speed of the object video image displacement on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement at the change for forming of the management signals of the tracking system video camera field of view axis displacement over the extrapolated coordinates; former 23 of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame using the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement and the processor 24, intended for the computation processing of the local data, transmitted over rtw. bidirectional bus (including virtually implemented parallel algorithmic branches) and which do not need any flow processing of data file.<br>
The unit 4 contains, for example, (see Fig. 9) the selector 25 intended for choosing the video images signals of the three pairs of terrain reference marks from the current video image field signals; calculator 26 intended for forming of the first pair of the teuain reference mark? video images dissitnilatily measure signals from the correspondent first pair of the teuain reference marks video image signals of the current video image field and from the correspondent first pair of the teuvn reference marks video image signals of the former video image field and for determination with their help of displacement of images of the first pair of the terrain reference marks at the time between the reception of signals of the current and former video images fields; the calculator 27 intended for<br>
of the object generalised binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows and for forming on the basis of this analysis of the signal of the change for fonntng of the tracking system video camera field of view axis displacement management signals using the object video image extrapolated coordinates; the unit 21 intended for the averaging of (low-pass filtering) the object video image displacement current speed complex estimate and of the storage of the averaged (filtered) speed values; the extrapolator 22, intended for the detemiination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame and for the restoring of the true speed of the object video image displacement on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement at the change for forming of the management signals of the tracking system video camera field of view axis displacement over the extrapolated coordinates; former 23 of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame using the object video image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement and the processor 24, intended for the computation processing of the local data, transmitted over the bidirectional bus (including virtually implemented parallel algorithmic branches) and which do not need any flow processing of data file.<br>
The unit 4 contains, for example, (see Fig. 9) the selector 25 intended for choosing the video images signals of the three pairs of terrain reference marks from the current video image field signals; calculator 26 intended for forming of the first pair of the terlain reference marks video images dissimilarity measure signals from the correspondent first pair of the terrain reference marks video image signals of the current video image field and from the correspondent first pair of the teuain reference marks video image signals of the former video image field and for determination with their help of displacement of images of the first pair of the terrain reference marks at the time between the reception of signals of the current and former video images fields; the calculator 27 intended tor<br>
forming of the second pair of the terrain reference marks video images dissimilarity measure signals from the conespondent second pair of the terrain reference marks video image signals of the current video image field and from the correspondent second pair of the terrain reference marks video image signals of the former video image field and for deteiinitiation with their help of displacement of images of the second pair of the terrain reference marks at the time between the reception of signals of the current and former video images fields; the calculator 28, intended for foiniing of the third pair of the terrain reference marks video images dissimilarity measure signals from the correspondent third pair of the terrain reference marks video image signals af the current video image field and from the correspondent third pair of the terrain reference marks video image signals of the former video image field and for determination with their help of displacement of images of the third pair of the terrain reference marks at th« time between the reception of signals of the current and former video images fields; the calculator 29 intended for detei initiation of the current video image field signals shift and roll parameters relatively to the former video image field using the three pairs of the terrain reference marks video images signals coordinates displacements.<br>
At tha.t, in the given example of realization of the device for simplification of consideration number N=3 of the used pairs of references is set.<br>
The unit 10 contains, for example, (see Fig. 10) the first buffer random access memory (the BRAM) 30, intended for the scaled video image signals storage in the current analysis window; the commutator 31 intended for electronic commutation to the former 32 of the scaled video image signals in the current analysis window directly from the unit 8 (sec Fig. 8) or after buffering in the BRAM 30; former 32 of the histogiam classifier binary video image signals in the curieat analysis window; former 33 of horizontal and vertical projections of the histogiam classifier binary video image signals; the unit 34 intended for determination of the confidence factors of the of the histogiam classifier binary video image signals; the unit 35 intended for determination of the ratio object/background Q and of the minimal value<br>
forming of the second pair of the terrain reference marks video images dissimilarity measure signals from the correspondent second pair of the terrain reference marks video image signals of the current video image field and from the correspondent second pair of the terrain refereace marks video image signals of the former video image field and for detemunation with their help of displacement of images of the second pair of the terrain reference marks at the time between the reception of signals of the current and former video images fields; the calculator 28, intended for forming of the third pair of the terrain reference marks video images dissimilarity measure signals from the correspondent third pair of the terrain reference marks video image signals of the current video image field and from the correspondent third pair of the terrain reference marks video image signals of the former video image field and for determination with their help of displacement of images of the third pair of the terrain reference marks at the time between the reception of signals of the current and former video images fields; the calculator 29 intended for determination of the current video image field signals shift and roll parameters relatively to the former video image field using the three pairs of the terrain reference marks video images signals coordinates displacements.<br>
At that, in the given example of realisation of the device for simplification of consideration number N=3 of the used pairs of references is set.<br>
The unit 10 contains, for example, (see Fig. 10) the first buffer random access memory (the BRAM) 30, intended for the scaled video image signals storage in the current analysis window; the commutator 31 intended for electronic commutation to the foitaer 32 of the scaled video image signals in the current analysis window directly from the unit 8 (see Fig. 8) or after buffering in the BRAM 30; former 32 of the histogram classifier binary video image signals in the current analysis window; former 33 of horizontal and vertical projections of the histogram classifier binary video image signals; the unit 34 intended for determination of the confidence factors of the of the histogram classifier binary video image signals; the unit 35 intended for deteunination of the ratio object/background Q and of the minimal value<br>
σBmm(n) of the root-mean-sqnare value of the background video image signals of the video image of a background in M windows of the analysis of a background,<br>
(Equation Removed) <br>
 the average value of the video image signals in the object window,<br>
LQ is the average value of the video image signals in the background window, CTB is the root-mean-square drift of video image signals image in the background window from Lg,<br>
former 36 of the background change detector initial binary video image signals using the forming of the difference background change detector video image by subtraction of the sample background video image signals from scaled image signals in the current analysis window and for the comparison of the obtained difference background change detector video image signals with a threshold; the filter 37 intended for a low-frequency spatial filtration of the background change detector initial binary video image signals; former 38 of the background change detector secondary binary video image signals; former 39 of the horizontal and vertical projections of the background change detector secondary binary video image signals; the unit 40, intended for deteuiiination of the confidence factors of the background change detector secondary binary video image signals; the second BRAM 41, intended for the scaled video image signals storage in the current analysis window; the unit 42 intended for adjusting to the current scale of the scaled video image signals in the analysis window and and their displacement on the tracking system video camera field of view axis displacement value; BRAM 43, intended for adjusted to the current scale in the analysis window and displaced on the video camera field of view axis displacement value signals of the scaled video image; the node 44 intended for forming of the moving objects indicator difference video image signals; former of the moving objects indicator initial binary video image signals; the filter 46, intended for a low-frequency spatial<br>
σBmin(n) of the root-mean-square value of the background video image signals of the video image of a background in windows of the analysis of a background,<br>
(Equation Removed) <br>
L0 is the average value of the video image signals in the object window, LB is the average value of the video image signals in the background window, 0B is the root-mean-square drift of video image signals image in the background window from Lg,<br>
former 36 of the backgiound change detector initial binary video image signals using the forming of the difference background change detector video image by subtraction of the sampk background video image signals from scaled image signals in the current analysis window and for the comparison of the obtained difference background change detector video image signals with a threshold; the filter 37 intended for a low-frequency spatial filtration of the background change detector initial binary video image signals; former 38 of the background change detector secondary binary video image signals; former 39 of the horizontal and vertical projections of the background change detector secondary binary video image signals; the analyzer 40, intended for detennination of the confidence factors of the background change detector secondary binary video image signals; the second BRAM 41, intended for the scaled video image signals storage in the current analysis window; the unit 42 intended for adjusting to the current scale of the scaled video image signals in the analysis window and and their displacement on the tracking system video camera field of view axis displacement value; BRAM 43, intended for adjusted to the current scale in the analysis window and displaced on the video camera field of view axis displacement value signals of the scaled video image; the node 44 intended for forming of the moving objects indicator difference video image signals; former of the moving objects indicator initial binary video image signals; the filter 46, intended for a low-frequency spatial<br>
filtration of the moving objects indicator initial binary video image signals; former 47 of the moving objects indicator secondary binary video image signals; former 48 of the horizontal and vertical projections of the moving objects indicator secondary binary video image signals; the unit 49 intended for deteuiiination of the confidence factors of the moving objects indicator secondary binary video image signals; former of the generalized horizontal and vertical projections of the moving objects indicator generalized binary video image signals; the unit 51 intended for determination of the coordinates and dimensions, of the current and averaged area of the object generalized binary video image signals in the analysis window and the unit 52, intended for the object video image automatic coordinates determination failure conditions analysis and for forming of attribute PF_ACD of the automatic object coordinates determination stop.<br>
The unit 11 contains, for example, (see Fig. 11) the commutator 53 intended for electronic commutation of the scaled video image signals in the current analysis window to the units of the device directly from the unit 8 (see Fig. 8) or of those filtered in the nonlinear high-pass filter 54; the analyzer 55 intended for comparison of the dissimilarity measure signals parameters and of the movement pattern parameters of the object video, obtained using the object static and dynamic reference images or the object static reference video image only and for forming of the object static reference video image renewal signal: BRAM 56, intended for the storage of the scaled video image signals in the current analysis window: former 57 of the object static and dynamic reference images signals or of the object static reference video image signals only; the unit 58, intended for adjusting to the current scale the object static and dynamic reference images signals or of the object static reference video image signals only; BRAM 59, intended for adjusting to the current scale the object static and dynamic reference images signals or object static reference video image signals only; former 60 intended for adjusting to the current scale object static and<br>
filtration of the moving objects indicator initial binary video image signals; former 47 of the moving objects indicator secondary binary video image signals; former 48 of the horizontal and vertical projections of the moving objects indicator secondary binary video image signals; the analyzer 49 intended for determination of the confidence factors of the moving objects indicator secondary binary video image signals; former of the generalized horizontal and vertical projections of the moving objects indicator generalized binary video image signals; the calculator 51 intended for determination of the coordinates and dimensions, of the current and averaged area of the object generalized binary video image signals in the analysis window and the analyzer 52, intended for the object video image automatic coordinates determination failure conditions analysis and for forming of attribute Pr_ACD of the automatic object coordinates determination stop.<br>
The unit 11 contains, for example, (see Fig. 11) the commutator 53 intended for electronic commutation of the scaled video image signals in the current analysis window to the units of the device directly from the unit 8 (see Fig. 8) or of those filtered in the nonlinear high-pass filter 54; the analyzer 55 intended for comparison of the dissimilarity measure signals parameters and of the movement pattern parameters of the object video, obtained using the object static and dynamic reference images or the object static reference video image only and for forming of the object static reference video image renewal signal; BRAM 56, intended for the storage of the scaled video image signals in the current analysis window; former 57 of the object static and dynamic reference images signals or of the object static reference video image signals only; the unit 58, intended for adjusting to the current scale the object static and dynamic reference images signals or of the object static reference video image signals only; BRAM 59. intended for adjusting to the current scale the object static and dynamic reference images signals or object static reference video image signals only; former 60 intended for adjusting to the current scale object static and<br>
dynamic reference images signals or object static reference video image signals only; the unit 61 intended for determination of the minimal values of the video images dissimilarity measure signals along lines and columns of the object video image displacement two-dimensional search area, for forming and determination of the succession type of the minimal values of the video images dissimilarity measure signals along the lines and columns of the object video image displacement two-dimensional search area; the commutator 62 intended for commutation of the successions data of the minimal values of the video images dissimilarity measure signals to the input of one of three units 64, 65 or 66; approximator 63, intended for analytical approximation of the successions of the minimal values of the video images dissimilarity measure signals by a polynomial of the fourth degree; the unit 64 intended for object video image coordinates detemiination in the analysis window after the location of the minimum of the approximating polynomial of the fourth degree of the successions data of the minimal values of the video images dissimilarity measure signals; the unit 65, intended for object video image coordinates determination in the analysis window after the boundary displacement of the area of fast growth of the values of the video images dissimilarity measure signals relatively to the analysis window center; the unit 66, intended for object video image coordinates determination in the analysis window after the analysis window center coordinates.<br>
Former 32 (see Fig. 12) of the unit 10 (see Fig. 8) contains, for example, former 67 tion of the normalized histograms of the intensity distribution of the object and background video images, and the unit 68, (histogram classifier), intended for classification of video image elements in the current analysis window into the background and object on the basis of comparison of the noinialized intensity histograms of the object and background video images along with forming of the histogram classifier binary video image.<br>
former 36 (see Fig 13) of the unit 10 (see Fig. 8) contains, for example, BRAM 69, intended for storage of the scaled video image signals in the current analysis window; former 70 of the sample background video image signals; the unit 71, intended for<br>
dynamic reference images signals or object static reference video image signals only; the analyzer 61 intended for determination of the minimal values of the video images dissimilaiity measure signals along lines and columns of the object video image displacement two-dimensional search area, for forming and determination of the succession type of the minimal values of the video images dissimilarity measure signals along the lines and columns of the object video image displacement two-dimensional search area; the selector 62 intended for commutation of the successions data of the minimal values of the video images dissimilarity measure signals to the input of one of three coordinators 64, 65 or 66; approximator 63, intended for analytical approximation of the successions of the minimal values of the video images dissimilarity measure signals by a polynomial of the fourth degree; the coordinator 64 intended for object video image coordinates determination in the analysis window after the location of the minimum of the approximating polynomial of the fourth degree of the successions data of the minimal values of the video images dissimilarity measure signals; the coordinator 65, intended for object video image coordinates determination in the analysis window after the boundary displacement of the area of fast growth of the values of the video images dissimilarity measure signals relatively to the analysis window center; the coordinator 66, intended for object video image coordinates deteunination in the analysis window after the analysis window center coordinates.<br>
Former 32 (see Fig. 12) of the unit 10 (see Fig. 8) contains, for example, calculator 67 tion of the nonnalized histograms of the intensity distribution of the object and background video images, and the unit 68, (histogram classifier), intended for classification of video image elements in the current analysis window into the background and object on the basis of comparison of the normalized intensity histograms of the object and background video images along with fomn'ng of the histogram classifier binary video image.<br>
Former 36 (see Fig 13) of the unit 10 (see Fig. 8) contains, for example, BRAM 69, intended for storage of the scaled video image signals in the current analysis window; former 70 of the sample background video image signals: the unit 71, intended for<br>
for adjusting of the reference background video image signals, received in the former n-1 frame, to the current scale and for the shift considering the analysis window shift in inertial coordinates system at last frame, BRAM 72, intended for storage of the reference background video image signals, received in former n-1 frame, adjusted to the current scale and displaced on the analysis window shift value in inertial coordinates system at last frame; the unit 73 intended for forming of the background change detector difference video image signals by subtraction of the reference background video image signals from the scaled image signals in the current analysis window; and the unit 74 intended for deteimination of the threshold THRESHOLDBCD(ix,iy) of the binarization of the background change detector as the value, proportional to the local parameter of dispersion of values of the background change detector difference video image to the neighborhood of the pixel with the coordinates ix,iy and for forming of the prior binary image LI BIN BCD(ix,iy) of the background change detector according to a rule:<br>
LIBINBCD(ixjy)= 1, if LpBcD(ixjy) &gt; THRESHOLDBCD(ix,jy),   or LIBINBCD(ixjy) = 0, if LpBcD(ix,jy) 
For reali/ation of the invention on the second variant the object positioning device may contain, for example, the unit 75 (see Fig. 14), intended for reception and storage of the current video image field image from the tracking system video camera and for forming of the signals of device operability synchronization; the unit 76, intended for reception and storage of the tracking system video camera field of view uncontrolled displacement and roll signals; the unit 77 intended for determination of the field of view axis controlled displacement at the time between reception of signals of the previous and current video image fields, caused by the impact of the video camera field of view displacement management signals on the tracking system video camera; the generator 78, intended for generation of the video image signals of the current n-frame from the video image signals of the former frame taking into consideration the field of view axis displacement at the time between reception of signals of the (n-1) and (n-2) video image fields and of the current video image field signals with the elementwise transformation of the current video<br><br>
image field signals coordinates, providing the compensation of the tracking system video camera field of view uncontrolled displacements and roll; the unit 79 intended for forming and scaling of the video image signals in the current analysis window using the signals of the video image current analysis window location and dimensions; the unit 80 intended for forming of the M analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the M background analysis windows; the unit 81 intended for detemiination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object; the unit 82 intended for the object video image coordinates determination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals fanning; the unit 83, intended for determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming; the commutator 84 intended for electronic switching of the codes of the prior (received from the tracking system) or of the current (received as a result of the video image processing in the current analysis window) object dimensions and coordinates in tracking system video camera field of view to the units of the device; the unit 85 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; the unit 86 intended for the determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals; the unit 87 intended for the determination of the histogram classifier binary video images boundaries area and coordinates of binary video images in M background analysis windows after the received of the histogram classifier binary video images signals in M background analysis windows; the unit 88 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; the unit 89 intended for forming of the current speed of the object video image displacement in the inertial coordinate<br>
image field signals coordinates, providing the compensation of the tracking system video camera field of view uncontrolled displacements and roll; the former 79 intended for forming and scaling of the video image signals in the current analysis window using the signals of the video image current analysis window location and dimensions; the former 80 intended for forming of the analysis windows about the analysis window perimeter and the histogram classifier binary video image signals projection determination in the background analysis windows; the unit 81 intended for deteimination of the current coordinates of the generalized binary video image of the object using the generalized horizontal and vertical projections of the generalized binary video image of the object: the unit 82 intended for the object video image coordinates deteimination relatively to the current analysis window center on the basis of the video image similarity and dissimilarity measure signals forming; the unit 83, intended for determination of the current speed of the object video image displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video images dissimilarity measure signals forming; the commutator 84 intended for electronic switching of the codes of the prior (received from the tracking system) or of the current (received as a result of the video image processing in the current analysis window) object dimensions and coordinates in tracking system video camera field of view to the units of the device; the analyzer 85 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system confidence factors; the analyzer 86 intended for the determination of the confidence factors of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals; the unit 87 intended for the determination of the histogram classifier binary video images boundaries area and coordinates of binary video images in background analysis windows after the received of the histogram classifier binary video images signals in background analysis windows; the unit 88 intended for the determination of the current speed of the object generalized binary video image displacement in the inertial coordinate system; the unit 89 intended for forming of the current speed of the object video image displacement in the inertial coordinate<br>
system complex estimate from the object video image displacement current speed estimate data, received on the basis of the video images dissimilarity measure signals forming and on the basis of the object video image displacement current speed estimate after the object generalized binary video image signals generalized projections, taking into consideration confidence factors of the object video image displacement speed components and of the object maneuvering speed prior limitations, the unit 90 intended for forming of the complex estimate of the object video image coordinates in the tracking system video camera field of view by the integration of the difference of the object image displacement current speed complex estimate and of the field of view axis controlled displacement speed in the inertial coordinates system using the initial coordinates of the object video image in the video camera field of view, received from the tracking system video at the moment of the start of the tracking the object, the analyzer 91, intended for the analysis of the of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows and for forming on the basis of this analysis of the signal of the change for forming of the tracking system video camera field of view axis displacement management signals using the object video image extrapolated coordinates; the unit 92 intended for averaging of (low-pass filtering) the object video image displacement current speed complex estimate and of the storage of the averaged (filtered) speed values; the unit 93, intended for the determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame and for the restoring of the true speed of the object video image displacement on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement at the change for forming of the management signals of the tracking system video camera field of view axis displacement over the extrapolated coordinates; former 94 of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video<br>
system complex estimate from the object video image displacement current speed estimate data, received on the basis of the video images dissimilarity measure signals forming and on the basis of the object video image displacement current speed estimate after the object generalized binary video image signals generalized projections, taking into consideration confidence factors of the object video image displacement speed components and of the object maneuvering speed prior limitations, the unit 90 intended for forming of the complex estimate of the object video image coordinates in the tracking system video camera field of view by the integration of the difference of the object image displacement current speed complex estimate and of the field of view axis controlled displacement speed in the inertial coordinates system using the initial coordinates of the object video image in the video camera field of view, received from the tracking system video at the moment of the start of the tracking the object, the analyzer 91, intended for the analysis of the of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the M background analysis windows and for forming on the basis of this analysis of the signal of the change for forming of the tracking system video camera field of view axis displacement management signals using the object video image extrapolated coordinates; the unit 92 intended for averaging of (low-pass filtering) the object video image displacement current speed complex estimate and of the storage of the averaged (filtered) speed values; the extrapolator 93, intended for the determination of the prognosticated coordinates and of the displacement speed of the object video image in the next frame and for the restoring of the true speed of the object video image displacement on the basis of the analysis of the stored values of the averaged complex estimate of the speed of the object video image displacement at the change for forming of the management signals of the tracking system video camera field of view axis displacement over the extrapolated coordinates; former 94 of the tracking system video camera field of view axis displacement management signals and of the signals of location and dimensions of the analysis window in the next video image frame with the use of the object video<br>
image coordinates in the tracking system video camera field of view or coordinates and speed of the object video image displacement and the processor 95, intended for the computation processing of the local data, transmitted over the bidirectional bus (including virtually implemented parallel algorithmic branches) and which do not need any flow processing of data file.<br>
The unit 81 may be realized in the same way as the unit 10 according to the diagram represented on Fig. 10.<br>
The unit 82 may be realized according to the diagram represented on the Fig. 11, i.e. in the same way as the unit 11.<br>
The example of realization of the first said variant of the device can be taken for the case of realization, at which:<br>
-	functions of the flow computing processing and management logic are realized as the<br>
functions of programmable logic integrated circuits (PLIS) on microcircuits, for example, of the<br>
EPF10K130E type... made by Altera Co. (USA);<br>
-	functions of the local data computing processing (not demanding flow processing of<br>
data files) are realized in the general processor on a microcircuit,  for example, of the<br>
TMS320C80 type made by Texas Instruments Inc. (USA), used by the corresponding unit of the<br>
device in the time sharing using the general bidirectional bus (GB) (including virtual use at<br>
realization of the coucsponding parallel branches of algorithm);<br>
-	small files of the processable data (of the variable parameters) are stored in internal<br>
RAM PLIS and in the processor 24;<br>
-	constant paiameters of the flow processing and management are stored in internal<br>
registers and in the RAM PLIS (are loaded together with loading of the PLIS configurations at<br>
the turning on power from the loading PROM realized on microcircuits, for example, of the<br>
EPCLI20) made by the Altera Co. (USA);<br>
-	large files of the processable data are stored in independent (external in relation to PLIS)<br>
RAM, realized on standard microcircuits, for example, of the IDT71V424... type made by the<br>
Integrated Device Technology (USA):<br>
-	programs (subprograms) and parameters of processing are stored in the external RAM of<br>
the processor realized on microcircuits, for example, o! the MT48LC1M16... type made by<br>
Micron Technology Inc. (USA) (are loaded h\ the piocessor at turning on<br>
power from the  loading FLASH-PROM  realized on a microcircuit,  for example,  of the AM29LV017B... type) Advanced Micro Devices Inc. (USA).<br>
The unit 1 may be realized, for example, on the basis of the independent RAM and of the following functions of PLIS:<br>
-	external videodata communication link (for example, digital video signal), received to<br>
the first input 1.1 of the unit 1 (see Fig. 8.1), with the input of the independent RAM (recording<br>
channel);<br>
-	independent RAM communication link using the videodata with the first output 1-1 of<br>
the unit 1 (reading channel);<br>
-	addressing of the recording channel after reception of the current video image field<br>
(VIF) videodata to the independent RAM;<br>
-	addressing of the reading channel after the distribution of the previous VIF videodata<br>
from the independent RAM;<br>
-	extraction of the videotiming signals from digital composite video: of the frame<br>
synchronizing pulse (FSP), of the line synchronization signal (LSS), of the timing pulses of the<br>
videodata word-by-word synchronization (TSP) for use in other functions of the unit 1 and<br>
functions of other units of the device - are translated as the constituent of the synchronization<br>
bus (SB) to the second output 1-2 of the unit 1;<br>
-	forming of a automatic positioning mode signal (AP: AP=1 the start of the mode, AP=O<br>
the end of the mode) on the basis of the external signal START/STOP (see actions 108/110 Fig.<br>
19), received to the second input 1.2 of the unit 1 and of the AP failure signal (action 143 on Fig.<br>
19), received by the third input 1.3 of the unit 1 from the former 50 (see Fig. 10.2), for use in<br>
other functions of the unit 1 and functions of other units of the device - is translated as the<br>
constituent of the SB to the second output 1-2 of the unit 1;<br>
-	forming of the code of n number VIF (at AP=0 - n=0, at AP=1 - value n is equal to the<br>
current VIF number from the start of AP) - is translated as the constituent of the SB to the second<br>
output 1-2 of the unit 1;<br>
The unit 2 may be realized, for example, as PLIS function of buffering (in internal registers of PLIS) of the codes signals (digital, for example) of the field of vie\\ axis displacement angular speed, received to the input of the unit 2 from the third input of the dexice and their translation from outputs of buffer registers to the output of the unit 2.<br>
The unit 3 may be realized, for example, by the following PLIS functions:<br>
-	expressions realization subprogram call (39) in the processor 24 by means of the<br>
corresponding interrupt input into the general bus to the second output 3-2 of the unit 3, on<br>
condition that ACD=1, otherwise:<br>
-	appropriation of code implications to dx and dy displacements, calculated (operation 99<br>
of the algorithm on Fig. 19) according to the code implications of the control console angle<br>
position, received to the second input of 3.2 unit 3 (operation 98 of the algorithm on Fig. 19);<br>
at that:<br>
-	code implications of management signals XMAN[n], YMAN[n] are received from the<br>
first input of the 3.1 unit 3 from former 23 and buffered in internal PLIC registers, program-<br>
accessible to processor 24 through the GB, connected to the second output 3-2 of the unit 3, in<br>
the reading mode;<br>
-	ACD   signals,   FSP   (determining   the   calculation   start   in   current   VIF),   TSP<br>
(synchronizing the unit operation) are received through the SB from the third input 3.3 of the unit<br>
3 from the unit 1;<br>
-	expressions realisation subprogram (39) and pulse characteristics implications hx[ij,<br>
hy[i] are stored in processor 24 RAM; resultant value dx[n] and dy[n] buffered in internal PLIC<br>
registers, program-accessible to processor 24 through the GB in recording mode, and are<br>
received (from the buffer registers outputs) by the first output 3-1 of the unit 3 and etc.<br>
Complete operation cycle of first filed variant of device, realized for conditions, when low-pass uncontrolled tracking system video camera field of view displacements take place (which causes shift and roll of all video image field elements as a whole), and also with the possible absence hi tracking system of tracking system video camera field of view stabilization error indicators, is illustrated in the operation algorithm block diagram in the Fig. 19.<br>
Operability of second variant of device, realized for conditions, when low-pass uncontrolled tracking system video camera field of view displacements take place (field of view "jitter") with the higher object moving dynamics, is illustrated by the algorithm block diagram in the Fig. 20.<br>
List of all reference indications and element names on drawings, which these indications belong to<br>
1,	75 - unit of the reception and storage of the current video image field of the tracking<br>
system video camera and of forming of the device operability synchronisation signals<br>
2,	76 - unit of the reception and storage of the tracking system video camera uncontrolled<br>
displacement and roll signals,<br>
3,	77 - unit of the computation of the controlled displacement of the tracking system<br>
video camera field of view axis at the time between the reception of the (n-1) and (n-2) video<br>
image fields signals, where n=3, 4, 5,... is the current video image field number,<br><br>
4	- unit of forming of the signals of video image dissimilarity measure of the 2N terrain<br>
reference marks and of the forming of the rotation and shift parameters of the current video<br>
image field at the time between the reception of the current and former video image fields,<br>
5	- unit of the division of the current video image field signals shift parameters obtained<br>
on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain<br>
reference marks into the constituents of the tracking system video camera field of view axis<br>
controlled displacement and of the tracking system video camera field of view axis uncontrolled<br>
displacement at the time between the reception of the current and the former video image fields<br>
signals,<br>
6	- switching unit,<br><br>
7,	78 - former of the current n-frame video image signals from the former frame video<br>
image signals with regard to the field of view axis controlled displacement at the time between<br>
the reception of the video image (n-1) and (n-2) fields signals and of the current video image<br>
field signals with the elementwise current video image field signals coordinates transfounation<br>
which supports the compensation of the tracking system video camera field of view uncontrolled<br>
displacements and roll,<br>
8,	79 - unit of the video image signals forming and scaling in the current analysis window<br>
using the video image current analysis window location and dimensions signals.<br>
List of all reference indications and element names on drawings, which these indications belong to<br>
1,75- unit of the reception and storage of the current video image field of the tracking system video camera and of forming of the device operability synchronisation signals<br>
2,	76 - unit of the reception and storage of the tracking system video camera uncontrolled<br>
displacement and roll signals,<br>
3,	77 - unit of the computation of the controlled displacement of the tracking system<br>
video camera field of view axis at the time between the reception of the preceding and current<br>
video image fields signals,<br><br>
4	- unit of foiling of the signals of video image dissimilarity measure of the 2N terrain<br>
reference marks and of the deteimination of the rotation and shift parameters of the current video<br>
image field at the time between the reception of the current and former video image fields,<br>
5	- unit of the division of the current video image field signals shift parameters obtained<br>
on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain<br>
reference marks into the constituents of the tracking system video camera field of view axis<br>
controlled displacement and of the tracking system video camera field of view axis uncontrolled<br>
displacement at the time between the reception of the current and the former video image fields<br>
signals,<br>
6	- switching unit,<br><br>
7,	78 - former of the current video image signals from the former frame video image<br>
signals with regard to the tracking system video camera field of view axis controlled<br>
displacement at the time between the reception of the video image preceding and current fields<br>
signals and of the ciuient video image field signals with the elementwise current video image<br>
field signals coordinates transfounation which supports the compensation of the tracking system<br>
video camera field of view uncontrolled displacements and roll,<br>
8,	79 - video image signals formers in the current analysis window using the video image<br>
current analysis window location and dimensions signals.<br>
9,	80 - unit of the M analysis windows forming about the analysis window perimeter and<br>
the  histogram  classifier  binary  video  image  signals  projection   determination  in  the  M<br>
background analysis windows,<br>
10,	81 - unit of the detei urination of the current coordinates of the object generalized<br>
binary video image using the generalized horizontal and vertical projections of the object<br>
generalized binary video image,<br>
11,	82 - unit of the object video image coordinates determination relatively to the current<br>
analysis window center on the basis of the video image similarity and dissimilarity measure<br>
signals forming,<br>
12,	83 - unit of determination of the current speed of the object video image displacement<br>
in the inertial coordinate system using the object video image coordinates obtained on the basis<br>
of video images dissimilarity measure signals forming,<br>
13,84-commutators  of the codes  of the  ingoing  or current object video  image dimensions and of the object coordinates in the tracking system video camera field of view,<br>
14,	85 - unit of detemiination of the current speed of the object generalized binary video<br>
image displacement confidence factors in the inertial coordinate system,<br>
15,	86 - unit of detetmination of the confidence factors of the current speed of the object<br>
video image displacement in the inertial coordinate system obtained on the basis of video images<br>
dissimilarity measure signals,<br>
16,	87 - unit of the binary video images boundaries area and coordinates determination in<br>
M background analysis windows,<br>
17,	88 - unit of deteunination of the current speed of the object generalized binary video<br>
image displacement in the inertial coordinate system,<br>
18,	89 - unit of the forming of the current speed of the object video image displacement<br>
in the inertial coordinate system complex estimate,<br>
19,	90-unit of the complex estimate of the video image coordinates in the tracking<br>
system video camera field of view,<br>
20,	91 - analyzer of the conditions of the transition to the use of the object video image<br>
extrapolated coordinates on the basis of the current and averaged area of the object generalized<br>
binary video image, current and averaged speed of the object video image displacement, area and<br>
boundaries of the histogram classifier binary video images  in the M  background analysis<br>
windows.<br>
9,	80 - analysis windows formers about the analysis window perimeter and the histograrrt<br>
classifier binary video image signals projection determination in the background analysis<br>
windows,<br>
10,	81 -unit of the deteunination of the current coordinates of the object generalized<br>
binary video image using the generalized horizontal and vertical projections of the object<br>
generalized binary video image,<br>
11,	82 - unit of the object video image coordinates determination relatively to the current<br>
analysis window center on the basis of the video image similarity and dissimilarity measure<br>
signals forming,<br>
12,	83 - unit of detemiination of the current speed of the object video image displacement<br>
in the inertial coordinate system using the object video image coordinates obtained on the basis<br>
of video images dissimilarity measure signals forming,<br>
13,	84 - commutators of the codes of the ingoing or current object dimensions and of the<br>
object coordinates in the tracking system video camera field of view,<br>
14,	85-analyzers of the current speed of the object generalized binary video image<br>
displacement in the inertial coordinate system,<br>
15,	86-analyzers of the current speed of the object video image displacement in the<br>
inertial coordinate system obtained on the basis of video images dissimilarity measure signals,<br>
16,87 - units of the histogram classifier binary video images boundaries area and coordinates deteunination in background analysis windows,<br>
17,	88 - units of deteunination of the current speed of the object generalized binary video<br>
image displacement in the inertial coordinate system,<br>
18,	89 - units of the foaning of the current speed of the object video image displacement<br>
in the inertial coordinate system complex estimate,<br>
19,	90 - units of the complex estimate of the video image coordinates in the tracking<br>
system video camera field of view,<br>
20,91 -analyzers of the conditions of usage of the object video image extrapolated coordinates on the basis of the current and averaged area of the object generalized binary video image, current and averaged speed of the object video image displacement, area and boundaries of the histogram classifier binary video images in the background analysis windows.<br>
21,	92 - unit of averaging the complex estimate of the current speed of the object video<br>
image displacement and of the storage of the averaged speed values,<br>
22,	93 - unit of determination of the prognosticated coordinates and of the displacement<br>
speed of the object video image in the next frame on the basis of the analysis of the stored values<br>
of the averaged complex estimate of the speed of the object video image displacement,<br>
23,	94 - former of the tracking system video camera field of view axis displacement<br>
management signals and of the signals of location and dimensions of the analysis window in the<br>
next video image frame using the object video image coordinates in the tracking system video<br>
camera field of view or coordinates and speed of the object video image displacement,<br>
24,	95 - the processor of the computation processing of the local data, being transmitted<br>
over the bidirectional bus,<br><br>
25	- video images terrain reference marks selector,<br>
26	- device for coordinate determination of the first pair of video images terrain reference<br>
marks,<br>
27	- device for coordinate determination of the second pair of video images terrain<br>
reference marks,<br>
28	- device for coordinate determination of the third pair of video images terrain<br>
reference marks,<br>
29	- device for current video image field signals shift and roll parameters detemiination,<br>
30	- the first buffer random access memory (BRAM),<br>
31	- the first videodata commutator,<br>
32	- former of the histogram classifier binary video image signals in the current analysis<br>
window,<br>
33	- former of the histogram classifier binary video image signals horizontal and vertical<br>
projections,<br>
34	- unit of the histogram classifier binary video  image  signals confidence  factor<br>
determination,<br>
35	- the unit of the object/background ratio and background video image signals mean<br>
square value minimum determination.<br>
21,	92 - unit of averaging the complex estimate of the current speed of the object video<br>
image displacement and of the storage of the filtered speed values,<br>
22,	93 - extrapolator of the coordinates and of the displacement speed of the object video<br>
image in the next frame on the basis of the analysis of the averaged values of the speed of the<br>
object video image Displacement,<br>
23,	94 - former of the tracking system video camera field of view axis displacement<br>
management signals and of the signals of location and dimensions of the analysis window in the<br>
next video image frame using the object video image coordinates in the tracking system video<br>
camera field of view or coordinates and speed of the object video image displacement,<br>
24,	95 - processors for the computation processing of the local data, in time sharing over<br>
the bidirectional bus,<br><br>
25	- video images tenain reference marks selector,<br>
26	- device for coordinate determination of the first pair of video images terrain reference<br>
marks,<br>
27	- device for coordinate determination of the second pair of video images terrain<br>
reference marks,<br>
28	- device for coordinate determination of the third pair of video images terrain<br>
reference marks,<br>
29	- device for ciureat video image field signals shift and roll parameters detemiination,<br>
30	- the first buffer random access memory (BRAM),<br>
31	- video data commutator,<br>
32	- former of the histogiam classifier binary video image signals in the current analysis<br>
window,<br>
33	- former of the histogram classifier binary video image signals horizontal and vertical<br>
projections,<br>
34	- analyzer of the histogram classifier binary video image signals,<br>
35	- the unit of the object/background ratio and background video image signals mean<br>
square value minimal deteunination.<br>
36	- former of the background change detector primary video image signals,<br>
37	- spatial low-pass filter of the background change detector primary binary video image<br>
signals,<br>
38	- former of the background change detector secondary video image signals,<br>
39	- former of the background change detector secondary binary video image signals<br>
horizontal and vertical projections,<br>
40	- unit of the background change detector binary secondary video image signals<br>
confidence factor determination,<br>
41	- the second BRAM,<br>
42	- the first node of video image signals scaling and shift,<br>
43-the third BRAM,<br>
44	- the node of forming of the moving-objects indicator difference video image signals,<br>
45	- former of the moving-objects indicator primary video image signals,<br>
46	- spatial low-pass filter of the moving-objects indicator primary binary video image<br>
signals,<br>
47	- former of the moving-objects indicator secondary video image signals,<br>
48	- former of the moving-objects indicator secondary binary video image signals<br>
horizontal and vertical projections,<br>
49	- unit moving-objects indicator binary secondary video image signals confidence<br>
factor determination,<br>
50	- former of the generalized vertical and horizontal projections of the object generalized<br>
binary video image,<br>
51	- unit of title detemiination of the coordinates and sizes, of the current and averaged<br>
area of the object biiwy video image in the analysis window,<br>
52	- analyzer of the averaged binary video image coordinates automatic detemiination<br>
break,<br>
53	- second video data commutator,<br>
54	- non- linear low- pass filter.<br>
36	- former of the background change detector primary video image sigoals,<br>
37	- spatial low-pass filter of the background change detector primary binary video image<br>
signals,<br>
38	- former of the background change detector secondary video image signals,<br>
39	- former of the background change detector secondary binary video image signals<br>
horizontal and vertical projections,<br>
40	- analyzer of the background change detector binary secondary video image signals,<br>
41	- the second BRAM,<br>
42	- the first node of video image signals scaling and shift,<br>
43	- the third BRAM,<br>
44	- the node of forming of the moving-objects indicator difference video image signals,<br>
45	- former of the moving-objects indicator primary video image signals,<br>
46	- spatial low-pass filter of the moving-objects indicator primary binary video image<br>
signals,<br>
47	- former of the moving-objects indicator secondary video image signals,<br>
48	- former of the moving-objects indicator secondary binary video image signals<br>
horizontal and vertical projections,<br>
49	- analyzer of the moving-objects indicator binary secondary video image signals,<br>
50	- former of the generalized vertical and horizontal projections of the object generalized<br>
binary video image,<br>
51	- calculator of the object coordinates and sizes, of the current and averaged area of the<br>
object binary video image in the analysis window,<br>
52	- analyzer of the averaged binary video image coordinates automatic determination<br>
break,<br>
53	- scaled image signals commutator,<br>
54	- non- linear low- pass filter,<br>
55	- analyzer of the object static video reference video image renewal conditions,<br>
56	- the third BRAM,<br>
57	- former of the object static and dynamic reference video images signals,<br>
58	- the second node of the object video image signals scaling and shift,<br>
59	- the fourth BRAM,<br>
60	- former of the dissimilarity measure signals between the video image signals in the<br>
binary search area of the object video image shifts,<br>
61	- the node of forming and analysis of the video image dissimilarity measure signals<br>
minimum value sequence type along the lines and columns of the binary search area of the object<br>
video image shifts,<br>
62	- commutator of the video image dissimilarity measure signals minimum value<br>
sequence data,<br>
63	- the approximator of the video image dissimilarity measure signals minimum value<br>
sequence by the forth degree polynomial,<br>
64	- the node of the object video image coordinates deteimination in the analysis window<br>
by the status of the approximating forth degree polynomial of the video image dissimilarity<br>
measure signals minimum value sequence data,<br>
65	- the node of the object video image coordinates deteimination in the analysis window<br>
by the boundaries shift of the fast growth area of the video images dissimilarity measures signals<br>
values relatively to analysis window center,<br>
66	- the node of the object video image coordinates deteimination in the analysis window<br>
after the analysis window center coordinates,<br>
67	- former of the noimalized histograms of the intensity distribution of the object and<br>
background video images signals,<br>
68	- node of the histogram classifier binary video image forming,<br>
69	- the fifth BRAM,<br>
70	- former of the reference background video image signals,<br>
71	- the third node of video image signals scaling and shift.<br>
72	- analyzer of the object static video reference video image renewal conditions,<br>
73	- the first BRAM,<br>
74	- former of the object static and dynamic reference video images signals,<br>
75	- node of the video image signals scaling,<br>
76	- the second BRAM,<br>
77	- former of the dissimilarity measure signals between the video image signals in the<br>
binary search area of the object video image shifts,<br>
78	- former of the video image dissimilarity measure signals minimum value sequence<br>
type along the lines and columns of the binary search area of the object video image shifts,<br>
79	- selector of the video image dissimilarity measure signals minimum value sequence,<br>
80	- the approximator of the video image dissimilarity measure signals minimum value<br>
sequence by the forth degree polynomial,<br>
81	- coordinator of the object video image in the analysis window by the status of the<br>
approximating polynomial,<br>
82	- coordinator of the object video image in the analysis window by the boundaries shift<br>
of the fast growth area of the video images dissimilarity measures signals values relatively to<br>
analysis window center,<br>
83	- coordinator of the object video image in the analysis window after the analysis<br>
window center coordinates,<br>
84	- calculator of the nouualized histograms of the intensity distribution of the object and<br>
background video images signals,<br>
85	- node of the histogram classifier binary video image forming,<br>
86	- the first BRAM,<br>
87	- former of the reference background video image signals,<br>
88	- the node of reference background video image signals scaling and shift,<br>
72	- the sixth BRAM,<br>
73	- node of the background change detector difference video image signals forming,<br>
74	- node of the background change detector primary binary video image forming.<br>
Besides on figure drafts 8.1, 8.2, 8.3, 10.1, 10.2, 11.1, 11.2, 14.1, 14.2 and 14.3 near<br>
fields there are used such indications as 13(8), 1(9,10,13-23) etc., put down near connecting lines. They mean that, for example, 13(8) "connection of the output of the unit 13 to the input of the unit 8 " and for 1(9,10,13-23) it means "connection of the output of the unit 1 to the inputs of the units 9, 10, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23 ".<br>
- the second BRAM,<br>
72	- node of the background change detector difference video image signals forming,<br>
73	- node of the background change detector primary binarization.<br>
Besides on figure drafts 8.1, 8.2, 8.3, 10.1, 10.2, 11.1, 11.2, 14.1, 14.2 and 14.3 near fields there are used such indications as 13(8), 1(9,10,13-23) etc., put down near connecting lines. They mean that, for example, 13(8) "connection of the output of the unit 13 to the input of the unit 8 " and for 1(9,10,13-23) it means "connection of the output of the unit 1 to the inputs of the units 9, 10, 13,14,15. 16. 17, 18. 19.20.22.23".<br><br><br><br><br><br><br><br>
WE CLAIM:<br>
1. Device   (60)   for   object   coordinates   determination,   observed   in   the<br>
sequence of video images, comprising :<br>
processor (95) for the computation processing of the local data in the time sharing over the bidirectional bus characterized by the following means<br>
means of analysis (85, 10) of the current speed of the object generalized binary video image displacemont in the inertial coordinate system;<br>
means of analysis (86,15) of the current speed of the object video image displacement in the inertial coordinate system obtained on the basis of video images dissimilarity measure signals;<br>
means of analysis (91,20) of the conditions of the transition to the use of the object video image<br>
said means of the current speed of the object video image displacement, means of analysis of the current speed of the object generalized binary video image and means of analysis being connected to each other,<br>
extrapolated coordinates on the basis of the current and averaged area of the object generalized<br>
binary video image, current and average speed of the object video image displacement,area and<br>
boundaries of the histogram classifier binary video images in the background analysis windows;<br>
means   for   extrapolation   (93,22)   of  the   coordinates   and   of  the<br>
displacement speed of the object video image in the next frame on the<br>
basis of the analysis of the values of the averaged speed of the object<br>
video image displacement;<br>
means of the reception and storage (75,1) of the current video image field<br>
of the tracking system video camera and of the forming of the device<br>
operability signals,<br>
means of the reception (76,2) of computation of the controlled rotation of the tracking system video camera,<br>
said means for extrapolation, means for reception and storage andjnears for reception and computation being connected so as to track the video camera field of axis,<br>
means for computation (77,3) of the controlled rotation of the tracking system video camera field of view axis at the time between the reception of the former and current video image fields axis,<br>
means of forming (78,7)of the current frame video image signals from the former frame video image signals with regard to the tracking system video camera field of view axis controlled displacement at the time between the reception of the video image former and current fields signals and., of the current video image field signals with the elementwise current video image filed signals coordinates transformation which supports the compensation of the tracking system video camera field of view uncontrolled displacement and roll;<br>
Means of forming (79,8) of the video image signals in the current analysis window using the video image current analysis window location and dimensions signals;<br>
means of forming (80,9) of the analysis windows about the analysis window perimeter and the histogram classifier binary video image-signals projection determination in the background analysis windows; means (83,12) for determinatio of the current speed of the object video image of displacement in the inertial coordinate system using the object video image coordinates obtained on the basis of video imagos dissimilarity measure signals forming;<br>
means (87,16) of the histogram classifier binary video images boundaries area and coordinates determination in background analysis windows;<br>
means   (88,17)   of  determination   of  the   current   speed   of  the   object<br>
generalized  binary video  image  displacement  in  the  inertial  coordinate<br>
system;<br>
means of forming (89,18) of the current speed of the object of the video<br>
image displacement in the inertial coordinate system complex estimates;<br>
means (90,19) of the complex estimate of the video image coordinates in the<br>
tracking system video camera field of view;<br>
means (92,21) of averaging the complex estimate of the current speed of the<br>
object video image displacement and of the storage of the filtered speed<br>
values;<br>
means of forming (94,23) of the tracking system video camera field view axis;<br>
displacement  management signals and  of the signals of location  and<br>
dimensions of the analysis window in the next video image frame with the<br>
use of the object video image coordinates in the tracking system video<br>
camera field of view or coordinates and speed of the object video image<br>
displacement;<br>
said   means of forming (77,3),(78,7),(78,9), (80,9),(83,12), (87,16), (88,17),<br>
(89,18) and (94,23) connected to access the displacement of the object<br>
video image;<br>
means  (81,10)  of the  determination  of the  current  coordinates  of the<br>
generalized   binary   video   image   of  the   object   using   the   generalised<br>
horizontal and vertical projections of the generalized binary video image of<br>
the object;<br>
means (82,11) of the object video image coordinates determination relatively<br>
to the current analysis window center on the basis of the video image<br>
similarity and dissimilarity measure signals forming;<br>
means for commutation (84,13) of the codes of the ingoing or current object<br>
dimensions and of the object coordinates in the tracking system video<br>
camera field of view<br><br>
said means for the determination of the current coordinates and means for video image co-ordinate determination and means for commutation of the codes being linked together thereby providing the desired result of locating of the positioning of the said object.<br>
2. Device as claimed in claim 1 comprising means of forming (4) of the signals of video   image dissimilarity measure of the 2N terrain reference marks and of the rotation and shift parameters of the current video image field determination at the   time between the reception of the current and former video image fields;<br>
means of the division (5) of the current video image field signals shift parameters obtained on the basis of the estimation of the dissimilarity measure of the video images of the 2N terrain reference marks into the constituents of the tracking system video camera field of view axis controlled shift and of the tracking system video camera field of view axis uncontrolled shift at the time between the reception of the current and the former video image fields signals and switching means (6),<br>
3.      Device  as  claimed   in   claim   1,   wherein   said   means  (81,   10)  of the determination of the current coordinates comprising: videodata commutator (31); the first buffer random access memory (30); the second buffer random access memory (41) and the third buffer random access memory (43) and<br>
means of forming (32) of the histogram classifier binary video image signals in the current analysis window;<br>
means of forming (36) of the background change detector primary binary video image;<br>
means of forming (38) of the background change detector secondary vi image signals;<br>
means of forming (45) of the moving-objects indicator primary video image<br>
signals;<br>
means of forming (47) of the moving-objects indicator secondary video<br>
image signals;<br>
means of scaling (42) of video image signals and shift;<br>
means of forming   (44) of the moving-objects indicator   difference video<br>
image signals;<br>
means of forming (33) of the histogram classifier binary video image signals<br>
horizontal and vertical projections;<br>
means of forming (39)    of the background change    detector secondary<br>
binary video image signals horizontal and vertical projections;<br>
means of forming (48) of the moving-objects indicator secondary binary<br>
video image signals horizontal and vertical projections;<br>
means (35) for the object/background ratio and background video image<br>
signals minimum mean square value determination    in the backgrounc<br>
analysis windows;<br>
means of spatial low-pass filtering (37) of the background change detecton<br>
primary binary video image signals;<br>
means of spatial low-pass filtering  (46) of the moving-objects indicator<br>
primary binary video image signals;<br>
means of analysis (34) of the histogram classifier binary video image<br>
signals<br>
means of analysis (40) of the background change detector binary secondary<br>
video image signals;<br>
means of analysis (49) of moving-objects indicator binary secondary video<br>
image signals, and<br>
means of forming (50) of the generalized  vertical and horizontal projections<br>
of the generalised object binary video image,<br>
means of calculation (51) of the object coordinates and sizes of the current<br>
and averaged area of the object binary video image, and<br>
means of analysis   (52)    of the object video image coordinates automatic determination break conditions means of analysis.<br>
Device as claimed in claim 2, wherein<br>
means of forming (32) of the histogram classifier binary video image signals<br>
in the current analysis window comprising:<br>
means of calculation (67)    of the  normed  histograms of the intensity<br>
distribution of the signals of the object video images, and<br>
means of forming (68) of the rrstogram classifier binary video image.<br>
Device    as    claimed in claim 2, wherein means of forming (36) of ttie background change detector primary video image signals comprising: buffer random access memories (69) and (72);<br>
means of forming (70) of the reference background video image signals; means of scaling (71) of reference background video image signals shift; means of forming (73) of the background change detector difference video image signals, and means of primary binarization    (74) of the background change detector.<br>
Device as claimed in claim 1, wherein said means (82, (11) of the object<br>
video image coordinates determination relatively to the current analysis<br>
window    center on the basis of the video image dissimilarity    measure<br>
signals forming comprising:<br>
high-pass nonlinear filter (54);<br>
buffer random access memories (56) and (59) and<br>
means of commutation (53) of the scaled image signals;<br>
means  of  analysis  (55)  of the  static  video   reference  image   renewal'<br>
conditions.<br>
means of analysis (61) of the video   image dissimilarity   measure signals<br>
minimum value  sequence type along  the lines and columns  of the binary<br>
search area of the object video image shifts;<br>
means of forming (57) of the static and dynamic video images signals;<br>
means of   forming (60) of the dissimilarity measure signals between the-<br>
video    image signals in the binary search are of the object video image<br>
shifts;<br>
means of scaling (58) of the object video image signals;<br>
means for selection (62) of the video image dissimilarity   measure signals<br>
minimum values sequence data;<br>
means of approximation (63) of the video image dissimilarity    measure<br>
signals minimum values sequence by the forth degree polynomial;<br>
means (64) for determination of the coordinates of the object video image in<br>
the analysis window by the status of the polynomial;<br>
means (65) for determination of the coordinates of the object video image In.<br>
the analysis window by the boundaries shift of the fast growth area of the<br>
video images dissimilarity measures   signals values relatively to analysis<br>
window center;<br>
means (66) for determination of the object video image  coordinates in the<br>
analysis window after the analysis window center coordinates.<br>
7.      Method of signal processing for determination of object    coordinates as claimed in claim 1 having:<br>
reception and storage of signals of the current video image field in means 75<br>
(1):<br>
determination of the controlled video camera field of view axis displacement in means 77 (3) before the reception of the signals of the current video image field at the time between the reception of former and current video<br>
image fields signals caused by the impact of the displacement of the field of view of the tracking system video camera;<br>
determination of the speed of the controlled tracking system video camera field of view displacement in means 77(3) from the controlled tracking system video camera field of view displacement data at the time between the reception of former and current video image fields signals,<br>
reception and storage of the signals of the uncontrolled displacement and roll of the field of view of the tracking system video camera in means 76(2), which are performed simultaneously with the reception of the current video image field signals, and such signals are used for current video image<br>
frame forming,<br>
forming of the video image signals of the current frame from image signals of the former frame in means 78(7) with allowance for controlled field of view axis displacement at the time between reception of signals of the former and current video image fields and signals of the current video image field with bit transformation of signals coordinates of the current video image field which compensate for the video camera field of view uncontrolled displacement and roll;<br>
forming and scaling of the video image signals in the current analysis window in means 79{8) using location and dimensions of the current video image field analysts window signals obtained after video image processing in the former video image frame in means 78(7) or using the object extrapolated coordinates and speed with the initial conditions of location and dimensions of the video image field analysis window signals are formed at the start of tracking over an object on the basi$ of the exterria. signals START/STOP on the input 2 of the device;<br>
storage of signals of the scaled video image in the current analysis window in BRAM41;<br>
forming of differential video image signals of the moving-objects indictor in means 44 by subtraction the scaled video image signals, which were stored in means 41 and corrected to the current video image scale in the analysis window and shifted in means 42 on the value of the tracking system video camera field of view axis displacement, from the scaled video image'signate in the current analysis window;<br>
forming of the primary binary image signals of the moving-objects indicator in means 45 from the differential video image signals of the moving-objects<br>
indicator;<br>
forming of the secondary binary image signals of the moving-objects indicator in means 47 from the primary video image signals of the moving-objects indicator exposed to fow-pass filter in means 46;<br>
forming of the primary binary image signals of the background change detector in means 36 and<br>
forming of the binary image signals of the histogram classifier in the current analysis window in means 32 comprising means 67 and 68 simultaneously with the storage of the scaled video image signals in the current analysis window, with the forming of the primary and secondary binary image signals of the moving-objects indicator from the scaled video image signals in the current analysis window signals with allowance to the controlled shift of the field of view axis of the trackinq system video camera;<br>
forming of the secondary binary image signals of the background change detector in means 38 from the primary binary image signals of the background change detector exposed to low-pass filtering in means 37; forming of horizontal and vertical projections of the secondary binary images signals of the moving-objects indicator in means 48 and background change detector in means 39, and of horizontal and vertical projections of binary image signals of the histogram classifier in means 33; determination of confidence factors of the secondary binary images signals of the background change detector in means 40 and moving-objects indicator in means 49, as well as of binary image signals of the histogram classifier in means 34;<br>
forming of the generalized horizontal and vertical signal projections of the object generalized binary image in means 50 from horizontal and vertical' signal projections of the secondary binary images of the moving-objects indicator and background change detector, as well as from horizontal and vertical signal projections of the histogram classifier binary image on the basis of their joint processing which uses confidence factors of binary images signals of the histogram classifier, the secondary binary images signals of the background change detector and moving-objects indicator;<br>
determination of horizontal and vertical boundaries as well as the object image sizes at the cut-off levels on all four sides of the assigned area percent of generalized horizontal and vertical signals projections of the generalized binary object image in means 51 of means 81(10);<br>
determination of the current and average areas of the generalized binary object image located inside the formed object image boundaries in means 51 of means 81 (10);<br>
determination of the current coordinates of the generalized binary object image using generalized horizontal and vertical projections of the generalized binary object image in means 51 of means 81(10);<br>
determination of the current traverse speed of the generalized binary object image in the inertial coordinates system in means 88 (17);<br>
determination of the confidence factor of the generalized binary object current traverse speed in the inertial coordinates system in means 85 (14);<br>
simultaneously with forming of signals of the primary and secondary binary images of the moving-objects indicator and background change detector, binary image of the histogram classifier, as well as generalized horizontal and vertical signal projections of the generalized binary object image object image coordinates are defined in relation to the centre of the current analysis window on the basis of :<br>
forming of images dissimilarity measure signals in means 82 (11) as a result of non-linear high-pass filtering of scaled image signals in the current analysis window, fulfilled on condition that:<br>
the average area of the object generalized binary image exceeds the<br>
threshold value (means 55),<br>
on condition of storage of received signals in BRAM 56,<br>
on condition of forming of object static reference image signals or object static and dynamic reference images signals in means 57,<br><br>
on condition of reduction of object static reference image signals or object static and dynamic reference image signals to the current scale in means 58,<br>
on condition of forming in means 60 and storage in means 59 of the signals of the dissimilarity measure between the video image signals after the nonlinear high-pass filtering of the scaled video image signals in the current analysis window and object static and dynamic reference images signals in the two-dimensional search area of object video image shifts,<br>
on condition of definition of minimum values of video images dissimilarity measure signals along the lines and columns of two-dimensional search area of object video image shifts in means 61.<br>
on condition of forming of minimum values sequences of images dissimilarity measure signals along the lines and columns of two-dimensional search area of object image shifts in means 61,<br>
on condition of determination of the appropriate object image coordinate in the analysis window according to the type of minimum values sequence of images dissimilarity measure signals for this coordinate, namely:<br>
by analytical approximation of the minimum values sequence of imayes dissimilarity measure signals by the fourth degree polynomial in means 63 and<br>
by determination of object image coordinate in means 64 as the approximation polynomial minimum position on condition that the sequence is related to the type of sequences with two boundaries of values fast growth areas of images dissimilarity measure signals close to the position of: its<br>
minimum, or by determination of the boundary shift of values fast growth area of images dissimilarity measure signals in relation to the analysis window centre and<br>
by object coordinate forming in the analysis window in means 65 as an amount proportional to the obtained boundary shift of values fast growth area of images dissimilarity measure signals on condition that the sequence is referred to the type of sequences with plane neighbourhood of the minimum position and existence of one values fast growth area of images dissimilarity measure signals, or<br>
by forming of the object coordinate in means 66 equal to the coordinate of the analysis window centre on condition that the sequence is referred to the type of sequences with plane neighbourhood of values minimum position of images dissimilarity measure signals in the whole search area of object<br>
image shifts;<br>
determination of the current object image traverse speed in the inertia! coordinates system in means 83 (12) using object image coordinates, obtained on the basis of forming of images dissimilarity measure signals;<br>
determination of the confidence factor of the current object image traverse speed in the inertial coordinates system obtained on the basis of forming of images dissimilarity measure signals in means 86, 15;<br>
forming of the complex estimate of the object image traverse speed in the inertial coordinates system in means 89 (18) from estimate data of the current object image traverse speed got on the basis of forming of image dissimilarity measure signals, and estimate of the current object traverse speed across the generalized signals projections of the object generalized;<br>
binary image with allowance for confidence   factors which form the image traverse speeds and prior restrictions of the object manoeuvring speed;<br>
averaging of the complex estimate signals of the object image current traverse speed in the inertial coordinates system in means 92 (21);<br>
storage of the complex estimate signals of the object image current traverse speed in the inertial coordinates system in means 92 (21);<br>
determination of the object image coordinates in the video camera field of view in means 90 (19) by difference integration of the complex estimate of the current object image traverse speed in the inertial coordinates system and controlled displacement speed of the field of view axis in the inertial coordinates system with the initial conditions of object video image coordinates and traverse speed, formed at the start of the tracking over an object on the basis of the external signal START/STOP;<br>
forming of the background analysis window in means 80 (9) along the analysis window perimeter and signals projections of the histogram classifier binary images are determined in windows of background analysis simultaneous with forming of the primary and secondary binary images signals of the moving-objects indicator and background change detector, binary image of the histogram classifier, generalized horizontal and vertical signals projections of the generalized binary object image;<br>
determination of the areas and coordinates of binary images boundaries of the histogram classifier in windows of background analysis in means 87 (16) after the obtained signals projections of the histogram classifier binary image in windows of background analysts.<br><br>
forming of video camera field of view axis displacement management signals in means 94 (23) using object image coordinates in the video camera field of view obtained in the result of image processing in the current analysis window in means 81 (10) or using the extrapolated coordinates from means 93 (22) and object image traverse speed according to results of analysis of the current and average area of the object generalized binary image, current and average object image traverse speed in means 19 (20), and according to results of analysis of area and coordinates of binary images boundaries of the histogram classifier in windows of background analysis in means 35,<br>
at that, the extrapolated object image traverse speed is formed in means 93 (22) on the basis of analysis of the stored values of the average complex estimate of the object image current traverse speed;<br>
forming of signals of the location and dimensions of the video image current analysis window for the following frame in means 94 (23) using the signals of the object video image coordinates in the field of view of the tracking system video camera, obtained as a result of the video image processing in the current analysis window or using the extrapolated object video image coordinates and traverse speed with initial conditions, formed at the start of the tracking over an object on the basis of the external signals START/STOP,<br>
8.   Method of signal processing for determination of object coordinates as<br>
claimed in claim 6, wherein controlled displacement of the tracking system<br>
video camera field of view axis horizontally and vertically at    the time<br>
between reception of the former and current video image fields signals   is<br>
defined    in means 77 (3) by calculating the convolution of management<br>
signals of the tracking system video camera field of view displacement with corresponding pulse characteristics of its drives.<br>
9.	Method of signal processing for determination of object coordinates as<br>
claimed in claim   6, wherein forming of video image signals of  the curren:<br>
video image frame at interlacing   from signals   of the video image current<br>
half-frame and video image of the former frame is executed in means 78 (7)<br>
by prognostication of signals of the current video image frame with the help<br>
of the    former    video image frame shift by    the amount    of controlled<br>
displacement   of the video   camera  field   of view    axis   horizontally   and<br>
vertically at the time between reception of video image half-frames and by<br>
substitution   of the current  frame video image pixels with the current half-<br>
frame video image pixels   with compensation for the current uncontrolfod<br>
shifts and roll of the tracking system video camera field of view.<br>
10.	Method of signal processing for determination of object coordinates as<br>
claimed in claim    6, wherein signals of the primary binary image of the<br>
background change detector are formed in means 36:<br>
by adjustment of reference background image signals obtained in the former frame in means 70 to the current scale in means 71, by formation of differential image signals of the background change detector in means 73 by subtraction of reference background image signals of the former frame from means 71 from scaled image signals in the current analysis window with a shift which accounts for displacement of the analysis window center in the inertial coordinates system at the last frame , by determination of the binarization threshold of the background change detector in means 74 as a value proportional to the local values spreading parameter of the differential image of the background change detector in the neighborhood of the point with coordinates.<br>
by assigning values to the primary binary image of the background change detector 1, if the module of signal values of the differential image of the background change detector equals or exceeds value of binarization threshold of the background change detector or equals 0, if the module of signal values of the differential image of the background change detector is less than value of binarization threshold of the background change detector,<br>
at that , the reference background video signals are formed by sharing the<br>
scaled video image signals in the current analysis window into three types<br>
of image signals;<br>
video image signals in the object window,<br>
video image signals in the background window and video image   signals in<br>
the<br>
" New Background" window,<br>
where video image signals in rectangle which lies in the center of the current<br>
analysis   window and includes predominantly the object   image   elements<br>
are defined as image signals in the object window,<br>
image elements on outer boundaries of the current analysis window where<br>
new background  video  image elements     appear on  account of object<br>
movement and the video   camera field   of view  displacement  are defined<br>
as video image signals of the window "New Background",<br>
all the rest image elements of the analysis  window   are defined as image<br>
signals of the background window,<br>
by storage of signals from the current analysis window are defined as image<br>
signals of the background window,<br>
by storage of signals from the current analysis window of the current frame<br>
scaled video image in the window "New Background",<br>
by averaging scaled video image signals in the background window from the<br>
current analysis   window   with the   constant and with account for analysis<br>
window shift in the inertial coordinates system at the last frame in the inertial coordinates system<br>
by re-recording in the object window of reference background video image signals of the former frame with a shift which accounts for the analysis window center displacement at the last frame.<br>
11.	Method of signal processing for determination of object coordinates as<br>
claimed in claim 6, wherein<br>
secondary binary video image signals of the background change detector formed in means 38 from the low-pass filter signals equal 1, if the low-pass: filter signal values exceed decision threshold value for unit elements of the primary binary video image of the background change detector or if the low-pass filter signal values exceed decision threshold value for null elements of the primary binary video image of the background change detector, otherwise they equal 0.<br>
12.	Method of signal processing for determination of object coordinates as<br>
claimed in claim 6, where secondary binary video image signals of the<br>
moving-objects indicator are formed in means 47 from the low-pass filter<br>
signals equal 1, if the low-pass filter signal values exceed decision threshold<br>
value for unit elements of the primary binary vid image of the moving-<br>
objects indicator or if the low-pass   filter signal values   exceed   decision<br>
threshold value   for null elements of the primary binary video image of the<br>
moving-objects indicator, otherwise they equal 0.<br>
13.	Method of signal processing for determination of object coordinates as claimed<br>
in claim 6, where horizontal and vertical projections of the secondary binary<br>
video image signals of the moving-objects indicator are defined in means 48<br>
as sums of the values of the secondary binary video image of the moving-<br>
objects indicator horizontally and vertically, respectively.<br>
14.	Method of signal processing for determination of object coordinates as claimed<br>
in claim 6, wherein horizontal and vertical projections of the secondary binary<br>
video image signals of the background change detector are defined in means<br>
39  as  sums of the values of the secondary  binary  video  image  of the<br>
background change detector horizontally and vertically, respectively.<br>
15.	Method of signal processing of determining of object coordinates as claimed in<br>
claim 6, wherein horizontally and vertical projections of binary video image<br>
signals of the histogram classifier are defined in means 33 as sums of the<br>
values of the binary video image of the histogram classifier horizontally and<br>
vertically, respectively.<br>
16.	Method of signal processing for determination of object coordinates as<br>
claimed in claim   6, where confidence factors of the background change<br>
detector, moving-objects indicator and histogram classifier are defined in means 34 as composition of functions of initial conditions input and the normalized average densities of binary video images of the background change detector, moving-objects indicator and histogram classifier, respectively, at that, the average densities of binary video images of the background change detector, moving-objects indicator, and histogram classifier are obtained as a result of minimum and maximum values limitation and further averaging of the current densities of the appropriate binary video images by the first order recursive filters, at that ratios of the current areas of binary video images of the background change detector, moving-objects indicator and histogram classifier, respectively, inside the object image boundaries to the current areas of regions inside the object video image boundaries are taken as the current densities of the appropriate binary video images.<br>
17. Method of signal processing for determination of object coordinates as<br>
in claim 6, 14, 15 and 17, where generalized horizontal and vertical signal projections of the generalized binary object video image are formed in means 50 by weighted summation of binary video image projections of the background change detector, moving-objects indicator and histogram classifier.<br>
18.	Method of signal processing for determination of object coordinates as claimed<br>
in     claims 6 and 17, where current coordinates of the object generalised<br>
binary video image are defined in means 51 as a weighted sum of-gravity<br>
center coordinates and area median coordinates of the generalized binary<br>
object video image, at that, the weighting coefficient is increased at reduction<br>
of object coordinates video image mean  deviation from their predictable<br>
values.<br>
19.	Method of signal processing for determination of object coordinates as claimed<br>
in   claim 6, where current horizontal and vertical estimate components of the<br>
generalized binary object video image speed in the inertial coordinates system<br>
are defined in means 88 (17) as the sums of values of horizontal and-vertieal<br>
axis displacement of the tracking, system video camera field of view at the time<br>
between reception of the current and former video image fields, repositioning of<br>
the analysis window in the current frame in relation to the former one and<br>
change of generalized binary object image coordinates in the analysis window<br>
in the current frame in relation to the former one to the time period of the<br>
current and previous video image fields horizontally and vertically, respectively.<br>
20. Method of signal processing for determination of object coordinates as daimcd in claim 6, where signals of the object dynamic reference video image are formed in means 57 by reading signals in each frame from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image, and with the center which coordinate<br>
are defined by the difference between the object video image coordinates in the tracking system video camera field of view and analysis -window coordinates in the tracking system video camera field of view.<br>
21.	Method of signal processing for determination of object coordinates as<br>
claimed in claim 6, where signals of the object static reference video image<br>
are formed in means 57 by reading   and storage   of video   image   signals<br>
from the current analysis window in a rectangular window   with dimensions<br>
equal to dimensions of the  object  generalized   binary video image with the<br>
center which coordinates are defined by the difference between object video<br>
image   coordinates in the tracking system video camera field of   vie.w and<br>
coordinates of the center of the analysis window in the   tracking system video<br>
camera field of view<br>
meeting the conditions of the object static reference video image change formed on the basis of analysis of parameters of dissimilarity measures signals of image signals after the nonlinear high-pass filtering in the current analysis window and object static and dynamic reference video image signals, as well as object video image trajectory parameters analysis obtained on the basis of object static and dynamic reference video image signals usage.<br>
22.	Method of signal processing for determination of object coordinates as claimed<br>
in claim 6, where signals of the object static reference video image are<br>
formed in means 57 by reading and storage of video image signals from the current analysis window in a rectangular window with dimensions equal to dimensions of the object generalized binary video image with the center which coordinates are defined by the difference between object video image coordinates in the tracking system video camera field of view and analysis window coordinates in the tracking system video camera field of view<br>
meeting tne conditions of the object static reference video image change formed on the basis of analysis of parameters of dissimilarity measure .signals of video image signals after the nonlinear high-pass filtering in the current analysis window and object static and dynamic reference image signals, as well as object video image trajectory parameters analysis obtained on the basis of analysis of video images dissimilarity measures signals.<br>
23.	Method of signal processing for determination of object coordinates as claimed<br>
in claim  6, where current horizontal and vertical estimate   components of the<br>
binary object video image traverse speed in the inertial coordinates   system<br>
obtained   on the forming of video   images dissimilarity measure signals are<br>
defined in means 83 as the ratio of the sum of value of displacement of the axis<br>
of the tracking system video camera field    of view at the time between<br>
reception of the current and former video image fields, change of the analysis<br>
window position in the current frame in relation to the former one and change<br>
of object coordinates in the analysis window in the current frame in relation to<br>
the former  one and change of object coordinates in the analysis   window   in<br>
the current frame in relation to the former one, obtained on the basis of forming<br>
of video images dissimilarity  measure signals, to the time period of reception<br>
of the current and previous video image fields horizontally and vertically,<br>
respectively.<br>
24.	Method of signal processing for determination of object coordinates as claimed<br>
in claim 6, where confidence factor of the object generalized binary video<br>
image current traverse speed is calculated in means 85 (14)   by defining  the<br>
current    density of the generalized    binary video    image; by    limiting the<br>
minimum    and maximum values of   the current density of the generalised<br>
binary   video image; further averaging of the generalized binary video image<br>
delimited density by the first order recursive filter; normalizing the average<br>
density of the object generalized binary video image.<br>
25.	Method of signal processing for determination of object coordinates as claimed<br>
in claim 6, where confidence factor of the object video image current traverse<br>
speed, obtained on the basis of forming of video images dissimilarity measure<br>
signals, is obtained in means 85 (15) by calculating the current similarity factor<br>
by limiting its maximum and minimum values and averaging by the first order<br>
recursive filter, normalizing the calculated average similarity factor.<br>
26.	Method of signal processing for determination of object coordinates as claimed<br>
in claim  6, where complex estimate of horizontal and vertical  constituents of<br>
the object   video image   current traverse speed in the   inertial coordinates<br>
system is defined   in means 89 (18) by limiting the estimate of the object<br>
binary video image traverse speed and object image traverse speed obtained<br>
on the basis of forming of video images dissimilarity measure signals, by the<br>
minimum and maximum values formed with allowance for preceding values of<br>
the complex estimate of the object video image current traverse speed, and<br>
forming the   weighted sum of delimited estimates of the binary   object video<br>
image traverse speed and object video image traverse speed obtained on the<br>
basis of forming of video images dissimilarity measure signals.<br>
27.Method for determination of object coordinates observed in a sequence of video images as claimed in claim 6, comprising after the reception and storage of signals of the current video image fields;<br>
selection of video images signals of 2N terrain reference marks from signals of the current video image field,<br>
Where N=3, 4, 5,	, and<br>
forming of images dissimilarity measure signals of 2N terrain reference marks, in means 5 from video image signals of 2N terrain reference marks of the current video image field and video images signals of the appropriate 2M terrain reference marks in the former video image field, and<br>
determination with their help of 2N terrain reference marks image shifts at the time between reception of signals of the former and current video images fields and determination of shift and rotation parameters of the current video image field signals at the time between reception of the signals of the former and current video images fields using 2N terrain reference marks image shifts, usage of the stored signals of the uncontrolled displacement and roll of the field of view of the tracking system video camera for forming of the video image predictable coordinates signals of the 2N terrain reference marks in the current video image field with the following division in means 5 of the shift parameter^ of the current video image field signals obtained on the basis of 2N terrain reference marks images dissimilarity measure signals at the time between reception of signals of the current video images fields into constituents of controlled and uncontrolled video camera field of view axis displacement.<br>
28. Device and method of determination of object coordinates, observed in a sequence of video images substantially as herein described and shown in Figs. 8 to 20 of the accompanying drawings.<br></n></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtYWJzdHJhY3QucGRm" target="_blank" style="word-wrap:break-word;">132-delnp-2003-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtQ29ycmVzcG9uZGVuY2UgT3RoZXJzLSgwMS0wNC0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Correspondence Others-(01-04-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtQ29ycmVzcG9uZGVuY2UgT3RoZXJzLSgzMC0wMy0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Correspondence Others-(30-03-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtQ29ycmVzcG9uZGVuY2UtT3RoZXJzLSgwNC0wOS0yMDA5KS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Correspondence-Others-(04-09-2009).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtQ29ycmVzcG9uZGVuY2UtT3RoZXJzLSgyNS0wMy0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Correspondence-Others-(25-03-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtQ29ycmVzcG9uZGVuY2UtT3RoZXJzLSgyOC0wMy0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Correspondence-Others-(28-03-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtY29ycmVzcG9uZGVuY2Utb3RoZXJzLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtY29ycmVzcG9uZGVuY2UtcG8ucGRm" target="_blank" style="word-wrap:break-word;">132-delnp-2003-correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtRGVzY3JpcHRpb24gKENvbXBsZXRlKSAoMzEtMTItMjAwNCkucGRm" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Description (Complete) (31-12-2004).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZGVzY3JpcHRpb24gKGNvbXBsZXRlKS5wZGY=" target="_blank" style="word-wrap:break-word;">132-delnp-2003-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZHJhd2luZ3MuLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-drawings..pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZm9ybS0xLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZm9ybS0xMy0oMDQtMDktMjAwOSkucGRm" target="_blank" style="word-wrap:break-word;">132-delnp-2003-form-13-(04-09-2009).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZm9ybS0xOS5wZGY=" target="_blank" style="word-wrap:break-word;">132-delnp-2003-form-19.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtRm9ybS0yICgzMS0xMi0yMDA0KS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Form-2 (31-12-2004).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZm9ybS0yLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZm9ybS0zLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZm9ybS01LnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtZ3BhLnBkZg==" target="_blank" style="word-wrap:break-word;">132-delnp-2003-gpa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtcGN0LTEwMS5wZGY=" target="_blank" style="word-wrap:break-word;">132-delnp-2003-pct-101.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLWRlbG5wLTIwMDMtcGN0LTMwOC5wZGY=" target="_blank" style="word-wrap:break-word;">132-delnp-2003-pct-308.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTMyLURFTE5QLTIwMDMtUGV0aXRpb24gT3RoZXJzLSgyNS0wMy0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">132-DELNP-2003-Petition Others-(25-03-2011).pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="218492-remote-control-lock-operation-system-for-vehicels.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="218494-an-indolinone-compound-useful-for-treatment-of-dpression-and-or-anxiety.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>218493</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>132/DELNP/2003</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>13/2009</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>27-Mar-2009</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>02-Apr-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>07-Feb-2003</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>JOINT STOCK COMPANY &quot;SCIENTIFIC DESIGN BUREAU OF COMPUTER SYSTEMS&quot;</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>SHEVCHENKO STR.D.2, 347928 -TAGANROG, RUSSIA</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>MARKOV, ALBERT LEONIDOVICH</td>
											<td>INITSIATIVANAYA ST. 44, FL. 28 TAGANROG, 347942, RUSSIA</td>
										</tr>
										<tr>
											<td>2</td>
											<td>FOMENKO, GUENNADY ALEXEEVICH</td>
											<td>KHALTURINA ST. 49-A, FL. 1, TAGANROG, 347909, RUSSIA</td>
										</tr>
										<tr>
											<td>3</td>
											<td>NAOUMOV, VLADIMIR VASILYEVICH</td>
											<td>S. LAZO ST. 7/1, FL. 55, TAGANROG 347924, RUSSIA</td>
										</tr>
										<tr>
											<td>4</td>
											<td>ITENBERG, IGOR ILYICH</td>
											<td>VOSKOVA ST.102, FL. 31, TAGANROG 347924, RUSSIA</td>
										</tr>
										<tr>
											<td>5</td>
											<td>SIVTSOV, SERGUEI ALEXANDROVICH</td>
											<td>OKTYABRSKAYA SQ. 2, FL.8, TAGANROG, 347922, RUSSIA</td>
										</tr>
										<tr>
											<td>6</td>
											<td>BATCHILO, SERGUEI, ALEXANDROVICH</td>
											<td>VODOPROVODNAYA ST. 17/1, FL. 5, TAGANROG, 347902, RUSSIA</td>
										</tr>
										<tr>
											<td>7</td>
											<td>ARTSATBANOV, ALEXANDER YURYEVICH</td>
											<td>SVOBODY ST. 29/2, FL. 12, TAGANROG, 347902, RUSSIA</td>
										</tr>
										<tr>
											<td>8</td>
											<td>KALASHNIKOV, VLADIMIR MAKAROVICH</td>
											<td>K. LIBKNEHTA ST. 143, TARANROG, 347935, RUSSIA</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H04N 7/18</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/RU01/00328</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2001-08-06</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>2000120929</td>
									<td>2000-08-10</td>
								    <td>Russia</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/218493-method-and-device-for-positioning-an-object by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 12:53:21 GMT -->
</html>
