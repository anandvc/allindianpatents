<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/257747-method-for-tracking-an-object-in-an-avatar-based-video-conferencing-system by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 07:34:26 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 257747:METHOD FOR TRACKING AN OBJECT IN AN AVATAR-BASED VIDEO CONFERENCING SYSTEM</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">METHOD FOR TRACKING AN OBJECT IN AN AVATAR-BASED VIDEO CONFERENCING SYSTEM</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>The invention describes a system and a method to capture a pre-defined object from a camera generated video in a mobile environment. The object to be captured is first superimposed on a cue. A user then analyses whether the cue fits the object precisely or not. The key features to be tracked from the captured object are selected through a user interface and matched with the features of the cue. Once the user is satisfied with the arrangement of the cue and the actual object, the user sends a feedback to capture the image. The object is further tracked by reading the expressions of the object from the selected cue outline. If the tracking of the object stops functioning properly then a re-initialization of the object for tracking can be done.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>FIELD OF INVENTION<br>
This invention in general relates to the field of mobile communication. More particularly this invention relates to a method and system for user assisted object tracking in an Avatar based video-conferencing system.<br>
DESCRIPTION OF RELATED PRIOR ART<br>
The basic concept of an Avatar-based video conferencing and messaging system is as follows:<br>
Avatars are virtual representations of actual objects which are meant to replicate the object's behavior in a given environment, without the actual object coming into view. For example, a cartoon's face could be used by a person to represent him.<br>
Such systems are already in place in popular chatting portals like Yahoo! Etc. The level of representation varies in different applications. In some cases, the avatars are static images. In others, they are displayed as emoticons which can do some pre-defined actions based on user selection.<br>
Recently, a company called Sensorylnc has developed text-to-3D and voice-to-3D avatars to be deployed in mobile environments.<br>
LIMITATIONS<br>
All the existing Avatar based systems, mentioned in related art, are limited in their capabilities of replicating their source's behavior in real-time. Camera-based systems allow the possibility of real-time tracking of source's behavior (for example a user's face) in real-time. The tracked motion can be interpreted suitably to extract source behavior and this behavior can be mapped onto the avatar.<br>
One of the key practical problems which   arise in implementing a camera-based<br><br>
Avatar is the issue of identifying the source object in the camera image. SUMMARY OF THE INVENTION<br>
The present invention relates to a system and method for capturing a pre-defined object from a camera generated video in a mobile environment.<br>
The present invention relates to a method of using customizable, parameterized mask (depending on object) in achieving user-assisted capture from camera images to arrive at an initial estimate of the location of the object.<br>
The present invention relates to the method of allowing a customizable mask to allow best possible mapping of mask to the desired object in the camera image.<br>
The present invention further related to the Method of applying re-sync mechanism in case tracking is going wrong.<br>
Accordingly, this invention explains a method for a user assisted object tracking in an avatar-based video conferencing system comprising the steps of:<br>
(a)	capturing an image of an object in a camera;<br>
(b)	superimposing the object captured in the camera to a cue;<br>
(c)	adjusting the parameters associated with the cue to make the object fit in the said cue;<br>
(d)	locking the image   and   assigning plurality of cue points if the image and the cue fits;<br>
(e)	storing the cue points in a memory and mapping the cue points to plurality of featured points on the object image to form a parameterized cue model; and<br>
(f)	Deriving the motion and   the location of the object from the mapped cue points and the feature points.<br><br>
The cue is adapted to be configured before tracking or while the said tracking is in progress. The shape and size of the cue is adapted to be controlled through parameters associated with the cue. The camera is moved accordingly to make the object fit in the said cue. The feature points comprise feature point coordinates located at fixed positions in the parameterized cue model. On changing the cue's shape or size, the feature point coordinates also gets changed accordingly. A reinitialization of the object for a fresh object location for tracking is carried out if the tracking of the object stops functioning. Re-initialization involves the process of capturing and superimposing the cue with the fresh object location.<br>
Accordingly, this invention further explains a system for a user assisted object tracking in an avatar-based video conferencing system comprising:<br>
(a)	a camera unit for capturing an image of an object;<br>
(b)	display unit for superimposing the object captured in the camera  unit to a cue;<br>
(c)	memory module to store plurality of cue points which are mapped to plurality of feature points; and<br>
(d)	an object tracking module for tracking the feature-points of the object; and<br>
are-initialization module for tracking a fresh object location if the tracking of the object stops functioning.<br>
The tracking module outputs the average or regularized motion of the object derived from the motion of the feature points.<br>
These and other objects, features and advantages of the present invention will become more apparent from the ensuing detailed description of the invention taken in conjunction with the accompanying drawings.<br>
BRIEF DESCRIPTION OF ACCOMPANYING DRAWINGS<br>
Figure 1 illustrates an example system where a camera is used to capture image<br><br>
And the object to be tracked is a human face in the camera image.<br>
Figure 2 illustrates the using of a cue / mask which resembles a human face, superimposed onto the camera image.<br>
Figure 3 illustrates the flowchart of operation of the invention.<br>
Figure 4 illustrates an example coordinate system to classify points in the cue /mask.<br>
Figure 5 illustrates an example of an initial cue being modified by the user to fit the object in a better fashion.<br>
DETAILED DESCRIPTION OF THE INVENTION<br>
The preferred embodiments of the present invention will now be explained with reference to the accompanying drawings. It should be understood however that the disclosed embodiments are merely exemplary of the invention, which may be embodied in various forms. The following description and drawings are not to be construed as limiting the invention and numerous specific details are described to provide a thorough understanding of the present invention, as the basis for the claims and as a basis for teaching one skilled in the art how to make and/or use the invention. However in certain instances, well-known or conventional details are not described in order not to unnecessarily obscure the present invention in detail.<br>
The present invention improves upon existing methods of object-tracking by offering a robust, low-complexity mechanism to acquire object location to be used for tracking purposes. Due to recent advance in computer vision technology, object tracking systems are now coming into the mainstream. The success of object tracking rests heavily on obtaining a good initial estimate of the object's location - and then using motion and other parameters to track the object.<br><br>
Obtaining a good initial estimate is a tricky problem in mobile scenarios such as a hand-held camera phone system. Our present invention lays down a simple and robust system to get initial estimate of object location. We use human-assisted feedback and customizable cues to allow the user to help the system get an accurate initial estimate of the object location. A mechanism for resynchronization is also provided, in case the tracking starts becoming erroneous, so that a new location estimate can be provided to the tracker.<br>
The invention relates to the field of avatar-based video conferencing systems for mobile phones. The invention describes a system and a method to capture a predefined object from a camera generated video in a mobile environment. The object to be captured is first superimposed on a cue. A user then analyses whether the cue fits the object precisely or not. The key features to be tracked from the captured object are selected through a user interface and matched with the features of the cue. Once the user is satisfied with the arrangement of the cue and the actual object, the user sends a feedback to capture the image. The object is further tracked by reading the expressions of the object from the selected cue outline. If the tracking of the object stops functioning properly then a reinitialization of the object for tracking can be done.<br>
The present invention involves a camera system and a display system which displays the camera captured image. The objective is to track the spatial motion (in time) of an object (in the camera image). To start the tracking, an estimate of the initial location of the object is required. An example system is shown in figure 1. The figure 1 describes an example system where a camera is used to capture image and the object to be tracked is a human face in the camera image. The figure shows a avatar-based duplex communication scenario where two users -Useri and User2, are communicating using avatars. At UserVs end, the Camera captures images at periodic intervals which are given to the "Camera Frame Capture" block which buffers the images. These images are then given to the "Face Analysis and Abstraction Engine". This block is responsible for locating/extracting facial features and tracking the facial movement. The facial<br><br>
Features are in form of a set of parameters which define salient points on the head. These parameters could be raw 2D-coordinates of facial feature points (like center of each eye, tip og nose, corners of mouth, etc.) or they could be higher level semantic interpretations of graded facial expressions (like smile, big smile, frown, angry, wide-eyed, etc.). These parameters are sent to the Tx-engine to be transmitted to User2. Similarly, the User2 sends his parameters to Useri which are received by the Rx engine. The parameters are sent to the mapping engine, which uses these parameters to modify the facial expressions/head orientation of the Avatar representation of User2. After the ampping the display frame containing the updated representation of User2 is sent to UseiTs display engine which renders it onto the display device (e.g. LCD) of UseM. A similar process happens at User2's end. In this manner, the avatar representations of both users get updated in real-time and are displayed to each other.<br>
The present invention is structured around the concept of using user feedback to get a reliable initial estimate of the location of the object in the camera image. The user is provided (or can select from an available database) a cue whose shape and size can be controlled through parameters .Figure 2 illustrates the using of a cue / mask which resembles a human face, superimposed onto the camera image.<br>
The User customizes the cue according to the object he wishes to track, by modifying these parameters .Figure 5 illustrates an example of a initial being modified by the user to fit the object in a better fashion. The first image shows the initial cue superimposed on the camera image. This cue does not fit the face in the camera image properly. The second image shows the cue after being modified by the user. This modified cue fits the face in a better manner. This cue-designing activity can be done beforehand (wherein the customized cue can be stored into a database), or done while using the application.<br>
Once the user selects the cue, he aligns the object with the cue using camera movements. The process continues until the user is satisfied with the best fit<br><br>
Possible under the circumstances. Following this, the user signals a "lock" event to the application.<br>
Upon receiving the "lock" event from the user, the application derives the initial estimate of object's location through a set of feature points in the cue.<br>
These feature points are at fixed positions in the parameterized cue model. When the cue's shape or size is changed, the feature point coordinates change accordingly. The coordinates are defined as referenced by the display coordinates. Figure 4 illustrates an example coordinate system to classify points in the cue /mask. After superimposition, the location of the feature points in the cue (e.g. eyes, mouth, nose, etc.) will be input to the tracker. The tracker will start its tracking algorithm centered on these feature point coordinates. The origin of the coordinate system is located at the top left corner of the displayable area on the screen/display device. The convention followed is that the coordinates advance in a positive manner while going down or right from the origin. Following are the definitions of the terms used in the figure 4.<br>
Sh = Height of the screen<br>
Sw = Width of the screen<br>
Ew = distance between both eyes (measured from center of pupil of each<br>
eye)<br>
Eh = vertical distance of eye from center of display area (Sh/2, Sw/2)<br>
Mw = horizontal distance of mouth from center of display area<br>
Mh = vertical distance of mouth from center of display area The positions of the cues are defined in terms of the above coordinate system.<br>
The User first describes the object in terms of a cue or a mask outline. On start of the application, the cue is presented on the display unit (example a LCD) superimposed on the camera-generated image.<br>
One can think of the cue and the camera-generated image as two image planes superimposed and shown on the LCD. As the camera moves, the camera-<br><br>
Generated image on the LCD moves. While the cue image stays static on the LCD. The user then aligns the camera so that the desired object in the camera-image fits the cue as best as possible. Once the user is satisfied with the fit, a signal is sent by the user to the system indicating that the cue has been synchronized with the object.<br>
Once the synchronization is achieved, the system, which is already aware of the location of cue on the display, captures the Points or Region of Interest from the camera image upon receiving the signal from the user. These points can be now used to track the object in future frames in the camera captured video.<br>
For example, the object to be captured could be human face in a camera image from a mobile camera. The cue could be a generic face mask. The user will fit his face in the mask and send a signal to the system. The system will then capture the salient points of the face, after receiving the signal (for e.g. location of eyes, mouth, nose, etc.).<br>
Once the feature-points are obtained, they are tracked in the camera image by the -object-tracking module. The Tracking module can have as an output the average or regularized motion of the object derived from the motion of the feature points.<br>
There can be situations where the object tracking stops functioning properly. For example, the object goes out of the camera view, and then reappears after some time. In such cases, a re-initialization of the object for tracking needs to be done. The user can signal such an event to the application, when he notices that the tracking is not going well through some display-based feedback mechanism.<br>
When a "resync" is signaled by the application, then the tracking module stops operation. The application gives control to the user to go through the process of .aligning the cue with the fresh object location again. In the "resync" mode, the cue used at the start of the beginning can be reused.<br><br>
A block flowchart of full system operation is shown in the drawings.<br>
1.	The User invokes the application using the application screen.<br>
2.	Cue/Mask pops up on the screen superimposed on the camera image.<br>
3.	User then signals to align object with the cue/mask<br>
4.	Adjusting the cue/mask by changing its parameters is done.<br>
5.	Using the cue and camera image the user aligns the relavent object with the cue/mask.<br>
6.	Once the user is satisfied with the alignment he signals to the application to lock.<br>
7.	On receiving the lock signal from the user the application stores the location of cue points in the database.<br>
8.	The cue points are treated as feature points on the objects image and are used for tracking the object using an object tracking system.<br>
9.	User indicates need for resync and the system goes into resync mode.<br>
It will also be obvious to those skilled in the art that other control methods and apparatuses can be derived from the combinations of the various methods and apparatuses of the present invention as taught by the description and the accompanying drawings and these shall also be considered within the scope of the present invention. Further, description of such combinations and variations is therefore omitted above. It should also be noted that the host for storing the applications include but not limited to a microchip, microprocessor, handheld communication device.<br>
Although the present invention has been fully described in connection with the preferred embodiments thereof with reference to the accompanying drawings, it is to be noted that various changes and modifications are possible and are apparent to those skilled in the art. Such changes and modifications are to be understood as included within the scope of the present invention as defined by the appended claims unless they depart therefrom.<br><br>
ADVANTAGES<br>
The above system allows the following advantages in a practical mobile camera environment<br>
1.	Use of custom designed and customizable cues suited to the object<br>
2.	Using User feedback to enable a good initial location estimate of the object<br>
3.	Easy to implement and use<br>
4.	Greatly reduces the time to achieve "Lock"<br>
5.	Allows re-synchronization by user if object tracking starts going off the mark<br>
WE CLAIM<br>
1.	A method  for a  user assisted  object tracking  in  an  avatar-based  video conferencing system comprising:<br>
(a)	capturing an image of an object in a camera ;<br>
(b)	superimposing the object captured in the camera to a cue;<br>
(c)	adjusting the parameters associated with the cue to make the object fit in the said cue;<br>
(d)	locking the image and assigning plurality of cue points if the image and the cue fits;<br>
(e)	storing the cue points in a memory and mapping the cue points to plurality of featured points on the object image to form a parameterized cue model; and<br>
(f)	deriving the motion and the location of the object from the mapped cue points and the feature points.<br><br>
2.	A method as claimed in claim 1 wherein the cue is adapted to be configured before the tracking or while the said tracking is in progress.<br>
3.	A method as claimed in claim 1 wherein the shape and size of the cue is adapted to be controlled through parameters associated with the cue.<br>
4.	A method as claimed in claim 1 wherein the camera is moved accordingly to make the object fit in the said cue.<br>
5.	A method as claimed in claim 1 wherein the feature points comprise feature point coordinates located at fixed positions in the parameterized cue model.<br>
6.	A method as claimed in claim 1 wherein on changing the cue's shape or size, the feature point coordinates also gets changed accordingly.<br><br>
7.	A method as claimed in claim 1 wherein a re-initialization of the object for a fresh object location for tracking is carried out if the tracking of the object stops functioning.<br>
8.	A method as claimed in claim 1 wherein re-initialization involves the process of capturing and superimposing the cue with the fresh object location.<br>
9.	A system for a user assisted object tracking in an avatar-based video conferencing system comprising:<br><br>
(a)	a camera unit for capturing an image of an object;<br>
(b)	display unit for superimposing the object captured in the camera  unit to a cue;<br>
(c)	memory module to store plurality of cue points which are mapped to plurality of feature points;<br>
(d)	an object tracking module for tracking the feature-points of the object; and<br>
(e)	a re-initialization module for tracking a fresh object location if the tracking of the object stops functioning.<br><br>
10.	A system as claimed in claim 1 wherein the tracking module outputs the average or regularized motion of the object derived from the motion of the feature points.<br>
11.	A method for a user assisted object tracking in an avatar-based video conferencing system substantially as herein described particularly with reference to the drawings.<br><br>
12. A system for a user assisted object tracking in an avatar-based video conferencing system substantially as herein described particularly with reference to the drawings.<br>
Dated this the 17th day of November 2006</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1DSEUtMjAwNiAgICBBTUVOREVEICBQQUdFUyAgT0YgU1BFQ0lGSUNBVElPTiAgMTQtMTAtMjAxMy5wZGY=" target="_blank" style="word-wrap:break-word;">2136-CHE-2006    AMENDED  PAGES  OF SPECIFICATION  14-10-2013.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1DSEUtMjAwNiAgIEFNRU5ERUQgICBDTEFJTVMgICAgMTQtMTAtMjAxMy5wZGY=" target="_blank" style="word-wrap:break-word;">2136-CHE-2006   AMENDED   CLAIMS    14-10-2013.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1DSEUtMjAwNiAgIEVYQU1JTkFUSU9OICBSRVBPUlQgUkVQTFkgUkVDRUlWRUQgICAxNC0xMC0yMDEzLnBkZg==" target="_blank" style="word-wrap:break-word;">2136-CHE-2006   EXAMINATION  REPORT REPLY RECEIVED   14-10-2013.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1DSEUtMjAwNiAgIEZPUk0tMSAgMTQtMTAtMjAxMy5wZGY=" target="_blank" style="word-wrap:break-word;">2136-CHE-2006   FORM-1  14-10-2013.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1DSEUtMjAwNiAgIEZPUk0tMTMgICAgMTQtMTAtMjAxMy5wZGY=" target="_blank" style="word-wrap:break-word;">2136-CHE-2006   FORM-13    14-10-2013.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1DSEUtMjAwNiAgIFBPV0VSICAgT0YgQVRUT1JORVkgICAgIDE0LTEwLTIwMTMucGRm" target="_blank" style="word-wrap:break-word;">2136-CHE-2006   POWER   OF ATTORNEY     14-10-2013.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1hYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">2136-che-2006-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1jbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">2136-che-2006-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1jb3JyZXNwb25kbmVjZS1vdGhlcnMucGRm" target="_blank" style="word-wrap:break-word;">2136-che-2006-correspondnece-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1kZXNjcmlwdGlvbihjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">2136-che-2006-description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1kcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">2136-che-2006-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1mb3JtIDEucGRm" target="_blank" style="word-wrap:break-word;">2136-che-2006-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjEzNi1jaGUtMjAwNi1mb3JtIDI2LnBkZg==" target="_blank" style="word-wrap:break-word;">2136-che-2006-form 26.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="257746-escalator-or-moving-walkway-with-handrail-entry-handrail-entry-of-such-an-escalator-or-moving-walkway-and-method-of-reducing-a-gap-in-the-handrail-entry.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="257748-decoder-encoder-and-methods-of-encoding-decoding-precision-scalable-bit-stream-with-encoded-predetrmined-picture.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>257747</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>2136/CHE/2006</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>44/2013</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>01-Nov-2013</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>31-Oct-2013</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>17-Nov-2006</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>SAMSUNG INDIA SOFTWARE OPERATIOS PRIVATE LIMITED</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>BAGEMANE LAKEVIEW, BLOCK B, NO. 66/1, BAGMANE TECH PARK, C. V. RAMAN NAGAR, BYRASANDRA, BANGALORE- 560 093, KARNATAKA, INDIA</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>ANSHUL SHARMA</td>
											<td>EMPLOYED AT SAMSUNG INDIA SOFTWARE OPERATIONS PVT LTD, HAVING ITS OFFICE AT, BAGEMANE LAKEVIEW, BLOCK B, NO. 66/1, BAGMANE TECH PARK, C. V. RAMAN NAGAR, BYRASANDRA, BANGALORE- 560 093, KARNATAKA, INDIA</td>
										</tr>
										<tr>
											<td>2</td>
											<td>RAVINDRA SHET</td>
											<td>EMPLOYED AT SAMSUNG INDIA SOFTWARE OPERATIONS PVT LTD, HAVING ITS OFFICE AT, BAGEMANE LAKEVIEW, BLOCK B, NO. 66/1, BAGMANE TECH PARK, C. V. RAMAN NAGAR, BYRASANDRA, BANGALORE- 560 093, KARNATAKA, INDIA</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06F01/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>N/A</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td></td>
									<td></td>
								    <td>NA</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/257747-method-for-tracking-an-object-in-an-avatar-based-video-conferencing-system by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 07:34:27 GMT -->
</html>
