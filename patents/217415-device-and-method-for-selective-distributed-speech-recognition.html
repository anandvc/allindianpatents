<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/217415-device-and-method-for-selective-distributed-speech-recognition by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:43:44 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 217415:&quot;DEVICE AND METHOD FOR SELECTIVE DISTRIBUTED SPEECH RECOGNITION</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;DEVICE AND METHOD FOR SELECTIVE DISTRIBUTED SPEECH RECOGNITION</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A method and apparatus, for selective distributed speech recognition includes an embedded speech recognition engine (101) and a dialog manager (102), such has browser coupled to the embedded speech recognition engine (104). The method and (106), such as a WLAN speech recognition engine (108) or a network speech recognition engine (110). The method and apparatus futher includes proference information (114) environment information (112) and a speech input (116) all provided to the dialog manager (102). The dialog manager (102),in response to the preference information (114) and the environment information (113), provides the speech input (116) to the embedded speech recognition, engine (104), the WLAN speech recognition engine (108) or the network speech recognition engine (110).</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td><br>
		[0001] The invention relates generally to speech recognition, and more<br>
	specifically, to distributed speech recognition between a wireless device, a<br>
	communication server, and a wireless local area network.<br>
		[0002] With the growth of speech recognition capabilities, there is a<br>
	corresponding increase in the number of applications and uses for speech recognition.<br>
	Different types of speech recognition applications and systems have been developed,<br>
	based upon the location of the speech recognition engine with respect to the user.<br>
	One such example is an embedded speech recognition engine, otherwise known as a<br>
	local speech recognition engine, such as SpeechToGo speech recognition engine sold<br>
	by Speech Works International, Inc., 695 Atlantic Avenue, Boston, MA 02111.<br>
	Another type of speech recognition engine is a network-based speech recognition<br>
	engine, such as Speech Works 6, as sold by Speech Works International, Inc., 695<br>
	Atlantic Avenue, Boston, MA 02111.<br>
		[0003] Embedded or local speech recognition engines provide the added<br>
	benefit of speed in recognizing a speech input, wherein a speech input includes any<br>
	type of audible or audio-based input. One of the drawbacks of embedded or local<br>
	speech recognition engines is that these engines typically contain a limited<br>
	vocabulary. Due to memory limitations and system processing requirements, in<br>
	conjunction with power consumption limitations, embedded or local speech<br>
	recognition engines are limited to providing recognition to only a fraction of the<br>
	speech inputs which would be recognizable by a network-based speech recognition<br>
	engine.<br>
		[0004] Network-based speech recognition engines provide the added benefit<br>
	of an increased vocabulary, based on the elimination of memory and processing<br>
	restrictions. Although a downside is the added latency between when a user provides<br>
	a speech input and when the speech input may be recognized, and furthermore<br>
1<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	provided back to the end user for confirmation of recognition. In a typical speech<br>
	recognition system, the user provides the speech input and the speech input is<br>
	thereupon provided to a server across a communication path, whereupon it may then<br>
	be recognized. Extra latency is incurred in not only transmitting the speech input to<br>
	the network-based speech recognition engine, but also transmitting the recognized<br>
	speech input, or N-best list back to the user.<br>
		[0005] Moreover, with the growth of wireless local area networks (WLAN),<br>
	such as Bluetooth or IEEES02.11 family of networks, there is an increased demand in<br>
	providing a user the ability to utilize the WLAN and services disposed thereon, as<br>
	opposed to services which may be accessible through a standard cellular network<br>
	connection. WLANs provide, among other things, the benefit of improved<br>
	communication speed through the increased amount of available bandwidth for<br>
	transmitting information.<br>
		[0006] One current drawback to speech recognition are limitations of<br>
	recognition caused by factors, such as, an individual user's speech patterns, external<br>
	noise, transmission noise, vocabulary coverage of the speech recognition system, or<br>
	speech input beyond a recognition engine's capabilities. It is possible to provide a<br>
	speech recognition engine which is adaptable or predisposed to a specific type of<br>
	interference, such as excess noise. For example, a speech recognition engine may be<br>
	preprogrammed to attempt to recognize speech input where the speech input is<br>
	provided in a noisy environment, such as an airport. Thereupon, a user may provide<br>
	the speech input while within an airport and if the speech input is provided to the<br>
	specific speech recognition engine, the speech recognition engine may have a higher<br>
	probability of correctly recognizing the specific term, based on an expected noise<br>
	factor, typically background noise associated with an airport or an echoing or<br>
	hollowing effect, which may be generated by the openness of terminal hallways.<br>
		[0007] Furthermore, simply because a WLAN may provide a specific service,<br>
	an end user may not necessarily wish to utilize the specific service, for example, a<br>
	user may have a subscription agreement with a cellular service provider and may<br>
2<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	incur further toll charges for utilizing a WLAN, therefore the user may wish to avoid<br>
	excess charges and use the services already within the user's subscription agreement.<br>
BRIEF DESCRIPTION OF THE DRAWINGS<br>
		[0008] The invention will be more readily understood with reference to the<br>
	following drawings wherein:<br>
		[0009] FIG. 1 illustrates one example of an apparatus for distributed speech<br>
	recognition;<br>
		[0010] FIG. 2 illustrates one example of a method for distributed speech<br>
	recognition;<br>
		[0011] FIG. 3 illustrates another example of the apparatus for distributed<br>
	speech recognition;<br>
		[0012] FIG. 4 illustrates an example of elements within a dialog manager;<br>
		[0013] FIG. 5 illustrates another example of a method for distributed speech<br>
	recognition;<br>
		[0014] FIG. 6 illustrates an example of a method of an application utilizing<br>
	selective distributed speech recognition and<br>
		[0015] FIG. 7 illustrates another example of an apparatus for distributed<br>
	speech recognition.<br>
3<br><br>
WO 2004/061820								PCT/US2003/037899<br>
DETAILED DESCRIPTION<br>
		[0016] Briefly, a method and apparatus for selective distributed speech<br>
	recognition includes receiving a speech input, wherein the speech input is any type of<br>
	audio or audible input, typically provided by an end user that is to be recognized using<br>
	a speech recognition engine and typically an action is thereupon to be performed in<br>
	response to the recognized speech input. The method and apparatus further includes<br>
	receiving preference information, wherein the preference information includes any<br>
	type of information or preference directed to how and/or where speech input may be<br>
	distributed. The method and apparatus also includes receiving environment<br>
	information, wherein the environment information includes information that describes<br>
	the particular environment within which the speech recognition may be performed.<br>
	For example, environment information may include timing information which<br>
	indicates the exact time upon which the speech recognition may be selectively<br>
	distributed, such as therein a WLAN or a cellular network may provide variant<br>
	pricing structures based on time of day (e.g. peak and off-peak hours).<br>
		[0017] The method and apparatus includes providing the speech input to a<br>
	first speech recognition engine, such as an embedded speech recognition engine, or<br>
	one of a plurality of second speech recognition engines, such as external speech<br>
	recognition engines, more specifically, for example, a WLAN speech recognition<br>
	engine or a network speech recognition engine. The WLAN speech recognition<br>
	engine may be disposed within a WLAN and the network speech recognition engine<br>
	may be disposed within or in communication with a cellular network.<br>
		[0018] The method and apparatus includes providing the speech input to the<br>
	selected speech recognition engine based on the preference information and the<br>
	environment information, wherein a wireless device selectively distributes speech<br>
	input to one of multiple speech recognition engines based on preference information<br>
	in response to environment information, A wireless device may be any device<br>
	capable of receiving communication from a wireless or non-wireless device or<br>
	network, a server or other communication network. The wireless device includes, but<br>
	is not limited to a cellular phone, a laptop computer, a desktop computer, a personal<br>
4<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	digital assistant (PDA), a paper, a smart phone, or any other suitable device to receive<br>
	communication, as recognized by one having ordinary skill in the art.<br>
		[0019] FIG. 1 illustrates a wireless device 100 that includes a dialog manager<br>
	102, such as a VoiceXML, SALT and XHTML or other such browser, and an<br>
	embedded speech recognition engine 104. The dialog manager 102 is operably<br>
	coupleable to external speech recognition engines 106, more specifically a WLAN<br>
	speech recognition engine 108 and a network speech recognition engine 110. In one<br>
	embodiment, the dialog manager 102 receives environment information 112, typically<br>
	provided from a WLAN (not shown). The dialog manager 102 also receives<br>
	preference information 114, wherein the preference information may be provided<br>
	from a memory device (not shown) disposed within the wireless device 100.<br>
		[0020] The dialog manager receives a speech input 116 and thereupon<br>
	provides the speech input to either the embedded speech recognition engine 104, the<br>
	WLAN speech recognition engine 108 or the network speech recognition engine 110<br>
	in response to the environment information 112 and the preference information 114.<br>
	As discussed below, the preference information typically includes conditions and the<br>
	environment information includes factors, whereupon if specific conditions within the<br>
	preference information 114 are satisfied, by a comparison with the environment<br>
	information 112, a specific speech recognition engine may be selected.<br>
		[0021] If, in response to the environment information 112 and preference<br>
	information 114, the embedded speech recognition engine 104 is selected for<br>
	distribution of the speech input 116, the speech input is provided across<br>
	communication path 118, which may be an internal connection within the wireless<br>
	device 100. If the WLAN speech recognition engine 108 is selected, the dialog<br>
	manager 102 provides the speech input 116 to the WLAN speech recognition engine<br>
	108 across communication path 120, which may be across a WLAN, through a<br>
	WLAN access point (not shown). Furthermore, if the network speech recognition<br>
	engine 110 is selected, the dialog manager 102 may provide the speech input 116 to<br>
	the network speech recognition engine 110 across communication path 122, which<br>
	may include across a cellular network (not shown) and further across a<br>
5<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	communication network, such as an internet, an intranet, a proprietary network, or any<br>
	other suitable interconnection of servers or network computers that provide<br>
	communication access to the network speech recognition engine 110.<br>
		[0022] FIG. 2 illustrates a flowchart representing the steps of the method for<br>
	distributed speech recognition. The method begins 130 by receiving a speech input,<br>
	step 132. As discussed above, the speech input is provided to the dialog manager 102,<br>
	but as recognized by one having ordinary skill in the art, the wireless device may<br>
	further include an audio receiver and the speech input is provided from the audio<br>
	receiver to the dialog manager 102. The next step, step 134, includes receiving<br>
	preference information, as discussed above with respect to FIG. 1, preference<br>
	information 114 may be provided from a memory device disposed within the wireless<br>
	device 100.<br>
		[0023] Thereupon, the method further includes receiving environment<br>
	information, step 136. The environment information 112 may be provided from the<br>
	WLAN, but in another embodiment, the environment information may also be<br>
	provided from alternative sources, such as a CPS receiver (not shown) which provides<br>
	location information or a cellular network which may provide timing information or<br>
	toll information. The audio receiver 142 may be any typical audio receiving device,<br>
	such as a microphone, and generates the speech input 116 in accordance with known<br>
	audio encoding techniques such that the speech input may be recognized by a speech<br>
	recognition engine. Thus, the method includes providing the speech input to either a<br>
	first speech recognition engine or a second speech recognition engine bused on the<br>
	preference information and the environment information, step 138. As discussed<br>
	above with respect to FIG. 1, the first speech recognition engine may be embedded<br>
	within the wireless device 100, such as the embedded speech recognition engine 104<br>
	and the second speech recognition engine may be disposed externally, such as the<br>
	WLAN Speech recognition engine 108 and/or the network speech recognition engine<br>
	110, Thereupon, one embodiment of the method is complete, seep 140.<br>
		[0024] In an alternative embodiment, the dialog manager 102 may provide<br>
	feedback information to be stored within the memory device 150. The feedback<br>
6<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	information may be directed to reliability and quality of service based upon previous<br>
	speech recognitions conducted by the WLAN speech recognition engine 108. For<br>
	example, the memory device 150 may store information relating to a particular<br>
	WLAN speech recognition engine, such as a manufacturing type of speech<br>
	recognition engine, a specific location speech recognition engine or other variant<br>
	factors which are directed to quality of service. Thereupon, this quality of service<br>
	information may be included within the preference information 114 which is provided<br>
	to the dialog manager 102 and utilized by the dialog manager 102 in determining to<br>
	which speech recognition engine the speech input 116 is provided.<br>
		[0025] FIG. 3 illustrates another example of the apparatus for selective<br>
	distributed speech recognition including the wireless device 100 and a dialog manager<br>
	102 and the embedded speech recognition engine 104 disposed therein. The wireless<br>
	device also includes an audio receiver 142 coupled to the dialog manager 102,<br>
	wherein the audio receiver 142 provides the speech input 116 to the dialog manager<br>
	102. The audio receiver 142 receives an audio input 144, typically from an end user.<br>
	The dialog manager 102 is operably coupled to a transmitter/receiver 146 coupled to<br>
	an antenna 148, which provides for wireless communication.<br>
		[0026] The wireless device 100 further includes a memory device 150 which<br>
	in one embodiment includes a processor 152 and a memory 154 wherein the memory<br>
	154 provides executable instructions 156 to the processor 152. In another<br>
	embodiment the memory device 150 may further include any type of memory storing<br>
	the preference information therein. The processor 152 may be, but not limited to, a<br>
	single processor, a plurality of processors, a DSP, a microprocessor, ASIC, state<br>
	machine, or any other implementation capable of processing or executing software or<br>
	discrete logic or any suitable combination of hardware, software and/or firmware.<br>
	The term processor should not be construed to refer exclusively to hardware capable<br>
	of executing software, and may implicitly include DSP hardware, ROM for storing<br>
	software, RAM, and any other volatile or non-volatile storage medium. The memory<br>
	154 may be, but not limited to, a single memory, a plurality of memory locations,<br>
	shared memory, CD, DVD, ROM, RAM, EEPROM, optical storage, or any other<br>
	non-volatile storage capable of storing digital data for use by the processor 152.<br>
7<br><br>
WO 2004/061820								PCT/US2003/037899<br>
		[0027] The wireless device 100 further includes an output device 158, wherein<br>
	the output device may be a speaker for audio output, a display or monitor for video<br>
	output, or any other suitable interface for providing an output, as recognized by one<br>
	having ordinary skill in the art. Output device 158 receives an output signal 160 from<br>
	the dialog manager 102.<br>
		[0028] The wireless device 100 may be in wireless communication with a<br>
	wireless local area network 162 across communication path 164. through the<br>
	transmitter/receiver 146 and the antenna 148. The WLAN 162 includes a WLAN<br>
	access point 166, a WLAN server 168, wherein the WLAN access point 166 is in<br>
	communication with the WLAN server 168 across communication path 170 and the<br>
	WLAN server is in communication with the WLAN speech recognition engine 108<br>
	across communication path 172.<br>
		[0029] The wireless device 100 may further be in communication with a<br>
	cellular network 174 across communication path 176, via the transmitter/receiver 146<br>
	and the antenna 148. The cellular network may be in communication with a<br>
	communication network 178, wherein the communication network 178 may be a<br>
	wireless area network, a wireless local area network, a cellular communication<br>
	network, or any other suitable network for providing communication information<br>
	between the wireless device 100 and a communication server 180. The cellular<br>
	network 174 is in communication with the communication server 180 and the network<br>
	speech recognition engine 110 via communication path 182, which may be a wired or<br>
	wireless communication path. Furthermore, within the communication network 178,<br>
	the communication server 180 may be in communication with the network speech<br>
	recognition engine 110 via communication path 134.<br>
		[0030] FIG. 4 illustrates an alternative embodiment of the dialog manager<br>
	102, having a processor 186 operably coupled to a memory 188 for storing executable<br>
	instructions 190 therein. The processor 186 receives the speech input 116, the<br>
	preference information 114 and the environment information 112. In response<br>
	thereto, the processor 186, upon executing the executable instructions 190, generates a<br>
	routing signal 192 which provides for the direction of the speech input 116. In an<br>
8<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	alternative embodiment, the processor 186 may not receive the speech input 116, but<br>
	rather only receive the environment information 112 and the preference information<br>
	114. In this alternative embodiment, the routing information 192 may be provided to<br>
	a router (not shown) which receives the speech input 116 and routes the speech<br>
	information 116 to the designated speech recognition engine, such as 104, 108 or 110.<br>
		[0031] The executable instructions 190 provide for the processor 186 to<br>
	perform comparison tests of environment information 112 with preference<br>
	information 114. In one embodiment, the preference information includes an if, then<br>
	command and the environment information 112 provides conditions for the if<br>
	statements within the preference information 114. The executable instructions 190<br>
	allow the processor 186 to conduct conditional comparisons of various factors and<br>
	thereupon provide for the specific routing of the speech input 116 to a preferred,<br>
	through comparison of the preference information 114 with the environment<br>
	information 112, speech recognition engine.<br>
		[0032] The processor 186 may be, but not limited to, a single processor, a<br>
	plurality of processors, a DSP, a microprocessor, ASIC, a state machine, or any other<br>
	implementation capable of processing and executing software or discrete logic or any<br>
	suitable combination of hardware, software and/or firmware. The term processor<br>
	should not be construed to refer exclusively to hardware capable of executing<br>
	software, and may implicitly include DSP hardware, ROM far storing software,<br>
	RAM, and any other volatile or non-volatile storage medium. The memory 188 may<br>
	be but not limited to, a single memory, a plurality of memory locations, a shared<br>
	memory, CD, DVD, ROM. RAM, EEPROM, optical storage, or any other non-<br>
	volatile storage capable of storing digital data for use by the processor 186.<br>
		[0033] FIG. 5 illustrates the steps of a flowchart of the method for selective<br>
	distributed speech recognition, in accordance with the apparatus of FIG. 3. The<br>
	method begins 200 by receiving a speech input in a wireless device from an end user,<br>
	step 202. As illustrated, an audio input 144 is provided to the audio receiver 142<br>
	which thereupon provides the speech input 116 to the dialog manager 102, within the<br>
	wireless device 100. The method includes receiving preference information from a<br>
9<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	memory device disposed within the wireless device, wherein the preference<br>
	information may include a pricing preference, a time preference, a quality of service<br>
	preference, a language preference and a system availability preference, step 134,<br>
		[0034] Within the wireless device 100, the memory device 150 provides the<br>
	preference information 114 to the dialog manager 102. The pricing preference may<br>
	be an indication that a user may prefer to avoid using a particular network or a<br>
	particular speech recognition engine, based upon a specific price preference, for<br>
	example, having a toll charge above a specific dollar amount. A time preference may<br>
	indicate a user's preference to select a network or a speech recognition engine based<br>
	upon the specific time in which the communication or speech recognition may occur,<br>
	for example, a user may have a greater quantity of available minutes after a specific<br>
	time, therefore a time preference may indicate preference, for example, for the cellular<br>
	network 174 after peak hours and the WLAN 162 during peak hours. A quality of<br>
	service preference may indicate a reliability requirement that the user or the wireless<br>
	device prefers with respect to communication with or speech recognition from the<br>
	cellular network or the WLAN 162. For example, the WLAN 162 may provide a<br>
	reliability indicator and the dialog manager 102 may determine whether to provide<br>
	communication for speech recognition based on the stated reliability of fhe WLAN<br>
	162 or the WLAN speech recognition engine 108. A language preference may<br>
	indicate a preference that the user wishes for specific speech recognition, including,<br>
	but not limited to, a regional dialect, colloquialisms, a specific language (e.g. English,<br>
	French, Spanish), vocabulary coverage, ethnic speech patterns, or other linguistic<br>
	aspects.<br>
		[0035] A system availability preference may provide an indication that the<br>
	user or communication device has a preference for a system with a predefined level of<br>
	availability, for example, a minimum amount of available bandwidth for the<br>
	transmission of speech to be recognized. As recognized, by one having ordinary skill<br>
	in the art, preference information may include further preferences designated by the<br>
	wireless device 100 for the interpretation and determination of optimizing distributed<br>
	speech recognition and the above provided list is for illustration purposes only and not<br>
	meant to be so limiting herein.<br>
10<br><br>
WO 2004/061820								PCT/US2003/037899<br>
		[0036] The next step, step 206, includes receiving environment information<br>
	from a wireless local area network profile transmitted by a wireless local area<br>
	network, wherein the environment information may include location information,<br>
	time information, quality of service information, price information, system<br>
	availability information and language information. The location information may<br>
	include, but not limited to information relating to a specific location within which the<br>
	WLAN 162 may be disposed. For example, if the WLAN 162 is disposed within an<br>
	airport, the environment information may provide an indication of the location being<br>
	within an airport or may provide further information such as a city, state, zip code,<br>
	area code or a general global positioning system location. Time information may<br>
	include information such as the current time in which the WLAN profile is<br>
	transmitted, restrictions and toll information based on time, such as peak and off-peak<br>
	hours for communication, which may directly affect toll charges. Quality of service<br>
	information may be directed to the level of quality that, the WLAN 162 or the WLAN<br>
	speech recognition engine 108 may be able to provide to the wireless device, such as a<br>
	indication of the abilities of the WLAN speech recognition engine 108, or a reliability<br>
	factor, such as an average confidence value output provided from recognized terms<br>
	generated by the WLAN speech recognition engine 108. Price information may<br>
	include information to the toll charges or accepted subscription agreements that may<br>
	exist between different communication network 178 carriers and WLAN network 162<br>
	providers. System availability information may be directed to information related to<br>
	the availability of the system at the given time of the generation of the wireless local<br>
	area network profile, including bandwidth availability, or other pertinent information<br>
	for determining the availability of effectively utilizing the WLAN 162 and/or the<br>
	WLAN speech recognition engine 108. Language information may include<br>
	information directed to the different types of language that the WLAN speech<br>
	recognition engine 108 is capable of recognizing, such as specific dialects, specific<br>
	languages (e.g. English, French, Spanish), vocabulary coverage, accents, or other<br>
	linguistic aspects.<br>
		[0037] Thereupon, the method includes providing the speech command to<br>
	either an embedded speech recognition engine, a network speech recognition engine,<br>
11<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	or a wireless local area network speech recognition engine based on the preference<br>
	information and the environment information, step 208. If the dialog manager 102, in<br>
	response to the comparison of specific preference information to environment<br>
	information, selects the embedded speech recognition 104, the speech input 116 is<br>
	provided via communication path 118. If the dialog manager 102 selects the WLAN<br>
	speech recognition engine 108, the speech input is provided via the<br>
	transmitter/receiver 146 through the antenna 148 across communication path 164 to<br>
	the access point 166. Within the WLAN 162, the speech input is thereupon provided<br>
	to the WLAN speech recognition engine 108. As recognized by one having ordinary<br>
	skill in the art the speech input may be directed directly to the WLAN speech<br>
	recognition engine 108, bypassing the WLAN server 168. Furthermore, if it is<br>
	determined that the WLAN speech recognition engine 108 and ike embedded speech<br>
	recognition engine 104 are not to be used, the dialog manager 102 may, in one<br>
	embodiment, default to the network speech recognition 108 which is provided via the<br>
	communication path 176 through the cellular network 174.<br>
		[0038] The next step, 210, includes receiving at least one recognized term<br>
	from the selected speech recognition engine. For example, if the WLAN speech<br>
	recognition engine 108 is selected, the engine 108 generates a recognized term, or in<br>
	another embodiment, generates an n-best list of recognized terms, and provides the at<br>
	least one recognized term back to the wireless device 100 via communication path<br>
	164, through the access point 166, across the antenna 148. The transmitter/receiver<br>
	146 may provide the at least one recognized term to the dialog manager 102, via<br>
	communication 186. In one embodiment, the next step of the method for distributed<br>
	speech recognition includes providing the at least one recognized term to an output<br>
	device, step 212. The dialog manager 102 provides the at least one recognized term<br>
	to the output device 158, wherein a user may readily ascertain the recognized term or<br>
	n-best list of terms from the output device. For example, if the output device 158 is a<br>
	screen, the screen may display the list of recognized terms, if there is more than one<br>
	term, or the recognized term if there is only one recognized term.<br>
		[0039] A final step, step 214, includes receiving a final confirmation of the<br>
	correct recognized term of the at least one recognized term provided on the output. In<br>
12<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	one embodiment, the user may provide confirmation via an audio receiver 142 or may<br>
	provide it via a toggle switch or keyboard (not shown), wherein the dialog manager<br>
	102 receives the final confirmation. As such, select distributed speech recognition is<br>
	generated based on the wireless device 100 comparing at least one preference<br>
	information with at least one environment information provided from the WLAN 162<br>
	and a proper speech recognition is thereupon selected in response thereto, step 216,<br>
		[0040] FIG. 6 illustrates the steps of a method of an example of distributed<br>
	speech recognition. The method begins, step 220, when the Wireless device receives a<br>
	pricing preference indicating that if the WLAN 162 charges more than X using the<br>
	WLAN speech recognition engine 108, that the dialog manager should choose a<br>
	different speech recognition engine, step 222. Next, step 224, the dialog manager 102<br>
	receives pricing information, within the environment information, a part of the<br>
	WLAN profile, indicating that the WLAN 162 charges Y per minute for usage of the<br>
	WLAN speech recognition engine 108.<br>
		[0041] The dialog manager 102 thereupon compares the pricing preference<br>
	with the pricing information, step 226. Illustrated at decision block 228, if the charge<br>
	X is greater than the charge Y, the dialog manager provides a speech input to the<br>
	WLAN speech recognition engine 108, indicating that the cost for using the WLAN<br>
	speech recognition engine 108 is within an acceptable price range. Also indicated at<br>
	decision block 228, if X is not greater than Y, the dialog manager 102 chooses<br>
	between the embedded speech recognition engine 104 and the network speech<br>
	recognition engine 110. In one embodiment, the network speech recognition engine<br>
	110 may be the default speech recognition engine and only when further factors<br>
	provide, the embedded speech recognition engine 104 may be utilized, such as the<br>
	speech input being within the speech recognition capabilities of the embedded speech<br>
	recognition engine 104. Thereupon, the dialog manager 102 provides the speech<br>
	input to the selected embedded speech recognition engine 104 or the selected network<br>
	speech recognition engine 110, based on the selection within step 232.<br>
		[0042] As discussed above with respect to FIG. 5, once the speech input has<br>
	been recognized by a chosen speech recognition engine, the dialog manager receives<br>
13<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	at least, one recognized term from the selected speech recognition engine, step 236.<br>
	Thereupon, the dialog manager may provide the at least one recognized term to the<br>
	output device 158 step 238. Whereupon, step 240, the dialog manager may receive<br>
	confirmation of a correct recognized term. As such, the method is complete step 242.<br>
		[0045] FIG.7 illustrates an alternative embodiment of a wireless device 100<br>
	having a router 250 disposed within the wireless device 100 and coupled to the dialog<br>
	manager 102, While this device 100 includes the embedded speech recognition<br>
	engine 104, the output device 158, the memory device 150 and the audio receiver 142.<br>
	In this Embodiment, the dialog manager 102 receives the performance information<br>
	114 from the memory device 150 and the environments information 112 from the<br>
	transmitter/receiver 146 through the antenna 148 from the WLAN 162.<br>
		[0044] The dialog manager 102, as discussed above, based on me preference<br>
	information 114 and the environment information 112 generates a routing signal 252<br>
	which is provided to the router 250. The router 250, receives the speech input 116<br>
	and routes the speech input 116 to the appropriate speech recognition engine, such as<br>
	108, 110, or 104 based on the routing signal 252. If either the WLAN speech<br>
	recognition engine 108 or the network speech recognition engine 110 is selected, the<br>
	router provides the speech input via communication path 254 and if the embedded<br>
	speech recognition engine 104 selected, the router 250 provides the speech input 116<br>
	via communication path 236. In this alternative embodiment the dialog manager<br>
	never receives the speech input 116, the speech input 116 is directly provided to the<br>
	router 250 which is thereupon provided to the selected speech recognition engine.<br>
		[0045] It should be understood that there exists implementations of other<br>
	variations and modifications and the invention and its various aspects, as may be<br>
	readily apparent to those of ordinary skill in the art, and that the invention is not<br>
	limited by the specific embodiments described herein. For example, the network<br>
	speech recognition engine 110 and the WLAN speech recognition engine 108 may<br>
	further be accessible across alternative networks, such as through the cellular network<br>
	174 and across intercommunication paths within the communication network 178, a<br>
	speech input may be eventually provided to the WLAN speech recognition engine 108<br>
14<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	through internal routing. The transmission of the speech input through the WLAN<br>
	access point 166 may provide for higher bandwidth availability and quicker access to<br>
	the WLAN speech recognition engine, but as recognized by one having ordinary skill<br>
	in the art, beyond the cellular network 174, the communication network 178 may be<br>
	able to be in communication with the wireless local area network 162 via other<br>
	network connections, such as an internet routing connection. It is therefore<br>
	contemplated and covered by the present invention, any and all modifications,<br>
	variations, or equivalence that fall within the spirit and scope of the basic underlying<br>
	principals disclosed and claimed herein.<br>
15<br><br>
WO 2004/061820								PCT/US2003/037899<br>
CLAIMS<br>
	What is claimed is:<br>
	1.	A wireless device comprising:<br>
		an embedded speech recognition engine;<br>
		a dialog manager operably coupled to the embedded speech recognition engine<br>
			and operably couple able to an at least one external speech recognition<br>
			engine;<br>
		preference information received by the dialog manager: and<br>
		environment information received by the dialog manager, wherein the dialog<br>
			manager receives a speech input and the dialog manager, in response to<br>
			the preference information and the environment information, provides<br>
			the speech input to at least one of the embedded speech recognition<br>
			engine and the at least one external speech recognition engine.<br>
	2.	The wireless device of claim 1, wherein the environment information is<br>
	provided from a wireless local area network.<br>
	3.	The wireless device of claim 2, wherein the environment information is<br>
	disposed within a wireless local area network profile and the environment information<br>
	includes at least one of the following location information, time information, quality<br>
	of service information, price information, system availability information and<br>
	language information.<br>
	4.	The wireless device of claim 1, wherein the at least one external speech<br>
	recognition engine includes a wireless local area network speech recognition engine<br>
	and a network speech recognition engine.<br>
16<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	5.	The wireless device of claim 4, wherein when the speech input is provided to<br>
	the wireless local area network speech recognition engine, the speech input is<br>
	provided through a wireless area network access point.<br>
	6.	The wireless device of claim 1 further comprising<br>
		a memory device operably coupled to the dialog manager, wherein memory<br>
			device provides the preference information to the dialog manager,<br>
			wherein the memory device is capable of receiving preference<br>
			information from the dialog manager.<br>
17<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	7.	A method for selective distributed speech recognition comprising:<br>
		receiving a speech input;<br>
		receiving preference information;<br>
		receiving environment information; and<br>
		providing the speech input to at least one of the following: a first speech<br>
			recognition engine and at least one second speech recognition engine<br>
			based on the preference information and the environment information.<br>
	8.	The method of claim 7, wherein the first speech recognition engine is an<br>
	embedded speech recognition engine and the at least one second speech recognition<br>
	engine is at least one external speech recognition engine.<br>
	9.	The method of claim 8, wherein the at least external speech recognition engine<br>
	includes a wireless local area network speech recognition engine and a network<br>
	speech recognition engine.<br>
	10.	The method of claim 7, wherein the environment information is disposed<br>
	within a wireless local area network profile and the environment information includes<br>
	at least one of the following location information, time information, quality of service<br>
	information, price information, system availability information and language<br>
	information.<br>
	11.	The method of claim 10, wherein the wireless local area network profile is<br>
	received from a wireless local area network.<br>
18<br><br>
WO 2004/061820								PCT/US2003/037899<br>
	12.	The method of claim 7 further comprising:<br>
		receiving at least one recognized term from at least one of the following: the<br>
			first speech recognition engine and the at least one second speech<br>
			recognition engine;<br>
		providing the at least one recognized term to an output device; and<br>
		receiving a final confirmation of a correct recognized term of the at least one<br>
			recognized term.<br>
19<br><br>
A method and apparatus, for selective distributed speech recognition includes an embedded speech recognition engine<br>
(101) and a dialog manager (102), such has browser coupled to the embedded speech recognition engine (104). The method and<br>
(106), such as a WLAN speech recognition engine (108) or a network speech recognition engine (110). The method and apparatus<br>
futher includes proference information (114) environment information (112) and a speech input (116) all provided to the dialog<br>
manager (102). The dialog manager (102),in response to the preference information (114) and the environment information (113),<br>
provides the speech input (116) to the embedded speech recognition, engine (104), the WLAN speech recognition engine (108) or<br>
the network speech recognition engine (110).<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
		<br>
		<div class="pull-left">
			<a href="217414-substituted-dihydrophenanthridinesulfonamides.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="217416-skin-piercing-microprojections-having-piercing-depth-control.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>217415</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>01005/KOLNP/2005</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>13/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>28-Mar-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>26-Mar-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>27-May-2005</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>MOTOROLA. INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>1303 EAST ALGONQUIN ROAD, SCHAUMBURG, IL 60196, UNITED STATES OF AMERICA.</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>ANASTASAKOS, TASOS</td>
											<td>1026 MONICA LANE, SAN JOSE, CA 95128, UNITED STATES OF AMERICA.</td>
										</tr>
										<tr>
											<td>2</td>
											<td>BALASURIYA, SENAKA</td>
											<td>1405 CRANE STREET, ARLINGTON HEIGHTS, IL 60004, UNITED STATES OF AMERICA.</td>
										</tr>
										<tr>
											<td>3</td>
											<td>VAN WIE, MICHAEL</td>
											<td>24 PORTSMOUTH TERRACE #3, ROCHESTER, NEW YORK 14607, UNITED STATES OF AMERICA.</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H04M 1/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/US2003/037899</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2003-11-24</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>10/334,158</td>
									<td>2002-12-30</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/217415-device-and-method-for-selective-distributed-speech-recognition by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:43:45 GMT -->
</html>
