<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/251665-an-airborne-reconnaissance-system-for-obtaining-images-from-an-area-of-interest by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 13:47:44 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 251665:AN AIRBORNE RECONNAISSANCE SYSTEM FOR OBTAINING IMAGES FROM AN AREA OF INTEREST</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">AN AIRBORNE RECONNAISSANCE SYSTEM FOR OBTAINING IMAGES FROM AN AREA OF INTEREST</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>An airborne reconnaissance system comprising: (1) Gimbals having at least two degrees of freedom; (2) At least one array of light sensors positioned on the gimbals, for being directed by the same within at least two degrees of freedom; (3) Map storage means for storing at least one Digital Elevation Map of an area of interest, divided into portions; (4) Inertial Navigation System for real-time providing to a gimbals control unit navigation and orientation data of the aircraft with respect to a predefined global axes system;(5) Portion selection unit for selecting, one at a time, another area portion from the area of interest; and (6) servo means for directing the gimbals. The system uses data from the inertial navigation system and from the digital elevation map for real-time calculating direction to selected area portions, and for maintaining the direction during integration of light from the terrain, and for producing corresponding images of area portions.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Field of the Invention<br>
The present invention relates to a system for carrying out airborne<br>
reconnaissance. More particularly, the present invention relates to an airborne<br>
reconnaissance system which comprises one or more arrays of light-sensitive<br>
sensors, such as UV, visible, IR, multi/hyper-spectral, or active illumination,<br>
the line of sight of which being directed by means of gimbals having at least two<br>
degrees of freedom, said system further uses an Inertial Navigation System<br>
(INS) for providing accurate tracking and capturing, and for providing 3D<br>
motion compensation. The sensors and INS are preferably mounted on the<br>
gimbals.<br>
Background of the Invention<br>
Airborne reconnaissance systems have been widely used for many years now,<br>
particularly for obtaining images from the air of areas of interest.<br>
Originally, a film camera was used on board aircraft for capturing the images.<br>
The main problem of the airborne, film-camera based reconnaissance system is<br>
the length of time required for developing the film, an operation that can be<br>
performed only after landing. This problem has been overcome in more modern<br>
systems by the use of a one-dimensional vector or a two-dimensional array of<br>
light-sensitive sensors in the camera for obtaining electronic images that are<br>
then electronically stored within the aircraft, and/or transmitted to a ground<br><br>
base station. This is generally done in such systems by scanning by the light-<br>
sensitive sensors of the area of interest in the direction of the flight.<br>
Airborne reconnaissance systems are generally used to obtain images of hostile<br>
areas, and therefore the task of obtaining such images involves some particular<br>
requirements, such as:<br>
1.	Flying the aircraft at high elevations and speeds in order to reduce the<br>
risk of being targeted by enemy weapons, and in order to widen the area<br>
captured by each image;<br>
2.	Trying to capture as much relevant image information as possible during<br>
as short as possible flight;<br>
3.	Trying to operate under various visibility conditions, while not<br>
compromising the resolution of the images and their quality.<br>
4.	Trying to photograph rough terrains (e.g., high mountains, areas having<br>
sharp ground variations), in high resolution and image quality.<br>
The need for securing the reconnaissance aircraft, while flying above or close<br>
to hostile areas has significantly increased flying costs and risks, as<br>
sometimes the reconnaissance mission requires escorting of the aircraft by<br>
other, fighter aircrafts. Therefore, the need for enabling a short and reliable<br>
mission is of a very high importance.<br><br>
There are several other problems generally involved in carrying out airborne<br>
reconnaissance. For example, capturing images from a fast-moving aircraft<br>
introduces the need for the so-called Forward Motion Compensation<br>
(Hereinafter, the term "Forward Motion Compensation" will be shortly<br>
referred to as FMC. Motion Compensation in general will be referred to as<br>
MC), to compensate for aircraft movement during the opening of the camera<br>
shutter (whether mechanical or electronic; in the latter case, the opening of<br>
the camera shutter for the purpose of exposure is equivalent to the<br>
integration of light photons by the light-sensitive components).<br>
When light-sensitive sensors are used in the camera (hereinafter, this type of<br>
image capturing will be referred to as "electronic capturing" in contrast to<br>
"film capturing", wherein a film-type camera is used), three major scanning<br>
types are used:<br>
i. The Along-Track Scanning (also known as "push-broom scanning") - In a<br>
first configuration of the Along-Track Scanning, the light-sensitive<br>
sensors are arranged in a one-dimensional vector (row), perpendicular to<br>
the flight direction. The scanning of the imaged area is obtained by the<br>
progression of the aircraft. In one specific configuration of Along-Track<br>
Scanning, generally called Along-Track TDI (Time Delayed Integration)<br>
configuration, a plurality of such parallel one-dimensional vectors (pixel-<br>
rows) perpendicular to the flight direction are provided at the front of the<br><br>
camera forming a two-dimensional array. In that case, however, the first<br>
row of the array captures an area section, while all the subsequent rows<br>
are used to capture the same section, but at a delay dominated by the<br>
aircraft progression. Then, for each row of pixels, a plurality of<br>
corresponding pixels of all the rows in the array, as separately measured,<br>
are first added, and then averaged in order to determine the pixel<br>
measured light intensity value. More particularly, each pixel in the<br>
image is measured N times (N being the number of rows) and then<br>
averaged. This Along-Track TDI configuration is found to improve the<br>
signal-to-noise ratio, and to improve the image quality and the reliability<br>
of measuring.<br>
ii. The Across-Track Scanning (also known as "Whiskbroom Scanning") - In<br>
the Across-Track Scanning, a one-dimensional sensing vector of light-<br>
sensitive sensors, arranged parallel to the flight direction, is used. The<br>
sensing vector is positioned on gimbals having one degree of freedom,<br>
which, during the flight, repeatedly moves the whole vector right and left<br>
in a direction perpendicular to the direction of flight, while always<br>
keeping the vector in an orientation parallel to the direction of flight.<br>
Another Across-Track Scanning configuration uses a moving mirror or<br>
prism to sweep the line of sight (hereinafter, LOS) of a fixed vector of<br>
sensors across-track, instead of moving the vector itself. In such a case,<br>
the Across-Track Scanning of the area by the gimbal having one degree<br><br>
of freedom, while maintaining the forward movement of the aircraft,<br>
widens the captured area. Another configuration of the Across-Track<br>
Scanning is the Across-Track TDI configuration. In this configuration<br>
there exists a plurality of vectors (columns) in a direction parallel to the<br>
flight direction, forming a two-dimensional array. This Across-Track TDI,<br>
in similarity to the Along-Track Scanning TDI, provides an improved<br>
reliability in the measuring of pixel values, more particularly, an<br>
improvement in the signal-to-noise ratio,<br>
iii. Digital Framing Scanning: In Digital Framing Scanning, a two-<br>
dimensional array of light-sensitive sensors is positioned with respect to<br>
the scenery. In US 5,155,597 and US 6,256,057 the array is positioned<br>
such that its column-vectors (a column being a group of the array's<br>
columns) are parallel to the flight direction. Forward motion<br>
compensation (FMC) is provided electronically on-chip (in the detector<br>
focal plane array) by the transferring of charge from a pixel to the next<br>
adjacent pixel in the direction of flight during the sensor's exposure time<br>
(also called "integration time"). The charge transfer rate is determined<br>
separately for each column (or for the whole array as in US 6,256,057<br>
where a slit is moved in parallel to the columns direction), depending on<br>
its individual distance (range) from the captured scenery, assuming flat<br>
ground. In WO 97/42659 this concept is extended to handle transferring<br>
of charge separately for each cell instead of column, a cell being a<br>
rectangular group of pixels. In the system of US 5,692,062, digital image<br><br>
correlation between successive frames captured by each column is<br>
performed, in order to measure the velocity of the scenery with respect to<br>
the array, and the correlation result is used for estimating the average<br>
range of each column to the scenery, for the purpose of motion<br>
compensation in terrain with large variations. This compensation method<br>
requires capturing of three successive frames for each single image, two<br>
for the correlation process and one for the final motion-compensated<br>
frame. The system of US 5,668,593 uses a 3-axis sightline stepping<br>
mechanism for expanding coverage of the area of interest, and it applies<br>
a motion compensation technique by means of transferring of charge<br>
along columns. US 6,130,705 uses a zoom lens that automatically varies<br>
the camera field of view based on passive range measurements obtained<br>
from digital image correlation as described above. The field of view is<br>
tuned in accordance with prior mission requirements for coverage and<br>
resolution.<br>
A significant problem which is characteristic of the prior art reconnaissance<br>
systems, particularly said electronically scanning Across-Track and Along-<br>
Track scanning methods, is the need for predefining for the aircraft an<br>
essentially straight scanning leg (and generally a plurality of such parallel<br>
straight legs), and once such a leg is defined, any deviation, particularly a<br>
rapid or large deviation, from the predefined leg, is not tolerated, as said<br>
systems of the prior art are not capable of mamtaining a desired line of sight<br><br>
direction during such a fast and/or large deviation from the predefined leg,<br>
resulting in image artifacts such as tearing (dislocation of image lines),<br>
smearing (elongation of pixels) or substantial gaps in the image information.<br>
This is particularly a significant drawback when carrying out a<br>
reconnaissance mission above or close to a hostile area, when the need arises<br>
for the aircraft to carry out fast maneuvering to escape enemy detection or<br>
targeting. Moreover, sometimes, in order to obtain good imaging of a<br>
complicated terrain, such as of a curved canyon, it is best to follow the course<br>
of the sharply curved edges of the canyon. However, in most cases the<br>
reconnaissance systems of the prior art cannot tolerate carrying out such a<br>
sharply curved maneuvering, involving sharp changes in the angles of the<br>
line of sight with respect to the photographed scenery.<br>
Another drawback characteristic of the reconnaissance systems of the prior art,<br>
for example, US 5,155,597, US 5,692,062, WO 97/42659, and US 6,256,057, is<br>
their need to handle vast amounts of data. The systems of the prior art do not<br>
enable an easy, selective imaging of small portions of an area of interest. Once<br>
operated, the system scans the entire area to which the camera is directed, with<br>
essentially no selection of specific portions of the whole possible. Therefore, even<br>
for a small area of interest, the systems of the prior art must handle a huge<br>
amount of data, i.e., be capable of storing the full image data obtained during<br>
the operation of the camera, and transmission of it to the ground (when such an<br>
option is desired). The transmission of a huge amount of data to the ground,<br><br>
sometimes in real-time, requires usage of a very wide bandwidth. Another<br>
particular problem which evolves from this limitation is the need for<br>
distinguishing and decoding a small data of interest within the said full, huge<br>
amount of data obtained.<br>
Still another drawback of reconnaissance systems of the prior art, for<br>
example, US 5,155,597, US 5,692,062, WO 97/42659, US 6,130,705, and US<br>
6,256,057 is their limited ability to capture images in a wide range of a field<br>
of regard. Hereinafter, the term "field of regard" refers to the spatial section<br>
within which the camera line of sight can be directed without obscuration.<br>
Systems of the prior art sometimes use separate dedicated sensors for<br>
different sight directions (e.g. separate sensors for down-looking, side-oblique<br>
or forward-oblique). The present invention provides to the aircraft the ability<br>
of capturing images, simultaneously from all sensors, of areas forward,<br>
backward, sideways and in any other arbitrary direction, and to rapidly<br>
switch between these directions.<br>
Yet another drawback of reconnaissance systems of the prior art, for<br>
example, US 5,155,597, US 5,668,593, US 5,692,062, WO 97/42659, US<br>
6,130,705, and US 6,256,057 is the use of large-sized two-dimensional<br>
sensors' arrays, which becomes a necessity for systems having limited or no<br>
control over their line of sight. The present invention enables usage of small<br>
or medium-sized, two-dimensional sensors' arrays, by taking advantage of<br><br><br>
the capability to quickly and accurately move the LOS within a large field of<br>
regard, to stably fix the LOS on the ground scenery while capturing an<br>
image, and to gather photographic image data by a multitude of<br>
small/medium frames rather than one single large frame at a time. A small-<br>
sized array would typically be up to 1 megapixels (million pixels), and a<br>
medium-sized array would typically be up to 5 megapixels. In contrast, large-<br>
sized arrays would typically be up to 50 megapixels and even larger. An<br>
important feature of the present invention is that both the small and<br>
medium-sized arrays are commercially available as universal sensors' arrays,<br>
not designed specifically for reconnaissance applications but rather for<br>
commercial applications such as stills and video cameras, and therefore they<br>
are widely available from a few vendors at low prices. This sensors'<br>
technology also benefits from the enormous investment by vendors in such<br>
commercial products due to the demands of the commercial market. In<br>
contrast, the large-sized reconnaissance sensors' arrays are uniquely<br>
developed by reconnaissance systems manufacturers, are complex due to the<br>
need for on-chip motion compensation, are expensive, and are not widely<br>
available. The limitations of prior art systems are more acute when the<br>
sensor is required to operate at the IR range rather than at the visible range,<br>
since the current IR array technology does not provide large-sized IR arrays.<br>
Another drawback of large-sized arrays is their lower frame rate with respect<br>
to small/medium-sized arrays, due to the large amount of pixels processed for<br>
each image.<br><br><br>
Some of the prior art systems employ on-chip motion compensation, for<br>
example, as described in US 5,155,597, US 5,692,062, and WO 97/42659.<br>
Several drawbacks are associated with the on-chip motion compensation<br>
concept. On-chip motion compensation is performed by transferring charges<br>
from one column/cell to an adjacent column/cell during the integration time at a<br>
specified rate. This process of transferring charges induces electronic noises and<br>
creates an ambiguity (resulting in smearing or loss of pixels) at the borders<br>
between columns/cells and at the edges of the chip, since the required charge<br>
transfer rate may be different between adjacent columns/cells. Some of the prior<br>
art systems assume flat and horizontal ground for estimating the range from<br>
the sensor to each part of the scenery in the captured image (i.e. longer range<br>
for the farther portion of the scenery and shorter range for the closer portion),<br>
and calculate the motion compensation rate based on simple aircraft velocity<br>
and attitude information with respect to the flat ground. When the terrain has<br>
large variations this generally results in substantial smearing as shown in<br>
example 1 of the present invention. In some cases, the sensor must be oriented<br>
during capturing so that its columns are accurately parallel to the flight<br>
direction without rotation, whereby any deviation from that orientation will<br>
result in further smearing, thus seriously limiting mission planning. The more<br>
advanced prior art systems use digital image correlation between successive<br>
frames for each cell in the chip, in order to estimate more accurately the range<br>
to the scenery for each cell. This process requires three successive image<br><br><br>
captures for each usable image, thus wasting system duty cycles. The<br>
correlation accuracy is limited by smearing of the first two images when<br>
photographing a terrain with large variations. Another problem associated with<br>
correlation is the large change of aspect angle with respect to the scenery<br>
between the two successive images. For example, an aircraft flying at a velocity<br>
of 250m/s at a range of 15km to the scenery in side oblique, using a chip with 2<br>
Hz frame rate, will have an LOS angular velocity (sometimes called V/R) of<br>
250/15 = 16.7 milirad/s, resulting in an aspect angle between successive images<br>
of 8.3 milirad. For a typical pixel Instantaneous FOV (IFOV) of 30 microrad this<br>
means a shift of 277 pixels in the image. Moreover, since the value of V/R is not<br>
constant at any time during the mission, especially. when the aircraft is<br>
maneuvering, the elapsed time between the two successive images will induce<br>
an additional error.<br>
Some of the prior art systems employ a step framing method to cover large<br>
areas, for example, as described in US 5,668,593. The step framing method<br>
does not provide mechanical/optical fixing of the LOS on the scenery during<br>
exposure time, and has a limited field of regard. On-chip motion<br>
compensation is used, but inaccuracies are induced due to vibrations of the<br>
aircraft, and delays in transferring the measurement of the vibrations to the<br>
reconnaissance system.<br><br>
It is therefore an object of the present invention to provide a reconnaissance<br>
airborne system capable of tolerating and compensating for very sharp<br>
maneuvers of the aircraft and for large terrain variations, while still<br>
providing high resolution and reliable images of the area of interest, within a<br>
very wide field of regard.<br>
It is still another object of the present invention to provide a reconnaissance<br>
system in which the amount of irrelevant data is significantly reduced,<br>
therefore reducing work needed for distinguishing relevant data from the<br>
fully obtained data, and reducing airborne and ground image storage and<br>
communication requirements.<br>
It is still another object of the present invention to enable the defining of very<br>
small areas of interest within a large area (i.e., a field of regard), of which<br>
images can be obtained.<br>
It is still another object of the present invention to reduce the communication<br>
load between the aircraft and a ground base station, when communicating<br>
images from the aircraft to the ground.<br>
It is still another object of the present invention to provide an airborne<br>
reconnaissance system with the ability to capture images in a wide range of<br>
the angle of sight (i.e., a wide field of regard).<br><br>
It is still another object of the invention to provide a new and efficient<br>
manner of obtaining the images required for creating stereoscopic-view<br>
images.<br>
It is still another object of the invention to provide the capability of<br>
combining in the same reconnaissance mission both manual mode operation<br>
and automatic mode operation.<br>
Other objects and advantages of the present invention will become apparent<br>
as the description proceeds.<br>
Summary of the Invention<br>
The present invention relates to an airborne reconnaissance system which<br>
comprises: a. Gimbals having at least two degrees of freedom; b. At least one<br>
array of light sensors positioned on the gimbals for being directed by the same<br>
within at least two degrees of freedom; c. Map storage means for storing at least<br>
one Digital Elevation Map of an area of interest, divided into portions; d.<br>
Inertial Navigation System for real-time providing to a gimbals control unit<br>
navigation and orientation data of the aircraft with respect to a predefined<br>
global axes system; e. Portion selection unit for selecting, one at a time, another<br>
area portion from the area of interest; f. Servo control unit for:<br><br>
A.	Receiving from said Digital Elevation Map one at a time, a<br>
coordinates set of the selective area portion, said set comprising the<br>
x:y coordinates of said area portion and the elevation z of the<br>
center of that portion;<br>
B.	Receiving continuously from said inertial navigation system present<br>
location and orientation data of the aircraft;<br>
C.	Repeatedly calculating and conveying into a gimbals servo unit in<br>
real time and at a high rate signals for:<br>
a.	during a direction period, signals for directing accordingly the<br>
gimbals including said LOS of at least one array of light-sensing<br>
units towards said x : y: z coordinates of the selected area portion,<br>
and;<br>
b.	during an integration period in which the array sensors<br>
integrate light coming from the area portion, providing to the<br>
gimbals unit signals for compensating for the change in direction<br>
towards the x:y:z coordinates of the selected portion evolving from<br>
the aircraft motion;<br>
g. Gimbals servo for effecting direction of the gimbals in at least two degrees of<br>
freedom according to the signals provided from said Servo Control Unit; h.<br>
Sampling means for simultaneously sampling at the end of the integration<br>
period pixel levels from each of said array sensors, a set of all of said sampled<br>
pixel levels forms an image of said area portion; and i. Storage means for<br>
storing a plurality of area portion images.<br><br><br>
Preferably, said one or more arrays are selected from at least a visual light-<br>
sensitive array, a UV light sensitive-array, an infrared light-sensitive array, a<br>
multi/hyper-spectral array, and an active illumination array.<br>
Preferably, said navigation data of the aircraft comprises data relating to the<br>
3D location of the aircraft, and its velocity and acceleration vectors with respect<br>
to a predefined coordinates system, and its orientation data relating to the<br>
orientation of the aircraft with respect to said predefined coordinate system.<br>
Preferably, said Inertial Navigation System comprises velocity, acceleration,<br>
and orientation sensors, at least some of said sensors being positioned on the<br>
gimbals.<br>
Preferably, at least some of said arrays of sensors are positioned on the gimbals.<br>
Preferably, the system uses two Inertial Navigation Systems, the first inertial<br>
navigation system being the main Inertial Navigation System of the aircraft<br>
and its sensors being positioned within the aircraft, and the second Inertial<br>
Navigation System being an a system dedicated to the reconnaissance system,<br>
at least some of the sensors of said second Inertial Navigation System being<br>
positioned on the gimbals unit, measuring navigation and orientation data of<br>
the gimbals with respect to the said predefined axes system, for better<br><br>
eliminating misalignments occurring between the gimbals and LOS and the<br>
said main Inertial Navigation System of the aircraft due to aero-elastic<br>
deflections and vibrations of the aircraft, by using a process of transfer<br>
alignment from the said first INS to the said second INS.<br>
Preferably, the Digital Elevation Map is a map comprising a grid of the area of<br>
interest, the x: y: z coordinate values at each of the nodal points in said grid<br>
being provided by said map.<br>
Preferably, the portion selecting unit is used for calculating and determining a<br>
center of a next area portion such that provides a predefined overlap between<br>
the said imaged area portion and the adjacent previously imaged area portion.<br>
Preferably, in an automatic mode of operation the gimbals are activated to<br>
cover in a sequential, step-wise manner, the area of interest, said coverage is<br>
made from a predefined starting portion and according to a stored mission plan,<br>
thereby sequentially scanning one after the other area portions of the area of<br>
interest, and sampling images from each of said portions.<br>
Preferably, in a manual mode of the system the pilot of the aircraft defines an<br>
area of interest during the flight, said area of interest being automatically<br>
divided into at least one area portion, all the area portions being automatically<br><br>
scanned one after the other by means of correspondingly directing to them the<br>
on-gimbals array, for capturing images of each of said scanned portions.<br>
Preferably, the gimbals comprise two gimbals mechanisms, an external gimbals<br>
mechanism and an internal gimbals mechanism.<br>
Preferably, the external gimbals mechanism is used for coarse directing the on-<br>
gimbals array to the center of a selected area portion.<br>
Preferably, the external gimbals mechanism has two degrees of freedom,<br>
elevation and roll.<br>
Preferably, the internal gimbals mechanism is used for fine directing the on-<br>
gimbals array to the center of a selected area portion, particularly for<br>
compensating the gimbals direction for the aircraft motion and orientation<br>
change during the integration period.<br>
Preferably, the internal gimbals mechanism has two degrees of freedom, yaw<br>
and pitch.<br>
Preferably, the external gimbals mechanism is slaved to the internal gimbals<br>
mechanism.<br><br>
Preferably, during the integration period each of the array sensors<br>
simultaneously senses light from a corresponding section of the area portion,<br>
and at the end of the integration period the data from all the array sensors is<br>
read simultaneously, and stored as an image of the area portion.<br>
Preferably, the arrays of light sensors are sensitive to light in the range of<br>
visual light, IR, UV, multi/hyper-spectral, and/or an active illumination.<br>
Preferably, the arrays are focal plane arrays.<br>
Preferably, the predefined axes system is a global axes system.<br>
In one embodiment of the invention, the system of the invention is assembled<br>
within a pod attached to the aircraft.<br>
In another embodiment of the invention, the system of the invention is<br>
assembled within a payload installed inside the aircraft with only its windows<br>
protruding for obtaining a clear, unobstructed Line Of Sight.<br>
Preferably, the gimbals are located at the front of the pod, behind a transparent<br>
window.<br><br>
In an embodiment of the invention, the system further comprising a back-<br>
scanning mechanism comprising a mirror or prism, positioned on the gimbals<br>
and rotatable with respect thereto, light coming from the area portion first<br>
passing through said mirror which deflects the same towards the array, and: a.<br>
the servo control unit applies to the gimbals a continuous row and/or column<br>
scanning movement without stopping; and b. while the direction towards an<br>
area portion is being established, applying to said back-scanning mirror during<br>
the integration period an opposite direction movement with respect to said row<br>
and/or column scanning continuous movement, thereby compensating for that<br>
continuous movement and ensuring a fixed orientation relationship of the array<br>
with respect to the area portion imaged.<br>
The invention further relates to a method for carrying out airborne<br>
reconnaissance, which comprises: a. Providing at least one array of light-<br>
sensitive pixels; b. Mounting the at least one array on gimbals having at least<br>
two degrees of freedom so that the gimbals can direct the array to a selected<br>
Line Of Sight; c. Providing a Digital Elevation Map of an area of interest,<br>
reconnaissance images from said area are to be obtained; d. Providing an<br>
Inertial Navigation System for obtaining at any time during the flight the<br>
updated xa: ya : za coordinates of the center of the array with respect to a<br>
predefined coordinates system; e. Providing a calculation unit for, given xp : y<br>
location coordinates of a center of specific area portion within the area of<br>
interest, and the zp elevation coordinate at said portion center as obtained from<br><br>
said Digital Elevation Map, and the said xa:ya: za coordinates of the array<br>
center at same specific time, determining the exact angles for establishing a<br>
line of sight direction connecting between the center of the array and the said<br>
XP '■ yP '■ ZP coordinates; f. Given the calculation of step e, directing accordingly<br>
the center of the array's Line Of Sight to the center of the area portion; g.<br>
During an integration period, effecting accumulation of light separately by any<br>
of the array light sensors; h. During the integration period, repeating at a high<br>
rate the calculation of step e with updated array xa : ya : za coordinates, and<br>
repeatedly, following each said calculation, correcting the direction as in step f;<br>
i. At the end of the integration period, sampling all the array sensors, and<br>
saving in a storage as images of the array portion; j. Selecting new portion<br>
coordinates x : y : zp within the area of interest, and repeating steps e to j for<br>
these new coordinates; and, k. When the coverage of all the area of interest is<br>
complete, terminating the process, or beginning coverage of a new area of<br>
interest.<br>
Preferably, the selection of xp :yp coordinates of a new area portion is<br>
performed to assure overlap between adjacent area portions within a predefined<br>
range, by calculating the 3-dimensional footprint of the new area portion on the<br>
ground, and then projecting it on the footprint of a previous area portion.<br><br>
Preferably, the overlap assurance is obtained by a trial and error selection,<br>
overlap calculation, and correction when necessary, or by an exact analytical<br>
calculation.<br>
Preferably, at least some of the sensors of the Inertial Navigation System are<br>
positioned on the gimbals, for improving the measuring of the orientation of the<br>
array with respect to the selective area portion.<br>
Preferably, at least some of the light sensitive sensors are positioned on the<br>
gimbals, for improving the measuring of the orientation of the Line Of Sight<br>
with respect to the selective area portion.<br>
Preferably, the Inertial Navigation System comprises a dedicated Inertial<br>
Navigation System of the reconnaissance system and the main Inertial<br>
Navigation System of the aircraft, to improve the measuring of the orientation<br>
of the array with respect to the selective area portion, by using a process of<br>
transfer alignment from the aircraft Inertial Navigation System to the<br>
dedicated reconnaissance system's Inertial Navigation System.<br>
The invention further relates to a method for providing motion compensation<br>
during airborne photographing which comprises: a. Providing at least one<br>
array of light-sensitive pixels; b. Mounting the at least one array on gimbals<br>
having at least two degrees of freedom so that the gimbals can direct its Line Of<br><br>
Sight towards a selective area portion; c. Providing a Digital Elevation Map of<br>
an area of interest, reconnaissance images from said area are to be obtained; d.<br>
Providing an Inertial Navigation System for obtaining at any instant during<br>
flight the updated xa: ya: za coordinates of the center of the array with respect<br>
to a predefined coordinates system; e. Providing a calculation unit for, given<br>
xp : yp location coordinates of a center of specific area portion within the area of<br>
interest, and the zp elevation coordinate at said portion center as obtained from<br>
said Digital Elevation Map, and the said xa : ya : za coordinates of the array<br>
center at same specific time, determining the exact angles for establishing a<br>
line of sight direction connecting between the center of the array and the said<br>
xp : yp : zp coordinates; f. During an integration period, when the center of the<br>
array's Line Of Sight is directed to a center of an area portion effecting<br>
accumulation of light separately by any of the array light sensors; g. During<br>
the integration period, repeating at a high rate the calculation of step e with<br>
updated array xa :ya :za coordinates, and repeatedly, following each said<br>
calculation, correcting the direction by keeping the center of the array directed<br>
to the center of the selected area portion, therefore compensating for aircraft<br>
movement; and h. At the end of the integration period, sampling all the array<br>
sensors, and saving in a storage as images of the array portion.<br>
The invention further relates to a method for carrying out airborne targeting,<br>
which comprises:<br><br><br>
a.	Providing at least one weapon;<br>
b.	Mounting the at least one weapon on gimbals having at least two degrees of<br>
freedom so that the gimbals can direct the weapon to a selected Line Of Sight;<br>
c.	Providing a Digital Elevation Map of an area of interest, selected targets<br>
within said area are to be obtained;<br>
d.	Providing an Inertial Navigation System for obtaining at any time during<br>
the flight the updated xa : ya : za coordinates of the center of the weapon with<br>
respect to a predefined coordinates system;<br>
e.	Providing a calculation unit for, given xp : yp location coordinates of a center<br>
of specific target within the area of interest, and the zp elevation coordinate at<br>
said target center as obtained from said Digital Elevation Map, and the said<br>
xa '■ ya '■ za coordinates of the weapon center at same specific time, determining the<br>
exact angles for establishing a Line of Sight Direction connecting between the<br>
center of the weapon and the said x : v : zp coordinates;<br>
f.	Given the calculation of step e, directing accordingly the center of the weapon<br>
Line Of Sight to the center of the target;<br>
h. During the effective targeting and shooting period, motion compensating for<br>
the motion of the aircraft.<br>
The motion compensation of the targeting can be made in any conventional<br>
manner known in the art. According to an embodiment of the present invention<br>
the motion compensation of step h is carried out by repeating at a high rate the<br><br>
calculation of step e with updated target xa: yg: za coordinates, and repeatedly,<br>
following each said calculation, correcting the direction as in step f.<br>
BRIEF DESCRIPTION OF THE ACCOMPANYING DRAWINGS(S)<br>
In the drawings:<br>
-	Fig. 1 shows a general structure of a reconnaissance system, assembled<br>
within a pod, according to one embodiment of the invention;<br>
-	Fig. 1A shows another embodiment of the invention. In this configuration<br>
the reconnaissance system is assembled as a payload into the aircraft<br>
body.<br>
Fig. 2 shows the mechanical structure of a gimbals system according to<br>
one embodiment of the invention;<br>
-	Fig. 3 illustrates several modes of operation, typical to the system of the<br>
invention;<br>
-	Fig. 3A shows an area of interest divides into a plurality of area portions,<br>
according to an embodiment of the invention;<br>
-	Fig. 3B illustrates several staring modes which are possible by the system<br>
of the invention;<br>
-	Fig. 4 is a block diagram illustrating the operation of the reconnaissance<br>
system of the invention;<br>
-	Fig. 5 is a flow diagram describing the operation principles of the<br>
reconnaissance system of the invention;<br><br><br>
-	Fig. 6 shows the structure of the INS system of the invention, which<br>
comprises the main INS of the aircraft, and the dedicated INS of the<br>
reconnaissance system of the invention;<br>
-	Fig. 7 shows how a stereoscopic image is constructed by the system of the<br>
present invention;<br>
-	Fig. 8 illustrates a specific case in which a system of the prior art is<br>
required to carry out a reconnaissance mission;<br>
-	Fig. 8A illustrates how the system of the present invention carries out the<br>
same reconnaissance mission of Fig. 8A;<br>
-	Fig. 9 exemplifies the significance of the elevation factor when carrying<br>
out a reconnaissance mission, and more particularly shows the<br>
importance of considering directly and in real-time the elevation of the<br>
imaged terrain;<br>
-	Fig. 10 illustrates the use of a back-scanning mirror in accordance with<br>
the system of the present invention;<br>
-	Fig. 11 is a perspective illustration of a billy terrain, and its division into<br>
area portions, including some overlap between area portions, as<br>
performed by the system of the present invention;<br>
-	Fig. 11A shows an upper view of the terrain of Fig. 11, and the manner of<br>
scanning of the said terrain by the airborne reconnaissance system of the<br>
present invention; and<br><br><br>
- Fig. 12 is an example illustrating how the system of the present invention,<br>
can photograph selective targets, thereby significantly reducing the<br>
amount of data handled;<br>
Detailed Description of Preferred Embodiments<br>
A preferred embodiment, of the reconnaissance system of the present<br>
invention is particularly characterized by the following main features:<br>
i. The one or more focal plan arrays that are used to sense and capture<br>
images from an area of interest have a Hne of sight (LOS) that is directed<br>
by gimbals having at least two degrees of freedom. The term 'gimbals',<br>
when used herein, refers to any type of mechanism, whether mechanical,<br>
optical (such as one including mirrors, prisms, etc.) or a combination<br>
thereof, which is capable of moving a line of sight of an array of light-<br>
sensitive sensors in at least two degrees of freedom. A mechanical<br>
mechanism is sometimes called 'payload-stabilized gimbals'; an optical<br>
mechanism is sometimes called 'mirror-stabilized gimbals'. One of said<br>
arrays may sense in the visual range, and another may sense, for<br>
example, in the IK, range, and/or the UV range. In another case, a<br>
multi/hyper spectral array or an active illumination array may be used.<br>
Hereinafter, throughout this application, the term "array" refers to any<br>
type of array of light-sensing means for obtaining an image from an area<br>
of interest. The arrays used in the invention may be of small or medium<br>
size, rather than large arrays which are used in prior art systems as are<br>
used in US 5,155,597, WO 97/42659, and US 6,256,057, taking advantage<br><br>
of the flexibility of the system in taking many snapshots at arbitrary line<br>
of sight directions, and with sharply varying terrain conditions. The<br>
preferred mounting of the sensors' arrays and their optics, when payload-<br>
stabilized gimbals are used, is on the gimbals; in case of mirror-stabilized<br>
gimbals, the sensors are mounted off the gimbals.<br>
ii. The reconnaissance system uses an Inertial Navigational System (INS)<br>
for continuously calculating the direction of the line of sight. In a<br>
preferable case, two INS systems are used: the first one is the main INS of<br>
the aircraft, and the second INS is an internal, dedicated INS of the<br>
system mounted in the reconnaissance system. The reconnaissance<br>
system of the invention continuously receives from the said Inertial<br>
Navigation Systems both navigational information regarding the location<br>
of the aircraft with respect to a fixed, predefined global axes system, and<br>
orientation information of the aircraft with respect to the ground, for<br>
point-directing the one or more arrays positioned on the gimbals to any<br>
desired area portion on the ground within the field of regard of the<br>
aircraft. The preferred mounting of the system INS is on the gimbals,<br>
whether payload-stabilized or mirror-stabilized gimbals.<br>
iii. The gimbals having at least two degrees of freedom, on which the arrays<br>
are preferably mounted, in one mode of the invention are systematically<br>
activated in a step-wise manner for sequentially scanning one after the<br><br>
other area portions of an area of interest, within a very large field of<br>
regard.<br>
iv. The system of the invention captures by its arrays, when activated, a<br>
snap-shot, two-dimensional image of an area portion, enabling long<br>
exposure times due to the compensation of the line of sight motion;<br>
v. When directed to an area portion, three-dimensional motion compensation<br>
is provided by means of adjusting the gimbals to keep track with the<br>
relevant area portion by means of data provided from the INS, and from a<br>
Digital Elevation Map of the area of interest, which is pre-stored in the<br>
reconnaissance system of the invention; and<br>
vi. In a preferable case, the division of the area of interest into area portions<br>
is performed in real-time, wherein the size of each area portion depends<br>
on several parameters, such as the shape of the terrain, and the range<br>
from the aircraft to the center of the area of interest, as determined from<br>
the Digital Elevation Map (DEM), particularly for assuring proper overlap<br>
between images of area portions.<br>
The above main features, as well as other structural features of the invention,<br>
will become apparent as the description proceeds.<br><br>
As said, reconnaissance systems of the prior art of the Along-Track or Across-<br>
Track scanning type, relate to the accumulated data of a specific leg as an<br>
essentially one-piece data. More particularly, once a leg is defined, while the<br>
aircraft flies along the leg, the accumulated data is essentially stored as one<br>
huge image file. Later, it is up to the operator to distinguish specific relevant<br>
data from this image. Furthermore, any airborne reconnaissance system must<br>
deal with the problem of motion compensation. As this latter problem is<br>
complicated, the solution provided in systems of the prior art do not allow for<br>
sharp maneuvering of the aircraft during the reconnaissance mission. The<br>
present invention provides a solution to said two problems in a compact and<br>
efficient manner.<br>
The reconnaissance system of the present invention is particularly adapted to<br>
be carried by a fighter aircraft, where environmental conditions, maneuvering<br>
dynamics, system size, aerodynamic limitations, and angular aspects with<br>
respect to the ground are extreme; however, the system is also suitable for<br>
other airborne platforms. In a preferable case, the system is assembled within a<br>
pod or a payload that is generally carried below the aircraft wing or fuselage.<br>
Because of the extreme operating conditions of a fighter aircraft, systems of the<br>
prior art, for example as disclosed in US 5,668,593 sometimes use a mirror to<br>
direct the LOS, a solution which limits the FOR substantially since a mirror<br>
essentially folds the LOS to point at a certain direction with relatively small<br>
angular variations. In the present invention the LOS is directed by means of<br><br>
the gimbals, a solution that enables a very wide field of regard since gimbals<br>
can be rotated towards any direction.<br>
Fig. 1 shows the general structure of a reconnaissance system, assembled<br>
within a pod, according to one embodiment of the invention. The pod 1<br>
comprises in its forward section 10 gimbals (not shown) carrying light-sensing<br>
arrays and the necessary optics (not shown). The said gimbals and optics are<br>
mounted behind a transparent window 12. At least one of such arrays exists, for<br>
example, in a CCD-type array, or a focal plan array for sensing light in the<br>
visual range. Optionally, more arrays may be included, for example, an IR<br>
array for sensing and capturing an image in the IR range. The plurality of<br>
arrays, when used, as well as the INS, are positioned on the same portion of the<br>
gimbals in such a manner as to be directed to, and cover exactly a same area<br>
portion. Optionally, in a less preferred embodiment, the sensors and/or INS<br>
may be located behind the gimbals, while the gimbals carry a set of mirrors<br>
and/or prisms that folds the LOS towards the sensors. The system further<br>
comprises an Image Handling Unit (IHU) 2 that processes the digital image<br>
information from the sensors, compresses the images, and combines them with<br>
mission data to facilitate later interpretation in the ground station, a Solid<br>
State Recorder (SSR) 3 or a similar fast-access storing device for storing a<br>
Digital Elevation Map (DEM) of the area of interest and a mission plan, and for<br>
recording the captured images. The system further comprises a Servo Unit (SU)<br>
for providing control and power signals to the gimbals servo, and an Interface<br><br>
Unit (IU) for enabling power interfaces with the aircraft. Other computer<br>
system and control electronics is included within the System Electronics Unit<br>
(SEU). Optionally, a Data Link 16 (DL) is used to transmit images and mission<br>
data to a ground station for near real-time interpretation. The pod is attached<br>
to the aircraft by means of lugs 11.<br>
Fig. 1A shows another embodiment of the invention. In this configuration the<br>
reconnaissance system is assembled as a payload into the aircraft body. The<br>
Forward Section is positioned vertically and is pointing down, with only its<br>
windows protruding outside the aircraft body. The same electronic units as in<br>
the pod configuration are installed inside the aircraft body. Although this<br>
solution may be used by a fast jet aircraft, its main objective is for other types<br>
of aircraft such as helicopters, KPVs (remotely piloted vehicles), and command<br>
&amp; control aircrafts.<br>
Fig. 2 shows the mechanical structure of the gimbals system 20, according to a<br>
preferred embodiment of the invention. The progression direction of the aircraft<br>
is indicated by numeral 27. As said, in order to carry out reconnaissance, the<br>
gimbals system according to the present invention has at least two degrees of<br>
freedom. The direction of the gimbals' axes and the gimbals' order are not<br>
important, provided they are capable of steering the LOS towards any spatial<br>
direction within their specified field of regard.<br><br>
According to a preferred embodiment of the invention shown in Fig. 2, the<br>
gimbals system 20 comprises two sub-mechanisms, as follows:<br>
Internal gimbals mechanism 36, having two degrees of freedom, Yaw<br>
(rotation around axis 22) and Pitch (rotation around axis 21); and<br>
External gimbals mechanism 37, having two degrees of freedom,<br>
elevation (rotation around axis 21) and roll (rotation around axis 23).<br>
The Pitch and the Elevation degrees of freedom relate essentially to a rotation<br>
about a same axis 21. However, the Pitch degree of freedom relates to a fine<br>
rotation by the internal gimbals mechanism 36, while the Elevation degree of<br>
freedom relates to a coarse rotation of the external gimbals mechanism 37. The<br>
external gimbals mechanism 37 is preferably slaved to the internal gimbals<br>
mechanism 36. Slaving is the process by which the internal gimbals are the<br>
prime gimbals to which LOS steering commands are directed, while the<br>
external gimbals are compensating for the internal gimbals' limited angular<br>
rotation by following the movement of the internal gimbals, always trying to<br>
minimize the angular displacement between the internal and external gimbals.<br>
Although tracking to any specific point in front and below the pod is possible by<br>
means of two degrees of freedom, the separation into two sub-mechanisms is<br>
made in order to obtain a better tracking precision and wider field of regard.<br>
The external gimbals mechanism is particularly used for coarse tracking, for<br>
example, for transferring the direction of the gimbals from one area portion to a<br>
second area portion, while the internal gimbal mechanism is particularly used<br><br>
for providing motion and orientation compensation, while capturing an image of<br>
a specific area portion. Other gimbals' arrangements, with different number of<br>
gimbals or different direction of axes, may also achieve these goals.<br>
As said, the external gimbals facilitate expansion of the field of regard (FOR).<br>
The limits of the FOR for a pod embodiment are indicated on Figure 1. This<br>
FOR is achieved by the ability of the external elevation gimbals to look<br>
backward as well as forward, combined with the ability of the roll gimbals to<br>
rotate a full turn of 360 degrees. The limits of the FOR for a payload<br>
embodiment are indicated on Figure 1A. This FOR is achieved by the ability of<br>
the external elevation gimbals to look sideways as well as downward, combined<br>
with the ability of the roll gimbals to rotate a full turn of 360 degrees. The only<br>
limitation to the FOR in both embodiments are the pod body and aircraft body,<br>
which obscure the line of sight at the edges of the FOR envelop.<br>
In the preferred embodiment of the invention of Fig. 2, the one or more arrays<br>
are mounted on the internal gimbal, to provide fine adjustment of the array<br>
towards a specific portion of an area of interest. This is required, for example, to<br>
provide motion and orientation compensation.<br>
As said, the one or more arrays of sensors, together with their associated optics,<br>
are positioned on the gimbals to maintain at least two degrees of freedom. In<br>
the embodiment of Fig. 2, an exemplary focal plane array 24 capable of<br><br>
capturing images in the visual range, is symbolically indicated. The boundaries<br>
of the sensor's field of view (FOV) are symbolically indicated by numerals 25,<br>
and the scene captured by the array is symbolically indicated by numeral 26.<br>
According to the present invention, when the scene 26 is a selected area portion,<br>
the gimbals system directs the center of array 24 towards center 29 of area<br>
portion 26, the line connecting the center of the array to the center of the<br>
selected area portion will be referred to herein as 'line of sight" (LOS). The<br>
sensors' optics may be either separate optics for each sensor, or shared optics<br>
for all/some of the sensors. Shared optics collects hght in a multi-spectral range<br>
and then splits it to each of the sensors according to its unique spectral<br>
waveband. The use of separate versus shared optics will depend on the specific<br>
design goals, taking into consideration available space and required<br>
performance, modularity and maintainability.<br>
Inertial Navigation Systems are well known in the art, and are widely used in<br>
aircraft and in airborne systems for determining with high precision in flight<br>
the aircraft or airborne-system location, its velocity and acceleration vectors,<br>
and its orientation with respect to a stationary, global axes system. The<br>
Inertial Navigation System comprises essentially two separate units (i.e<br>
functions), a Navigational Unit, for determining the location coordinates of the<br>
aircraft or airborne-system, and an Inertial Unit for determining, among<br>
others, the aircraft or airborne-system orientation with respect to a predefined,<br>
fixed, and generally global coordinate system. The INS may also provide the<br><br>
velocity and acceleration vectors of the aircraft or airborne-system. The<br>
Navigational System may use, for example, GPS information, and the Inertial<br>
Unit generally uses inertial sensors within the aircraft or airborne-system.<br>
Sometimes, a less accurate INS in an airborne system is communicating with a<br>
more accurate INS in the aircraft, and aligns itself continuously to the aircraft<br>
INS by using data received from it. The process is called 'transfer alignment',<br>
and is used by many systems to align two INS's (e.g. to align a missile's INS<br>
before dropping it from the aircraft). Once aligned, the less accurate INS<br>
further calculates independently the line of sight direction (angles) with respect<br>
to a global reference system, until the next alignment occurs.<br>
According to a preferred embodiment of the invention, the reconnaissance<br>
system may have various modes of operation, deriving from its capability to<br>
direct the LOS towards any desired direction with the system's field of regard.<br>
Referring to Fig. 3B, the LOS directions may be side-oblique, forward-oblique,<br>
down-looking or arbitrary. Referring to Fig. 3, the following modes of operation<br>
may typically be used:<br>
i. Path Mode: Images are captured along the flight path of the aircraft, with<br>
the line of sight directed forward oblique, down looking or side oblique. The<br>
path trajectory follows the actual aircraft flight path.<br>
ii. Strip Mode: A linear strip positioned along the flight path, or at an angle to<br>
it, is captured. In this mode the line of sight is usually directed side oblique.<br><br>
iii. Spot Mode: Images of a selected area are captured. In this mode the line of<br>
sight may be directed in any arbitrary direction,<br>
iv. Staring Mode: Images of the same selected area are taken successively. In<br>
this mode the line of sight may be directed in any arbitrary direction.<br>
In the last three modes, when the aircraft is approaching the selected area, the<br>
entrance observation angle may be any angle (i.e. start capturing well before<br>
arriving), and when the aircraft is leaving the selected area, the exit<br>
observation angle may be any angle (i.e. stop capturing well after leaving).<br>
This way, the reconnaissance system may linger more time on the selected<br>
area.<br>
Basically, the reconnaissance system may work at either automatic mode or<br>
manual mode, and combine both in the same mission. In automatic mode, the<br>
reconnaissance system provides automatic acquisition of reconnaissance<br>
imagery of preplanned targets and targets of opportunity. The modes selected<br>
for the system may contain any combination of path, strip, spot, and staring<br>
modes. Based on the mission plan, the system automatically configures the<br>
sensors as the aircraft approaches the targets' area, activates the selected<br>
sensors, controls the sensors' direction, and initiates/terminates recording and<br>
transmission of image data. The planning of the mission is done in advance at<br>
the ground station, and is uploaded to the system prior to the mission. In<br>
manual mode, the operator can manually change the mission plan during<br><br>
flight. The operator may interrupt an automatic operation and perform<br>
reconnaissance functions manually. All modes of operation may be available in<br>
both automatic and manual modes.<br>
Referring to Figs. 3 and 3A, according to a preferred embodiment of the<br>
invention, an area of interest is divided into a matrix of a plurality of area<br>
portions. For example, area 100, which is defined by points A, B, C, and D, is<br>
divided into a matrix of a plurality of area portions, for example, portions Pi.i,<br>
Pi,2, Pi,3, P2,i, P2,2, P2,3,..Pn,m, wherein the first subscripted number indicates<br>
the portion column within the matrix, and the second subscripted number<br>
indicates the portion row within the matrix area. Area 100 may assume any<br>
arbitrary quadrangular shape as desired by the mission plan. The size of the<br>
area portions Pn,m varies in accordance with the sensors' FOV and range to the<br>
scene of each area portion. In some cases, and as will be elaborated later, the<br>
area matrix is defined in such a manner that area portions partially overlap one<br>
another, as is shown in Fig. 11, for example, by about 10-20% of their area in<br>
order to assure full coverage of the area of interest, even when the area is a<br>
sharply changed terrain. When stereo photography is required, an overlap<br>
larger than about 56% is required. During the aircraft 102 flight, the gimbals of<br>
the reconnaissance system scan the area matrix 100 in a sequential,<br>
systematic, step-wise manner, by which the gimbals first direct the imaging<br>
array of it to a first area portion and capture its image simultaneously in all<br>
sensors, then to a second area portion to capture its image, and then, repeating<br><br>
this procedure, the gimbals sequentially "jump" the line of sight and field of<br>
view of the array through all the other portions, until completely capturing<br>
images of all the portions of the area of interest 100. For example, the system<br>
may scan the exemplary 9-portion matrix 100 in the following order:<br>
 When the gimbals of the system direct<br>
the light-sensitive array to a specific area portion, generally by directing the<br>
center of the array towards the center of the specific area portion and locking<br>
(i.e fixing the LOS) on it, a "snapshot" is taken, capturing the image of this area<br>
portion. More particularly, the "snap shooting" involves two stages, a light<br>
integration stage during which light from the area of interest is sensed by<br>
components of the array, and a sampling stage during which all the components<br>
of the array are simultaneously sampled at the end of the integration period. As<br>
said, this procedure is sequentially and systematically repeated for all the area<br>
portions of the area 100. Each time an image of a portion is captured, it is saved<br>
in a storage at the reconnaissance system (such as the Solid State Recorder 3 of<br>
Fig. 1), and optionally also transmitted to a ground base station (not shown)<br>
using a Data Link (DL). In order to provide accurate capturing of the image,<br>
location and navigation data are provided in real-time to the gimbals control<br>
unit by the INS.<br>
Fig. 4 is a block diagram illustrating the operation of the reconnaissance system<br>
of the invention. As said, the operation of the system involves three main<br><br>
phases. At the first phase, the line of sight of the array is directed towards a<br>
selected area portion. At the second phase, the array is "exposed" to light<br>
coming from the area portion, and charge is integrated correspondingly within<br>
the array components. During said second phase, motion compensation is<br>
provided by moving the line of sight with the gimbals, in order to compensate<br>
for the aircraft motion and change of orientation during the exposure<br>
(integration) period, particularly in order to eliminate smearing. At the end of<br>
the integration period, at the third phase, all the array light-sensitive sensors<br>
are simultaneously sampled, and the image is stored. Before takeoff, a Digital<br>
Elevation Map 310 of an area, which includes within it at least the area of<br>
interest, is stored at the reconnaissance system. The Digital Elevation Map 310<br>
is a digital file reflecting a map divided into a grid, wherein for each nodal point<br>
of the grid, the x-y coordinates (with respect to a global or predefined<br>
coordinates system) and the elevation z at that point are provided. The portion<br>
selection block 311 selects an area portion. More particularly,' the portion<br>
selection block 311 sequentially indicates a nodal point being a center of a<br>
selected area portion within the DEM 310, causing the DEM 310 to convey the<br>
coordinates of the center of the area portion to the servo control unit 305. The<br>
concept of finding the 3D center coordinates of a selected target using a DEM,<br>
as described in the present invention, can also be used in systems other than<br>
reconnaissance systems, such as targeting systems, where sometimes it is<br>
desired to measure the exact range to the scene or a selected target without<br>
employing active range finders. Preferably, and as will be elaborated later,<br><br>
several selection modes exist for selecting an area portion, and determining its<br>
borders, or more particularly, determining its central nodal point. The xp: yp<br>
coordinates of the center point of the selected area portion, and the elevation<br>
coordinate zp of the same point are conveyed to the servo control unit 305. The<br>
area portion direction module 306 of the servo control unit also periodically<br>
receives from the INS 303 the xa : y a: za coordinates of the center of the on-<br>
gimbals array. Having these two sets of x-y-z coordinates, the area portion<br>
direction module 306 geometrically calculates the gimbal angles required for<br>
establishing a Line Of Sight (LOS) between the center of the array (xa :ya : za)<br>
and the center of the selected area portion (xp :yp :zp) and converts said angles<br>
to the analog signals required by the gimbals servo unit 308 for estabhshing<br>
said direction of the gimbals 300. The said direction calculation is repeated and<br>
updated in short time intervals, in order to account for the change in the<br>
aircraft location and orientation. The gimbals servo unit 308 receives a signal<br>
315 from the gimbals unit indicating the state of the gimbals with respect to the<br>
desired LOS direction. When it determines that the LOS direction has been<br>
established, the servo unit conveys a signal 321 to the Integration/Sampling<br>
unit 304, for initiating the integration period. The Integration/Sampling unit<br>
304 provides a signal 322 to the array 301, causing it to begin light integration<br>
of incoming light from the area portion. From that instance, the light sensitive<br>
components of the array begin to accumulate charge relative to the level of light<br>
at each corresponding section of the area portion. During the integration period,<br><br>
motion compensation is repeatedly calculated by the motion compensation<br>
module 307. The motion compensation module 307, in similarity to the area<br>
portion direction module 306, also receives from the DEM the (xp :yp :z )<br>
coordinates of the center of the selected area portion and from the INS the<br>
(xa :ya'.za) of the center of the on-gimbals array 301. The motion compensation<br>
module 307 repeatedly calculates the gimbal angles required for estabhshing a<br>
Line of Sight (LOS) between the updated coordinates of the center of the array<br>
(xa : yd :za) as received from the INS, and the center of the selected area portion<br>
(xp :yp :zp) and accordingly converts said calculated angles to analog signals<br>
required by the gimbals servo unit 308 for estabhshing said direction (i.e., said<br>
angles) of the gimbals 300, or in other words, to repeatedly compensate for the<br>
motion and change of orientation of the aircraft during the integration period.<br>
Motion compensation for image roll around the LOS may also be done, by using<br>
an additional de-roll gimbals, but this is typically not needed in small or<br>
medium-sized arrays due to the small smearing effect of roll in such arrays. At<br>
the end of the integration period a "sampling" signal 322 is provided to the<br>
array 301, for simultaneously sampling the accumulated charge levels within<br>
all the array sensors, and storing in storage 302 the said charge levels as an<br>
image of the selected area portion. The image storage 302 is essentially the SSR<br>
3 of Fig. 1. Next, the portion selection block 311 selects a center of a next area<br>
portion from the DEM 310, and conveys the same to the area portion direction<br>
module 306, and the same procedure as described above repeats for this next<br><br>
area portion. The procedure repeats for all the area portions of the area of<br>
interest. It should be noted herein that there are several optional modes by<br>
which the portion selection block 311 operates. In one optional case, the portion<br>
selection block 311 first selects a center of a first area portion and obtains its<br>
image, and then, for any additional portion it determines in real time the center<br>
of a next area portion which satisfies, for example, a requirement of 10%<br>
overlap between the present portion and the previously imaged portion. This<br>
optional case will be described in more detail hereinafter. In another mode of<br>
portion selection, the pilot marks the center of a portion, and the image of that<br>
portion is accordingly obtained after carrying out the above procedure of<br>
direction, motion compensation, integration and sampling. In still another<br>
optional selection mode, all portions of the area of interest and their center are<br>
predefined, for example while the airplane is on the ground, and the selection is<br>
then carried out according to said predefined order, while during the flight the<br>
exact directions are updated automatically based on the actual position and<br>
attitude of the aircraft.<br>
The selection of an area portion, so that a pre-defined overlap will exist between<br>
successive images, is dependent on the overall geometric scenario including<br>
aircraft position with respect to the scene and the ground variations of the<br>
captured scene. Referring to Figs. 11 and 11A, for each snapshot, the footprint<br>
of the sensor's FOV on the ground is calculated using the 3-dimensional ground<br>
data of the DEM. Each footprint is a 3-dimensional plane, or a higher order<br><br>
surface, tilted in two directions in order to best fit the ground's gradients. After<br>
taking a snapshot, and before taking the next snapshot, the system estimates<br>
the direction of the LOS center using extrapolation from the previous snapshots<br>
or other techniques, and calculates the estimated ground footprint of the next<br>
snapshot. The overlap between this estimated footprint and the footprint of the<br>
previous snapshot is calculated by projecting the former on the latter, and then<br>
the direction of the LOS center is modified in order to ensure an overlap within<br>
a specified range of values. This process repeats iteratively a few times until the<br>
required overlap is achieved, and then the LOS is physically moved to the<br>
location of the new snapshot. The calculation may also be done analytically<br>
without iteration, depending on the mathematical model and the computing<br>
resources available to the reconnaissance system. The number of jumps of the<br>
LOS across track (i.e. along a row), which determines the width of the<br>
photographed strip, is continuously calculated to ensure maximum strip width<br>
without creating an excessive lag in jumping along track to the next row to<br>
compensate for the aircraft progression.<br>
Gimbals having at least two degrees of freedom are well known in the art, and<br>
are used, for example, in some aircrafts for inertial directing and tracking the<br>
targeting system to a target, a targeting system being a system enabling<br>
observation of targets and directing weapon systems towards them. Some of<br>
said targeting systems also use navigational and orientation data from an<br>
Inertial Navigation System for calibrating the tracking in real time. The<br><br>
present invention uses a gimbals system similar to the one used in said<br>
airborne targeting systems. The "tracking" of the gimbals with the array to the<br>
relevant area portions, each portion in its turn, is performed in a similar<br>
manner as some airborne targeting systems direct and track their weapons to a<br>
target. However, in such targeting systems the area of interest is not divided<br>
into a plurality of matrix-type portions, and there is no systematic scanning of<br>
the relevant area portions within an area of interest in a sequential step-wise<br>
manner. Moreover, in the targeting systems, the problem of motion<br>
compensation is solved by other means, such as electro-optical tracking based<br>
on image information, a solution that is not practical for area scanning systems,<br>
such as the reconnaissance system of the invention. As said, the problem of<br>
motion compensation is implemented in a completely different manner in some<br>
of the prior art reconnaissance systems, for example an on-chip motion<br>
compensation is disclosed in US 5,155,597, US 5,692,062, and WO 97/42659.<br>
The invention uses a combination of the INS and gimbals system having at<br>
least two degrees of freedom for (a) carrying out area scanning by which a<br>
"jump" between area portions is performed in a coarse manner, and (b) motion<br>
compensation during the integration period of an area portion which is<br>
performed in a fine manner. The facts that the elevation (i.e. altitude) at the<br>
center of each selected portion, as obtained from the DEM, and that the gimbals<br>
system has at least two degrees of freedom, enables full and fine motion<br>
compensation in all axes. This structure, as will be elaborated later, enables the<br>
aircraft to carry out sharp maneuvers in a superior manner in comparison to<br><br>
reconnaissance systems of the prior art. Furthermore, it has been found by the<br>
inventors that due to the longer sensors' exposure time to light (i.e., longer<br>
integration time) which becomes possible in the system of the invention due to<br>
the use of staring arrays, there is essentially no need for a TDI structure in<br>
which a same area pixel is scanned N times, and then averaged. The system<br>
instead can use a single "snapshot" capturing of each area portion.<br>
Fig. 5 is a flow diagram describing the operation principles of the<br>
reconnaissance system of the invention. The flow diagram of Fig. 5 assumes<br>
that an area of interest has been defined, more particularly, the borders of the<br>
area of interest. In a preferable embodiment of the invention, the center of the<br>
first portion of the area of interest to be imaged is determined automatically.<br>
For example, if the area of interest is viewed as a matrix, the first portion may<br>
be the farthest, leftmost portion, and its center is selected automatically by<br>
means of some predefined manner. Next, the centers of all the other area<br>
portions are determined in real time, in order to satisfy some predefined range<br>
of overlap between the images, as explained before. The definition of that range<br>
is necessary in order, on the one hand, to assure that no "holes" of imaging<br>
exist, and on the other hand, that no extreme overlapping exists between<br>
images of adjacent portions, involving more images than necessary. As will be<br>
elaborated hereinafter, this procedure involves using the DEM 310.<br><br>
In block 500, the center coordinates x1; y1, of the first selected portion is provided<br>
to the DEM 501, which in turn conveys the set x1: y1: z1 (z1 being the elevation<br>
at x1;y1) to the servo control unit 305. The Servo control unit 305, which also<br>
receives the real time present coordinates of the center of the array xa: ya: za<br>
from the INS (step 504), also calculates in step 503 the angles and signals<br>
required to establish a line of sight between the center of the array and the<br>
center of the said first area portion x1:y1:z1. The signals are conveyed to the<br>
gimbals servo unit, which establishes the desired LOS direction in step 505. In<br>
step 506, a check is made to determine whether the establishment of the proper<br>
direction has been completed. Of course, this is a dynamic operation which is<br>
repeated in a very frequent and rapid manner to perform correction according<br>
to the airplane progression, and any change of orientation, for example due to<br>
elasticity or aircraft maneuvering, as being reported by the INS 504, which in a<br>
preferable case has its inertial sensors on the gimbals. In step 507, the light<br>
integration by the array components takes place. Simultaneously, during the<br>
integration period a motion compensation takes place, again, to account for the<br>
aircraft progression and any change of its (or more particularly of the gimbals<br>
array) orientation. This operation is also performed repeatedly, in real time,<br>
with high frequency, typically around 100 Hz, that guaranties highly accurate<br>
motion compensation during the time of integration. At step 509, a check is<br>
made to determine whether the integration period has lapsed. At the end of the<br>
integration period, the light integration by the array terminates (step 511) and<br>
the motion compensation of step 508 may also terminate (step 510). At step<br><br>
512, all the array sensors are sampled at the same time (in a snap-shot<br>
manner), and the image is stored. At step 513, a check is made whether all the<br>
area of interest has been covered by the images already taken. In the<br>
affirmative case, the procedure terminates (step 514). If, however, the full area<br>
of interest has not been covered yet, the application assumes x: y coordinates of<br>
a next area portion (step 515), which are conveyed to the DEM to obtain the<br>
elevation z at the same portion center. The next area portion may either be<br>
located across-track (same row) or along track (new row) from the previous area<br>
portion, depending on the calculated row width so that a maximum strip width<br>
is achieved while not lagging behind the aircraft progression. Next, in a<br>
preferable case (step 516) a simulation is made to determine whether, if an<br>
image is taken when directing to said x:y:z coordinates, the overlapping area<br>
between the area of the previous image and the area of the said new image<br>
satisfies a predefined overlap range (for example between 10%-20% overlap). If<br>
the overlap is found to be too great, this center point is then positioned slightly<br>
farther from the center of the previous portion. If, however, the overlap is found<br>
to be too low, the portion center is positioned slightly closer to the center of the<br>
previous portion. This simulation uses the DEM, which is essentially a digital<br>
map which also includes elevation at all the grid nodal points. The use of the<br>
DEM for that purpose is advantageous, as the elevation is of high importance<br>
when checking the overlapping issue. It has been found that after one or two<br>
repeated simulations, a new portion center can be determined. The alignment of<br>
step 516 as described is preferable, but not essential. From step 516 the<br><br>
procedure continues to step 503 using the new x: y: z coordinates of the portion<br>
center as determined at step 516, and the procedure is repeated until coverage<br>
of all the portions of the area of interest is completed. Then, the system may<br>
switch to scan a new area of interest if such is desired. It should be noted that<br>
although it is preferable to scan an area of interest in a sequential order, as it<br>
simplifies the portions overlapping calculations and improves the scanning<br>
efficiency, the scanning may also be performed in any other, predefined (or not)<br>
order.<br>
As said, when directing the array to the center of an area portion, and<br>
compensating for the aircraft motion and the orientation change, the elevation<br>
of the relevant area portion is of particular importance for obtaining high<br>
accuracy. Therefore, according to a preferred embodiment of the invention, a<br>
Digital Elevation Map (DEM), i.e., 3-D map which includes a grid of the area of<br>
interest, is used. Generally, there is no problem in obtaining such Digital<br>
Elevation Maps of almost any area in the world; such information is<br>
commercially available, or can be extracted from a topographic map of the area.<br>
The DEM of the area of interest is loaded into the airborne system before the<br>
reconnaissance mission, generally to the SSR 3.<br>
As said, the reconnaissance systems of the prior art, for example, as disclosed in<br>
US 5,155,597, US 5,668,593, US 5,692,062, WO 97/42659, US 6,130,705, and<br>
US 6,256,057, do not directly consider the elevation of the terrain at the<br><br>
imaging area portion while calculating the Forward Motion Compensation<br>
(FMC), or Motion Compensation (MC) in general. Some of those systems, for<br>
example, as disclosed in US 5,692,062, US 6,130,705, use image-to-image<br>
correlation in order to indirectly estimate the smearing effects of the terrain<br>
variations and correct them using on-chip techniques, but these techniques<br>
have drawbacks as described before, and therefore are not considered in the<br>
following discussion. These drawbacks include the need for three successive<br>
image captures for each usable image, limited correlation accuracy due to<br>
smearing of the first two images, large pixel-shift between successive images,<br>
and varying V/R during the 3-image capture process. Fig. .9 exemplifies the<br>
significance of the elevation (i.e. altitude) factor, and shows how important it is<br>
to consider directly and in real-time the elevation of the imaged terrain.<br>
Generally, in prior art reconnaissance systems, the aircraft INS, when used,<br>
computes the aircraft position and attitude with respect to a global system, but<br>
has no knowledge whatsoever of the actual shape of the terrain 751 being<br>
photographed. The aircraft 750 therefore assumes some fixed, ground level 756,<br>
generally sea level, as depicted. Now, if the reconnaissance system, photographs<br>
the area whose center point is A, during the exposure time, the aircraft 750<br>
progresses a distance D. The reconnaissance LOS (Line of Sight) will keep<br>
pointing at point C assuming a fixed level 756, thus shifting point A to point B<br>
during the exposure time, and for the mountain 757, a smear of magnitude AB,<br>
or error angle 752 is created.<br><br>
Example 1<br>
The following is a numeric example for assessing the pixels smear in the prior<br>
art systems when capturing a terrain with large variations, showing a forward<br>
oblique scenario: Aircraft flying at a velocity of 250m/s, range from the aircraft<br>
to scene of 10km to level ground, exposure time of 10ms, and assuming<br>
mountain 757 is at 1/2 range (i.e. 5km), and altitude of aircraft is 5km.<br>
Therefore, the angular velocity of the LOS is approximately 12.5milirad/s<br>
(aircraftVelocity / aircraftAltitude) x SIN2(LOSdepressionAngle), and the<br>
angular travel of the aircraft during integration time is 12.5 x 0.01 = 125<br>
microrad. The angle 752 is therefore 125 microrad; for a typical pixel<br>
Instantaneous FOV (IFOV) of 30 microrad this means a smear of more than 4<br>
pixels.<br>
The situation is even worse for a side oblique scenario, where, in this example,<br>
the angular velocity is 250 / 10,000 = 25milirad/s, and the resulting smear is 8<br>
pixels.<br>
For the system of the invention, the pixel smear is much smaller, as<br>
demonstrated in the following example. The error in the LOS angular velocity<br>
due to range R and velocity V uncertainties is calculated by the following<br>
formula:<br><br><br>
For Vnom of 200m/s, Rnom of 15km, LOS depression angle of 20 degrees, terrain<br>
slope up to 15%, a typical INS velocity error of 0.045m/s, a typical LOS angular<br>
error of 2mrad, a typical aircraft position error of 30m, a typical aircraft<br>
altitude error of 42m, and a typical DEM altitude error of 15m, we calculate (all<br>
values 3σ) the range error as 160m, and the LOS angular velocity error:<br><br>
The pixel smear during the integration time of 10ms will then be 0.14 x 0.01 =<br>
1.4 microrad, which, for a typical IFOV of 30 microrad is a small sub-pixel<br>
smear, i.e., a smear of less than 5% of a pixel.<br>
Various types of scanning may be used in the system of the invention, as<br>
follows:<br>
a.	Sequential matrix scanning: The portions of the area of interest are<br>
captured according to their sequential order within the area matrix.<br>
b.	Selective scanning: Any selection can be predefined, and the portion<br>
capturing is performed accordingly.<br>
c.	Manual capturing: the capturing of an area portion is carried out<br>
manually, according to the pilot selection.<br>
It should be further noted that a stereoscopic imaging could also be obtained by<br>
the system of the invention. Unlike systems of the prior art, especially those<br>
disclosed in US 5,155,597, US 5,692,062, and US 6,256,057, the system of the<br><br>
present invention can enhance the stereoscopic effect by "revisiting" an area<br>
portion after the aircraft progressed to a substantial angular displacement with<br>
respect to the area portion. Fig. 7 shows how a stereoscopic image can be<br>
constructed by the present invention. As is known in the art, a stereoscopic<br>
image of an area or object can be constructed by using two images, each<br>
covering a substantial overlapping portion of the area or object, if said two<br>
images are taken from two points of views angularly remote enough one from<br>
the other with respect to the imaged area. In Pig. 7, the reconnaissance aircraft<br>
200 flies from the left to the right, in the route as shown. When passing, for<br>
example point A, the aircraft sequentially captures the portion images 201, 202,<br>
and 203, by directing the on-gimbals array to these portions accordingly.<br>
Thereafter, the aircraft continues to point B, at which the gimbals are directed<br>
again to capture correspondingly the images 201', 202', and 203' of the same<br>
area portions; however, now, from the point of view of B. If a substantial<br>
portion, for example, about 56%, of each area portion overlaps in the images as<br>
taken from points A and B respectively, a stereoscopic image of the area portion<br>
can be constructed in a known manner. As shown, the invention provides an<br>
easy, simple, and compact manner for obtaining the images required for<br>
constructing stereoscopic images. When interpreting reconnaissance photos,<br>
the stereoscopic effect may be obtained, for example, by displaying the two<br>
images of the same area portion at different light polarities, and then viewing<br>
the display using polarizing glasses, directing one image to the left eye and the<br>
other to the right eye.<br><br>
In a more preferred embodiment of the invention, two Inertial Navigation<br>
Systems are used by the reconnaissance system of the invention.. More<br>
particularly, and as is shown in Fig. 6, the INS 303 of Fig. 4 comprises two<br>
separate Inertial Navigation Systems. The first INS is the aircraft main INS<br>
604, usually combined with GPS, and the second INS is the Internal<br>
Reconnaissance INS 603. As said, the INS 303 is used for providing<br>
navigational data, such as the present coordinates of the aircraft with respect to<br>
a predefined, preferably global coordinates system, and orientation data<br>
relating to the orientation of the aircraft with respect to said global coordinates<br>
system. This data has to. be very accurate, and has to be continuously updated<br>
in order to assure accurate direction to the captured area, and not less<br>
important, in order to assure accurate motion compensation, even during fast<br>
and sharp maneuvers of the aircraft. This task involves even more<br>
complication, as due to the-elasticity of the aircraft the portions suffer from very<br>
high accelerations, and very intense airflow. Therefore, in order to best direct<br>
the array of light-sensitive sensors, and to best compensate for the aircraft<br>
motion, it has been found by the inventors that it is essential to position an INS<br>
inside the reconnaissance system, and preferably on the gimbals themselves, for<br>
measuring navigation and orientation data of the gimbals, with respect to a<br>
predefined global coordinates system. Therefore, the Internal Reconnaissance<br>
INS is preferably positioned on the gimbals, proximate to the array that is<br>
preferably positioned on the gimbals as well, and accurately measures said<br><br>
data. However, as the Internal INS must be limited in size, and therefore may-<br>
suffer from some inaccuracies and drifts, according to the present invention the<br>
Internal INS 603 is connected to the Aircraft Main INS 604. The main aircraft<br>
INS periodically updates the internal INS with navigational data for aligning it<br>
for possible drifts, using the prior art transfer alignment process described<br>
before. In such a manner wider bandwidth and higher accuracy of the gimbals<br>
servo is obtained. Typical values for mechanical misalignment between the<br>
aircraft and the reconnaissance system LOS are 10-20 mrad, while the aligned<br>
on-gimbals INS can measure this misalignment to an accuracy of 1-2 mrad.<br>
It should be noted that the area portions are typically captured by the system of<br>
the invention in a "snapshot" manner, typically with much longer integration<br>
time than the vector or array in systems of the prior art. While the typical<br>
integration period in the system of the invention is in the order of milliseconds,<br>
in systems of the prior art using vectors, such as, US ,5,155,597, US 5,692,062,<br>
and US 6,256,057, the typical integration periods are two to three orders of<br>
magnitude shorter (i.e., between 100 to 1000 times shorter), resulting in a much<br>
lower photonic sensitivity to light. This evolves from the fact that the invention<br>
uses an array having several hundreds or thousands of rows and columns, in<br>
which each pixel is exposed to light during all the area- capturing time. In the<br>
prior art systems using a one-dimension vector, a same capturing period is<br>
divided between the pixels of a same vector, which must be exposed hundreds<br>
or thousands of times in order to cover a same area portion. Moreover, the fact<br><br>
that the system of the invention allows great flexibility in selecting areas and<br>
portions within areas of interest enables a significant reduction in the amount<br>
of data that the system has to deal with (i.e., to capture, store, and/or transfer<br>
by communications means). More particularly, images of only portions and<br>
areas of interest are captured by the system of the invention.<br>
EXAMPLE 2<br>
The following is an example showing the saving in the amount of data which<br>
the system of the invention handles (i.e., storing, transmitting to a ground<br>
station, etc.), in comparison with a pushbroom or a large-sized array<br>
reconnaissance system:<br>
-	Mission duration: 2 hours;<br>
-	Fig. 12 illustrates this scenario. Area of high priority targets with<br>
respect to the photographed area: 40% for snap shooting. The term<br>
"snapshot", refers herein to a manner in which all the array pixels are<br>
simultaneously exposed to light from an area portion, and data from all<br>
the array pixels simultaneously is read at the end of the said exposure;<br>
5% for pushbroom or large-sized array - due to the efficiency of LOS and<br>
FOR mission planning (this is an assumption resulting from the ability<br>
of the system of the invention to better select high priority targets<br>
within an area of interest, and to ignore area portions of no interest);<br>
-	Sensors' data throughput rate uncompressed: 20 Mbytes/s<br>
-	Low compression rate: 1:5<br><br>
-	High compression rate: 1:10<br>
-	Overlap area for snapshots reconnaissance (according to the invention):<br>
40% total of along-track and across-track overlap;<br>
-	Overlap area for pushbroom: 20%;<br>
Total recording:<br>
1.	Snapshooting = (2hr x 60 x 60) x 20MB/s x (0.4/5 + 0.6/10) x 1.4 = 28 GB<br>
2.	Pushbroom = (2hr x 60 x 60) x 20MB/s x (0.05/5 + 0.95/10) x 1.2 = 18 GB<br>
As said, the number of high priority targets obtained for snap-shooting<br>
reconnaissance (according to the invention) is 40% / 5% = 8 times higher than<br>
for push-broom or large-sized array reconnaissance, and therefore the overall<br>
efficiency of the mission is: 8 x (18/28) = 5.1 in favor of snapshooting according<br>
to the system of the invention.<br>
This is a significant increase in efficiency.<br>
It should be noted herein that the reconnaissance system of the invention, by<br>
directing the LOS of the array/s of light sensitive sensors using gimbals with at<br>
least two degrees of freedom, enables the defining of an area of interest of<br>
arbitrary shape which is divided, preferably in real time, to a plurality of area<br>
portions that are sequentially scanned in a stepwise systematic and accurate<br>
manner to obtain images of those portions until covering the full area. The<br><br>
system of the invention enables not only efficient covering of a specific area of<br>
interest, it also eliminates the need for providing dedicated means for forward<br>
motion compensation, as required in reconnaissance systems of the prior art,<br>
for example in US 5,155,597, US 5,668,593, US 5,692,062, WO 97/42659, US<br>
6,130,705, and US 6,256,057. By directing the LOS of the array using gimbals<br>
with at least two degrees of freedom, and by continuously correcting the<br>
direction to the selected area portion during the "exposure", not only forward<br>
motion compensation with respect to the forward axis is provided, but also<br>
consideration is made to the'3D shape of the terrain for providing improved<br>
motion and orientation compensation with respect to all three axes. This fact is<br>
of particular importance, as it enables sharp and wide maneuverings of the<br>
aircraft. Moreover, no matter where the aircraft is positioned with respect to<br>
the area of interest or to any portion within said area, and no matter in what<br>
orientation, the system provides means for obtaining appropriate images of<br>
such area portions (assuming no obstruction from the aircraft body).<br>
Example 3<br>
The invention was successfully implemented with the following parameters:<br>
Airborne pod configuration;<br>
Number of pixels: Visual array: 2000 x 2000, IE array 640 x 480;<br>
Integration times: 1-15 ms;<br>
Operational ranges: up to 30 km and altitude up to 10 km;<br>
Snapshot rate: 3 per second, both sensors arrays operating simultaneously.<br><br><br>
: FOR: full spherical coverage, excluding a ±30 degrees backward looking cone;<br>
See also Figs. 11 and 11A that are the result of an actual simulation of the<br>
scanning and overlapping process.<br>
As said, in the embodiment of the invention as described above, the area<br>
scanning operation is performed by the gimbals that first direct the center of<br>
the array's LOS to the center of the relevant area portion, then, an exposure of<br>
the array to the light coming from the area portion occurs (i.e., the array starts<br>
integration of light from the area portion), and at the end of the exposure period<br>
the image of the area portion is captured. Next, the array's LOS is directed to<br>
the center of a next area portion, and the procedure of integration and<br>
capturing repeats. This procedure repeats for all the area portions of the area of<br>
interest. More specifically, in the above embodiment the gimbals operate during<br>
the scanning of the area of interest in a "jumping" manner, by which the<br>
gimbals first move until an alignment to the predefined area portion is<br>
obtained, then the gimbals are stationary during the exposure period (except for<br>
motion compensation movement), and next the gimbals move again to direct the<br>
array to the center of the next area portion, etc. This type of operation is limited<br>
in the number of snapshots per second as demonstrated in Example 3, as the<br>
acceleration-deceleration and stopping of the relatively heavy gimbals is time-<br>
consuming.<br><br>
According to still another embodiment of the invention, the scanning efficiency<br>
is improved by using a back-scanning mechanism, as is shown in Fig. 9. As<br>
said, in the "jumping"-type scanning, the array and. its associated optics is<br>
preferably positioned on the internal gimbals, and all these are stationary with<br>
respect to said internal gimbals. In the improved, back-scanning embodiment of<br>
the invention a back-scanning array assembly 860 is mounted on the internal<br>
gimbals. The assembly essentially comprises lenses 861, and 862, stationary<br>
mirror 864, and a low-mass single or dual-axis back-scanning rotating mirror of<br>
prism 863. The back-scanning rotating mirror is mounted on a dedicated motor.<br>
In the back-scanning method, the scanning of the area portions is performed<br>
continuously, or, in other words, the gimbals continuously scan columns (or<br>
rows) of the area of interest at high LOS angular velocity, and this scanning<br>
movement comes on top of the direction alignment and motion compensation, as<br>
described above. Whenever an exact alignment to the center of an area portion<br>
is obtained, the light integration (exposure of the array) period begins, and the<br>
back-scanning mirror 863 compensates for the scanning continuous movement<br>
of the gimbals, only during the integration period by maintaining angular<br>
movement to the opposite direction in half the angular velocity. With reference<br>
to Fig. 10, if the gimbals maintain a scanning constant angular inertial velocity<br>
in the direction 870, the back-scanning mirror 863 rotates during the<br>
integration period (only) in the opposite direction 880, in half the angular<br>
velocity. In this manner, the area portion is maintained stationary at the array<br>
305. At the end of the integration (exposure) period, the mirror 863 returns to<br><br>
its initial position, until a new integration period, in which the same constant<br>
movement of the mirror repeats. The back-scanning enables the gimbals to<br>
move at higher velocity without having to stop for each snapshot. Stopping the<br>
gimbals consumes most of the duty cycle time due to the high<br>
acceleration/deceleration and to the high inertial mass of the gimbals and their<br>
payload (i.e. the sensors). The back-scanning mirror can move much faster<br>
thanks to its much smaller inertial mass.<br>
EXAMPLE 4<br>
The angular range of the back-scanning mirror is very small. For example, if<br>
the gimbals move at 60 deg/s and the exposure time is 10 ms, the angular<br>
displacement of the back-scanning mirror is 60 x 0.01 = 0.6 deg, which is very<br>
small.<br>
A typical comparison: Using gimbals without back-scanning enables a snapshot<br>
rate of 3 frames per second in a typical installation (e.g. Bxamle 3). The average<br>
gimbals velocity, for a Field of View of 3 degrees, would be approximately 3x3<br>
= 9 deg/s. On the other hand, using back-scanning, the gimbals can move at a<br>
velocity of 60 deg/s, resulting in 60 / 3 = 20 snapshots/s, a rate which is more<br>
than 6 times higher. The maximum allowable rate is limited by the electronic<br>
frame rate of the sensor, which is typically 30 or 60 Hz,and therefore higher<br>
than 20 Hz.<br><br>
Fig. 8 and Fig. 8A illustrate a specific case in which the present invention is<br>
advantageous over prior art reconnaissance systems. Suppose that the aircraft<br>
700 has a mission to obtain images of the two posts 705, and of a highway 710<br>
located between the mountains. In a reconnaissance system of the prior art, in<br>
which the camera (i.e., the vector of light sensitive elements) is essentially fixed<br>
with limited FOR, the aircraft can cover from point Q (while having a field of<br>
view limited by lines 701 and 702) only the area between points B and D of the<br>
first (right) mountain and the area between points A and C of the second (left)<br>
mountain. Images of the two posts 705 and of the highway 710 are not<br>
obtained. In long range oblique photography (LOROP) the aircraft will fly<br>
towards the page, and the LOS will be side-oblique with substantial obscuration<br>
due to the small LOS depression angle. If the camera, however, is not fixed, as<br>
in the case of the prior art push-broom or whiskbroom systems, there is also no<br>
assurance of full coverage of the posts 705 and highway 710, as there is no<br>
synchronization between the movement of the field of view of the camera and<br>
the shape of the terrain. According to the present invention, this problem is<br>
easily solved. While preparing the mission and the division of the area of<br>
interest, it is possible to select any coordinate to be the center of an area of<br>
interest, and to program the reconnaissance system to direct its array/s' LOS to<br>
these selected coordinates from any predefined or manually selected location of<br>
the aircraft, so long as the LOS is within the system's FOR. Therefore, as<br>
shown in Fig. 8A, while being at location R the array's LOS may be forward<br>
directed to cover the area between points F and E, and later, while reaching<br><br>
point S, the array's LOS may be backward directed to cover the area between<br>
points G and H. In that case, the full area between the mountains, including<br>
the two posts 705 and the full image of the highway, can be obtained. The lines<br>
711 and 712 indicate the limits of the field of regard of the reconnaissance<br>
system from point R, while the lines 721 and 722 indicate the limits of the field<br>
of regard of the reconnaissance system from point S.<br>
Fig. 11 is a perspective view of a hilly terrain 560, and its division into area<br>
portions, including some overlap between area portions. Also shown in Fig. 11 is<br>
an instance in which one area portion is captured by aircraft 561, and a later<br>
instance in which another area portion, 564, is captured by the same aircraft.<br>
Fig. 11A shows an upper view of the same terrain, and the manner of scanning<br>
of the area of interest by the aircraft reconnaissance system, as indicated by the<br>
arrows 565.<br>
To summarize, the present invention is characterized by the following main<br>
advantages over the prior art systems:<br>
- The ability to photograph in any LOS direction within a large Field of<br>
Regard (FOR). In order to have this ability (e.g., forward-oblique, side-<br>
oblique, down, and arbitrary looking) the prior art reconnaissance<br>
requires use of separate light- sensing units or a plurality of separate<br>
pods. This ability of the present invention enables the coverage of more<br><br>
targets (i.e., area portions) during a mission, with reduced storage<br>
requirements;<br>
-	The ability to photograph in any aircraft flight direction within a large<br>
FOR;<br>
-	The ability to focus on selective quality targets for a long duration and<br>
get many pictures at the highest quality while the aircraft is<br>
progressing. The overall mission area is not recorded, but only selective<br>
portions, thus saving storage;<br>
-	The ability to photograph in terrain with large variations, by directing<br>
the LOS so that no obscuring will occur;<br>
-	The ability to photograph while the aircraft is maneuvering, thus<br>
increasing mission flexibility and aircraft survivability;<br>
-	The ability to operate manually or automatically in the same mission.<br>
While some embodiments of the invention have been described by way of<br>
illustration, it will be apparent that the invention can be carried into practice<br>
with many modifications, variations and adaptations, and with the use of<br>
numerous equivalents or alternative solutions that are within the scope of<br>
persons skilled in the art, without departing from the spirit of the invention or<br>
exceeding the scope of the claims.<br><br><br>
WE CLAIM:<br>
1. An airborne reconnaissance system for obtaining images from an area of<br>
interest comprising:<br>
-	Gimbals having at least two degrees of freedom;<br>
-	At least one array of light sensors positioned on the gimbals, for being<br>
directed by the same within at least two degrees of freedom;<br>
-	Map storage means for storing at least one Digital Elevation Map of an<br>
area of interest, divided into portions;<br>
-	Inertial Navigation System for real-time providing to a gimbals control<br>
unit navigation and orientation data of the aircraft with respect to a<br>
predefined global axes system;<br>
-	Portion selection unit for selecting, one at a time, another area portion<br>
from the area of interest;<br>
-	Servo control unit for:<br>
A.	Receiving from said Digital Elevation Map one at a time, a<br>
coordinates set of the selective area portion, said set comprising the x: y<br>
coordinates of said area portion, and the elevation z of the center of that<br>
portion;<br>
B.	Receiving continuously from said inertial navigation system<br>
present location and orientation data of the aircraft;<br>
C.	Repeatedly calculating and conveying into a gimbals servo unit<br>
in real time and at a high rate signals for:<br><br><br>
a.	during a direction period, signals for directing accordingly<br>
the gimbals including said at least one array of light-sensing units towards<br>
said x: y : z coordinates of the selected area portion, and;<br>
b.	during an integration period, in which the array sensors<br>
integrates light coming from the area portion, providing to the gimbals unit<br>
signals for compensating for the change in direction towards the x: y : z<br>
coordinates of the selected portion evolving from the aircraft motion;<br>
-	Gimbals servo for effecting direction of the gimbals in at least two degrees<br>
of freedom as claimed in the signals provided from said Servo Control Unit;<br>
-	Sampling means for simultaneously sampling at the end of the integration<br>
period pixel levels from each of said array sensors, a set of all of said<br>
sampled pixel levels forms an image of said area portion; and<br>
-	Storage means for storing a plurality of area portion images.<br>
2.	System as claimed in claim 1, wherein said one or more arrays are<br>
selected from at least a visual light-sensitive array, a UV light sensitive-<br>
array, an infrared light-sensitive array, a multi/hyper-spectral array, and<br>
an active illumination array.<br>
3.	System as claimed in claim 1, wherein said navigation data of the<br>
aircraft comprises data relating to the 3D location of the aircraft, and its<br>
velocity and acceleration vectors with respect to a predefined coordinates<br><br><br>
system, and its orientation data relating to the orientation of the aircraft<br>
with respect to said predefined coordinates system.<br>
4.	System as claimed in claim 1, wherein said Inertial Navigation System<br>
comprises velocity, acceleration, and orientation sensors, at least some of<br>
said sensors being positioned on the gimbals.<br>
5.	System as claimed in claim 1, wherein at least some of said arrays of<br>
sensors being positioned on the gimbals.<br>
6.	System as claimed in claim 1, comprising two Inertial Navigation<br>
Systems, the first inertial navigation system being the main Inertial<br>
Navigation System of the aircraft and its sensors being positioned within<br>
the aircraft, and the second Inertial Navigation System being a system<br>
dedicated to the reconnaissance system, at least some of the sensors of said<br>
second Inertial Navigation System being positioned on the gimbals unit,<br>
measuring navigation and orientation data of the gimbals with respect to<br>
the said predefined axes system, for better eliminating misalignments<br>
occurring between the gimbals and LOS and the said main Inertial<br>
Navigation System of the aircraft due to aero-elastic deflections and<br>
vibrations of the aircraft, by using a process of transfer alignment from the<br>
said first INS to the said second INS.<br><br>
7.	System as claimed in claim 1, wherein the Digital Elevation Map is a<br>
map comprising a grid of the area of interest, the x: y : z coordinate values<br>
at each of the nodal points in said grid being provided by said map.<br>
8.	System as claimed in claim 1, wherein the portion selecting unit is used<br>
for calculating and determining a center of a next area portion that<br>
provides a predefined overlap between the said imaged area portion and the<br>
adjacent previously imaged area portion.<br>
9.	System as claimed in claim 1, wherein in an automatic mode of<br>
operation the gimbals are activated to cover in a sequential, step-wise<br>
manner, the area of interest, said coverage is made from a predefined<br>
starting portion and according to a stored mission plan, thereby<br>
sequentially scanning one after the other area portions of the area of<br>
interest, and sampling images from each of said portions.<br>
10. System as claimed in claim 1, wherein in a manual mode of the system<br>
the pilot of the aircraft defines an area of interest during the flight, said<br>
area of interest being automatically divided into at least one area portion,<br>
all the area portions being automatically scanned one after the other by<br>
means of correspondingly directing to them the on-gimbals array, for<br>
capturing images of each of said scanned portions.<br><br><br>
11.	System as claimed in claim 1, wherein the gimbals comprise two gimbals<br>
mechanisms, an external gimbals mechanism and an internal gimbals<br>
mechanism.<br>
12.	System as claimed in claim 1, wherein the external gimbals mechanism<br>
is used for coarse directing the on-gimbals array to the center of a selected<br>
area portion.<br>
13.	System as claimed in claim 11 wherein the external gimbals mechanism<br>
has two degrees of freedom, elevation and roll.<br>
14.	System as claimed in claim 10, wherein the internal gimbals mechanism<br>
is used for fine directing the on-gimbals array to the center of a selected<br>
area portion, particularly for compensating the gimbals direction for the<br>
aircraft motion and orientation change during the integration period.<br>
15.	System as claimed in claim 11, wherein the internal gimbals mechanism<br>
has two degrees of freedom, yaw and pitch.<br>
16.	System as claimed in claim 10, wherein the external gimbals mechanism<br>
is slaved to the internal gimbals mechanism.<br>
17.	System as claimed in claim 1, wherein during the integration period<br>
each of the array sensors simultaneously senses light from a corresponding<br><br><br>
section of the area portion, and at the end of the integration period the data<br>
from all the array sensors is read simultaneously, and stored as an image of<br>
the area portion.<br>
18.	System as claimed in claim 1, wherein the array light sensors are<br>
sensitive to light in the range of visual light, IR, UV, multi/hyper-spectral,<br>
and/or an active illumination.<br>
19.	System as claimed in claim 1, wherein the arrays are focal plane arrays.<br>
20.	System as claimed in claim 1, wherein the predefined axes system is a<br>
global axes system.<br>
21.	System as claimed in claim 1, assembled within a pod attached to the<br>
aircraft.<br>
22.	System as claimed in claim 1, assembled within a payload installed<br>
inside the aircraft with only its windows protruding for obtaining a clear,<br>
unobstructed Line of Sight.<br>
23.	System as claimed in claim 21, wherein the gimbals are located at the<br>
front of the pod, behind a transparent window.<br><br><br>
24.	System as claimed in claim 1, further comprising a back-scanning<br>
mechanism comprising a mirror or prism, positioned on the gimbals and<br>
rotatable with respect thereto, light coming from the area portion first<br>
passing through said mirror which deflects the same towards the array,<br>
and,<br>
a.	the servo control unit applies to the gimbals a continuous row and/or<br>
column scanning movement without stopping; and<br>
b.	while the direction towards an area portion is being established,<br>
applying to said back-scanning mirror during the integration period an<br>
opposite direction movement with respect to said row and/or column<br>
scanning continuous movement, thereby compensating for that continuous<br>
movement and ensuring a fixed orientation relationship of the array with<br>
respect to the area portion imaged.<br>
25.	A method for carrying out airborne reconnaissance, comprising:<br>
a.	Providing at least one array of light-sensitive pixels;<br>
b.	Mounting the at least one array on gimbals having at least two degrees<br>
of freedom so that the gimbals can direct the array to a selected Line of<br>
Sight;<br>
c.	Providing a Digital Elevation Map of an area of interest,<br>
reconnaissance images from said area are to be obtained;<br><br><br>
d.	Providing an Inertial Navigation System for obtaining at any time<br>
during the flight the updated xa : ya : za coordinates of the center of the<br>
array with respect to a predefined coordinates system;<br>
e.	Providing a calculation unit for, given xp : yp location coordinates of a<br>
center of specific area portion within the area of interest, and the zp<br>
elevation coordinate at said portion center as obtained from said Digital<br>
Elevation Map, and the said xa : ya : za coordinates of the array center at<br>
same specific time, determining the exact angles for establishing a line of<br>
sight direction connecting between the center of the array and the said<br>
XP : yP : zp coordinates;<br>
f.	Given the calculation of step e, directing accordingly the center of the<br>
array's Line of Sight to the center of the area portion;<br>
g.	During an integration period, effecting accumulation of light<br>
separately by any of the array light sensors;<br>
h. During the integration period, repeating at a high rate the calculation<br>
of step e with updated array xa : ya : za coordinates, and repeatedly,<br>
following each said calculation, correcting the direction as in step f;<br>
i. At the end of the integration period, sampling all the array sensors,<br>
and saving in a storage as images of the array portion;<br>
j. Selecting new portion coordinates xp : yp : zp within the area of interest,<br>
and repeating steps e to j for these new coordinates;<br>
k. When the coverage of all the area of interest is complete, terminating<br>
the process, or beginning coverage of another area of interest.<br><br><br>
26.	Method as claimed in claim 25, wherein the selection of xp : yp<br>
coordinates of a new area portion is performed to assure overlap between<br>
adjacent area portions within a predefined range, by calculating the 3-<br>
dimensional footprint of the new area portion on the ground, and then<br>
projecting it on the footprint of a previous area portion.<br>
27.	Method as claimed in claim 26, wherein the overlap assurance is<br>
obtained by a trial and error selection, overlap calculation, and correction<br>
when necessary, or by an exact analytical calculation.<br>
28.	Method as claimed in claim 25, wherein at least some of the sensors of<br>
the Inertial Navigation System are positioned on the gimbals, for improving<br>
the measuring of the orientation of the array with respect to the selective<br>
area portion.<br>
29.	Method as claimed in claim 25, wherein at least some of the light<br>
sensitive sensors are positioned on the gimbals, for improving the<br>
measuring of the orientation of the Line of Sight with respect to the<br>
selective area portion.<br><br><br>
30.	Method as claimed in claim 25, wherein the Inertial Navigation System<br>
comprises a dedicated Inertial Navigation System of the reconnaissance<br>
system and the main Inertial Navigation System of the aircraft, to improve<br>
the measuring of the orientation of the array with respect to the selective<br>
area portion, by using a process of transfer alignment from the aircraft<br>
Inertial Navigation System to the dedicated reconnaissance system's<br>
Inertial Navigation System.<br>
31.	A method for providing motion compensation during airborne<br>
photographing comprising:<br>
a.	Providing at least one array of light-sensitive pixels;<br>
b.	Mounting the at least one array on gimbals having at least two degrees<br>
of freedom so that the gimbals can direct its Line of Sight towards a<br>
selective area portion;<br>
c.	Providing a Digital Elevation Map of an area of interest,<br>
reconnaissance images from said area are to be obtained;<br>
d.	Providing an Inertial Navigation System for obtaining at any instant<br>
during flight the updated xa : ya : za coordinates of the center of the array<br>
with respect to a predefined coordinates system;<br>
e.	Providing a calculation unit for, given xp : yp location coordinates of a<br>
center of specific area portion within the area of interest, and the zp<br>
elevation coordinate at said portion center as obtained from said Digital<br>
Elevation Map, and the said xa : ya : za coordinates of the array center at<br><br>
same specific time, determining the exact angles for establishing a line of<br>
sight direction connecting between the center of the array and the said<br>
xp : yp : zp coordinates;<br>
f.	During an integration period, when the center of the array's Line<br>
of Sight is directed to a center of an area portion, effecting accumulation of<br>
light separately by any of the array light sensors;<br>
g.	During the integration period, repeating at a high rate the<br>
calculation of step e with updated array xa : ya : za coordinates, and<br>
repeatedly, following each said calculation, correcting the direction by<br>
keeping the center of the array directed to the center of the selected area<br>
portion, therefore compensating for aircraft movement; and<br>
h. At the end of the integration period, sampling all the array<br>
sensors, and saving in a storage as images of the array portion.<br>
32. Method as claimed in claim 25, for further carrying out airborne<br>
targeting, comprising:<br>
a.	Providing said gimbals having at least two degrees of freedom, so that<br>
the gimbals can be directed to a selected Line of Sight;<br>
b.	Providing said Digital Elevation Map of an area of interest, selected<br>
objects within said area are to be targeted;<br>
c.	Providing said Inertial Navigation System for obtaining at any time<br>
during the flight the updated xa : ya : za coordinates of the center of the<br>
gimbals with respect to a predefined coordinates system;<br><br>
d.	Providing said calculation unit for, given xp : yp location coordinates<br>
of a center of a specific target within the area of interest, and the zp<br>
elevation coordinate at said target center as obtained from said Digital<br>
Elevation Map, and the said xa:ya: za coordinates of the gimbals center at<br>
same specific time, determining the exact angles for establishing a Line of<br>
Sight direction connecting between the center of the gimbals and said<br>
XP : yP : ZP coordinates;<br>
e.	Given the calculation of step d, directing accordingly the center of the<br>
gimbals Line of Sight to the center of the selected target;<br>
f.	During the effective targeting period, motion compensating for the<br>
motion of the aircraft by repeating at a high rate the calculation of step d<br>
with updated target xa : ya : za coordinates, and repeatedly, following each<br>
said calculation, correcting the direction as in step e.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LSgxNC0xMi0yMDExKS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-(14-12-2011)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUFCU1RSQUNUIDEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-ABSTRACT 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWFic3RyYWN0LnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUFNQU5ERUQgUEFHRVMgT0YgU1BFQ0lGSUNBVElPTi5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-AMANDED PAGES OF SPECIFICATION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWFzc2lnbm1lbnQucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-assignment.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWFzc2lnbm1lbnQxLjEucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-assignment1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUNMQUlNUyAxLjEucGRm" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-CLAIMS 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUNPUlJFU1BPTkRFTkNFIDEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-CORRESPONDENCE 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUNPUlJFU1BPTkRFTkNFLTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-CORRESPONDENCE-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUNPUlJFU1BPTkRFTkNFLTEuMi5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-CORRESPONDENCE-1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWNvcnJlc3BvbmRlbmNlLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-correspondence.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWNvcnJlc3BvbmRlbmNlMS4zLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-correspondence1.3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LURFU0NSSVBUSU9OIChDT01QTEVURSkgMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-DESCRIPTION (COMPLETE) 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWRlc2NyaXB0aW9uIChjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LURSQVdJTkdTIDEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-DRAWINGS 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWRyYXdpbmdzLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUVYQU1JTkFUSU9OIFJFUE9SVCBSRVBMWSBSRUNJRVZFRC5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-EXAMINATION REPORT REPLY RECIEVED.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWV4YW1pbmF0aW9uIHJlcG9ydC5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-examination report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUZPUk0gMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-FORM 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUZPUk0gMTMucGRm" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-FORM 13.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gMTguMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 18.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gMTgucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUZPUk0gMi4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-FORM 2.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LUZPUk0gMi5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gMy4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 3.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gMy5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gNS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 5.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWZvcm0gNS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-form 5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdwYS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-gpa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdwYTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-gpa1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtYWJzdHJhY3QucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtZGVzY3JpcHRpb24gKGNvbXBsZXRlKS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtZHJhd2luZ3MucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtZm9ybSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtZm9ybSAyLnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-form 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LWdyYW50ZWQtc3BlY2lmaWNhdGlvbi5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-granted-specification.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1LT0xOUC0yMDA0LU9USEVSUyBET0NVTUVOVFMgIDEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1784-KOLNP-2004-OTHERS DOCUMENTS  1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LW90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LXBldGV0aW9uIHVuZGVyIHJ1bGUgMTM3LnBkZg==" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-petetion under rule 137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LXJlcGx5IHRvIGV4YW1pbmF0aW9uIHJlcG9ydC5wZGY=" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-reply to examination report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTc4NC1rb2xucC0yMDA0LXNwZWNpZmljYXRpb24ucGRm" target="_blank" style="word-wrap:break-word;">1784-kolnp-2004-specification.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="251664-a-process-for-manufacturing-a-high-strength-ductile-corrosion-fresistant-carbon-steel.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="251666-a-locking-system.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>251665</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>1784/KOLNP/2004</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>13/2012</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>30-Mar-2012</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>27-Mar-2012</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>24-Nov-2004</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>RAFAEL-ARMAMENT DEVELOPMENT AUTHORITY LTD.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>P.O. BOX 2250, 31021 HAIFA</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>YAVIN ZVI</td>
											<td>TOPAZ STREET, 20103 GILON-MISGAV</td>
										</tr>
										<tr>
											<td>2</td>
											<td>UHL BERND</td>
											<td>34/5 HERMLINSTRASSE, D-73434 AALEN, GERMANY</td>
										</tr>
										<tr>
											<td>3</td>
											<td>GREENFELD ISRAEL</td>
											<td>23 HERMON STREET, 25147 KFAR VRADIM</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G01C 11/02</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/IL2003/00422</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2003-05-22</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>149934</td>
									<td>2002-05-30</td>
								    <td>Israel</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/251665-an-airborne-reconnaissance-system-for-obtaining-images-from-an-area-of-interest by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 13:47:45 GMT -->
</html>
