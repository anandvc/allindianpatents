<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/220045-audio-intonation-calibration-method by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:11:02 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 220045:AUDIO-INTONATION CALIBRATION METHOD</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">AUDIO-INTONATION CALIBRATION METHOD</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>AUDIO INTONATION CALIBRATION METHOD Audio intonation calibration method in which an audio signal emitted by a subject (S) is regenerated after real time processing so that it can be heard by the subject. Method has the following steps: (E10) pattern voice signal acquisition; (El I) pattern signal spectral analysis; (E13) subject pattern imitation voice signal acquisition: (E15) imitation signal spectral analysis; {E16) spectra comparison; (El8) correction of the imitation signal based on the comparison; signal playback to the subject.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td> <br><br>
The present invention concerns an audio-intonation calibration method.<br>
It also concerns a method of acquiring oral skills in a language being studied by a subject and a method of performing a song by a subject.<br>
Generally speaking, the present invention concerns a method in which the emission of an audio signal by a person is modified by modifying the sound information that he receives when he speaks.<br>
A method of this kind is based on a known principle whereby the vocal provision of a subject, i.e. sounds that he emits, undergoes a major transformation as a function of the auditory provision applied to the same subject, i.e. sound information that he receives.<br>
Using equipment in which an audio signal emitted by a subject Is reproduced to the auditory organs of the subject after real time processing Is known, and especially in the particular field of teaching and speaking languages.<br>
A method of this kind is described in the document WO 92/14229.<br>
That document describes a device in which an audio signal emitted by a subject is modified by processing it to take account of the characteristics of a foreign language being studied and of the harmonic content of that language. The modified audio signal is then furnished to the subject in real time by a vibratory signal, generally by a sound signal, in order to modify the audio signal emitted by the subject.<br><br>
However, in the above document, the audio signal emitted by the subject is processed in a predetermined manner as a function of the bandwidth of the language being learned, and in particular as a function of the envelope curve of that bandwidth.<br>
In practice, the signal processing circuit comprises a multifrequency equalizer that is set in a predetermined manner as a function of the foreign language in question, and more particularly of the bandwidth of that language and the shape of that bandwidth, i.e. the envelope curve of the bandwidth. In practice, the equalizer is constituted by a plurality of successive filters set to different frequencies.<br>
Thus frequency parameters of the equalizer are set in a predetermined manner by characteristics of the language being studied.<br>
Similarly, the above document describes a second form of processing applied to the audio signal emitted by the subject in which adjustment parameters are established as a function of the sound vibration harmonic content of the utterance collected and parameters depending on the language being studied. .<br>
In this case, the parameters of a multifrequency equalizer are set by the difference between the processed signal coming from the actual vocal provision of the subject and predetermined characteristics of the language.<br>
Thus the above document describes the creation from a first signal representing the sound emitted by the subject, of a second signal modified with respect to the first signal in a predetermined manner and as a function of the bandwidth of the language being learned, and in particular as a function of the envelope curve of that language, and a third signal which is derived from the first signal by modifying it as a function of the actual harmonic content of the utterance and characteristics of the language.<br>
The signals are then selectively reproduced to the subject.<br>
The above kind of prior art system has the drawback of using predefined parameter settings, taking no account of the type of audio signal emitted by the subject.<br><br>
An object of the present invention is to improve existing audio-intonation calibration methods in order to simplify their use and widen their applications.<br>
To this end, the present invention provides an audio-intonation calibration method in which an audio signal emitted by a subject is reproduced to the auditory organs of said subject after real time processing.<br>
According to the invention, the above method is characterized in that it comprises the following steps:<br>
-	acquisition of a model audio signal to be imitated;<br>
-	spectral analysis of said model audio signal;<br>
-	acquisition of an audio signal imitated by the subject;<br>
-	spectral analysis of the imitated audio signal;<br>
-	comparison of the spectra of the model audio signal and the imitated audio signal;<br>
-	correction of the imitated audio signal as a function of the result of said comparison; and<br>
-	reproduction of the corrected audio signal to the auditory organs of the subject.<br>
By analyzing the frequency content of an audio signal to be imitated and of an audio signal imitated by a subject, the present invention transforms the intonation of the person by real time modification of the harmonic content of his utterance and reproduction of the signal that has been corrected on the basis of the frequency analysis.<br>
Thus the calibration method makes it possible to increase or decrease in real time the intensity of the various frequencies contained in the imitated audio signal by comparison with a prerecorded model audio signal.<br>
This kind of calibration method enables the subject to reproduce very accurately any type of voice, by working on the basis of an audio signal to be imitated. Consequently, it finds an application not only in the acquisition of oral skills in a foreign language, in which it is possible to imitate series of words or phrases pronounced by a chosen native speaker, but also in performing a<br><br>
song, karaoke style, in which case it becomes possible to reproduce the intonations of a singer when performing a song.<br>
In accordance with preferred features of the invention, the above audio-intonation calibration method further Includes the following steps:<br>
-	measurement of the dynamic range of the audio signal imitated by the subject;<br>
-	measurement of the dynamic range of the corrected audio signal;<br>
-	comparison of the dynamic range of the imitated audio signal and the corrected audio signal; and<br>
-	correction of the dynamic range of the corrected audio signal as a function of the result of said comparison before reproduction to the auditory organs of the subject of the corrected audio signal.<br>
In this way it is possible to modify the overall envelope of the corrected signal as a function of the imitated audio signal to avoid the corrected signal reproduced to the subject having a dynamic range that is too different from the audio signal emitted by the subject.<br>
According to preferred features of the invention, the above calibration method further includes a step of storing the spectral analysis of the model audio signal to be imitated.<br>
Accordingly, it is possible to use the spectral analysis of the model audio signal to implement the audio-intonation calibration method when the audio signal to be imitated is repeated by a subject after a time delay.<br>
Inclusion in the audio-intonation calibration method of a step of emitting the model audio signal to be Imitated to the auditory organs of the subject prior to the step of acquiring the audio signal imitated by the subject, is particularly practical, in particular when acquiring oral skills in a language.<br>
Having the subject listen to the signal to be imitated before imitating it further facilitates acquiring oral skills* in a language and improvement of the intonation when speaking a word or a phrase in a foreign language.<br>
In a first application of the above audio-intonation calibration method, the present invention provides a method of acquiring oral skills in a<br><br>
language being studied, in which method an audio signal emitted by a subject is reproduced to the auditory organs of the subject after real time processing. This acquisition method uses the audio-intonation calibration method according to the invention.<br>
Alternatively, in a second application, the present invention also provides a method of performing a song by a subject, in which method an audio signal emitted by a subject is reproduced to the auditory organs of the subject after real time processing. This method of performing a song also uses the audio-intonation calibration method according to the invention.<br>
Finally, the present invention also provides fixed or removable information storage means containing software code portions adapted to execute the steps of the audio-intonation calibration method, of the method according to the Invention of acquiring oral skills in a language being studied, or of the method according to the invention of performing a song.<br>
Still other features and advantages of the invention will appear in the course of the following description.<br>
In the appended drawings, which are provided by way of non-limiting example:<br>
-	Figure 1 is an algorithm showing an audio-intonation calibration method according to a first embodiment of the invention;<br>
-	Figures 2a, 2b, 2c are diagrams showing steps of the audio-intonation calibration method shown in Figure 1 or Figure 5;<br>
-	Figure 3 is an algorithm showing a method of acquiring oral skills in a language in accordance with an embodiment of the invention;<br>
-	Figure 4 is an algorithm showing a calibration step implemented in Figure 3 and Figure 6;<br>
-	Figure 5 is an algorithm showing an audio-intonation calibration method according to a second embodiment of the invention:<br>
-	Figure 6 is an algorithm showing a method of performing a song in accordance with an embodiment of the invention ; and<br>
-	Figure 7 is a block diagram showing a computer adapted to implement the invention.<br><br>
An audio-intonation calibration method in accordance with a first embodiment of the invention is described first with reference to Figure 1.<br>
In this example the audio-intonation calibration method is adapted to be used in a method of acquiring oral skills In a language.<br>
It is based on the fact that each language uses one or more spectral bands with intensities that are specific to it. Thus the ear of each ' subject becomes accustomed to perceiving the spectral fields that are specific to his native language. Now, the voice reproduces only what the ear hears, so that it is difficult for a subject to pronounce correctly words in a foreign language when his ear is not accustomed to hearing the fields specific to the new language being studied.<br>
The audio-intonation calibration method according to the invention therefore re-educates the ear of a subject by causing the subject to hear in a pronounced manner the fields specific to the language being studied.<br>
This method comprises first of all a step E10 of acquiring a model audio signal to be imitated.<br>
In the case of a foreign language, this audio signal can be a word or a set of words pronounced by a native speaker of the language being studied.<br>
A model is preferably acquired from a computer file on which different model audio signals can be stored.<br>
These sound files F can be stored on a computer hard disk or any other data medium, such as a CD-ROM, a memory card, etc, or may be downloaded via a communication network such as the Internet.<br>
The audio signal to be imitated is then analyzed in a spectral analysis step E11.<br>
The intensity specific to each of the frequency bands analyzed is measured in the analysis step Ell,<br>
The result of this spectral analysis is illustrated in an example in Figure 2a. The analysis step E11 is therefore executed over a series of frequency bands within the range of audible frequencies from 30 Hz to 10 000 Hz.<br><br>
The series of frequency bands corresponds to a subdivision of the frequency range.<br>
In practice, the frequency range is divided into at least 50 frequency bands, and preferably into at least 160 frequency bands, in order to obtain a sufficiently fine analysis of the audio signal.<br>
Figure 2a shows this kind of subdivision over a range of frequencies from 50 Hz to 1 500 Hz.<br>
Thus the intensity in decibels of each frequency band present in the audio signal to be imitated can be measured.<br>
In this first embodiment, the result of this spectral analysis (i.e. the analysis shown in Figure 2a in the present example) is stored in a file in a storage step E12.<br>
The audio-intonation calibration method also includes a step E13 of acquiring an audio signal imitated by a subject S. In this acquisition step E13, an imitated audio signal is picked up, for example via a microphone, from an audio signal emitted by the subject S .<br>
In this embodiment, the dynamic range of the imitated audio signal is first measured in a measurement step E14.<br>
The measurement step E14 thus makes it possible to determine the overall envelope of the imitated audio signal.<br>
According to the invention, a spectral analysis step E15 is then applied to the imitated audio signal.<br>
The spectral analysis step E15 is similar to that previously described for the audio signal to be imitated, and analyzes the intensity of the audio signal collected in this way from the subject S over a series of frequency bands.<br>
As shown in Figure 2b, this provides a spectrum enabling the intensity of the signal in each frequency band to be determined over a range of frequencies from 30 Hz to 10 000 Hz.<br>
Figure 2b shows one example of this spectrum over the range of frequencies from 50 Hz to 1 500 Hz.<br><br>
The subdivision into frequency bands is identical to that used in the step El 1 of spectral analysis of the audio signal to be imitated.<br>
The spectra of the model audio signal and the imitated audio signal are then compared in a comparison step E16.<br>
The result of this comparison, as shown in Figure 2c, is used in a calculation step El 7 to calculate the spectral modifications to be made to the imitated audio signal.<br>
As shown clearly in Figure 2c, and band by band, each frequency band of the imitated audio signal is compared with each frequency band of the audio signal to be imitated, and is corrected so that the intensity values of the imitated audio signal correspond to those of the model to be imitated.<br>
In practice, a correction to be added to or subtracted from the level of each frequency band is deduced from this calculation step.<br>
The calculation step therefore defines the increases or reductions to be applied in each frequency band with respect to the model audio signal.<br>
The result of the calculation is used to correct the imitated audio signal in a step E18. In practice, the parameters of a dynamic multiband equalizer are set so that the frequency fields of the imitated audio signal are equal to those of the acquired model audio signal.<br>
The parameters are set by automatic gain control in each frequency band.<br>
In this preferred embodiment, the dynamic range of the corrected audio signal after the correction step E18 is measured in order to compare the dynamic range of the imitated signal and the dynamic range of the corrected audio signal in a comparison step El 9.<br>
A calculation step E20 defines the dynamic range modification to be applied to the signal. The result of this calculation is used to correct the dynamic range of the corrected audio signal in a correction step E21.<br>
In practice, the result of the calculation obtained in the calculation step E20 is used to set the parameters of a variable gain amplifier adapted to adjust the dynamic range of the signal.<br>
The parameters are set by overall automatic gain control.<br><br>
The calibration method then includes an emission step E22 which reproduces the corrected audio signal to the auditory organs of the subject S.<br>
This emission is effected conventionally by means of an amplifier whose output is connected to headphones.<br>
The amplifier enables the subject S to listen selectively either to the imitated audio signal to which the processing previously described has been applied or to the model audio signal acquired in the acquisition step E10.<br>
The step of emitting the model audio signal to be imitated to the auditory organs of the subject S is therefore preferably executed before the step El 3 of acquisition of the imitated audio signal emitted by the subjects.<br>
A modification step E23 is preferably adapted to modify the model audio signal to be imitated as a function of parameters representing a language being studied.<br>
In practice, the audio signal to be imitated passes through a preset multiband graphic equalizer, i.e. an equalizer whose parameters are preset as a function of a chosen language to accentuate frequency bands specific to that language.<br>
In this way, the subject S perceives more clearly the frequency areas in which his ears are usually relatively insensitive.<br>
In order additionally to facilitate the work and in particular the repetition of the audio signal to be imitated by the subject S, if the audio signal is a text, the method preferably includes a step E24 of displaying the text, for example on a screen associated with the computer.<br>
A method of acquiring oral skills in a language being studied using the audio-intonation calibration method shown in Figure 1 is described next with reference to Figure 3.<br>
This method of acquiring oral skills in a language thus improves the pronunciation of a subject S when speaking a chosen language.<br>
The acquisition method shown in Figure 3 includes first of all a calibration step E30.<br>
This calibration step is shown in Figure 4.<br><br>
In its general principle, this calibration step automatically adjusts the input level of a computer sound card before it begins to be used by comparing the level of a phrase that a subject S pronounces with a prerecorded example.<br>
This calibration enables the student to work autonomously on the method of acquiring oral skills, while preventing too low or too high sound input levels, which could interfere with the correct functioning of the method.<br>
To this end, this calibration step includes a test step E41 during which the subject can decide whether to carry out the calibration or not. In particular, if the computer and the associated sound card have just been used by the same subject, the calibration step can be omitted.<br>
If the calibration is to be carried out, a step E42 of generating an example generates a reference audio signal.<br>
A display step E43 displays text corresponding to the audio signal generated in the generation step E42.<br>
The subject then repeats the audio signal example and his voice is recorded in a recording step E44.<br>
Comparing the overall intensity of the audio signal example and the audio signal emitted by the subject in a comparison step E45 enables an intensity level difference between the two audio signals to be calculated in a calculation step E46.<br>
The input gain of the computer sound card is then adjusted in an adjustment step E47.<br>
Returning to Figure 3, when the calibration step has been executed, a loading step E31 loads software comprising the code of the method of acquiring oral skills In a language.<br>
-^__	In this embodiment, the same software can be used for different<br>
languages.<br>
Of course, separate software could be used for different foreign languages.<br>
In this embodiment the required language is chosen in a choosing step E32.<br><br>
The choosing step E32 loads into the computer all of the parameters associated with the language concerned.<br>
In particular, the parameters include one or more frequency spectra specific to the language being studied.<br>
A particular lesson can be chosen in a second choosing step E33.<br>
There can be a plurality of lessons for each language, each lesson comprising a particular number of prerecorded words or phrases.<br>
The lessons can be graded depending on a level of language difficulty or depending on different phonemes to be worked on and specific to each language.<br>
In   practice,   the   parameters   and   the   lesson   are   loaded conventionally into the random access memory (RAM) of the cornputer.<br>
The acquisition method then includes a step E34 of emitting a phrase or a word to be repeated.<br>
This step of emitting a phrase corresponds to the acquisition step E10 and the spectral modification step E23 shown in Figure 1.<br>
In the emission step E34, the active input of the amplifier is a recording stored in the sound file F.<br>
Thus the emission step E34 enables the subject S to hear an audio signal to be imitated corresponding to a word or a phrase of a text spoken by a native speaker of the language being studied.<br>
The analysis step El 1 and the spectral analysis storage step E12 shown in Figure 1 are executed at the same time.<br>
To facilitate memorization by the subject of the term to be repeated, a display step E35 corresponding to the display step E24 shown in Figure 1 is executed at the same time as the step E34 of emitting a phrase.<br>
The acquisition method then includes a time-delay step E36 in which all of the steps El 3 to E22 shown in Figure 1 are executed.<br>
Accordingly, during this time-delay, the subject repeats the audio signal to be imitated, which is corrected as previously described and reproduced to the ears of the subject in real time, so that the subject unconsciously and spontaneously modifies his own utterance.<br><br>
The duration of this time-delay step E36 substantially corresponds to the duration of the audio signal to be imitated, plus a few seconds to allow the subject to repeat the word or phrase.<br>
The time-delay can be adjustable, for example by the subject, should he require less time or more time to repeat the various words or phrases.<br>
Progress through the lesson, i.e. through the succession of words to be repeated and the repeated words, can be automatic or manual.<br>
In this embodiment, a test step E37 asks the subject if he wishes to work again on the same phrase or the same word.<br>
If so, the emission step E34, the display step E35, and the time-delay step E36 are executed on the same audio signal.<br>
If not, a second test step E38 asks the subject if he wishes to work again on a subsequent phrase or word.<br>
If so, the emission step E34, the display step E35, and the time-delay step E36 are also executed on a subsequent prerecorded word or phrase in the current lesson.<br>
Otherwise, the lesson is stopped.<br>
Thus groups of phrases or words can be repeated in looped fashion, the student being able to do more or less work on certain parts of the lesson.<br>
After a number of sessions, the ears of the subject become accustomed to perceiving the language and the perception of the new frequency fields that are fed to his ear becomes permanent. The utterance of the subject is then also permanently modified.<br>
A similar method allowing a subject to execute a song is described next with reference to Figures 5 and 6.<br>
In this application, the audio-intonation calibration method modifies the intonation of a subject in order to adapt* his manner of singing optimally to that of a prerecorded singer.<br>
It therefore enables a subject to sing in the manner of a singer, and can be used in a karaoke system, for example.<br><br>
The audio-intonation calibration method in accordance with the invention is described first with reference to Figure 5, and is similar to that used in the Figure 1 method of acquiring oral skills in a language.<br>
In the present application, the sound file F is adapted to store one or more songs. In practice, for each song there are three sound files, one containing only the voice of the artist, the other the accompaniment associated with the song, and the third the text of the song.<br>
As previously, this sound file can be stored on a hard disk of a computer or on any other data medium (CD-ROM, memory card, etc). It can also be obtained by downloading it from a communication network such as the Internet.<br>
Unlike the audio-intonation calibration method shown in Figure 1 (in which the step E10 of acquiring a model and the step E11 of spectral analysis of the model, on the one hand, and the step E13 of recording and the step E15 of spectral analysis of the imitated audio signal, on the other hand, are carried out alternately, thereby necessitating a step E12 of storing the spectral analysis of the audio signal to be imitated, for later comparison), here a step E50 of acquiring an audio signal to be imitated, corresponding to the voice of the singer, is executed simultaneously with the step E53 of recording the audio signal imitated by the subject S,<br>
As previously, the spectral analysis steps E51 and E55 are applied to the audio signal to be imitated and to the imitated audio signal, respectively, and the result is fed to the input of a comparator in order to carry out a spectnjm comparison step E56.<br>
The comparison step E56 is followed by a calculation step E57 for calculating the modifications to be made to the imitated audio signal as a function of the recorded model.<br>
These comparison and calculation steps are similar to those described with reference to Figures 2a, 2b. 2c.<br>
To obtain a thorough spectral analysis, the frequency range is preferably divided into at least 160 frequency bands.<br><br>
The modifications calculated in this way are applied by automatic gain control In each band to modify the imitated audio signal in a correction step E58,<br>
As previously, the correction can be applied by a dynamically adjustable graphic equalizer for adjusting the imitated audio signal, frequency band by frequency band, as closely as possible to the acquired model-Before reproducing the corrected signal to the subject, an envelope modification step E61 is executed, as previously.<br>
In practice, a step E54 of analyzing the dynamic range of the audio signal recorded in the recording step E53 provides the overall intensity of the signal. A comparison step E59 compares the dynamic range of the recorded audio signal to the dynamic range of the audio signal corrected in the correction step E58.<br>
A calculation step E60 calculates the modifications to be made to the corrected audio signal. These modifications are applied by overall automatic gain control to modify the corrected audio signal in a modification step E61. This envelope modification is effected by means of a variable gain amplifier.<br>
The audio signal corrected in this way is reproduced to the subject via an amplifier in an emission step E62.<br>
Accordingly, in this audio-Intonation calibration method, the amplifier receives simultaneously at its input a corrected audio signal and an accompaniment signal acquired from the sound file F in an acquisition step E63.<br>
The subject therefore hears simultaneously the accompaniment and his voice as corrected by the processing previously described.<br>
To facilitate the execution of the song, a display step E64 displays the text simultaneously with the playing of the song.<br>
in practice, and as shown in Figure 6, this audio-intonation calibration method can be used in a more complex system for karaoke style execution of a song by a subject.<br><br>
In this method of performing a song, a calibration step E61 can be executed before performing a song.<br>
This calibration step corresponds to the calibration step described with reference to Figure 4 and adjusts the input gain of the computer sound card.<br>
A loading step E62 is adapted to load software comprising the code of the method of performing a song.<br>
A choosing step E63 in practice eriables the subject to choose a song from those stored in the sound file F.<br>
A step E64 of performing the song is then executed as described previously with reference to Figure 5.<br>
During the playing of a song, the file containing only the accompaniment is fed to the subject S via the amplifier and headphones.<br>
Simultaneously, the text of the song is displayed on the screen of the computer in a display step E64. The model file containing the voice is read simultaneously and in perfect synchronization with the file containing the accompaniment. The subject also sings simultaneously with the accompaniment so that the spectral analysis of the two audio signals can be carried out simultaneously to apply the appropriate modifications in real time to the imitated audio signal.<br>
To facilitate simultaneity in the interpretation of the song, assistance can be provided by the emission of a metronome signal and possibly a cursor shown on the text of the song to indicate which portion of the text should be sung.<br>
The perception of the sound he emits as modified in real time causes the subject to modify the sound he emits unconsciously and also in real time.<br>
He is therefore caused to sing in the manner of the prerecorded singer.<br>
When the song is finished, a test step E65 enables the subject to choose to perform the same song again.<br><br>
If so, the performance step E64 is repeated on the same content of the sound file.<br>
If not, a second test step E66 enables the subject to choose another song.<br>
If the subject opts to choose another song, the choosing step E63 Is repeated to select one of the prerecorded songs in the sound file.<br>
The performance step E64 is then executed for the new song.<br>
The method of performing a song terminates when no other song is selected after the test step E66.<br>
The audio-intonation calibration method used to perform a song or to acquire oral skills in a language being studied can be implemented on a computer 10 as shown in Figure 7.<br>
All of the means employed by the method are incorporated into a microprocessor (CPU) 11 and a read only memory (ROM) 12 is adapted to store an audio-intonation calibration program and a program for performing a song or acquiring oral skills in a language.<br>
A random access memory (RAM) 13 is adapted to store in registers the values modified during execution of these programs.<br>
In practice, the RAM comprises registers adapted to store the sound files F and the spectral analysis results.<br>
The microprocessor 11 is integrated into a computer which may be connected to a communication network via a communication interface.<br>
The computer further includes means for storing documents, such as a hard disk 14, or is adapted to cooperate with removable document storage means, such as disks 5, by means of a disk drive 15, (for floppy disk, compact disk or computer card).<br>
The fixed or removable storage means can therefore contain the code of the audio-intonation calibration method and the code of the method of acquiring oral skills in a language or of the method of performing a song.<br>
The code of these methods can be stored on the hard disk of the computer 10, for example, and the sound files used in the various applications<br><br>
can be stored separately on disks 5 adapted to cooperate with the disk drive    15.<br>
Alternatively, the program for implementing the invention can be  stored in the read only memory 12.<br>
The computer 10 also has a screen 16 providing an interface with<br>
the subject S, in particular to display to the subject the text to be repeated or sung.<br>
A sound card 17 is also provided and is adapted to cooperate with a microphone 6 and headphones 7 to emit an audio signal or to receive an audio signal emitted by the subject.<br>
The central processor unit 11 then executes instructions relating to implementation of the invention. On powering up, the programs and methods relating to the invention stored in a memory, for example the read only memory 12, are transferred into the random access memory 13, which then contains the executable code of the invention and the variables necessary to implement the invention.<br>
A communication bus 18 provides communication between various subsystems that are part of the computer 10 or connected to it.<br>
The representation of the bus 18 is not limiting on the invention, and in particular the microprocessor 11 could communicate instructions to any subsystem, either directly or via another subsystem.<br>
Thanks to the invention, the audio-intonation calibration method can be implemented on a personal computer and can be used by a subject S without requiring external intervention.<br>
In particular, on acquiring oral skills in a language, the subject S can work on his intonation using the different prerecorded sound files, without necessitating the presence of a tutor.<br>
Of course, many modifications can be made to the embodiments previously described without departing from the scope of the invention.<br><br><br>
WE CLAIM :<br>
1.	An audio-intonation calibration method in which an audio signal emitted by a subject (S)<br>
is reproduced to the auditory organs of said subject (S) after real time processing, which method<br>
is characterized in that it comprises the following steps:<br>
-	acquisition (ElO, E60) of a model audio signal to be imitated;<br>
-	spectral analysis (El 1, E51) of said model audio signal;<br>
-	acquisition (E13, £53) of an audio signal imitated by the subject (S);<br><br>
-	spectral analysis (E15, E55) of the imitated audio signal;<br>
-	comparison (El 6, E56) of the spectra of the model audio signal and the imitated audio signal;<br>
-	correction (El8, E58) of the imitated audio signal as a function of the result of said comparison; and<br>
-	reproduction (E22, E62) to the auditory organs of the subject (S) of the corrected audio signal.<br>
2.	The audio-intonation calibration method as claimed in claim 1, wherein it comprises the<br>
following steps:<br>
-	measurement (E14, E24) of the dynamic range of the audio signal imitated by the subject (S);<br>
-	measurement (E18, E28) of the dynamic range of the corrected audio signal;<br>
-	comparison (E19, E59) of the dynamic range of the imitated audio signal and the corrected audio signal; and<br>
-	correction (E21, E61) of the dynamic range of the corrected audio signal as a function of the result of said comparison before reproduction to the auditory organs of the subject (S) of the corrected audio signal.<br>
3.	The audio-intonation calibration method as claimed in either claim 1 or claim 2. wherein<br>
the comparison steps (E16, E56) and correction steps (E18, E58) are executed over a series of<br>
frequency bands in the range of audible frequencies.<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGFic3RyYWN0IGR1cGxpY2F0ZS5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 abstract duplicate.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGFic3RyYWN0LnBkZg==" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGNsYWltcyBkdXBsaWNhdGUucGRm" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 claims duplicate.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGNvcnJlc3BvbmRlbmNlIG90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 correspondence others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGNvcnJlc3BvbmRlbmNlIHBvLnBkZg==" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 correspondence po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGRlc2NyaXB0aW9uIChjb21wbGV0ZSkgZHVwbGljYXRlLnBkZg==" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 description (complete) duplicate.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGRlc2NyaXB0aW9uIChjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGRyYXdpbmdzLnBkZg==" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGZvcm0tMS5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGZvcm0tMTgucGRm" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 form-18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGZvcm0tMjYucGRm" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 form-26.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGZvcm0tMy5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IGZvcm0tNS5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IG90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IHBjdC5wZGY=" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 pct.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDMzNS1jaGVucC0yMDA1IHBldGl0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">0335-chenp-2005 petition.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzM1LUNIRU5QLTIwMDUgQ0xBSU1TIEdSQU5URUQucGRm" target="_blank" style="word-wrap:break-word;">335-CHENP-2005 CLAIMS GRANTED.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="220044-a-plunger-for-user-manipulated-operation.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="220046-a-process-for-manufacture-of-compound-of-formula-i-an-immunosuppresant.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>220045</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>335/CHENP/2005</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>30/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>25-Jul-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>15-May-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>07-Mar-2005</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>SPEEDLINGUA SA</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>MUNGER, JACKY</td>
											<td></td>
										</tr>
										<tr>
											<td>2</td>
											<td>LEMOINE, Hubert</td>
											<td></td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G09B 19/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/EP2003/009514</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2003-08-06</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>02/10051</td>
									<td>2002-08-07</td>
								    <td>France</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/220045-audio-intonation-calibration-method by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:11:03 GMT -->
</html>
