<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/254841-a-method-for-processing-a-user-s-speech-using-a-mobile-computer by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:14:13 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 254841:A METHOD FOR PROCESSING A USER&#x27;S SPEECH USING A MOBILE COMPUTER</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">A METHOD FOR PROCESSING A USER&#x27;S SPEECH USING A MOBILE COMPUTER</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A computer-implemented method for processing a user&#x27;s speech using a mobile computer that includes a microphone, a display, and a reduced-character keypad is disclosed. The method involves the steps of receiving user speech via the microphone (300); performing speech recognition upon the speech to compute an original N-best list of words for each discrete utterance of the speech (302); operating the display to present a proposed sequence of multiple words; receiving and processing user entered correction to at least a given one of the displayed best words of the proposed sequence of words (306).</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td> <br><br>
BACKGROUND OF THE INVENTION<br>
TECHNICAL FIELD<br>
The invention relates to user entry of information into a system with an input<br>
device. more particularly, the invention relates to speech recognition<br>
combined With disambiguating systems for text input.<br>
DESCRIPTION OF THE PRIOR ART<br>
For many years, portable computers have been getting smaller and smaller.<br>
The principal size-limiting component in the effort to produce a smaller<br>
portable computer has been the keyboard. If standard typewriter-size keys<br>
are used, the portable computer must be at Jeast as large as the standard<br>
keyboard, Miniature keyboards have been used on portable computers, but<br>
the miniatujre keyboard keys have been found to be too small to be<br>
manipulated easily or quickly by a user. Incorporating a full-size keyboard in<br>
a portable computer also hinders true portable use of the computer. Most<br>
portable computers cannot be operated without placing the computer on a flat<br>
work surface to allow the user to type with both hands. A user cannot easily<br>
use a portable computer while standing or moving.<br>
Presently, a tremendous growth in the wireless industry has spawned reliable,<br>
convenient,  and very popular mobile devices available to the average<br>
consumer, such as cell phones, PDAs, etc. Thus, handheld wireless<br>
communications and computing devices requiring text input are becoming<br>
smaller still.' Recent advances in cellular telephones and other portable<br>
wireless technologies have led to a demand for small and portable two-way<br>
messaging systems.   Most wireless communications device manufacturers<br><br>
also desire  to provide to consumers devices that can be operated by a user<br>
with the same hand that is holding the device.<br>
Speech recognition has long been expected to be the best means for text<br>
input, both  as an enhancement to productivity on the desktop computer and<br>
as a solution for the size limitations of mobile devices. A speech recognition<br>
system typically includes a microphone to detect and record the voice input.<br>
The voice input is digitized and analyzed to extract a speech pattern. Speech<br>
recognition  typically requires a powerful system to process the voice input.<br>
Some speech recognition systems with limited capability have been<br>
implemented on small devices, such as command and control on cellular<br>
phones, but for voice-controlled operations a device only needs to recognize a<br>
few commands. Even for such a limited scope of speech recognition, a small<br>
device may not have satisfactory speech recognition accuracy because voice<br>
patterns vary dramatically across speakers and environmental noise adds<br>
complexity to the signal.<br>
Suhm et a discuss a particular problem of speech recognition in the paper<br>
Multimodal Error Correction for Speech User Interfaces, in ACM Transactions<br>
on Computer-Human Interaction (2001). The "repair problem" is that of<br>
correcting the errors that occur due to imperfect recognition. They found that<br>
using the same modality (respeaking) was unlikely to correct the recognition<br>
error, due in large part to the "Lombard" effect where people speak differently<br>
than usual after they are initially misunderstood, and that using a different<br>
modality, such as a keyboard, was a much more effective and efficient<br>
remedy. Unfortunately, mobile devices in particular lack the processing power<br>
and memory to offer full speech recognition capabilities, resulting in even<br>
higher recognition errors, and lack the physical space to offer full keyboard<br>
and mouse input for efficiently correcting the errors.<br>
Disambiguation<br>
Prior development work has considered use of a keyboard that has a reduced<br>
number of keys.    As suggested by the keypad layout of a touch-tone<br><br>
telephone, many of the reduced keyboards have used a 3-by-4 array of keys.<br>
Each key in the array of keys contains multiple characters. There is therefore<br>
ambiguity as a user enters a sequence of keys because each keystroke may<br>
indicate one of several letters. Several approaches have been suggested for<br>
resolving the ambiguity of the keystroke sequence. Such approaches are<br>
referred to as disambiguation.<br>
Some suggested approaches for determining the correct character sequence<br>
that corresponds to an ambiguous keystroke sequence are summarized by J.<br>
Arnott, M. Javad in their paper Probabilistic Character Disambiguation for<br>
Reduced keyboards Using Small Text Samples, in the Journal of the<br>
International Society for Augmentative and Alternative Communication.<br>
T9Â® Text Input is the leading commercial product offering word-level<br>
disambiguation for reduced keyboards such as telephone keypads, based on<br>
U.S. 5,818,437 and subsequent patents. Ordering the ambiguous words by<br>
frequency of use reduces the efficiency problems identified in earlier research,<br>
and the ability to add new words makes it even easier to use over time. Input<br>
sequences may be interpreted simultaneously as words, word stems and/or<br>
completions, numbers, and unambiguous character strings based on stylus<br>
tap location or keying patterns such as multi-tap.<br>
T9 and similar products are also available on reduced keyboard devices for<br>
languages with ideographic rather than alphabetic characters, such as<br>
Chinese. These products typically take one of two approaches: basic<br>
handwritten strokes or stroke categories are mapped to the available keys,<br>
and the user enters the strokes for the desired character in a traditional order;<br>
or a phonetic alphabet is mapped to the keys and the user enters the phonetic<br>
spelling of the desired character. In either case, the user then has to locate<br>
and select! the desired character among the many that match the input<br>
sequence. The input products often benefit from the context of the previously<br>
entered character to improve the ordering of the most likely characters<br>
displayed, as two or more ideographic characters are often needed to define a<br>
word or phrase.<br><br>
Unfortunately, mobile phones are being designed with ever-smaller keypads,<br>
with keys that are more stylish but also more difficult for typing quickly- and<br>
accurately. And disambiguating ambiguous keystroke sequences could<br>
benefit from further improvements. For example, the syntactic or application<br>
context is not typically taken into account when disambiguating an entered<br>
sequence or when predicting the next one.<br>
Another commonly used keyboard for small devices consists of a touch-<br>
sensitive panel on which some type of keyboard overlay has been printed, or<br>
a touch-sensitive screen with a keyboard overlay displayed. Depending on the<br>
size and nature of the specific keyboard, either a finger or a stylus can be<br>
used to interact with the panel or display screen in the area associated with<br>
the key or letter that the user intends to activate. Due to the reduced size of<br>
many portable devices, a stylus is often used to attain sufficient accuracy in<br>
activating each intended key. The small overall size of such keyboards<br>
results in a small area being associated with each key so that it becomes<br>
quite difficult for the average user to type quickly with sufficient accuracy.<br>
A number of built-in and add-on products offer word prediction for touch-<br>
screen keyboards like those just mentioned. After the user carefully taps on<br>
the first letters of a word, the prediction system displays a list of the most<br>
likely complete words that start with those letters. If there are too many<br>
choices, however, the user has to keep typing until the desired word appears<br>
or the user finishes the word. Switching visual focus between the touch-<br>
screen keyboard and the list of word completions after every letter tends to<br>
slow text entry rather than accelerate it.<br>
The system described in U.S. Patent 6,801,190 uses word-level auto-<br>
correction to resolve the accuracy problem and permit rapid entry on small<br>
keyboards.  Because tap locations are presumed to be inaccurate, there is<br>
some ambiguity as to what the user intended to type. The user is presented<br>
with one or more interpretations of each keystroke sequence corresponding to<br>
a word sucjh that the user can easily select the desired interpretation. This<br><br>
approach epables the system to use the information contained in the entire<br>
sequence of keystrokes to resolve what the user's intention was for each<br>
character of the sequence. When auto-correction is enabled, however, the<br>
system may not be able to offer many word completions since it does not<br>
presume that the first letters are accurate, cannot determine whether the user<br>
is typing the entire word, and there may be many other interpretations of the<br>
key sequence to display.<br>
Handwriting recognition is another approach that has been taken to solve the<br>
text input problem on small devices that have a touch-sensitive screen or pad<br>
that detects; motion of a finger or stylus. Writing on the touch-sensitive panel<br>
or display screen generates a stream of data input indicating the contact<br>
points. The handwriting recognition software analyzes the geometric<br>
characteristics of the stream of data input to determine each character or<br>
word.<br>
Unfortunately, current handwriting recognition solutions have many problems:<br>
1)	Handwriting is generally slower than typing;<br>
2)	On small devices, memory limitations reduce handwriting recognition<br>
accuracy; and<br>
3)	Individual handwriting styles may differ from those used to train the<br>
handwriting software.<br>
It is for these reasons that many handwriting or 'graffiti' products require the<br>
user to learn a very specific set of strokes for the individual letters. These<br>
specific set of strokes are designed to simplify the geometric pattern<br>
recognition process of the system and increase the recognition rate. These<br>
strokes may be very different from the natural way in which the letter is<br>
written. Thisj results in very low product adoption.<br>
Handwriting on mobile devices introduces further challenges to recognition<br>
accuracy: the orientation of handwriting while trying to hold the device may<br>
vary or skew the input; and usage while on the move, e.g. the vibration or<br>
bumpiness during a bus ride, causes loss of contact with the touch-screen<br>
resulting in "noise" in the stream of contact points.<br><br>
Therefore, current ambiguous and recognizer-based systems for text input,<br>
while compensating somewhat for the constraints imposed by small devices,<br>
have limitations that reduce their speed and accuracy to a level that users<br>
might consider unacceptable.<br>
In Suhm's paper, "multimodal error correction' is defined as using an alternate<br>
(non-speech) modality to re-enter the entire word or phrase that was<br>
misrecognizjed. This is found to be more efficient than respeaking in part<br>
because the speech modality has already been shown to be inaccurate. That<br>
the alternate input modality has its own recognition accuracy problems is<br>
considered by the user in deciding which modality to use next, but each of the<br>
modalities are operated independently in an attempt to complete the text entry<br>
task.<br>
It would be advantageous to provide an apparatus and method for speech<br>
recognition that offers smart editing of speech recognition output.<br>
It would be advantageous to provide an apparatus and method for speech<br>
recognition that maximizes the benefits of an alternate input modality in<br>
correcting recognition errors.<br>
It would be advantageous to provide an apparatus and method for speech<br>
recognition that offers an efficient alternate input modality when speech<br>
recognition is not effective or desirable given the current task or environment.<br>
SUMMARY OF THE INVENTION<br>
The present invention provides a speech recognition system combined with<br>
one or more alternate input modalities to ensure efficient and accurate text<br>
input. The speech recognition system achieves less than perfect accuracy<br>
due to limited processing power, environmental noise, and/or natural<br>
variations in speaking style. The alternate input modalities use disambiguation<br>
or recognition engines to compensate for reduced keyboards, sloppy input,<br><br>
and/or natural variations in writing style. The ambiguity remaining in the<br>
speech recognition process is mostly orthogonal to the ambiguity inherent in<br>
the alternate input modality, such that the combination of the two modalities<br>
resolves the recognition errors efficiently and accurately. The invention is<br>
especially Well suited for mobile devices with limited space for keyboards or<br>
touch-screen input.<br>
One embodiment of the invention provides a method for processing language<br>
input in a cjata processing system that comprises the steps of receiving a first<br>
input comprising voice input; determining a first plurality of word candidates<br>
according jo the first input; receiving a second input comprising a non-voice<br>
input; and determining one or more word candidates according to the first<br>
input and the second input. The one or more word candidates are<br>
determined based on the second input under constraint of the first input.<br>
Alternately, the union or intersection of the two word candidate lists is<br>
determined, rather than one input filtering the other.<br>
In another embodiment, the one or more word candidates are determined<br>
based on the first input in view of word context. The word context is based<br>
any of a N-gram language model and a language model of a speech<br>
recognition engine.<br>
In another embodiment, the determining of the one or more word candidates<br>
comprises the step of correcting or filtering the first plurality of word<br>
candidates  based on the second input.<br>
In another embodiment, the second input is received on a mobile device; and<br>
speech recognition on the voice input is partially performed on the mobile<br>
device and partially performed on a server coupled to the mobile device<br>
through a wireless communication connection.<br>
In a furtheri embodiment, the speech recognition is activated by a push-to-talk<br>
button on the mobile device.<br><br>
In a further embodiment, the second input is received while one or more of the<br>
word candidates is presented for selection or editing.<br>
In a further embodiment, the second input comprises any of a touch screen<br>
keyboard, handwriting gesture recognition, and a keypad input.<br>
One embodiment of the invention provides a machine readable medium<br>
having instructions stored therein which, when executed on a data processing<br>
system, cause the data processing system to perform a method for<br>
processing language input, the method comprising the steps of: receiving a<br>
first input comprising a voice input; determining a first plurality of word<br>
candidatesj according to the first input; receiving a second input comprising a<br>
non-voice input; and determining one or more word candidates according to<br>
the first input and the second input.<br>
i<br>
In another embodiment, the one or more word candidates are determined<br>
based on the second input under constraint of the first input, and in view of<br>
word context; and the word context is based any of a N-gram language model<br>
and a language model of a speech recognition engine.<br>
In another embodiment, the step of determining of the one or more word<br>
candidates: comprises the step of correcting a list of the first plurality of word<br>
candidates;<br>
In yet another embodiment, the second input is received on a client computing<br>
device; speech recognition on the voice input is partially performed on the<br>
device and partially performed on a server coupled to the device through a<br>
data connection; and the speech recognition is activated by a push-to-talk<br>
button on the device.<br>
In a further embodiment, the second input is received while one of the first<br>
plurality of! the word candidates is presented for editing or while the first<br>
plurality of the word candidates is presented for selection; and the second<br><br>
input comprises any of a touch screen keyboard; handwriting gesture<br>
recognition and a keypad input.<br>
One embodiment of the invention provides a mobile device for processing<br>
language input that comprises a speech recognition module to process a first<br>
input comprising a voice input; and one or more modules to process a second<br>
input comprising a non-voice input; a processing module coupled to the one<br>
or more input modules and the speech recognition module, the processing<br>
module to determine a first plurality of word candidates according to the first<br>
input and subsequently to determine one or more word candidates according<br>
to the first input and the second input.<br>
In another embodiment, the one or more word candidates are determined<br>
based on a second input under constraint of the first input and in view of word<br>
context; and the word context is based on any of a N-gram language model<br>
and a language model of a speech recognition engine.<br>
In yet another embodiment, the one or more word candidates are determined<br>
through correcting a list of the first plurality of word candidates.<br>
In a further embodiment, speech recognition of the voice input is partially<br>
performed on the mobile device and partially performed on a server coupled<br>
to the mobile device through a wireless communication connection; and the<br>
speech recognition is activated by a push-to-talk button on the mobile device.<br>
In a further embodiment, the second input is received while one of the first<br>
plurality of the word candidates is presented for editing or while the first<br>
plurality of the word candidates is presented for selection, and the second<br>
input comprises any of a touch screen keyboard, handwriting gesture<br>
recognition, and a keypad input.<br>
In another embodiment, a discrete input mode could be used to speak words<br>
representing punctuation. A temporary mode (like T9's Symbols mode) may<br><br>
be invoked! to recognize only single characters such as symbols or digits. For<br>
instance, saying the word "period" and is recognized.<br>
In one embodiment, "Smart" punctuation may be entered during the second<br>
input to interpret part of the voice input as punctuation. In another<br>
embodiment, there is no need to enter any special mode to recognize<br>
punctuation, For example, when a user says "period", both the word "period"<br>
and ". could be part of the list.<br>
BRIEF DESCRIPTION OF ACCOMPANYING FTGTJRFS<br>
Figure 1 is a diagram that illustrates a system for recognizing user input on a<br>
data processing system according to the invention;<br>
Figure 2 is block diagram of a data processing system for recognizing user<br>
	'<br>
input according to the invention;<br>
FIG. 3 is a flow diagram of a method for processing language input in a data<br>
processing! system according to the invention;<br>
FIG. 4 is a block diagram that provides an example where a user has dictated<br>
a word according to one embodiment of the invention; and<br>
FIGS. 5A-5C are block diagrams that provide an example where a user has<br>
dictated a word according to one embodiment of the invention.<br>
DETAILED DESCRIPTION OF THE INVENTION<br>
The invention provides an apparatus and method for smart editing of speech<br>
recognition output, which offers the most likely choice, or hypothesis, given<br>
the users input. The speech recognition engine scores alternate hypotheses<br>
which add value to information provided to the user. For example, if the<br><br>
speech recognition offers the user the wrong first-choice hypothesis, then the<br>
user may want to access the other N-best hypotheses to correct what was<br>
returned by the recognizer, in a mufti-modal environment, the N-best list of<br>
hypotheses from thQ speech recognition output is available. Specifically, the<br>
N-best list is incorporated into the current word choice list for easy editing.<br>
One embodiment of the invention makes use of both acoustic information and<br>
word context in offering the N-best hypotheses. This could be syntax-<br>
dependent or independent. That is, the language model may provide syntactic<br>
information that affects the probability of a given word or it may simply provide<br>
some type of N-gram model which indicates the probabilities of a particular<br>
word following a word or words.<br>
Acoustically similar utterances appear in the N-best list. The information is<br>
facilitated by a confusability matrix that informs N-best hypothesis formulation<br>
about the frequency of specific phonemic errors. For example, if /p/ is<br>
confused with Ibl in word final position by the speech recognition engine, the<br>
resulting N-best hypotheses with these phonemes would take this into<br>
account. Information may also be available to indicate how frequently each<br>
phoneme in a given language is confused with every other phoneme,<br>
including positional context, e.g. whether it occurs at the beginning, middle, or<br>
end of a wqrd. Information on when phonemes are deleted or inserted may be<br>
provided in addition to the confusability information.<br>
In the invention, a user's text input created in this multi-modal environment is<br>
also used to update any ambiguous or recognition system language<br>
databases. Ideally, databases that can be applied to any modality are updated<br>
in every modality. lf a word offered by the speech recognition engine is not In,<br>
for example, the T9 dictionary, it may be added. In addition, word and phrase<br>
frequency and N-gram information can also be updated with use.<br>
The invention provides a smart edit feature. For example, a user dictates into<br>
the mobile device. The resulting text output from the recognizer is returned to<br>
the user wherever the cursor is in the text entry screen. The output is rich in<br><br>
that it is tagged with the N-best information for the purpose of editing and<br>
correction.<br>
One embodiment of the invention also provides a client-server feature,<br>
whereby the utterances are preprocessed on the device, recognized on a<br>
server connected e.g. via an available wireless data channel and returned as<br>
N-best lists to the device for text display and editing. Hypotheses are more<br>
dynamic and relevant given any changes that the user is making to the text.<br>
For example, if the speech recognition engine proposed the word "winner"<br>
and the user corrects it with "winter*, this action will increase the likelihood<br>
that the following word "storm" is accurately recognized if the user's correction<br>
is also passed back to the server. Server-side language models provide a<br>
more comprehensive morpho-syntactic analysis of the input to improve<br>
recognition performance. The models have more power to predict the user's<br>
next word, enhancing both word prediction and word completion algorithms.<br>
Additionally, language-specific features such as subject-verb agreement,<br>
case, gender, and number agreements, etc., can be implemented more easily<br>
on a powerful server to increase recognition accuracy. The system may allow<br>
the user to control the flow of corrections and updates to the server through<br>
client-side configuration or prompting.<br>
The invention also provides "smart" punctuation. Speech recognition systems<br>
may have difficulty detecting when a user intends to insert a symbol rather<br>
than the word, e.g., "." instead of "period", or ":-)" instead of "smiley".<br>
Ambiguous text input systems have a limited number of keys or gestures to<br>
select a symbol rather than a letter. But correcting speech with an ambiguous<br>
"smart" punctuation feature informs the system that the proper interpretation<br>
of the utterance is a symbol.<br>
The invention allows a temporary mode for "push-to-dictate," which is similar<br>
to the "push-to-talk" feature except that the speech is converted into text<br>
instead of being transmitted as an audio signal to another phone or kept as an<br>
audio attachment to an email.<br><br>
In addition, the invention allows for vector quantization, which can be<br>
performed on the device, with the matching/hypothesis lists generated on<br>
either the device or the server.<br>
Figure 1 is a diagram that illustrates a system for recognizing user input on a<br>
data processing system according to the invention. The user 101 begins by<br>
dictating a word, phrase, sentence, or paragraph. The digitizer 105 and<br>
decoder 109 convert the acoustic input, using an acoustic model (not shown),<br>
to phonetic data. That data is analyzed by the recognition engine 111, based<br>
on the lexicon and/or language model in the linguistic, databases 119.,<br>
optimally including frequency or recency of use, and optionally based on the<br>
surrounding context in the text buffer 113. The best interpretation is added to<br>
the text buffer 113 and shown to the user 101 via the text and list display 103.<br>
Alternately, the N-best list of interpretations is stored in the text buffer 113 for<br>
later reference and/or presented to the user 101 for confirmation via the text<br>
and list display 103.<br>
At some point afterwards, the user 101 selects a word or phrase for correction<br>
via the text and list display 103. Depending on the input capabilities of the<br>
alternate modality, the user presses keys or taps or writes on a touch-screen,<br>
which is converted to an input sequence by an appropriate digitizer 107. The<br>
disambiguation engine 115 determines possible interpretations based on the<br>
lexicon and/or language model in the linguistic databases 119, optimally<br>
including frequency or recency of use, and optionally based on the<br>
surrounding context in the text buffer 113. The multimodal disambiguation<br>
engine 117 compares the ambiguous input sequence and/or interpretations<br>
against the best or N-best interpretations of the speech recognition and<br>
presents revised interpretations to the user 101 for confirmation via the text<br>
and list display 103. In an alternate embodiment, the disambiguation engines<br>
115 and 117 are combined, and mutual disambiguation occurs as an inherent<br>
part of processing the input from the alternate modality.<br>
In another embodiment, the multimodal disambiguation engine 117 directs the<br>
ambiguous   interpretations   back   to   the   recognition   engine   111   for<br><br>
reinterpretation along with the best or N-best list of speech interpretations. In<br>
one such embodiment, the original vectors or phoneme tags are stored in the<br>
text buffer 113; in another, the multimodal disambiguation engine 117 or<br>
recognition engine 111 maps the characters (graphs) of the words in the best<br>
or N-best and/or ambiguous interpretations back to vectors or phonemes for<br>
reinterpretation by the recognition engine 111.<br>
The recognition and disambiguation engines 111, 115, 117 may update one<br>
or more of the linguistic databases 119 to add novel words or phrases that the<br>
user 101 has explicitly spelled or compounded, and to reflect the frequency or<br>
recency of use of words and phrases entered or corrected by the user 101.<br>
In another embodiment of the invention, the system recognizes handwriting,<br>
(whether block, cursive, or even shorthand) instead of speech. The system<br>
components 105, 109,111 serve similar functions for handwriting as they do<br>
for speech. The alternate modality may be ambiguous input from a keypad or<br>
touch-screen keyboard, or speech recognition (whether continuous, discrete,<br>
or by letter), depending on the input capabilities and processing power of the<br>
equipment.<br>
Figure 2 is block diagram of a data processing system for recognizing user<br>
input according to the invention. Although Figure 2 illustrates various<br>
components of an example data processing system, it is understood that a<br>
data processing system according to the invention in general may include<br>
other components than those illustrated in Figure 2. For example, some<br>
systems may have communication circuitry on a cellular phone embodiment.<br>
Figure 2 illustrates various components closely related to at least some<br>
features of the invention. For this description, a person skilled in the art would<br>
understand that the arrangements of a data processing system according to<br>
the invention are not limited to the particular architecture illustrated in Figure<br>
2.<br>
The display 203 is coupled to the processor 201 through appropriate<br>
interfacing circuitry. A handwriting input device 202, such as a touch screen, a<br><br>
mouse, or a digitizing pen, is coupled to the processor 201 to receive user<br>
input for handwriting recognition and/or for other user input A voice input<br>
device 204, such as a microphone, is coupled to the processor 201 to receive<br>
user input for voice recognition, and/or for other sound input. A key input<br>
device 206, such as a phone keypad, a set of dedicated or configurable<br>
buttons, or a small keyboard displayed on a touch screen, is coupled to the<br>
processor 201 to receive user input for typing and/or for other user input.<br>
Optionally, a sound output device 205, such as a speaker, is also coupled to<br>
the processor.<br>
The processor 201 receives input from the input devices, e.g. the handwriting<br>
input device 202 or the voice input device 204 or the key input device 206,<br>
and manages output to the display and speaker. The processor 201 is<br>
coupled to a memory 210. The memory comprises a combination of<br>
temporary storage media, such as random access memory (RAM), and<br>
permanent storage media, such as read-only memory (ROM), floppy, disks,<br>
hard disks, or CD-ROMs. The memory 210 contains all software routines and<br>
data necessary to govern system operation. The memory typically contains an<br>
operating system 211 and application programs 220. Examples of application<br>
programs include word processors, messaging clients, and foreign language<br>
translators. Speech synthesis software may also be provided as part of the<br>
data processing system.<br>
In one embodiment of the invention, the memory 210 includes separate<br>
modules for each part of the recognition and/or disambiguation process,<br>
which may include one or more of a word-based disambiguating engine 216,<br>
a phrase-based recognition or disambiguating engine 217, a context-based<br>
recognition or disambiguating engine 218, a selection module 219, and<br>
others, such as a word list 214 and a phrase list 215. In this embodiment, the<br>
context based disambiguating engine applies contextual aspects of the user's<br>
actions toward input disambiguation. For example, a vocabulary may be<br>
selected based upon selected user location, e.g. whether the user is at work<br>
or at home; the time of day, e.g. working hours vs. leisure time; recipient; etc.<br><br>
In one embodiment of the invention, the majority of the components for<br>
recognition and disambiguation are shared among different input modalities,<br>
e.g. for speech recognition and for reduced keypad input. The word list 214<br>
comprises a list of known words in a language for all modalities, so that there<br>
are no differences in vocabulary between input modalities. The word list 214<br>
may further comprise the information of usage frequencies for the<br>
corresponding words in the language. In one embodiment, a word not in the<br>
word list 214 for the language is considered to have a zero frequency.<br>
Alternatively, an unknown word may be assigned a very small frequency of<br>
usage. Using the assumed frequency of usage for the unknown words, the<br>
known and unknown words can be processed in a substantially similar<br>
fashion. The word list 214 can be used with the word based recognition or<br>
disambiguating engine 216 to rank, eliminate, and/or select word candidates<br>
determined based on the result of the pattern recognition engine, e.g. the<br>
stroke/character recognition engine 212 or the phoneme recognition engine<br>
213, and to predict words for word completion based on a portion of user<br>
inputs. Similarly, the phrase list 215 may comprise a list of phrases that<br>
includes two or more words, and the usage frequency information, which can<br>
be used by the phrase-based recognition or disambiguation engine 217 and<br>
can be used to predict words for phrase completion.<br>
FIG. 3 is a flow diagram of a method for processing language input in a data<br>
processing system according to the invention. The method starts with the<br>
step of receiving first input comprising voice input 300. The method proceeds<br>
to determine a first plurality of word candidates according to the first input<br>
302. The method continues to the step of receiving second input comprising<br>
non-voice input 304. Finally, the method determines one or more word<br>
candidates according to the first input and the second input 306.<br>
The speech recognition system converts the acoustic signal into a digital<br>
sequence of vectors which are matched to potential phones given their<br>
context. Further, the phonetic forms are matched against a lexicon and<br>
language model to create an N-best list of words for each discrete utterance.<br><br>
In continuous speech recognition there may not be clear pauses between<br>
words, so the recognition output may be one or more likely phrase or<br>
sentence interpretations. By default the most likely interpretation is shown in<br>
the application's current input field at the text insertion point.<br>
Following the steps of the method, the user then determines that some of the<br>
previously recognized words are incorrect. Using a stylus, arrow keys, or<br>
voice command, the user selects one or more words for correction. The input<br>
system may display a list of most likely interpretations at this point, but it will<br>
not always show the desired word or words especially if there are display<br>
constraints.<br>
Using the available or preferred alternate modality, such as T9 Text Input on a<br>
phone keypad, the user begins to retype the first highlighted word. Because<br>
the letters mapped to each key, such as A B C on the 2 key, are typically not<br>
acoustically similar, the system is able to immediately determine that the first<br>
phoneme, such as a plosive Ibl or /p/, is in fact a B rather than a P because<br>
the 2 key was pressed rather than the 7 key containing P Q R S. Similarly,<br>
tapping the auto-correcting QWERTY keyboard in the V B N neighborhood<br>
rather than in the I O P neighborhood Increases the likelihood that the B was<br>
desired. Similarly, making a pen gesture that a handwriting recognition engine<br>
interprets as closer to a B or 3 than a P or R mutually resolves the ambiguity<br>
in both recognizers.<br>
As the user continues to rewrite the incorrect word or words, a system<br>
implementing one embodiment of the method will immediately offer better<br>
interpretations of the original recognizer output given each ambiguous<br>
correction. As indicated in the examples above, re-entering only the first letter<br>
or two may be sufficient for the system to mutually disambiguate the entire<br>
word and offer the desired word as the best choice. The context and grammar<br>
of the preceding and/or following words in the input field, which were not<br>
selected for correction and thus may be presumed to be correct, may further<br>
prioritize and refine the interpretations of the utterance being corrected by the<br>
user. Given the most likely word reinterpretation of the current utterance,<br><br>
subsequent utterances, associated e.g. by phoneme tag with other words<br>
selected for correction, may be reinterpreted as other more likely words. In<br>
another embodiment, the other selected words are mapped back to<br>
phonemes, using the lexicon or using language-specific rules that that specify<br>
a pronunciation for each letter, before reinterpretation as other more likely<br>
words.<br>
In one embodiment, the method has the vectors or phoneme tags and the<br>
ambiguous correction input directed back to the speech recognition system for<br>
a refined hypothesis search. In another embodiment, the method requires the<br>
disambiguation system to use the vectors or phoneme tags to refine and filter<br>
the correction so that only ambiguous interpretations with characters<br>
compatible with the vectors or phonemes are considered.<br>
As the user corrects the words, the speech recognition system may determine<br>
that its segmentation of continuous speech was in error and reinterprets the<br>
boundaries between words in light of the user's corrections; or that a pause is<br>
less likely to have represented a delimiter between words and so reinterprets<br>
the utterance and displays it as a single word.<br>
If the input options are limited on the device, the user may be able to select<br>
only one word at a time for correction. In that case, after the user selects the<br>
corrected word the method may include the step of reconsidering the following<br>
word in light of the corrected word context and/or how the original vectors<br>
map to the end of the corrected word and the beginning of the following word.<br>
The system may indicate that the following word has a lower confidence score<br>
or may automatically display the.list of interpretations for the associated<br>
utterance.<br>
In one embodiment of the invention, the system automatically interprets<br>
ambiguous input following a recognized utterance as a correction of the<br>
preceding word or phrase. In another embodiment, the system simultaneously<br>
interprets the input as a correction to a preceding word and as the start of a<br>
new word to be added to the text; by the time the user completes entry of the<br><br>
FIG. 4 is a block diagram illustrating an example where a user has dictated a<br>
word according to one embodiment of the invention. The speech engine<br>
recognizes an utterance 400. The word is displayed to the user 402. If the<br>
user reselects the word or words in the application's input field, the word<br>
choice list provides the alternate hypotheses from the speech recognition<br>
output 404. The user may then select the correct interpretation from the word<br>
choice list and continue with speech recognition input 406. If the user presses<br>
one or more ambiguous keys when a word is active, the word choice list<br>
reflects only words from the N-best list that fit the key sequence 408.<br>
FIG. 5A-5C are diagrams and sample displays illustrating an example where a<br>
user has dictated the words "The top" according to one embodiment of the<br>
invention. The speech engine recognizes the utterance as "The stop" which is<br>
returned to the user's mobile device (Fig. 5A). If the user makes the word<br>
"stop" active in multi-modal T9, the word choice list provides the alternate<br>
hypotheses from the speech recognition output (Fig. 5B). The user may then<br>
select his utterance from the word choice list and continue with T9 input or<br>
speech recognition input.<br>
If the user enters a key press the word choice list displays words from the N-<br>
best list that are constrained by this key press (Fig. 5C). When a word is<br>
active, an additional key press extends the letter sequence. Thus, a soft key<br>
"Edit" option may invoke the correction method.<br>
It quickly becomes evident that the invention works as well with reduced<br>
keyboards or recognizers for languages written with ideographic characters.<br>
For example, correcting the utterance "bing", incorrectly recognized as "ping",<br>
with Pinyin letters mapped to each key, such as A B C on the 2 key; after<br>
pressing the 2 key, the system is able to immediately determine that the first<br>
phoneme is in fact a B rather than a P. Similarly, with a stroke-order input<br>
system, after the user presses a correcting key representing the first stroke<br>
category for the desired character, the speech recognition engine would be<br>
able to consider characters beginning with a stroke in another category and<br><br>
word, few valid corrections or new word interpretations may remain and the<br>
most likely will be offered.<br>
In an alternate embodiment of the invention, the first and second inputs are<br>
nearly simultaneous or overlapping; in effect, the user is voicing what he or<br>
she is typing. The system automatically interprets both inputs and mutually<br>
disambiguates them to produce the best interpretation of both. The user<br>
doesn't need to go back and correct words or phrases very often since<br>
combining the two inputs increases the likelihood that the system chooses the<br>
correct interpretation. Entering only a few ambiguous inputs representing the<br>
beginning of each word may be sufficient in many cases. In another<br>
embodiment of the invention, the two inputs are concurrently entered,<br>
recognized, and mutually disambiguated only after a word or phrase is<br>
selected for correction.<br>
For instance, a user can press the 2 key for 'a' and speak a word that starts<br>
with 'a'. In one embodiment, the key press could be taken to represent the<br>
first letter of the intended word. Thus, when both forms of input seem to<br>
agree, one form of input could reinforce the other and increase the system's<br>
confidence of the words that it presents. However, the two forms of input<br>
could disagree. In that case, words matching both forms of input could be<br>
presented in the word candidate list. The user would then be able to further<br>
clarify using either mode or both.<br>
In addition, one form of input could be used to "build around" words from the<br>
other. For example, the user can speak the word "home" then press the 9 key<br>
shortly thereafter. Since these seem to conflict, the list of word possibilities<br>
should include words that are phonetically like "home" but also start with the<br>
letters "W, 'x', y, or 'z', which appear on the 9 key. The press of the 9 key<br>
could also be considered as the start of the next part of the compound word<br>
so that when the user says 'work' the press of the 9 key can be used to help<br>
disambiguate the next spoken input.<br><br>
would be able to offer a better interpretation of the utterance. Similarly,<br>
beginning to draw the first character using a handwrittep ideographic<br>
character recognition engine can correct the speech interpretation.<br>
Though an ambiguous stroke-order entry system or a handwriting recognition<br>
engine may not be able to determine definitively which handwritten stroke was<br>
intended, the combination of the acoustic interpretation and the stroke<br>
interpretation sufficiently disambiguates the two modalities of input to offer the<br>
user the intended character. And as noted previously for alphabetic language<br>
speech input correction, after the user selects the corrected ideographic<br>
character the method may include the step of reconsidering the following<br>
character in light of the corrected context and/or how the original acoustic<br>
vectors map to the end of the corrected character and the beginning of the<br>
following character. Due to the corrections, the speech recognition system<br>
may also determine that a momentary pause is less likely to have represented<br>
a delimiter between words or phrases, and so reinterprets the utterance and<br>
displays it as a series of characters representing a single word or phrase<br>
instead of two separate words or phrases; or vice-versa.<br>
The combination of speech recognition and ambiguous entry has other<br>
benefits. In a noisy environment, such as on a city sidewalk, in a busy<br>
cafeteria, or on a construction site, for example, the speech recognition<br>
accuracy may fall below a level acceptable to the user. Or, in a quiet<br>
environment, such as in a library or during a meeting, or when the subject<br>
matter is private or sensitive, it may be unacceptable to use speech dictation.<br>
The user then has the ambiguous input system as a fallback for free text<br>
entry. In addition, it is challenging to recognize or spell out a word that the<br>
speech recognition system doesn't have in its vocabulary, whereas the<br>
ambiguous input system typically offers a reliable means to type any<br>
character sequence and add it to its vocabulary. In addition, the speech<br>
recognition engine may be used to select a word from the list of candidates<br>
displayed by the ambiguous input system.<br><br>
In one embodiment of the invention, the word or phrase interpretations are<br>
ordered relative to the frequency of those words or phrases in common use of<br>
the language. In one embodiment of the invention, the ordering is adapted,<br>
continuously or on occasion, to the user's frequency and/or recency of use of<br>
each word or phrase relative to the others.<br>
In one embodiment of the invention, word completions or predictions that<br>
match the keystrokes or stylus taps entered thus far are offered along with the<br>
other word interpretations, to make retyping of corrections and additional<br>
words faster and easier. In one embodiment of the invention, diacritics such<br>
as vowel accents are placed on the proper characters of the word being<br>
spoken or corrected without the user indicating that a diacritic mark is needed.<br>
In one embodiment of the invention, some or all of the inputs from the<br>
alternative modality are not ambiguous. This may reduce or remove the need<br>
for the disambiguation engine 115 in Figure 1 but still requires the muttimodal<br>
disambiguation engine 117 to reinterpret the vectors or phoneme tags of the<br>
word or phrase being corrected in light of the new input sequence entered<br>
thus far.<br>
In one embodiment of the invention, such as when the ambiguous input<br>
system is an auto-correcting keyboard displayed on a touch-screen device,<br>
each character that is the best interpretation of the user's input during<br>
correction or retyping, such as the closest character to each stylus tap, forms<br>
a sequence that the system displays as an unambiguous interpretation, which<br>
the user may select if the desired word is not in the vocabulary.<br>
In one embodiment of the invention, such as when the ambiguous input<br>
system is utilizing a reduced keyboard such as a standard phone keypad, the<br>
unambiguous interpretation is a two-key or multi-tap interpretation of the key<br>
sequence.<br>
in one embodiment of the invention, the unambiguous interpretation is added<br>
to the vocabulary if the user selects it for correction or output. In one<br><br>
embodiment of the invention, the recognized or corrected word or<br>
unambiguous interpretation identifies a replacement word or phrase for<br>
output, such as an abbreviation for a longer phrase or an acceptable<br>
substitute for a term of profanity. In one embodiment of the invention, the<br>
system adapts to systematic differences between the user's input, e.g. tap<br>
location or slant of the handwritten shape, and the intended characters or<br>
words, based on the subsequent word or phrase interpretations actually<br>
selected by the user.<br>
In one embodiment of the invention, the user invokes a mode in which the<br>
utterances are recognized as discrete characters, e.g., a letter, digit, or<br>
punctuation symbol. The character sequence may be added to the vocabulary<br>
if it is novel, in one embodiment of the invention, alternate words for spelling,<br>
e.g. "Alpha Tango Charlie" or "A as in Andy, P as in Paul", are recognized as<br>
discrete characters.<br>
In one embodiment of the invention, the system may choose to disregard the<br>
vectors or phonetic tags when they no longer provide useful guidance for<br>
reinterpretation or disambiguation, in one embodiment of the invention, the<br>
system provides a means, e.g. a key or gesture, for the user to dismiss some<br>
or all of the acoustic data associated with the recognized words.<br>
In another embodiment, during the installation phase, or continuously upon<br>
the receipt of text messages or other data, information files are scanned for<br>
words to be added to the lexicon. Methods for scanning such information files<br>
are known in the art. As new words are found during scanning, they are<br>
added to a vocabulary module as low frequency words and, as such, are<br>
placed at the end of the word lists with which the words are associated.<br>
Depending on the number of times that a given new word is detected during a<br>
scan, it is assigned a higher priority, by promoting it within its associated list,<br>
thus increasing trie likelihood of the word appearing in the word selection list<br>
during information entry. Standard pronunciation rules for the current or<br>
determined language may be applied to novel words in order to arrive at their<br>
phonetic form for future recognition.<br><br>
Those skilled in the art will also recognize that additional vocabulary modules<br>
(whether by rule or specified in a lexicon) can be enabled within the computer,<br>
for example vocabulary modules containing legal terms, medical terms, and<br>
other languages. Further, in some languages, such as Indic languages, the<br>
vocabulary module may employ templates of valid sub-word sequences to<br>
determine which word component candidates are possible or likely given the<br>
preceding inputs and the word candidates being considered. Via a system<br>
menu, the user can configure the system to cause the additional vocabulary<br>
words to appear first or last in the list of possible words, e.g. with special<br>
coloration or highlighting, or the system may automatically switch the order of<br>
the words based on which vocabulary module supplied the immediately<br>
preceding selected word(s). Consequently, within the scope of the appended<br>
claims, ft will be appreciated that the invention can be practiced otherwise<br>
than as specifically described herein.<br>
Although the invention is described herein with reference to the preferred<br>
embodiment, one skilled in the art will readily appreciate that other<br>
applications may be substituted for those set forth herein without departing<br>
from the spirit and scope of the present invention. Accordingly, the invention<br>
should only be limited by the Claims included below.<br><br>
WE CLAIM:<br>
1.        A method for processing a user's speech using a mobile computer that comprises a<br>
microphone, a display, and a reduced-character keypad, the method comprising operations of:<br>
the computer receiving user speech via the microphone, the speech comprising a series<br>
of spoken words;<br>
the computer performing speech recognition upon the speech to compute an original N-<br>
best list of words for each discrete utterance of the speech;<br>
the computer operating the display to present a proposed sequence of multiple words,<br>
each word comprising: for each given one of the discrete utterances, a best word of the N-best<br>
list for said discrete utterance;<br>
the computer receiving and processing user entered correction to at least a given one of<br>
the displayed best words of the proposed sequence of words, comprising operations of:<br>
in response to the computer receiving user selection of the given word from the<br>
proposed sequence of words, the computer presenting a list of alternate hypothesis including<br>
others of the N-best list of words for the selected word;<br>
the computer receiving user input from the keypad spelling a desired word, where said<br>
user input is inherently ambiguous because the keypad includes multiple letters on some or all<br>
keys;<br>
responsive to receiving the user input, preparing a revised N-best list by limiting entries<br>
of the N-best list of words to words that are spelled by the user input from the keyboard;<br>
where the revised N-best list is further computed considering context and grammar of<br>
the selected word in conjunction with any words of the proposed sequence of words that the<br>
user has previously accepted or corrected;<br>
receiving user choice of a word from the revised N-best list in correction of the selected<br>
wprd;<br>
the computer updating the proposed sequence of words to incorporate the user entered<br>
correction; and<br>
the computer operating the display to present the updated proposed sequence of words;<br>
the operations of the computer receiving and processing user entered correction<br>
comprising:<br><br>
based on the user entered correction to the given one of the displayed best<br>
words, reinterpreting other words in the proposed sequence including any of (1) reinterpreting a<br>
boundary between the words of the proposed sequence, and (2) reinterpreting multiple words<br>
in the proposed sequence as being one word;<br>
wherein operation of computing any of the original and revised N-best lists of words<br>
considers contextual aspects of user's actions, including any of user location and time of day.<br>
2.	The method as claimed in claim 1, where the operation of the computer performing<br>
speech recognition upon the speech utilizes context based upon any of:<br>
a N-gram language model; and<br>
a language model of a speech recognition engine.<br>
3.	The method as claimed in claim 1, comprising:<br>
after user selection of the given word, responsive to receiving user input from the<br>
keypad associated with punctuation or symbols, the computer computing and operating the<br>
display to present a revised-N-best list of words for the selected word limited to punctuation or<br>
one or more symbols.<br>
4.	The method as claimed in claim 1, where the words comprise alphabetically formed<br>
words, and the keys of the keypad correspond to alphabetic letters or where the words<br>
comprise logographic characters formed by strokes, and the keys of the keypad correspond to<br>
said strokes or categories of said strokes.<br>
5.	The method as claimed in claim 1, the preparing operation including refining the<br>
revised N-best list substantially in real time given each keypress of user input received.<br>
6.	The method as claimed in claim 1, comprising:<br>
responsive to the user accepting or correcting one or more words of the proposed<br>
sequence of words, the computer automatically preparing a revised N-best list for one or more<br>
other words, in the sequence based on context of said one or more words relative to the<br>
accepted or corrected one or more words.<br><br>
7.	The method as claimed in claim 1, where the revised N-best list is further computed<br>
considering context and grammar of the selected word in conjunction with words of the<br>
proposed sequence of words, said context and grammar including subject-verb agreement,<br>
case, gender; and number agreements.<br><br><br>
ABSTRACT<br><br>
A METHOD FOR PROCESSING A USER'S<br>
SPEECH USING A MOBILE COMPUTER<br>
A computer-implemented method for processing a user's speech using a mobile<br>
computer that includes a microphone, a display, and a reduced-character keypad is disclosed.<br>
The method involves the steps of receiving user speech via the microphone (300); performing<br>
speech recognition upon the speech to compute an original N-best list of words for each<br>
discrete utterance of the speech (302); operating the display to present a proposed sequence of<br>
multiple words; receiving and processing user entered correction to at least a given one of the<br>
displayed best words of the proposed sequence of words (306).<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBhYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBjbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBjb3JyZXNwb25kZW5jZSBvdGhlcnMucGRm" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 correspondence others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBkZXNjcmlwdGlvbihjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBkcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBmb3JtLTEucGRm" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBmb3JtLTMucGRm" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBnZW5lcmFsIHBvd2VyIG9mIGF1dGhvcml0eS5wZGY=" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 general power of authority.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBpbnRlcm5hdGlvbmFsIHB1YmxpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 international publication.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBpbnRlcm5hdGlvbmFsIHNlYXJjaCBhdXRob3JpdHkgcmVwb3J0LnBkZg==" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 international search authority report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBwY3QgcmVxdWVzdC5wZGY=" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 pct request.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNiBwcmlvcml0eSBkb2N1bWVudC5wZGY=" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006 priority document.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNi1hc3NpZ25tZW50LnBkZg==" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006-assignment.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNi1jb3JyZXNwb25kZW5jZS0xLjEucGRm" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006-correspondence-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM2NDIta29sbnAtMjAwNi1mb3JtLTMtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">03642-kolnp-2006-form-3-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1BQlNUUkFDVC5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1BTUFOREVEIENMQUlNUy5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-AMANDED CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1ERVNDUklQVElPTiAoQ09NUExFVEUpLnBkZg==" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1EUkFXSU5HUy5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-DRAWINGS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1GT1JNIDEucGRm" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1GT1JNIDIucGRm" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1PVEhFUlMucGRm" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgxNC0xMC0yMDExKS1QQS5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(14-10-2011)-PA.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LSgyMy0wMS0yMDEyKS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-(23-01-2012)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUFTU0lHTk1FTlQucGRm" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-ASSIGNMENT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUNPUlJFU1BPTkRFTkNFLnBkZg==" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUVYQU1JTkFUSU9OIFJFUE9SVC5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1rb2xucC0yMDA2LWZvcm0gMTgucGRm" target="_blank" style="word-wrap:break-word;">3642-kolnp-2006-form 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUZPUk0gMy5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-FORM 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUZPUk0gNS5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-FORM 5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdQQS5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GPA.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtQUJTVFJBQ1QucGRm" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtQ0xBSU1TLnBkZg==" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtREVTQ1JJUFRJT04gKENPTVBMRVRFKS5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtRFJBV0lOR1MucGRm" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-DRAWINGS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtRk9STSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtRk9STSAyLnBkZg==" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LUdSQU5URUQtU1BFQ0lGSUNBVElPTi5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-GRANTED-SPECIFICATION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LU9USEVSUy5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzY0Mi1LT0xOUC0yMDA2LVJFUExZIFRPIEVYQU1JTkFUSU9OIFJFUE9SVC5wZGY=" target="_blank" style="word-wrap:break-word;">3642-KOLNP-2006-REPLY TO EXAMINATION REPORT.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="254840-a-method-for-deposition-of-anti-corrosive-coating-on-metallic-and-non-metallic-substrate-surface.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="254842-a-method-of-transmitting-and-receiving-data-in-a-multiple-input-multiple-output-mimo-communication-system-and-appartus-thereof.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>254841</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>3642/KOLNP/2006</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>52/2012</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>28-Dec-2012</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>26-Dec-2012</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>05-Dec-2006</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>AMERICA ONLINE,INCORPORATED</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>22000,AOL WAY, DULLES,VA 20166-9323, U.S.A.</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>LONGE, MICHAEL</td>
											<td>8756,13th AVE NW, SEATTLE,WA 98117, U.S.A.</td>
										</tr>
										<tr>
											<td>2</td>
											<td>HULLFISH, KEITH C.</td>
											<td>13326, -69th DRIVE SE, SNOHOMISH,WA 98296, U.S.A.</td>
										</tr>
										<tr>
											<td>3</td>
											<td>EYRAUD,RICHARD</td>
											<td>620 ,N.34th STREET, #615,SEATTLE,WA 98103, U.S.A.</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G09G5/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/US2005/019357</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2005-06-02</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/576,732</td>
									<td>2004-06-02</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>11/143,409</td>
									<td>2005-06-01</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>3</td>
									<td>10/866,634</td>
									<td>2004-06-10</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>4</td>
									<td>11/043,506</td>
									<td>2005-01-25</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>5</td>
									<td>60/651,302</td>
									<td>2005-02-08</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/254841-a-method-for-processing-a-user-s-speech-using-a-mobile-computer by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:14:14 GMT -->
</html>
