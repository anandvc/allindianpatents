<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/271390-dialogue-enhancement-techniques by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:43:43 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 271390:DIALOGUE ENHANCEMENT TECHNIQUES</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">DIALOGUE ENHANCEMENT TECHNIQUES</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A plural-channel audio signal (e.g., a stereo audio) is processed to modify a gain (e.g., a volume or loudness) of a speech component signal (e.g., dialogue spoken by actors in a movie) relative to an ambient component signal (e.g., reflected or reverberated sound) or other component signals. In one aspect, the speech component signal is identified and modified. In one aspect, the speech component signal is identified by assuming that the speech source (e.g., the actor currently speaking) is in the center of a stereo sound image of the plural-channel audio signal and by considering the spectral content of the speech component signal.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>DIALOGUE ENHANCEMENT TECHNIQUES<br>
RELATED APPLICATIONS<br>
[0001]	This patent application claims priority to the following co-pending U.S.<br>
Provisional Patent Applications:<br>
•	US. Provisional Patent Application No. 60/844,806, for "Method of<br>
Separately Controlling Dialogue Volume/' filed September 14, 2006,<br>
Attorney Docket No. 19819-047P01;<br>
•	VS. Provisional Patent Application No. 60/884,594, for "Separate<br>
Dialogue Volume (SDV)" filed January 11, 2007, Attorney Docket No.<br>
19819-120P01;and<br>
•	US. Provisional Patent Application No. 60/943,268, for "Enhancing<br>
Stereo Audio with Remix Capability and Separate Dialogue," filed June<br>
11,2007, Attorney Docket No. 19819-160P01.<br>
[0002]	Each of these provisional patent applications are incorporated by<br>
reference herein in its entirety.<br>
TECHNICAL FIELD<br>
{00031	The subject matter of this patent application is generally related to<br>
signal processing.<br>
BACKGROUND<br>
[00043	Audio enhancement techniques are often used in home entertainment<br>
systems, stereos and other consumer electronic devices to enhance bass frequencies<br>
and to simulate various listening environments (e.g., concert halls). Some techniques<br>
attempt to make movie dialogue more transparent by adding more high frequencies,<br>
for example. None of these techniques, however, address enhancing dialogue<br>
relative to ambient and other component signals.<br>
SUMMARY<br>
[0005]	A plural-channel audio signal (e.g., a stereo audio) is processed to<br>
modify a gain (e.g., a volume or loudness) of a speech component signal (e.g.,<br>
dialogue spoken by actors in a movie) relative to an ambient component signal (e.g.,<br><br>
reflected or reverberated sound) or other component signals. In one aspect, the<br>
speech component signal is identified and modified. In one aspect, the speech<br>
component signal is identified by assuming that the speech source (e.g., the actor<br>
currently speaking) is in the center of a stereo sound image of the plural-channel<br>
audio signal and by considering the spectral content of the speech component signal.<br>
[0006]	Other implementations are disclosed, including implementations<br>
directed to methods, systems and computer-readable mediums.<br>
DESCRIPTION OF DRAWINGS<br>
[0007]	FIG. 1 is block diagram of a mixing model for dialogue enhancement<br>
techniques.<br>
[0008]	FIG. 2 is a graph illustrating a decomposition of stereo signals using<br>
time-frequency tiles.<br>
[0009]	FIG. 3A is a graph of a function for computing a gain as a function of a<br>
decomposition gain factor for dialogue that is centered in a sound image.<br>
[0010]	FIG. 3B is a graph of a function for computing gain as a function of a<br>
decomposition gain factor for dialogue which is not centered.<br>
[0011]	FIG. 4 is a block diagram of an example dialogue enhancement system.<br>
[0012]	FIG. 5 is a flow diagram of an example dialogue enhancement process.<br>
[0013]	FIG. 6 is a block diagram of a digital television system for<br>
implementing the features and processes described in reference to FIGS. 1-5.<br>
DETAILED DESCRIPTION<br>
Dialogue Enhancement Techniques<br>
[0014]	FIG. 1 is block diagram of a mixing model 100 for dialogue<br>
enhancement techniques. In the model 100, a listener receives audio signals from left<br>
and right channels. An audio signal s corresponds to localized sound from a<br>
direction determined by a factor a. Independent audio signals n1 and n2, correspond<br>
to laterally reflected or reverberated sound, often referred to as ambient sound or<br>
ambience. Stereo signals can be recorded or mixed such that for a given audio<br><br>
source the source audio signal goes coherently into the left and right audio signal<br>
channels with specific directional cues (e.g., level difference, time difference), and<br>
the laterally reflected or reverberated independent signals n1 and n2 go into channels<br>
determining auditory event width and listener envelopment cues. The model 100<br>
can be represented mathematically as a perceptually motivated decomposition of a<br>
stereo signal with one audio source capturing the localization of the audio source<br>
and ambience. <br>
[0015]	To get a decomposition that is effective in non-stationary scenarios<br>
with multiple concurrently active audio sources, the decomposition of [1] can be<br>
carried out independently in a number of frequency bands and adaptively in time<br><br>
where i is a subband index and k is a subband time index.<br>
[0016]	FIG. 2 is a graph illustrating a decomposition of a stereo signal using<br>
time-frequency tiles. In each time-frequency tile 200 with indices i and k, the signals<br>
S, N1, N2 and decomposition gain factor A can be estimated independendy. For<br>
brevity of notation, the subband and time indices i and k are ignored in the following<br>
description.<br>
[0017]	When using a subband decomposition with perceptually motivated<br>
subband bandwidths, the bandwidth of a subband can be chosen to be equal to one<br>
critical band. S, N1, N2, and A can be estimated approximately every t milliseconds<br>
(e.g., 20 ms) in each subband. For low computation complexity, a short time Fourier<br>
transform (STFT) can be used to implement a fast Fourier transform (FFT). Given<br>
stereo subband signals, X1 and X2, estimates of S, A, N1, N2 can be determined. A<br>
short-time estimate of a power of X1 can be denoted<br><br>
where E{.} is a short-time averaging operation. For other signals, the same<br>
convention can be used, i.e., Px2, Ps and PN = PN1=PN2 are the corresponding short-<br><br>
time power estimates. The power of N1 and N2 is assumed to be the same, i.e., it is<br>
assumed that the amount of lateral independent sound is the same for left and right<br>
channels. <br>
[0018]	Given the subband representation of the stereo signal, the power (Px1,<br>
Px2) and the normalized cross-correlation can be determined. The normalized cross-<br>
correlation between left and right channels is<br><br>
[0019]	A, Ps, PN can be computed as a function of the estimated Px1, Px2, and<br>
Ø. Three equations relating the known and unknown variables are:<br><br>
[0020]	Equations [5] can be solved for A, Ps, and PN, to yield<br><br>
with <br>
Least Squares Estimation of S, N1, and N2<br>
[0021]	Next, the least squares estimates of S, N1 and N2 are computed as a<br>
function of A, Ps, and PN. For each i and k, the signal S can be estimated as<br><br><br>
where ω1 and ω2 are real-valued weights. The estimation error is<br>
  [9]<br>
The weights ω1 and ω2 are optimal in a least square sense when the error £ is<br>
orthogonal to X1 and X2 [6], i.e.,<br>
  [10]<br>
yielding two equations<br>
  [11]<br>
from which the weights are computed,<br>
  [12]<br>
[0022]	The estimate of N3 can be<br>
  [13]<br>
[0023]	The estimation error is<br>
  [14]<br>
[0024]	Again, the weights are computed such that the estimation error is<br>
orthogonal to X1 and X2, resulting in<br>
  [15]<br>
[0025]	The weights for computing the least squares estimate of N2,<br><br><br>
[0026]	In some implementations, the least squares estimates can be post-<br>
scaled, such that the power of the estimates equals to Ps and PN = Pm = Pm- The<br>
power of S is<br>
  [18]<br><br><br>
[0029]	Given the previously described signal decomposition, a signal that is<br>
similar to the original stereo signal can be obtained by applying [2] at each time and<br>
for each subband and converting the subbands back to the time domain.<br>
[0030]	For generating the signal with modified dialogue gain, the subbands<br>
are computed as<br><br><br>
where g(i,k) is a gain factor in dB which is computed such that the dialogue gain is<br>
modified as desired.<br>
[0031]	There are several observations which motivate how to compute g(i,k)'.<br>
•	Usually dialogue is in the center of the sound image, i.e., a component signal at<br>
time k and frequency i belonging to dialogue will have a corresponding<br>
decomposition gain factor A(i,k) close to one (OdB).<br>
•	Speech signals contain most energy up to 4 kHz. Above 8 kHz speech contains<br>
virtually no energy.<br>
•	Speech usually also does not contain very low frequencies (e.g., below about 70<br>
Hz).<br>
[0032]	These observations imply g(i,k) is set to 0 dB at very low frequencies<br>
and above 8 kHz, to potentially modify the stereo signal as little as possible. At<br>
other frequencies, g(i,k) is controlled as a function of the desired dialogue gain Gd<br>
and A(i,k):<br><br>
[0033]	An example of a suitable function f is illustrated in FIG. 3A. Note that<br>
in FIG. 3 A the relation between/and A(i,k) is plotted using logarithmic (dB) scale,<br>
but A{i,k) and / are otherwise defined in linear scale. A specific example for / is:<br><br>
where W determines the width of a gain region of the function/, as illustrated in FIG.<br>
3A. The constant W is related to the directional sensitivity of the dialogue gain. A<br>
value of PV = 6 dB, for example, gives good results for most signals. But it is noted<br>
that for different signals different W may be optimal.<br>
[0034]	Due to bad calibration of a broadcasting or receiving equipment (e.g.,<br>
different gains for left and right channels), it may be that the dialogue does not<br>
appear exactly in the center. In this case, the function/can be shifted such that its<br><br>
center corresponds to the dialogue position. An example of a shifted function / is<br>
illustrated in FIG. 3B.<br>
Alternative Implementations and Generalizations<br>
[0035]	The identification of dialogue component signals based on center-<br>
assumption (or generally position-assumption) and spectral range of speech is<br>
simple and works well in many cases. The dialogue identification, however, can be<br>
modified and potentially improved. One possibility is to explore more features of<br>
speech, such as formants, harmonic structure, transients to detect dialogue<br>
component signals.<br>
[0036]	As noted, for different audio material a different shape of the gain<br>
function (e.g., FIGS. 3A and 3B) may be optimal. Thus, a signal adaptive gain<br>
function may be used.<br>
[0037]	Dialogue gain control can also be implemented for home cinema<br>
systems with surround sound. One important aspect of dialogue gain control is to<br>
detect whether dialogue is in the center channel or not. One way of doing this is to<br>
detect if the center has sufficient signal energy such that it is likely that dialogue is in<br>
the center channel. If dialogue is in the center channel, then gain can be added to the<br>
center channel to control the dialogue volume. If dialogue is not in the center<br>
channel (e.g., if the surround system plays back stereo content), then a two-channel<br>
dialogue gain control can be applied as previously described in reference to FIGS. 1-<br>
3.<br>
[0038]	In some implementations, tine disclosed dialogue enhancement<br>
techniques can be implemented by attenuating signals other than the speech<br>
component signal. For example, a plural-channel audio signal can include a speech<br>
component signal (e.g., a dialogue signal) and other component signals (e.g.,<br>
reverberation). The other component signals can be modified (e.g., attenuated)<br>
based on a location of the speech component signal in a sound image of the plural-<br>
channel audio signal and the speech component signal can be left unchanged.<br>
Dialogue Enhancement System<br>
[0039]	FIG. 4 is a block diagram of an example dialogue enhancement system<br>
400. In some implementations, the system 400 includes an analysis filterbank 402, a<br><br>
power estimator 404, a signal estimator 406, a post-scaling module 408, a signal<br>
synthesis module 410 and a synthesis filterbank 412. While the components 402-412<br>
of system 400 are shown as a separate processes, the processes of two or more<br>
components can be combined into a single component.<br>
[0040]	For each time k, a plural-channel signal by the analysis filterbank 402<br>
into subband signals z. In the example shown, left and right channels x1(n), x2(n) of a<br>
stereo signal are decomposed by the analysis filterbank 402 into i subbands X1(i,k),<br>
X1(i,k). The power estimator 404 generates power estimates of Ps, A, and PN, which<br>
have been previously described in reference to FIGS. 1 and 2. The signal estimator<br>
406 generates the estimated signals S, N1,, and N2 from the power estimates. The<br>
post-scaling module 408 scales the signal estimates to provide S', N'2 and N'2 . The<br>
signal synthesis module 410 receives the post-scaled signal estimates and<br>
decomposition gain factor A, constant W and desired dialogue gain Gd, and<br>
synthesizes left and right subband signal estimates Y,(i,k) and Y2(i,k) which are<br>
input to the synthesis filterbank 412 to provide left and right time domain signals<br>
y,(n) and y2(ri) with modified dialogue gain based on Gj.<br>
Dialogue Enhancement Process<br>
[0041]	FIG. 5 is a flow diagram of an example dialogue enhancement process<br>
500. In some implementations, the process 500 begins by decomposing a plural-<br>
channel audio signal into frequency subband signals (502). The decomposition can<br>
be performed by a filterbank using various known transforms, including but not<br>
limited to: polyphase filterbank, quadrature mirror filterbank (QMF), hybrid<br>
filterbank, discrete Fourier transform (DFT), and modified discrete cosine transform<br>
(MDCT).<br>
[0042]	A first set of powers of two or more channels of the audio signal are<br>
estimated using the subband signals (504). A cross-correlation is determined using<br>
the first set of powers (506). A decomposition gain factor is estimated using the first<br>
set of powers and the cross-correlation (508). The decomposition gain factor<br>
provides a location cue for the dialogue source in the sound image. A second set of<br>
powers for a speech component signal and an ambience component signal are<br><br>
estimated using the first set of powers and the cross-correlation (510). Speech and<br>
ambience component signals are estimated using the second set of powers and the<br>
decomposition gain factor (512). The estimated speech and ambience component<br>
signals are post-scaled (514). Subband signals are synthesized with modified<br>
dialogue gain using the post-scaled estimated speech and ambience component<br>
signals and a desired dialogue gain (516). The desired dialogue gain can be set<br>
automatically or specified by a user. The synthesized subband signals are converted<br>
into a time domain audio signal with modified dialogue gain (512) using a synthesis<br>
filterbank, for example.<br>
Output Normalization for Background Suppression<br>
[0043]	In some implementations, it is desired to suppress audio of<br>
background scenes rather than boosting the dialogue signal. This can be achieved by<br>
normalizing the dialogue-boosted output signal with dialogue gain. The<br>
normalization can be performed in at least two different ways. In one example, the<br>
output signal and can be normalized by a normalization factor gnom.<br><br>
[0044]	The another example, the dialogue boosting effect is compensated by<br>
normalizing using weights with  The normalization factor , can take<br>
the same value as the modified dialogue gain <br>
[0045]	To maximize the perceptual quality,  can be modified. The<br>
normalization can be performed both in frequency domain and in time domain.<br>
When it is performed in frequency domain, the normalization can be performed for<br>
the frequency band where dialogue gain applies, for example, between 70 Hz and 8<br>
KHz.<br>
[0046]Alternatively, a similar result can be achieved as attenuating <br>
and while applying no gain to  This concept can be described with the<br>
following equations:<br><br><br>
Using Separate Dialogue Volume Based on Mono Detection<br>
[0047]	When input signals and  are substantially similar, e.g.,<br>
input is a mono-like signal, almost every portion of input might be regarded as S,<br>
and when a user provides a desired dialogue gain, the desired dialogue gain<br>
increases the volume of the signal. To prevent this, it is desirable to user a separate<br>
dialogue volume (SDV) technique to observe the characteristics of the input signals.<br>
[0048]	In [4], the normalized cross-correlation of stereo signals is calculated.<br>
The normalized cross-correlation can be used as a metric for mono signal detection.<br>
When phi in [4] exceeds a given threshold, the input signal can be regarded as a<br>
mono signal, and separate dialogue volume can be automatically turned off. By<br>
contrast, when phi is smaller than a given threshold, the input signal can be<br>
regarded as a stereo signal, and separate dialogue volume can be automatically<br>
turned on. The dialogue gain can be operated as an algorithmic switch for separate<br>
dialogue volume as:<br><br>
[0050]	One example is to apply weighting for   inverse-proportionality<br>
to is<br><br>
[0051]	To prevent sudden change of time smoothing techniques can<br>
be incorporated to get <br><br>
Digital Television System Example<br>
[0052]	FIG. 6 is a block diagram of a an example digital television system 600<br>
for implementing the features and processes described in reference to FIGS. 1-5.<br>
Digital television (DTV) is a telecommunication system for broadcasting and<br>
receiving moving pictures and sound by means of digital signals. DTV uses digital<br>
modulation data, which is digitally compressed and requires decoding by a specially<br>
designed television set, or a standard receiver with a set-top box, or a PC fitted with<br>
a television card. Although the system in FIG. 6 is a DTV system, the disclosed<br>
implementations for dialogue enhancement can also be applied to analog TV<br>
systems or any other systems capable of dialogue enhancement.<br>
[0053]	In some implementations, the system 600 can include an interface 602,<br>
a demodulator 604, a decoder 606, and audio/visual output 608, a user input<br>
interface 610, one or more processors 612 (e.g., Intel® processors) and one or more<br>
computer readable mediums 614 (e.g., RAM, ROM, SDRAM, hard disk, optical disk,<br>
flash memory, SAN, etc.). Each of these components are coupled to one or more<br>
communication channels 616 (e.g., buses). In some implementations, the interface<br>
602 includes various circuits for obtaining an audio signal or a combined<br>
audio/video signal. For example, in an analog television system an interface can<br>
include antenna electronics, a tuner or mixer, a radio frequency (RF) amplifier, a<br>
local oscillator, an intermediate frequency (IF) amplifier, one or more filters, a<br>
demodulator, an audio amplifier, etc. Other implementations of the system 600 are<br>
possible, including implementations with more or fewer components.<br>
[0054]	The tuner 602 can be a DTV tuner for receiving a digital televisions<br>
signal include video and audio content. The demodulator 604 extracts video and<br>
audio signals from the digital television signal. If the video and audio signals are<br>
encoded (e.g., MPEG encoded), the decoder 606 decodes those signals. The A/V<br>
output can be any device capable of display video and playing audio (e.g., TV<br>
display, computer monitor, LCD, speakers, audio systems).<br>
[0055]	In some implementations, dialogue volume levels can be displayed to<br>
the user using a display device on a remote controller or an On Screen Display<br>
(OSD), for example. The dialogue volume level can be relative to the master volume<br><br>
level. One or more graphical objects can be used for displaying dialogue volume<br>
level, and dialogue volume level relative to master volume. For example, a first<br>
graphical object (e.g., a bar) can be displayed for indicating master volume and a<br>
second graphical object (e.g., a line) can be displayed with or composited on the first<br>
graphical object to indicate dialogue volume level.<br>
[0056]	In some implementations, the user input interface can include circuitry<br>
(e.g., a wireless or infrared receiver) and/or software for receiving and decoding<br>
infrared or wireless signals generated by a remote controller. A remote controller<br>
can include a separate dialogue volume control key or button, or a separate dialogue<br>
volume control select key for changing the state of a master volume control key or<br>
button, so that the master volume control can be used to control either the master<br>
volume or the separated dialogue volume. In some implementations, the dialogue<br>
volume or master volume key can change its visible appearance to indicate its<br>
function.<br>
[0057]	An example controller and user interface are described in U.S. Patent<br>
Application No.	, for "Controller and User Interface For Dialogue<br>
Enhancement Techniques," filed September 14, 2007, Attorney Docket No. 19819-<br>
160001, which patent application is incorporated by reference herein in its entirety.<br>
[0058]	In some implementations, the one or more processors can execute code<br>
stored in the computer-readable medium 614 to implement the features and<br>
operations 618,620, 622,624, 626, 628, 630 and 632, as described in reference to FIGS.<br>
1-5.<br>
[0059]	The computer-readable medium further includes an operating system<br>
618, analysis/synthesis filterbanks 620, a power estimator 622, a signal estimator 624,<br>
a post-scaling module 626 and a signal synthesizer 628. The term "computer-<br>
readable medium" refers to any medium that participates in providing instructions<br>
to a processor 612 for execution, including without limitation, non-volatile media<br>
(e.g., optical or magnetic disks), volatile media (e.g., memory) and transmission<br>
media. Transmission media includes, without limitation, coaxial cables, copper wire<br>
and fiber optics. Transmission media can also take the form of acoustic, light or<br>
radio frequency waves.<br><br>
[0060]	The operating system 618 can be multi-user, multiprocessing,<br>
multitasking, multithreading, real time, etc. The operating system 618 performs<br>
basic tasks, including but not limited to: recognizing input from the user input<br>
interface 610; keeping track and managing files and directories on computer-<br>
readable medium 614 (e.g., memory or a storage device); controlling peripheral<br>
devices; and managing traffic on the one or more communication channels 616.<br>
[0061]	The described features can be implemented advantageously in one or<br>
more computer programs that are executable on a programmable system including<br>
at least one programmable processor coupled to receive data and instructions from,<br>
and to transmit data and instructions to, a data storage system, at least one input<br>
device, and at least one output device. A computer program is a set of instructions<br>
that can be used, directly or indirectly, in a computer to perform a certain activity or<br>
bring about a certain result. A computer program can be written in any form of<br>
programming language (e.g., Objective-C, Java), including compiled or interpreted<br>
languages, and it can be deployed in any form, including as a stand-alone program<br>
or as a module, component, subroutine, or other unit suitable for use in a computing<br>
environment.<br>
[0062]	Suitable processors for the execution of a program of instructions<br>
include, by way of example, both general and special purpose microprocessors, and<br>
the sole processor or one of multiple processors or cores, of any kind of computer.<br>
Generally, a processor will receive instructions and data from a read-only memory<br>
or a random access memory or both. The essential elements of a computer are a<br>
processor for executing instructions and one or more memories for storing<br>
instructions and data. Generally, a computer will also include, or be operatively<br>
coupled to communicate with, one or more mass storage devices for storing data<br>
files; such devices include magnetic disks, such as internal hard disks and removable<br>
disks; magneto-optical disks; and optical disks. Storage devices suitable for tangibly<br>
embodying computer program instructions and data include all forms of non-<br>
volatile memory, including by way of example semiconductor memory devices, such<br>
as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal<br>
hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-<br><br>
ROM disks. The processor and the memory can be supplemented by, or<br>
incorporated in, ASICs (application-specific integrated circuits).<br>
[0063]	To provide for interaction with a user, the features can be implemented<br>
on a computer having a display device such as a CRT (cathode ray tube) or LCD<br>
(liquid crystal display) monitor for displaying information to the user and a<br>
keyboard and a pointing device such as a mouse or a trackball by which the user can<br>
provide input to the computer.<br>
[0064]	The features can be implemented in a computer system that includes a<br>
back-end component, such as a data server, or that includes a middleware<br>
component, such as an application server or an Internet server, or that includes a<br>
front-end component, such as a client computer having a graphical user interface or<br>
an Internet browser, or any combination of them. The components of the system can<br>
be connected by any form or medium of digital data communication such as a<br>
communication network. Examples of communication networks include, e.g., a<br>
LAN, a WAN, and the computers and networks forming the Internet.<br>
[0065]	The computer system can include clients and servers. A client and<br>
server are generally remote from each other and typically interact through a<br>
network. The relationship of client and server arises by virtue of computer<br>
programs running on the respective computers and having a client-server<br>
relationship to each other.<br>
[0066]	A number of implementations have been described. Nevertheless, it<br>
will be understood that various modifications may be made. For example, elements<br>
of one or more implementations may be combined, deleted, modified, or<br>
supplemented to form further implementations. As yet another example, the logic<br>
flows depicted in the figures do not require the particular order shown, or sequential<br>
order, to achieve desirable results. In addition, other steps may be provided, or<br>
steps may be eliminated, from the described flows, and other components may be<br>
added to, or removed from, the described systems. Accordingly, other<br>
implementations are within the scope of the following claims.<br><br>
CLAIMS<br>
WHAT IS CLAIMED IS:<br>
1.	A method comprising:<br>
obtaining a plural-channel audio signal including a speech component<br>
signal and other component signals; and<br>
modifying the speech component signal based on a location of the speech<br>
component signal in a sound image of the audio signal.<br>
2.	The method of claim 1, where modifying further comprises:<br>
modifying the speech component signal based on the spectral content of<br>
the speech component signal.<br>
3.	The method of claim 1 or 2, where the modifying further comprises:<br>
determining the location of the speech component signal in the sound<br>
image; and<br>
applying a gain factor to the speech component signal.<br>
4.	The method of claim 3, where the gain factor is a function of the location of the<br>
speech component signal and a desired gain for the speech component signal.<br>
5.	The method of claim 4, where the function is a signal adaptive gain function<br>
having a gain region that is related to a directional sensitivity of the gain factor.<br>
6.	The method of any one of the preceding claims, where the modifying further<br>
comprises:<br>
normalizing the plural-channel audio signal with a normalization factor<br>
in a time domain or a frequency domain.<br>
7.	The method of any one of the preceding claims, further comprising:<br>
determining if the audio signal is substantially mono; and<br><br>
if the audio signal is not substantially mono, automatically modifying the<br>
speech component signal.<br>
8.	The method of claim 7, where determining if the audio signal is substantially<br>
mono, further comprises:<br>
determining a cross-correlation between two or more channels of the<br>
audio signal; and<br>
comparing the cross-correlation with one or more threshold values; and<br>
determining if the audio signal is substantially mono based on results of<br>
the comparison.<br>
9.	The method of any one of the preceding claims, where modifying further<br>
comprises:<br>
decomposing the audio signal into a number of frequency subband<br>
signals;<br>
estimating a first set of powers for two or more channels of the plural-<br>
channel audio signal using the subband signals;<br>
determining a cross-correlation using the first set of estimated powers;<br>
estimating a decomposition gain factor using the first set of estimated<br>
powers and the cross-correlation.<br>
10.	The method of claim 9, where the bandwidth of at least one subband is selected<br>
to be equal to one critical band of a human auditory system.<br>
11.	The method of claim 8, comprising:<br>
estimating a second set of powers for the speech component signal and<br>
an ambience component signal from the first set of powers and the cross-<br>
correlation.<br>
12.	The method of claim 11, further comprising:<br><br>
estimating the speech component signal and the ambience component<br>
signal using the second set of powers and the decomposition gain factor.<br>
13.	The method of claim 12, where the estimated speech and ambience component<br>
signals are determined using least squares estimation.<br>
14.	The method of claim 12, where the cross-correlation is normalized.<br>
15.	The method of claim 13 or 14, where the estimated speech component signal and<br>
the estimated ambience component signal are post-scaled.<br>
16.	The method of any one of claims 11 to 15, further comprising:<br>
synthesizing subband signals using the estimated second powers and a<br>
user-specified gain.<br>
17.	The method of claim 16, further comprising:<br>
converting the synthesized subband signals into a time domain audio<br>
signal having a speech component signal which is modified by the user-specified<br>
gain.<br>
18.	A method comprising:<br>
obtaining an audio signal;<br>
obtaining user input specifying a modification of a first component signal<br>
of the audio signal; and<br>
modifying the first component signal based on the input and a location<br>
cue of the first component signal in a sound image of the audio signal.<br>
19.	The method of claim 18, where the modifying further comprises:<br>
applying a gain factor to the first component signal.<br><br>
20.	The method of claim 19, where the gain factor is a function of the location cue<br>
and a desired gain for the first component signal.<br>
21.	The method of claim 20, where the function has a gain region that is related to a<br>
directional sensitivity of the gain factor.<br>
22.	The method of any one of claims 18 to 21, where the modifying further<br>
comprises:<br>
normalizing the audio signal with a normalization factor in a time<br>
domain or a frequency domain.<br>
23.	The method of any one of claims 18 to 22, where modifying further comprises:<br>
decomposing the audio signal into a number of frequency subband<br>
signals;<br>
estimating a first set of powers for two or more channels of the audio<br>
signal using the subband signals;<br>
determining a cross-correlation using the first set of powers;<br>
estimating a decomposition gain factor using the first set of powers and<br>
the cross-correlation;<br>
estimating a second set of powers for the first component signal and a<br>
second component signal from the first set of powers and the cross-correlation;<br>
estimating the first component signal and the second component signal<br>
using the second set of powers and the decomposition gain factor;<br>
synthesizing subband signals using the estimated first and second<br>
component signals and the input; and<br>
converting the synthesized subband signals into a time domain<br>
audio signal having a modified first component signal.<br>
24.	A system comprising:<br>
an interface configurable for obtaining a plural-channel audio signal<br>
including a speech component signal and other component signals; and<br><br>
a processor coupled to the interface and configurable for modifying the<br>
speech component signal based on a location of the speech component signal in a<br>
sound image of the audio signal.<br>
25. A method comprising:<br>
obtaining a plural-channel audio signal including a speech component<br>
signal and other component signals; and<br>
modifying the other component signals based on a location of the speech<br>
component signal in a sound image of the plural-channel audio signal.<br><br>
A plural-channel audio signal (e.g., a stereo audio) is processed to modify a gain (e.g., a volume or loudness) of<br>
a speech component signal (e.g., dialogue spoken by actors in a movie) relative to an ambient component signal (e.g., reflected or<br>
reverberated sound) or other component signals. In one aspect, the speech component signal is identified and modified. In one<br>
aspect, the speech component signal is identified by assuming that the speech source (e.g., the actor currently speaking) is in the<br>
center of a stereo sound image of the plural-channel audio signal and by considering the spectral content of the speech component<br>
signal.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=tlMgwJZ6A9G/AbbsixGs3w==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==" target="_blank" style="word-wrap:break-word;">http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=tlMgwJZ6A9G/AbbsixGs3w==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==</a></p>
		<br>
		<div class="pull-left">
			<a href="271389-transmit-power-initialization-for-secondary-reverse-link-carriers-in-a-wireless-communication-network.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="271391-method-for-transmitting-receiving-feedback-information-and-method-for-trnsmitting-receiving-data-using-the-same.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>271390</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>948/KOLNP/2009</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>09/2016</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>26-Feb-2016</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>18-Feb-2016</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>12-Mar-2009</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>LG ELECTRONICS INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>20, YOIDO-DONG, YONGDUNGPO-KU, SEOUL</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>FALLER, CHRISTOF</td>
											<td>ILLUSONIC LLC, ROUTE DE LA MALADIERE 6, CH-1022 CHAVANNES-PRES-RENENS</td>
										</tr>
										<tr>
											<td>2</td>
											<td>OH, HYEN-O</td>
											<td>306-403, GANGSEON MAEUL 3-DANJI AP, JUYEOP 1 (IL)-DONGILSAN-GU, GOYANG-SI, GYEONGGI-DO</td>
										</tr>
										<tr>
											<td>3</td>
											<td>JUNG, YANG, WON</td>
											<td>2-203, YEOKSAM HANSHIN APARTMENT, DOGOK-DONG, GANGNAM-GU, SEOUL 135-270</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H04S 3/00,G01L 19/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/EP2007/008028</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2007-09-14</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/943,268</td>
									<td>2007-06-11</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>60/884,594</td>
									<td>2007-01-11</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>3</td>
									<td>60/844,806</td>
									<td>2006-09-14</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/271390-dialogue-enhancement-techniques by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:43:44 GMT -->
</html>
