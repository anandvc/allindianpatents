<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/280051-methods-and-systems-for-digitally-re-mastering-of-2d-and-3d-motion-pictures-for-exhibition-with-enhanced-visual-quality by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 04 Apr 2024 22:34:20 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 280051:METHODS AND SYSTEMS FOR DIGITALLY RE-MASTERING OF 2D AND 3D MOTION PICTURES FOR EXHIBITION WITH ENHANCED VISUAL QUALITY</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">METHODS AND SYSTEMS FOR DIGITALLY RE-MASTERING OF 2D AND 3D MOTION PICTURES FOR EXHIBITION WITH ENHANCED VISUAL QUALITY</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>The present invention relates to methods and systems for the exhibition of a motion picture with enhanced per- ceived resolution and visual quality. The enhancement of perceived resolution is achieved both spatially and temporally. Spatial resolution enhancement creates image details using both temporal-based methods and learning-based methods. Temporal resolution enhancement creates synthesized new image frames that enable a motion picture to be displayed at a higher frame rate. The digitally enhanced motion picture is to be exhibited using a projection system or a display device that supports a higher frame rate and/or a higher display resolution than what is required for the original motion picture.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Methods and Systems for Digitally Re-mastering of 2D and 3D Motion Pictures<br>
for Exhibition with Enhanced Visual Quality<br>
CROSS REFERENCE TO RELATED APPLICATIONS<br>
[0001] The present application claims priority to U.S. provisional patent application<br>
no. 60/762,964, entitled "Methods and Systems for Digitally Re-mastering of 2D and<br>
3D Motion Pictures for Exhibition with Enhanced Visual Quality," filed January 27,<br>
2006, the entire contents of which is incorporated herein by reference.<br>
FIELD OF THE INVENTION<br>
[0002] The present invention relates generally to methods and systems for enhancing<br>
a motion picture and more specifically to enhancing the resolution and quality of the<br>
motion picture.<br>
BACKGROUND OF THE INTVENTION<br>
[0003] Motion pictures are composed of a sequence of image frames displayed to<br>
viewers at a fast frame rate. The perceived image resolution is a key indicator for the<br>
exhibition quality of a motion picture, and it is a combined result of both spatial<br>
resolution and temporal resolution. The spatial resolution measures the level of details<br>
within each image frame that can be perceived by audience, and it is determined by<br>
the quality of the display system as well as by the quality of motion picture content.<br>
The temporal resolution of a motion picture measures the level of motion smoothness<br>
of a moving image sequence, and it is determined by the frame rate at which the<br>
motion picture images are displayed. For cinematic presentations, the standard frame<br>
rate of a conventional motion picture is 24fps (frames per second). However, there<br>
exists a number of higher frame rate motion picture formats. An example of<br>
presenting a motion picture digitally at a higher frame rate is generally described in<br>
U.S. patent application no. 2002/0149696 as a method of temporally interpolating<br>
motion images to a higher frame rate so the motion picture can be presented digitally<br>
at the original frame rate or a higher frame rate. The frame Tate interpolation method<br>
may rely on motion vector analysis, such as the analysis used in Kodak's Cineon<br>
System. However, such system does not provide for digitally re-mastering an entire<br>
motion picture or producing acceptable image quality for cinematic or large format<br>
cinematic applications that demand relatively high visual and audio quality. For<br>
example, the system does not provide for artifact repair or sufficient processing speed<br>
to meet day and date release schedules.<br>
 <br>
[0004] Other examples of such high frame rate motion picture formats include<br>
Showscan (60fps), ToDD-AO (30fps) and MAX® HD (48fps). IMAX® HD is a 15-<br>
Perf/70mm film format that captures and displays a motion picture at a frame rate of<br>
48fps. The first IMAX® HD film Momentum was produced by the National Film<br>
Board (NFB) of Canada and premiered at EXPO92 in Seville, Spain, in 1992. The<br>
study on the IMAX® HD technology was subsequently presented to 135th SMPTE<br>
Technical Conference in 1993. The study indicated that, compared with a standard<br>
IMAX® format using a frame rate of 24fps, IMAX® HD dramatically improves<br>
image realism by enhancing clarity and sharpness, reducing film grain noise and<br>
virtually eliminating of motion artifacts like strobing and motion blur. The study<br>
further indicated that, even for still shots, the perceived image resolution was "notably<br>
greater than standard IMAX® (format)". The study provided evidence that temporal<br>
resolution enhancement through frame rate increase could improve perceived image<br>
resolution. Similar resolution improvement effects were later reported from the<br>
experiment work on two other 48fps film-based projection formats. One of such<br>
format was a 3-Perf/35mm format called MaxiVision, and the other was a 5-<br>
Perf/70mm format called Super Dimension-70 (SDS-70) by Super Vista.<br>
[0005] Over the past decade, relatively few motion pictures have been produced and<br>
exhibited at a frame rate higher than 24fps. There are both economical and technical<br>
limitations that prevent a film-based motion picture from being produced at a higher<br>
frame rate. On the production side, shooting at a higher frame rate increases the film<br>
costs and production costs. More lighting may be needed on a set due to reduced<br>
exposure time as the result of using a higher frame rate, which contributes to the<br>
production cost increase. On the exhibition side, projecting at a higher frame rate<br>
significantly increases the complexity and the cost of a film projector as well as the<br>
cost of film prints. Because of those limitations, neither IMAX® HD nor other<br>
proposed higher frame rate film formats became financially viable for mainstream<br>
motion picture productions.<br>
[0006] The advance of digital projection technology makes it possible to<br>
economically exhibit motion pictures at a higher frame rate. The Digital Cinema<br>
System Specification recently released by Digital Cinema Initiatives (DCI) includes<br>
48fps as a projection option. However, the cost of producing a motion picture at a<br>
48fps remains relatively high. One solution is to enhance the temporal resolution of a<br>
motion picture by converting the images to a higher frame rate. A frame rate<br>
 <br>
conversion method actually creates synthesized image frames digitally based on the<br>
original image frames. Over the past decades, a number of frame rate conversion<br>
methods were developed for motion pictures and for video format conversion. These<br>
methods range from simple frame (field) repeating, frame (field) averaging to more<br>
complex methods such as motion-compensated frame interpolation. A motion-<br>
compensated (MC) method analyzes the motion of image elements across neighboring<br>
image frames and creates a synthesized new frames based on the estimated motion<br>
information. An MC method usually produces smoother motion than other methods.<br>
[0007] A typical MC method has a motion estimator that calculates the movement of<br>
each image element of an image frame with respect to adjacent frames. An image<br>
element can be defined as a single pixel, a block of n x m pixels or a group of pixels<br>
describing an object. A single motion vector is normally used to indicate the direction<br>
and the strength of the movement of an image element from a present frame to a<br>
future frame. Sometimes, a pair of motion vectors is used to indicate the movement of<br>
an image element both from a present frame to a future frame and from the future<br>
frame back to the present frame. This is called bi-directional motion estimation.<br>
Motion vectors may not be sufficient to describe the movement of a group of pixels<br>
describing an object because the shape of the object may also change from a present<br>
frame to a future frame. In such a case, some forms of mathematical description of<br>
object shape warping may also be included along with motion vectors. There has been<br>
a plethora of MC methods proposed over the last decade for video format conversion.<br>
A majority of those methods can be fully automated with little or no need for human<br>
intervention. However, none of those methods are capable of producing adequate<br>
image quality required for motion picture applications.<br>
[0008] Some algorithms have been proposed for converting a motion picture to a<br>
video format at a field rate of 50/60 fields per second. Such applications typically<br>
require fully automated algorithms ranging from standard 3:2 pulldown to motion-<br>
compensation based frame rate conversation (MCFRC) algorithms. An MCFRC<br>
algorithm may create better image quality and smoother motion but it also produce<br>
other artifacts that result from motion estimation errors. The MCFRC algorithm<br>
generally includes three categories: (1) block-based methods; (2) object-based<br>
methods; and (3) pixel-based methods. The block-based methods can be implemented<br>
using common block-based motion estimation algorithms similar to those in MPEG<br>
and H.264/AVC codecs. The object based methods may produce fewer artifacts than<br>
 <br>
others, but are generally not very stable. An example of an advanced object-based<br>
MCFRC algorithm is generally disclosed in U.S. patent no. 6,625,333. The pixel-<br>
based methods are generally computationally expensive.<br>
[0009] Frame rate conversion methods are also used for creating special visual effects<br>
(VFX) such as to create slow motion, fast motion or variable-motion sequences,<br>
which are frequently practiced in the production of a motion picture, commercials and<br>
video. Examples of commercial software tools available for such VFX applications<br>
include ReTimer software by Realviz and Time Warp software by Algolith. Retimer<br>
provides the ability to create digital "slow motion" or variable motion and allows<br>
users to edit rate curves and motion vector fields to achieve desirable results. Some<br>
such commercial software tools deploy some forms of MC methods. For instance, the<br>
MC algorithms behind ReTimer are based on block image elements, while the<br>
algorithms behind TimeWarp are based on object image elements, such as those<br>
developed by Communications Research Centre Canada (CRC) and described in U.S.<br>
patent no. 6,625,333. Such commercial software tools, however, are not designed for<br>
automated computation, and they reply on human users to provide user inputs<br>
interactively through a GUI. Furthermore, these software tools inevitably produce<br>
unacceptable artifacts due to problems like occlusion and motion estimation errors,<br>
and they do not provide efficient tools and methods to handle those problems.<br>
Although the resulting artifacts can potentially be fixed through manual fixes by<br>
skillful human users, the process is relatively labor-intensive, costly and time<br>
consuming.<br>
[0010] Increasing the spatial resolution of each image frame can also improve the<br>
perceived resolution of a motion picture. A conventional motion picture shot on<br>
35mm negative film is limited to a spatial resolution of approximately 80 cycles per<br>
mm, or approximately 1,800 lines per picture height for 1.85 projection format Due<br>
to the generational modulation transfer function (MTF) losses from standard film lab<br>
processes, the spatial resolution of a release film print is reduced to approximately<br>
875 lines per picture height or lower.<br>
[0011] The advance of digital cinema technology eliminates some major sources of<br>
MTF losses, especially those from the standard film lab process, so that it becomes<br>
feasible to present a motion picture with a higher perceived resolution than a typical<br>
release film print. The DCI Digital Cinema System Specification recommends that a<br>
digital motion picture be presented at a 2K or 4K format. A 2K digital format can<br>
 <br>
theoretically support a spatial resolution up to 1,080 lines per picture height, while a<br>
4K digital format can support up to 2,160 lines per picture height. However, the<br>
quality of a digital cinema presentation cannot be guaranteed unless the quality of<br>
motion picture image content can match the spatial resolution of a digital cinema<br>
system. Because of MTF degradations from various stages of the motion picture<br>
production and post-production processes, including capture, scanning, VFX and data<br>
compression, the resulting motion picture images may have a much lower spatial<br>
resolution than what can be supported by a digital projector.<br>
[0012] It is a major challenge to improve the spatial resolution of motion picture<br>
images in order to produce a high quality cinematic experience, especially when a<br>
motion picture is to be presented in a large format cinema. A typical large format<br>
cinema, such as an IMAX® theatre, has a screen as large as 80 feet in height. In such<br>
a theatre, the audience seated much closer to the screen than in a conventional cinema.<br>
Delivering a satisfactory visual experience to the audience in such a theatre requires<br>
significant enhancement in image quality, such as the perceived resolution. Even<br>
when such enhancement methods are applied, it is difficult to complete all required<br>
processing within a relatively short time window in order for the enhanced motion<br>
picture to be released on schedule.<br>
SUMMARY OF THE INVENTION<br>
[0013] Certain aspects and embodiments of the present invention provide methods<br>
for digitally re-mastering a motion picture to achieve enhanced perceived resolution<br>
and visual image quality. The images of the motion picture can be enhanced through<br>
both spatial resolution enhancement and temporal resolution enhancement. Spatial<br>
resolution enhancement can be achieved though a combination of "motion-based" and<br>
"learning-based" spatial resolution enhancement methods. Temporal resolution can be<br>
achieved by computing motion vectors for every pixel in each motion picture<br>
sequence with relatively accurate motion estimation methods. Certain methods are<br>
designed to be implemented in a highly automated fashion with limited human<br>
interaction. Aspects of the present invention can be implemented with a system of one<br>
or more process-based devices.<br>
[0014] Certain methods and systems of the present invention can be applied to two-<br>
dimensional (2D) and/or three-dimensional (3D) motion pictures. A 2D motion<br>
picture is a sequence of image frames, which can be either captured by a motion<br>
picture camera or created one frame at a time by computer graphics. Enhancement<br>
 <br>
methods according to some embodiments of the present invention perform processes<br>
on digital data. Accordingly, if a motion picture already exists in a digital format,<br>
such as, for example, Source Master files, the image data may be directly used in the<br>
enhancement process. Motion picture in formats other than digital may be converted<br>
before the enhancement process. For example, if a motion picture is on film, it can be<br>
digitized to convert it into digital data through a film scanner.<br>
[0015] A 3D motion picture creates illusion of both movement and depth based on the<br>
principle of stereoscopic vision. A 3D motion picture consists of two sequences of<br>
image frames, one representing a view from the left eye and one from the right eye.<br>
Those two sequences are typically referred to as the left-eye (L) images and the right-<br>
eye (R) images. The L images and R images can be synchronized such that one frame<br>
from each sequence captures an action in a given instant from left eye and right eye<br>
view points and they form an image pair called an L-R image pair. Some<br>
embodiments of the present invention can be applied to 2D and 3D motion picture<br>
data because the L and R images can be treated as two separate image sequences.<br>
Other embodiments of the present invention apply different enhancement processes to<br>
the 3D motion picture data, such as using information from one eye image, for<br>
example L, to improve processing results of the other eye image, for example R.<br>
[0016] Some embodiments of the present invention provide methods for enhancing<br>
2D motion picture sequences using 3D motion picture data. For example, either L<br>
image sequences or R image sequences may be created for output, but the correlation<br>
between L and R images may be used to enhance the image sequence.<br>
[0017] One application of a digital re-mastering process, such as a spatial resolution<br>
enhancement process and a temporal resolution enhancement process, according to<br>
some embodiments is to enhance a conventional motion picture of 24 fps to be<br>
displayed at a frame rate of 48 fps or higher. Another application is to allow a motion<br>
picture to be captured at a lower frame rate, such as 12 fps, in order to increase the<br>
shooting time for a data storage device or a film roll. The resulting images can be<br>
enhanced to a normal frame rate of 24fps or higher by embodiments of the disclosed<br>
digital re-mastering process. Other similar applications of the present invention<br>
should be apparent for those skilled in the art.<br>
[0018] Embodiments of the present invention provide methods and systems for<br>
enhancing the perceived resolution of a motion picture through both spatial resolution<br>
 <br>
enhancement and temporal resolution enhancement that meet the requirement for<br>
motion picture release schedules.<br>
BRIEF DESCRIPTION OF THE DRAWINGS<br>
[0019] These and other features, aspects, and advantages of the present invention are<br>
better understood when the following Detailed Description is read with reference to<br>
the accompanying drawings, wherein:<br>
[0020] Fig. 1 is a flow diagram for digitally re-mastering a motion picture according<br>
to one embodiment of the present invention.<br>
[0021] Fig. 2 is a flow diagram for digitally re-mastering a 2D motion picture<br>
according to one embodiment of the present invention.<br>
[0022] Fig. 3 is a flow diagram for digitally re-mastering a 3D motion picture<br>
according to one embodiment of the present invention.<br>
[0023] Fig. 4 is a layout of a system for digitally re-mastering a motion picture<br>
according to one embodiment of the present invention.<br>
[0024] Fig. 5 is a flow diagram of data in a system for digitally re-mastering a motion<br>
picture according to one embodiment of the present invention.<br>
[0025] Fig. 6 is flow diagram of a temporal resolution enhancement process for the<br>
enhancement of motion picture image data according to one embodiment of the<br>
present invention.<br>
DETAILED DESCRIPTION OF THE INVENTION<br>
[0026] Fig. 1 shows one embodiment of a process 10 for enhancing motion picture<br>
image sequences. The process 10 starts by receiving motion picture image sequences<br>
that may be in a film format of 2D or 3D motion picture 12 or a digital source master<br>
of 2D or 3D motion pictures 14. If the motion picture is in the 2D or 3D motion<br>
picture film format 12, it is scanned using a film scanning process 16. If the motion<br>
picture is a digital source master 2D or 3D motion picture 14, the data can be used<br>
directly or can be converted into another digital format using a format conversion 18.<br>
[0027] After film scanning 16 or format conversion 18, the motion picture sequence<br>
data includes image sequences or frames. The image sequences are then enhanced by<br>
a digital re-mastering process 20. The digital re-mastering process 20 can include a<br>
spatial resolution enhancement method 22 and a temporal resolution enhancement<br>
method 24, embodiments of which are described in more detail below. The spatial<br>
resolution enhancement method 22 can create image details that are absent in an<br>
<br>
 <br>
image frame through image analysis. Two different spatial resolution enhancement<br>
methods may be used in order to achieve the desirable performance. One method can<br>
be a motion-based method in which additional image details are "stolen" from<br>
adjacent neighboring image frames through the analysis of the motion of image<br>
elements. The second method may be a learning-based method in which additional<br>
image details are "created" based on previously learned knowledge through the<br>
analysis of image feature space.<br>
[0028] The temporal resolution enhancement method 24 may improve perceived<br>
resolution by adding synthesized new image frames through frame interpolation to<br>
increase temporal sampling rate or frame rate. The temporal resolution enhancement<br>
method 24 may include a MCFRC method that is relatively accurate and stable. The<br>
temporal method 24 may also be adapted to handle occlusion.<br>
[0029] In some digital re-mastering processes, the motion picture image data are<br>
divided into smaller segments called shots. The shot segmentation process may use<br>
editorial information, available after the final cut of a motion picture is approved.<br>
With editorial information, image data can be divided into shots, and digital<br>
processing can be done independently for each shot. After digital re-mastering,<br>
editorial information 28 may need to be modified because of increased frame counts<br>
in each shot. With the modified editorial information 30, the enhanced shot segments<br>
can be put together in the right order. For example, after the motion picture data is<br>
enhanced, it undergoes a confirming process 26 that synchronizes the data in<br>
accordance with audio tracks 32. The confirmed enhanced image data is then<br>
converted into a standardized digital source master format 34, which can be similar to<br>
the original source master format, but with increased frame rate. Digital release<br>
master files 36 can then be produced ,based on the digital source master files 34, for<br>
display 38 in theatres or further mastered into other distribution formats, including<br>
video and broadcasting. Audio 32 may also be combined to create the digital release<br>
master file 36. Data compression may be applied in creating the digital release master<br>
file 36 to meet the storage and bandwidth requirements of release platforms.<br>
[0030] Enhancement of perceived resolution and visual quality of motion picture<br>
images are especially important for the release of a motion picture for large-format<br>
cinema venues that are capable of delivering substantially higher image resolution and<br>
visual quality. For that purpose, a frame from an enhanced motion picture has higher<br>
pixel resolution than what is needed for a conventional cinema. Converting a source<br>
 <br>
master to a release master for a conventional cinema may require reduction in frame<br>
pixel resolution. The original audio files may also be digitally re-mastered to support<br>
a higher audio quality standard as required for a large-format cinema venue. The<br>
digital release master file may also be recorded back to film for distribution.<br>
[0031] Figs. 2-3 show a process flow of creating a digital release master file with<br>
enhanced characteristics. Fig. 2 shows a process for enhancing 2D motion picture<br>
images while Fig. 3 shows a process for enhancing 3D motion picture images.<br>
[0032] The processes shown in Figs. 2-3 begin by receiving a 2D or 3D motion<br>
picture image sequence as an image data input 102. The 2D or 3D image sequence is<br>
either in digital format or converted, using a digitization process, to digital data if the<br>
image data is in a non-digital format, such as on celluloid film.<br>
[0033] The digital 2D or 3D motion image sequence is then divided using a scene<br>
segmentation process 104. For example, the image sequence can be divided into<br>
shots, where each shot is a sequence of image frames representing a continuous<br>
action. Scene segmentation 104 assists in preparing the image sequences for<br>
enhancement that is preferably performed on a sequence of image frames with<br>
continuous action. Scene segmentation 104 can be performed automatically by<br>
detecting abrupt changes of image frame characteristics. Examples of such<br>
characteristics include color, intensity, histogram, texture and motion. If automated<br>
scene segmentation 104 is used, the entire motion picture data can be treated as a<br>
continuous sequence.<br>
[0034] In some embodiments of the present invention, a motion picture is already<br>
separated into shots as a result of editing, and a shot list called an Edit Decision List<br>
(EDL) file is available. EDL records accurate information about every shot including<br>
time code data or frame count data representing the start and end of each shot. An<br>
EDL can be used to guide the scene segmentation process 104.<br>
 <br>
Spatial Resolution Enhancement<br>
[0035] After scene segmentation 104, the motion picture image sequence is enhanced<br>
by a spatial resolution enhancement process. Fig. 2 illustrates a spatial resolution<br>
enhancement process 106 that can be applied to 2D image sequences, while Fig. 3<br>
illustrates a spatial resolution enhancement process 206 that can be applied to a 3D<br>
image sequence. As described below, the spatial resolution enhancement processes<br>
106, 206 may be applied differently, but both can include a motion-based method and<br>
a learning-based method.<br>
Motion-Based Spatial Resolution Enhancement<br>
[0036] The motion-based methods 108, 208 can enhance the spatial resolution of<br>
image sequences based on motion analysis. The motion-based methods 108, 208 may<br>
include three general steps: (1) motion estimation; (2) motion field regulation; and (3)<br>
detail recovery. Motion estimation may be based on a hierarchical motion model in<br>
which every image frame is represented by a multi-level pyramidal data structure.<br>
Each pyramidal data structure can represent a certain level of image details. At each<br>
pyramid level, a motion estimate of every pixel can be computed from all frames<br>
within a sliding temporal window using a variable-size block-matching algorithm.<br>
The resulting motion fields can be regulated using constraints such as high-frequency<br>
features, smoothness and quality measure. Each motion estimate can be assigned a<br>
reliability measure. Any pixel with a lower reliability measure can be considered for<br>
regulation to reduce estimation error. A group of synthesized frames are constructed<br>
by mapping each neighboring frame to the present frame based on computed motion<br>
estimates. In some embodiments, image details are recovered through adaptive<br>
temporal interpolation of synthesized frames within the temporal window. Temporal<br>
filtering may be performed using single-pass, multi-pass, or iterated process. The<br>
motion-based spatial resolution enhancement methods 108, 208 may be implemented<br>
as an automated distributed computing system controlled by a processor-based device,<br>
such as an intelligent controller. Motion-based methods are described in more detail<br>
in U.S. patent application serial no. 19/474,780.<br>
[0037] For 3D motion picture image sequences, the motion-based spatial resolution<br>
method 208 can be applied to L and R image sequences separately. Furthermore,<br>
spatial resolution may be further improved using the correlation between the pixels of<br>
L and R image pairs, such as by estimating the disparity between L and R image<br>
sequences. Such estimating may include disparity estimation, disparity map<br>
<br>
 <br>
regulation and detail recovery. Disparity estimation begins by correcting the<br>
horizontal misalignment between two image sequences. For each pixel in one image,<br>
for example the L image, a matching pixel in the other image, for example the R<br>
image, is located. Matching may be performed in the horizontal directions with<br>
limited allowance to accommodate the remnant vertical misalignment that may not be<br>
eliminated. Pixels for which a match is not found may be ignored. The disparity<br>
matching can be done in both directions in order to improve the accuracy of the<br>
resulting disparity map. A disparity map is generated for each L/R image pair. The<br>
disparity map can be further refined by removing local abnormalities using similar<br>
constraints as those used in motion regulation. A synthesized R image is then<br>
generated using the disparity map to be mapped on to the L image to improve its<br>
spatial resolution, and a synthesized L image is generated and mapped to the R image<br>
to improve its spatial resolution.<br>
[0038] The motion estimates resulting from the above process can include multi-<br>
resolution motion vector fields, which can be stored and used as initial motion<br>
estimates 140 for a subsequent temporal resolution enhancement process.<br>
Learning-Based Spatial Resolution Enhancement<br>
[0039] Motion-based spatial resolution enhancement methods 108 may be effectively<br>
use for image sequences having relatively predictable motion. For images having<br>
relatively complex motion, the impact of detail enhancement diminishes as motion<br>
estimates become less accurate. In the embodiments illustrated in Figs. 2-3, a<br>
learning-based method 110, 210 may be used to match each pixel of an image to a<br>
library of pre-selected image patterns having higher resolution and replace the pixel<br>
with a value calculated from a matching higher resolution pattern. The higher<br>
resolution patterns can be generated using a set of selected sample images that contain<br>
a higher level of image details than the original motion picture images. Such a library<br>
of higher resolution patterns is a high-resolution codebook and each pattern is<br>
described by a codeword.<br>
[0040] The learning-based spatial resolution 110 can perform the following steps to<br>
enhance the spatial resolution of original image sequences. Each original image<br>
sequence is upsized to an intended higher resolution. A matching process is then<br>
applied to each pixel of the upsized image to find a matching codeword in a pre-set<br>
codebook. If a match is found, the pixel is replaced by the central pixel of the higher<br>
resolution image pattern associated with the matching codeword. After the above<br>
<br>
 <br>
process is repeated for each pixel of an upsized image, the resulting enhanced image<br>
may need an additional pass of temporal filtering process to ensure temporal<br>
consistency with other enhanced image frames. This matching process can be<br>
extended to a block of pixels of the upsized images. For example, each block of pixels<br>
is matched to a codeword, and it is replaced by the matched higher resolution pattern<br>
through a transformation. A blending process can be applied to reduce the spatial<br>
discontinuity between the blocks. A temporal filtering process can also be applied to<br>
ensure temporal consistency.<br>
[0041] A similar learning-based spatial resolution enhancement process 210 may be<br>
applied to a 3D motion picture. For example, the L and R images can share the same<br>
codebook. Resolution enhancement is then performed to each eye separately. A<br>
filtering process can be applied, using the disparity map produced in the motion-based<br>
resolution enhancement stage to remove inconsistency between resulting L and R<br>
images.<br>
[0042] One implementation of the codebook generation process is described as<br>
follows. First, all higher resolution image patterns are downsized to the same pixel<br>
resolution of the motion picture images such that they share a similar level of image<br>
details. The level of image details is measured through Fourier spectral analysis. The<br>
downsized image patterns are then upsized to a higher pixel resolution in which the<br>
motion picture is to be presented. The resulting pattern forms a pair with the higher<br>
resolution image pattern from which it is produced. The upsizing processing may<br>
increase image pixel count and not create additional image details. A training process<br>
can be applied to all image pattern pairs to calculate and extract a number of image<br>
features from each image pattern pairs for each pixel from surrounding pixels. Image<br>
features having a higher level of image details can be described by a fixed number of<br>
data bytes or word. The collection of all features from each pair of image form a data<br>
set or codeword. The length of the codeword can be reduced using a principle analysis<br>
process to remove redundant feature attributes. The codewords are then collected into<br>
a data library or codebook and saved in a data storage. In some embodiments, the size<br>
and content of the initial codebook are dependent on the size and the content of the<br>
image patterns selected. A clustering analysis can be applied to the codebook to<br>
reduce the codebook size. The clustering analysis may group pixels having similar<br>
image patterns.<br>
 <br>
Temporal Resolution Enhancement<br>
[0043] The output of the spatial resolution enhancement processes 110, 210 are<br>
applied to temporal resolution enhancement processes 112, 212. The temporal<br>
resolution enhancement process 112, 212 can increase perceived resolution by<br>
increasing the display frame rate. Because original images have a fixed frame rate,<br>
new image frames need to be synthesized based on the original image frames to<br>
achieve frame rate increase. In some embodiments of the present invention, the<br>
temporal resolution enhancement processes 112, 212 may include pre-processing,<br>
global motion estimation, local motion estimation, half-motion vector generation,<br>
frame interpolation, and artifact repair by a temporal consistency check. Synthesized<br>
frames are created based on high quality motion estimates calculated by an<br>
embodiment of MCFRC method described below with reference to Figs. 2-3 and Fig.<br>
6.<br>
Pre-processing<br>
[0044] First, the image sequence is pre-processed 114. Pre-processing 114 may be<br>
used to calculate edge mask maps and color segmentation maps from image frames.<br>
For example and referring to Fig. 6, edge mask map 612 can be generated from each<br>
frame of the image by an edge detection algorithm 602, such as a Canny edge<br>
detector. A color segmentation map 614 is then generated from each frame by a color<br>
segmentation algorithm 604, such as Meanshift or Watershed. For a 3D motion<br>
picture, separate edge mask maps 612 and color segmentation maps 614 are generated<br>
for each eye.<br>
Global Motion Estimation<br>
[0045] The global motion between adjacent frames is then calculated using a global<br>
motion estimation process 116. Global motion estimation 116 may be used to achieve<br>
relatively accurate local motion estimates. Approximate but correct global motion<br>
estimation 116 can be used to make a general first estimate for local motion<br>
estimation algorithms. Using global motion estimation 116, the motion of an image<br>
background can be computed using camera motion and depth information. If depth<br>
information is not available, such as for 2D or some 3D image sequences, global<br>
motion can be modeled approximately as a three-dimensional planar transform, which<br>
is a simplified version of three-dimensional perspective projection transformation and<br>
does not need depth information in calculation. An example of three-dimensional<br>
planar transform used in the method is shown here:<br>
 <br>
 <br>
where (x,y) and (x '.y') are positions of a pair of matching feature points from two<br>
adjacent frames. The coefficients al, a2,..., a8 are determined by fitting the three-<br>
dimensional planar model in equation (1) based on randomly tracked feature points.<br>
Feature points are computed based on standard 2x2 gradient cross correlation matrix<br>
 <br>
where fx and fy represent the local horizontal and vertical gradient, and the sums are<br>
taken over a small local region (3x3, 5x5, etc.) around each pixel. Feature points can<br>
be extracted by a number of methods. One method is to calculate the minimum Eigen<br>
value of matrix G, which is the basis of Kanada-Lucas Tomasi (KLT) feature<br>
detector. Another method is to calculate the maximum corner strength measure based<br>
on matrix G, which is the basis of another well-known Harris corner detector.<br>
Another method is to calculate the following values PEG, QEG and θEG, which are<br>
derived from matrix G:<br>
 <br>
[0046] The value of PEG represents the local edge scale, which can be used to<br>
generate edge mask values. The value of QEG represents the response to local<br>
gradient changes, which can be used to locate feature points by calculating its local<br>
maximum. Among the pixels with local maximum, true feature points are not far<br>
 <br>
away and they can be located by calculating the value of θEG, which represents the<br>
orientation of the local gradient, and an energy measure V and searching for local<br>
maximum positions:<br>
 <br>
[0047] L(θEG) is the projection length from centre pixel to the border of the local<br>
region. Because the local region is usually rectangle, this value varies depending on<br>
the angular values. Typically N directional angles are used, and N is usually set to 16<br>
or larger. Feature points extracted though the above method need to be matched in<br>
pairs between adjacent frames. For a feature point in the current frame, it is matched<br>
to a feature point in the next frame to form a pair they have the highest correlation<br>
value.<br>
[0048] Once feature points are faired, the coefficients of a three-dimensional planar<br>
transform (1) that models the global motion between any two adjacent frames can be<br>
calculated by randomly selecting at least four corresponding feature pairs from those<br>
two frames, each pair generating two linear equations such as:<br>
 <br>
[0049] The selected 4 feature pairs produce eight linear equations, so that coefficients<br>
a1,...,a8 can be solved. The resulting three-dimensional planar transform can be<br>
tested on all feature pairs and the transform as well as the number of inliers are saved<br>
for later use. Then the next four pairs of feature points are randomly selected and a<br>
three-dimensional planar transform is calculated and saved. This process is repeated<br>
over a sufficiently large of iterations. Among all three-dimensional planar transforms<br>
estimated from all iterations, the planar transform having the maximum number of<br>
 <br>
inliers is selected as an initial estimation. The inliers are used to estimate a second<br>
three-dimensional planar transform using a standard least square algorithm. This step<br>
is repeated until the number of inliers becomes stable. The resulting three-<br>
dimensional planar transform is an estimate of the dominant global motion between<br>
those two frames. If other dominance global motions exist, they can be calculated in<br>
the same way from remaining unclassified feature points. The required computation<br>
for this algorithm depends on the number of feature pairs and is independent of the<br>
image frame size.<br>
[0050] The most dominant motion between adjacent frames is usually the background<br>
motion, typically caused by camera motion. However, the motion of the foreground<br>
objects may become the dominant motion if the objects are large enough and move<br>
fast enough. In most cases, however, the background motion can be selected among<br>
dominant motions based on similarity in motion and percent of inliers among adjacent<br>
frames.<br>
[0051] For 3D image sequences, separate global motion estimates may be performed<br>
for each eye. Where stereoscopic disparity between L and R images are relatively<br>
small, a single global motion estimates may be performed for both eyes..<br>
Local Motion Estimation<br>
[0052] A local motion estimation process, such as local motion estimation 118, 218,<br>
can then be applied based on edge mask maps 612, color segmentation maps 614,<br>
global motion estimation 116, and initial motion estimates 140 received from the<br>
motion-based spatial resolution enhancement process 108, 208. Local motion<br>
estimation 118, 218 can include a pyramid voting-based algorithm. The pyramid<br>
voting-based algorithm can synthesize new frames based on resulting local motion<br>
vectors. The new frames can be combined with spatially enhanced image frames to<br>
achieve a desirable frame rate.<br>
[0053] The pyramid voting-based algorithms can further minimize errors during<br>
image sequence enhancement. A specific local motion estimation algorithm is usually<br>
optimized for a certain type of motion and may not remain accurate for other types of<br>
motion. In some embodiments of the present invention, a multiple of local motion<br>
estimation methods are used and a voting process is deployed to determine the best<br>
estimates and minimizing errors. In some of those methods, the motion estimates from<br>
the previous motion-based spatial resolution enhancement are used as initial<br>
estimates.<br>
 <br>
[0054] In one embodiment of the present invention, up to four local motion<br>
estimation algorithms or methods are used. One of the four algorithms is a block-<br>
matching method. The block-matching method includes dividing two adjacent image<br>
frames into small blocks. In one frame, its global motion is used as the initial guess<br>
and the average pixel shift of each block is calculated. A starting block in the second<br>
frame is determined and all blocks near the starting block are searched to find the best<br>
match based on minimum of block matching error. A morion vector is then assigned<br>
to each pixel in the first frame block that is equivalent to the shift of the block from<br>
the first frame to the best matching block in the second frame.<br>
[0055] A second local motion estimation algorithm is a feature propagation method.<br>
The feature propagation method estimates the motion of feature points and the results<br>
are propagated to the rest of pixels of the entire image frame. The feature point pairs<br>
extracted for global motion estimation can be used. All feature pairs with low<br>
correlation values and large initial motion estimates are removed. Among the<br>
remaining feature pairs, those with duplicate correspondence are also removed. The<br>
remaining feature points are considered accurate and can be used as seed pairs. Based<br>
on the motion vectors of seed pairs, a propagation algorithm is applied to spread the<br>
motion to more pixels. In some embodiments, the pixel status from the previous<br>
analysis are recorded to assist in speeding up the process. Missing vectors are then<br>
filled based on color segmentation maps.<br>
[0056] A third local motion estimation algorithm is a modified optical flow algorithm<br>
with control points and Lorenzo function. The control points are defined in a lattice<br>
grid and Lorenzo function is used to replace the standard Mean Square Error (MSE)<br>
to control errors. The method calculates horizontal and vertical gradients for each<br>
pixel of the next frame, determines the structure and distribution of the control points,<br>
and calculates weights of each pixel to the related control points. Starting from an<br>
initial guess of motion vectors of each pixel, motion vector modifications are<br>
computed for the control points. Motion vector modifications are computed for all<br>
pixels based on the control points. An energy cost function (motion tracking errors) is<br>
calculated using Lorenzo function. If the value changes are small, then the method<br>
stops, otherwise the computations are repeated until either value changes are small or<br>
the maximum number of iterations is reached.<br>
[0057] A fourth local motion estimation algorithm is an integral block matching with<br>
multiple block sizes. In this method, initial motion estimates between two frames are<br>
 <br>
used as starting points for pixel-based searching using block correlation values<br>
calculated using local central moments. For each pixel in the current frame, a<br>
matching pixel is found in the next frame based on the highest correlation values. This<br>
process is repeated for a multiple of different block sizes, resulting in a multiple of<br>
motion vectors for each block size for each pixel.<br>
[0058] Each pixel in an image frame can have both forward motion vectors and<br>
backward motion vectors. The former are estimated by searching for matching pixels<br>
from the next frame, and the latter are estimated by searching in the reverse direction<br>
from the previous frame. All algorithms discussed are applicable in either directions,<br>
resulting in a multiple of forward motion vectors and a multiple of backward motion<br>
vectors.<br>
[0059] A voting process is used to select a most accurate motion vector for each pixel<br>
from both forward and backward motion vectors. In one embodiment of the present<br>
invention, voting consists of the following schemes, each for a certain portion of both<br>
image frames. Scheme #1 is to select a motion vector by edge difference values,<br>
which is calculated based on the edge masks generated previously. For each motion<br>
vector associated with a certain block size, the normalized edge difference is<br>
calculated by summing the absolute edge differences of surrounding pixels within the<br>
block, and dividing by the absolute edge values of the block. A motion vector with a<br>
sufficiently small normalized edge difference value is selected and the motion vector<br>
is assigned to the pixel. If more than one motion vector has sufficiently small<br>
normalized edge differences, the motion vector obtained using the largest block size is<br>
selected. Selected pixels are marked with a relatively high accuracy value in a<br>
corresponding accuracy mask.<br>
[0060] If Scheme #1 fails to find a motion vector, the next Scheme #2 is used to<br>
select a motion vector by coherence check. In this scheme, each pixel with a forward<br>
motion vector tries to find a corresponding pixel in the next frame with a<br>
corresponding backward vector to form a pair based on sufficiently small matching<br>
error. If a pair is found, the motion vector pair is assigned to both pixels and mark<br>
them with relatively high accuracy values in a resulting accuracy mask.<br>
[0061] If Scheme #2 fails, the next voting scheme (Scheme #3) is used, which is also<br>
based on forward-backward checks. The corresponding backward motion vector is<br>
analyzed when a pixel is projected to the next frame based on a forward motion<br>
vector.. If the backward motion vector has a relatively low accurate value, it is<br>
 <br>
replaced with the forward motion vector with direction reversed because the former<br>
has a higher accuracy value. The accuracy value in the accuracy mask is modified<br>
accordingly.<br>
[0062] If Scheme #3 does not produce a motion vector, the next Scheme #4 is used to<br>
select a motion vector based on minimum pixel color difference. If such a motion<br>
vector is found, corresponding pixels are marked with a relative low accuracy value in<br>
the accuracy mask.<br>
[0063] For the remaining pixels, color segmentation maps are used to calculate an<br>
average for all motion vectors found previously in the same segmented region. The<br>
averaged motion vector is then used as a reference, and a motion vector that is both<br>
close to the reference motion vector and has minimum pixel matching errors is<br>
selected. This last voting method is Scheme #5.<br>
[0064] Although five voting scheme are disclosed, those skilled in the art will realize<br>
that other voting schemes can be devised and added to the method for the purpose of<br>
producing a pair of forward and backward motion vectors with the highest possible<br>
accuracy for every pixel of every frame.<br>
[0065] In the local motion estimation process 218 for a 3D motion picture, motion<br>
vectors can be estimated separately from L images or R images. However, because<br>
high correlations between L and R images exists, the motion estimates from one eye<br>
can be used as accurate initial motion estimates for the second eye. In one<br>
embodiment of the present invention, the motion vectors obtained from the L images<br>
are used as the initial motion estimates for the pyramid voting-based motion<br>
estimation process of the R images. This can improve the accuracy of the motion<br>
vectors for the R images and reduce inconsistency between L and R images.<br>
[0066] A voting-based local motion estimation in a multi-level pyramidal structure in<br>
which image data at an upper level represents a coarse version of the image data, and<br>
the lowest level (level 0) represents the finest details of the image data can be<br>
relatively efficient and accurate. Such a pyramid representation of an image sequence<br>
is generated by progressively low-pass filtering and subsampling each frame. A<br>
similar pyramid representations is implemented in the motion-based spatial resolution<br>
enhancement methods 108, 208 discussed previously, which also produces multi-<br>
level motion estimates.<br>
[0067] Pyramid voting-based local motion estimation methods 608 according to some<br>
embodiments of the present invention can be implemented at each pyramid level. For<br>
<br>
 <br>
example, and referring to Fig. 6, an edge mask map 612, a color segmentation map<br>
614 and global motion estimates 116 are generated for every frame at each level.<br>
Based on global motion estimates, all the data of the next frame (image data, the edge<br>
masks maps 612 and the color segmentation maps 614) are warped by a warping<br>
process 606 to the current frame to create warped data at each pyramid level.<br>
[0068] All the voting schemes discussed previously can be applied to all pyramid<br>
levels with slight or no modifications. For example, Scheme #2 can be extended to<br>
checking coherence of multiple motion vectors produced by more than one method.<br>
At each pyramid level, if at least two morion vectors from different methods have<br>
very small matching errors, the motion vector produced by a method with a higher<br>
accuracy is selected or an average of those vectors is determined, and accuracy values<br>
in a corresponding accuracy mask 618 are updated accordingly.<br>
[0069] For any pyramid level higher than level 0, both the motion vectors and<br>
accuracy masks can be progressively refined by using bilinear interpolation as the<br>
initial guesses of the next pyramid level. The accuracy mask values 618 are used as<br>
weights in the interpolation such that motion vectors with high accuracy values will<br>
be weighted more than those with lower accuracy in interpolation.<br>
[0070] For the pyramid level 0, motion vectors need to be adjusted, taking pixel<br>
occlusion into consideration. An intermediate mask is created by projecting pixels<br>
from second image frame using backward motion vectors to the first image frame.<br>
This mask is then dilated to generate a traceable mask of the first image frame. Those<br>
pixels that cannot be traced from the second image back to the first image are<br>
considered occlusion pixels. For occlusion pixels, their motion vectors are adjusted<br>
using color segmentation-based averages. This step is repeated in the reverse direction<br>
from the second image frame back to the first image frame. If warping 606 is applied,<br>
the resulting bi-directional motion vectors 616 and accuracy masks 618 are warped<br>
back to normal by an inverse warping process 610.<br>
[0071] The pyramid structure provides a possibility to select the performance of local<br>
motion estimation methods, ranging from maximum accuracy (using all multiple<br>
methods at all pyramidal levels) to maximum efficiency (using a single method at<br>
level 0). A 4-digit binary word called "flag" 624 is used to mark the combination of<br>
methods selected. For instance, a flag 624 having a value of 1010 can indicate that<br>
method #2 and method #4 are selected but method #1 and method #3 are not. More<br>
accurate results are achieved by selecting more methods but at same time increasing<br>
 <br>
computational cost. At the upper levels, more methods can be used. As a result, higher<br>
accuracy are achieved at upper levels without significant increase in computational<br>
costs. If upper level motion estimates are highly accurate, it can also improve the<br>
accuracy oflower level motion estimates so that fewer methods may be needed for<br>
lower levels.<br>
[0072] In the local motion estimation process 218 for a 3D motion picture, pyramid<br>
representations of L and R images can be obtained from the motion based resolution<br>
enhancement stage, and a voting-based motion estimation method similar to the<br>
pyramid voting-based local motion estimation 608 can be performed separately in L<br>
images and R images. However, because there exist strong correlations between L and<br>
R images, the motion estimates from one eye can be used as accurate initial motion<br>
estimates for the second eye. The motion vectors obtained from the L images can also<br>
be used as the initial motion estimates for the pyramid voting-based motion estimation<br>
process of the R images.<br>
Half-Motion Vector Generation<br>
[0073] A half-motion vector generation process 120 is then applied to the image<br>
sequences to convert the image sequence to a higher frame rate by synthesizing new<br>
frames at time intervals based on the desirable frame rate. Half motion vectors 620 are<br>
created based on bi-directional motion vectors 616 and the accuracy masks 618 to<br>
indicate the movement of pixels from an existing frame to a synthesized frame or<br>
otherwise. The term "half" is used here to indicate that the time interval of a<br>
synthesized frame is somewhere between two original image frames, and it does not<br>
restrict a synthesized frame to be created exactly half way between two original image<br>
frames. In a similar way, half accuracy masks 622 also created with respect to the<br>
time interval of a synthesized frame, which can be used later in frame interpolation.<br>
[0074] Half-motion vector generation 120 generally assumes that a synthesized frame<br>
is to be created at a time interval in between a first image frame and a second image<br>
frame. However, half-motion vector generation 120 can be use when the frame rate is<br>
more than doubled and more than one synthesized frames are to be created between<br>
two image frames. A synthesized frame is created by projecting all pixels of an<br>
existing frame to the synthesized frame based on forward or backward motion<br>
vectors. For example, new forward motion vectors called "half forward motion<br>
vectors" are assigned from the first frame to the synthesized frame, and also from the<br>
synthesized frame to the second frame. Similarly, new backward motion vectors<br>
 <br>
called "half backward motion vectors" are assigned from the second frame to the<br>
synthesized frame and from the synthesized frame to the first frame. If a pixel in the<br>
synthesized frame has both a half forward motion vector and a half backward motion<br>
vectors, it is considered accurate and marked accordingly in a half accuracy mask<br>
corresponding to the synthesized frame. If one of the half motion vector 620 exists,<br>
the pixel is considered an occlusion pixel and marked as such in the half accuracy<br>
mask 622. If a pixel that has neither half forward or backward motion vectors, it is<br>
considered inaccurate and marked accordingly in the half accuracy mask. The<br>
missing half motion vectors can be estimated from averaging the half motion vectors<br>
of neighboring accurate pixels. The resulting half accuracy mask can be used to<br>
locate potential artifact pixels in subsequent processing stages.<br>
Frame Interpolation<br>
[0075] Frame interpolation 122 is then performed using the half motion vectors 620<br>
and the half accuracy masks 622. In a preferred embodiment of the present invention,<br>
frame interpretation 122 uses the pyramid representations of two adjacent images and<br>
their accuracy masks 618.<br>
[0076] Frame interpolation 122 is generally applied to create a synthesized frame<br>
between a first image frame and a second image frame. A blank synthesized frame is<br>
first created in the same pyramid structure. The task of frame interpolation is to fill<br>
each pixel of the synthesized frame at each pyramid level with correct color values.<br>
Starting from a target pixel position in the synthesized frame of a certain level, the<br>
values of pixels at the same position and at the same level in both first and second<br>
frames are compared. If their values are sufficiently close, the averaged color is<br>
assigned to the pixel of the same position in the synthesized frame. This pixel is also<br>
marked as accurate.<br>
[0077] If a color match cannot be found, the target pixel in the synthesized frame is<br>
projected to the first frame and to the second frame using corresponding half motion<br>
vectors and one color value from each frame is obtained. If those two color values are<br>
sufficiently close, the average of those two color values are computed and assigned to<br>
the target pixel. In addition, the pixel is marked as accurate. If those two color values<br>
are not sufficiently close, one projected pixel whose color values share is selected by<br>
a majority of its neighboring pixels, and the pixel is marked as an occlusion pixel.<br>
[0078] If the target pixel has one half motion vector, either the half forward motion<br>
vector or the half backward motion vector, the pixel is assigned with the color values<br>
 <br>
of the pixel projected using the available half motion vector. This pixel is marked as<br>
an occlusion pixel.<br>
[0079] If the target pixel has no half motion vectors, accurate pixels are searched<br>
within a group of pixels at an upper pyramid level. Once found, the target pixel is<br>
assigned with color values obtained from a bilinear interpolation of those accurate<br>
pixels. If no accurate pixels are found within the neighborhood, the radius of the<br>
group of pixels is gradually expanded until it includes at least some accurate pixels<br>
and the target pixel is assigned with a bilinear interpolation of those accurate pixels.<br>
[0080] The frame interpolation 122 process can create a synthesized image frame<br>
from two existing image frames. This process is repeated for the entire image<br>
sequence until a desirable frame rate is reached. The result is a temporally enhanced<br>
image sequence that contains both the existing image frames and synthesized image<br>
frames. Depending on the desirable frame rate, not all exiting image frames may be<br>
kept in the resulting enhanced image sequence. For example, to convert a standard<br>
24fps motion picture sequence to an output frame rate of 48fps, a synthesized frame<br>
can be created in between each pair of neighboring image frames. As a result, nearly<br>
half of the image frames of the resulting enhanced image sequence are synthesized<br>
frames. In other cases where the output frame rate is not exactly a multiple of the<br>
original frame rate, a higher percentage (up to 100%) of image frames of an enhanced<br>
image sequence will be synthesized image frames.<br>
[0081] The temporally enhanced motion picture can be synchronized with the original<br>
audio track when displayed at a higher frame rate. One or more additional synthesized<br>
frames may need to be added after the last existing frame of the image sequence to<br>
assist with synchronization. Those synthesized end frames are typically created from<br>
the last image frame. There are a number of ways of creating synthesized frames from<br>
a single image frame. One method is to create a duplicate of the frame. A second<br>
method is to generate half forward motion vectors from the image frame to the<br>
synthesized frames and fill all pixels of the synthesized frames.<br>
[0082] A frame interpolation method based on more than two frames can significantly<br>
reduce the temporal inconsistency artifacts. In general, a temporal window with a<br>
length of 2M+1 frames is used to define the range of the image frames used for<br>
generating a synthesized frame. The length of the window can vary according to<br>
motion in an image sequence. A small window length is used for fast motion<br>
sequences and a relatively large window length is used for slow motion sequences.<br>
 <br>
[0083] Motion estimation is done between a frame immediately before the<br>
synthesized frame and every previous frame, and between a frame immediately after<br>
the synthesized frame and every future frames. The same pyramid voting-based<br>
motion estimation method can be used between each pair of frames and generate both<br>
forward and backward motion vectors.<br>
[0084] Half motion vectors 620 are generated between the synthesized frame and<br>
every other image frames within the temporal window using the same method as<br>
disclosed previously. The pixel value of the synthesized frames can be calculated by a<br>
weighted average of all projected pixels that exist<br>
[0085] For image sequences that contain fast motion, it becomes more challenging to<br>
enhance temporal resolution because motion estimates computed using the method<br>
described above become less accurate. Some embodiments of the present invention<br>
provide a layered approach 155 for dealing with scenes with fast motion. The layered<br>
approach 155 segments images into different motion layers and groups image<br>
elements that share similar motion into the same layer.<br>
[0086] The half motion vectors 620 estimated from the previous methods can be used<br>
for motion segmentation. Each original image frame is divided into small blocks. The<br>
motion representing each block is estimated by an affine motion model, which is<br>
calculated through the least square algorithm based on motion vectors of all pixels of<br>
the block. The fitting error is calculated to evaluate if the motion model is a good fit<br>
or a poor fit. For all good fitting blocks, their affine motion parameters are collected.<br>
Clustering algorithms are applied to cluster the affine motion models into a small<br>
number of classes representing the dominant motions in the whole image.<br>
[0087] With the dominant motion classes, each pixel is mapped to the closest motion<br>
class based on its motion vector. Then all pixels that belong to the same motion class<br>
are grouped into a segment. If a segmented region that is too small, it can be merged<br>
with a larger neighboring segment. If two segments have similar affine motions, they<br>
can be merged into one region. If a region has an affine motion model fitting error too<br>
large to stay as one region, it can be split into two regions, each having a distinct<br>
affine motion model. This segmentation process is repeated until the regions become<br>
stable or the maximum of iterations are reached.<br>
[0088] A final motion segmentation mask is created for each frame containing the<br>
segmentation index for all pixels. The segmentation mask defines the layer structure<br>
for image pixels of each frame.<br>
 <br>
[0089] The motion segmentation results, including the motion segmentation mask and<br>
the affine motion model parameters, can be are used as the initial estimation for the<br>
segmentation of the next frame. With the initial estimation, and the same processes as<br>
described previously are repeated until the next frame is segmented. This process is<br>
repeated until the motion segmentation masks are created for all original images.<br>
[0090] The motion segmentation masks for a synthesized frame can be interpolated<br>
from the motion segmentation masks of original frames using "AND" or "OR"<br>
operation. Within each layer, a global motion homography matrix for the layer is<br>
calculated and each frame is warped to its neighboring frame based on the global<br>
motion homography matrix before applying pyramid motion tracking. Finally new<br>
layered images are generated through interpolation at each layer using tracked motion<br>
vectors.<br>
[0091] Based on the motion segmentation masks, a new synthesized image frame is<br>
created through composition of all layered images.<br>
[0092] Compared with the layered approach 155 , the frame interpolation method 122<br>
described above can be considered as a single layer approach. Global motion<br>
estimation is applied for each layer in a layered approach so that the resulting motion<br>
vectors are relatively accurate. The layered approach can reduce artifacts occurring in<br>
the edge and occlusion areas.<br>
Artifact Repair by Temporal Consistency Check<br>
[0093] Any visible artifacts can be repaired through temporal consistency check 124,<br>
224 equipped with an automated occlusion detection and occlusion fill capability.<br>
The synthesized frames may contain artifact pixels, and the most visible ones are<br>
those that are inconsistent with their neighboring frames including original image<br>
frames. The temporal consistency check 124, 224 can automatically identify and<br>
repair temporally inconsistent artifacts by checking temporal consistency.<br>
[0094] Artifact pixels of a synthesized frame are those that are marked as anything<br>
but "accurate" in corresponding half accuracy masks generated from the previous<br>
process. Those artifact pixels can further be grouped by their "visibility" within a<br>
temporal window. An artifact pixel is "visible" if it can be "seen" from another frame<br>
if it can be projected onto that frame using a half motion vector and with a sufficiently<br>
small matching error.<br>
[0095] Artifact pixels can be grouped by their visibility. For example, the first group<br>
can include pixels which are visible from a majority of frames within a temporal<br>
<br>
 <br>
window. The second group can include those pixels which are visible from fewer than<br>
50% of past frames within the window. The third group can include those pixels that<br>
are visible from fewer than 50% of future frames within the window. The artifact<br>
pixels of the first group can be considered non-occlusion pixels, while the other two<br>
groups can be considered occlusion pixels. Half accuracy masks can be used to group<br>
artifact pixels. Artifact pixels as identified can be automatically removed.<br>
[0096] Artifact pixels that are inconsistent temporally produce the most objectionable<br>
artifacts. Although those artifacts can be repaired by a skilled user interactively with<br>
conventional painting, cloning or compositing software tools, the process is<br>
nevertheless time consuming and labor-intensive. The present invention discloses a<br>
far more efficient method of repairing temporally inconsistent artifacts.<br>
[0097] Half accuracy masks 622 can be used to group artifact pixels. For a pixel of<br>
the first group, pixel-matching errors are calculated with the pixel's projected pixels<br>
in past and future frames within the temporal window. If the matching error for a<br>
certain frame is noticeably larger than others, the pixel is marked as an artifact pixel.<br>
For a pixel of the second group, the pixel matching error is check with its projected<br>
pixel in future frames. If the matching error for a certain frame is noticeably larger<br>
than others, the pixel is marked as an artifact pixel. For a pixel of the third group, the<br>
temporal consistency check is done to past frames within the temporal window and<br>
the pixel is marked as an artifact pixel if the matching errors are large. For all<br>
identified artifact pixels, repair is performed by automatically replacing the pixel with<br>
the average color values of all projected pixels in corresponding past and/or future<br>
frames.<br>
[0098] The artifact repair process 224 for a 3D motion picture can repair artifacts<br>
from inconsistencies between L and R images, and can identify such artifacts by<br>
checking L-R consistency. For example, a stereo matching process is applied to each<br>
L-R image pair, and depth information can be estimated. The resulting depth maps are<br>
temporally interpolated to create depth maps for synthesized frames. Using<br>
interpolated depth maps, the pixels in the synthesized L and R images are checked<br>
and inconsistent pixels are marked and repaired using corresponding pixels from the<br>
other eye or directly from other frames of the same eye.<br>
[0099] The artifact pixels that are survived from the previous temporal consistency<br>
check 124, 224 can be further reduced or removed interactively by, for example,<br>
merging pixels of different versions. Different versions of a pixel are created under<br>
<br>
 <br>
different flags and merged to reduce artifacts. In a GUI environment, a user can<br>
review different versions of enhanced image sequences and select the best version or<br>
merge a number of selected versions together to generate a new version containing<br>
fewer artifacts.<br>
[00100] As described in more detail below, artifacts may also be repaired by user<br>
interaction 126, such as providing access to the data on a processor-based device<br>
having computer-executable code that allows users to repair the data.<br>
Quality Control<br>
[00101] The Quality Control (QC) 128, 228 is a process in which a trained human<br>
operator visually examines the enhanced image sequences to decide if the resulting<br>
quality is acceptable. As shown in Fig. 4, the QC process 128, 228 can be performed<br>
on a QC station 420. The QC station 420 can be, for example, a workstation equipped<br>
with a high-quality display system, a sufficiently large disk storage and software that<br>
allows the operator to perform all functions needed for QC inspection. The display<br>
system can be a 2D display system or a stereoscopic 3D display system or a system<br>
that supports both 2D and 3D. The enhanced image sequences are sent to the QC<br>
station 420 by a central server that hosts an Intelligent Controller (IC) 406 once the<br>
required processing is completed, and the tracking software tells the operator which<br>
sequences are available for QC inspection. If the operator identifies an artifact, he or<br>
she can report the nature of the issue back to the IC 406 through the QC software that<br>
provides necessary user inputs. The sequence is then reprocessed with adjustments<br>
based on the inputs of the operator, which may require the certain frames to be reprocessed<br>
by the automatic processing using a new set of parameters or may require<br>
using different artifact removal methods by a human operator. The re-processed<br>
image data can be brought back by the IC 406 to the QC station 420 for further<br>
inspection. This process is repeated until the resulting quality is seen as acceptable<br>
and then the processing is completed. The operator can notify the IC 406 that the<br>
sequence is acceptable, and the image data is ready for output.<br>
[00102] A completed and accepted image sequence can be a series of files on a<br>
central server, such as IC 406. These files have a standardized file format and serve as<br>
the source master files for the re-mastered motion picture. Upon output 130, 230 or<br>
release of a motion picture, the source master files are converted to release master<br>
files for cinematic releases. The format of release master files may depend on the<br>
release display platform deployed in a theatre. For a standardized digital cinema<br>
 <br>
release, the release master files may be directly used for exhibition. For other non-<br>
standardized digital releases, the release master files may be converted to an<br>
appropriate digital file format for exhibition. For film-based exhibition, the release<br>
master files (or the source master files) can be recorded onto an appropriate film<br>
format for a film print release. There are a myriad of file types and compression<br>
schemes in the digital realm that the source master files can be converted to release<br>
master files. In a typical release process, the release master files need to be written to<br>
a tape or external disk storage for transporting to cinemas or for archival purposes.<br>
The IC can schedule the work required for file format conversion based on a priority<br>
scheme established by users. An example of a priority scheme could be to prioritize<br>
certain sequences, like movie trailers, that appear before the main motion picture<br>
presentation. The specific release master format, the compression scheme used and<br>
the priority scheme for each cinema presentation are tracked by the IC so that the<br>
location and status of each motion picture release is known to the overall Production.<br>
[00103] The digital re-mastering methods can be implemented as a highly<br>
automated production computing system. Fig. 4 illustrates one embodiment of a<br>
computing system 400 implemented as a combination of two subsystems: a front-end<br>
system 402 that mainly supports applications that require user interaction and a back-<br>
end system 404 that can be totally automated. Both subsystems use a combination of<br>
networked software and hardware. Both hardware and software are monitored and<br>
controlled through a central software entity or Intelligent Controller (IC) 406 on a<br>
server.<br>
[00104] The back-end subsystem 404 includes the IC 406, a central data storage<br>
408 and many distributed render clients 410a-n forming a render farm. A data<br>
input/output 415 may be associated with the IC 406 in the back-end subsystem to<br>
provide access to the IC 406 for loading computer-executable code such as software<br>
and otherwise configuring the IC 406. The back-end subsystem 404 may also include<br>
cluster switches 414a-n and a backbone switch 417 for controlling data flow over the<br>
network. The render farm may have any suitable number of render clients 410a-n.<br>
Most computing tasks can be carried out in the back-end subsystem 404, and those<br>
tasks can include: automated scene segmentation, motion-based spatial resolution<br>
enhancement, learning-based spatial resolution enhancement, and a majority of tasks<br>
of temporal resolution enhancement. The IC 406 performs various control and<br>
 <br>
tracking functions. A number of daemons run on this server, continuously monitoring<br>
for work or updates required and performing them independently.<br>
[00105] The IC can perform functions in three main areas:<br>
•	Monitor the physical hardware and data, tracking all system resources being<br>
used and available;<br>
•	Respond to queries from users providing real-time reports; and<br>
•	Launch processes on data as actions are required and as system resources<br>
become available.<br>
[00106] The IC 406 can internally represent the state of all data and processes ran<br>
or being run for a production. All processes running, whether automatic machine tasks<br>
or manual human operator actions, report back their progress and status back to this<br>
IC 406. It monitors a central data storage 408 for data that is newly available as well<br>
as data that has been processed and may be ready for tape backup. The IC 406 can be<br>
queried to obtain accurate real-time information as the progress of a production or a<br>
single shot<br>
[00107] IC 406 information can be accessed remotely from any machine with<br>
network access to the IC 406. For example local front-end workstations 412a-n or<br>
external computers or communication devices can remotely communicate with the IC<br>
406 over the Internet. The information accessed is user specific enabling a level of<br>
security and access available to each individual. The information is reported through<br>
fixed reports or a proprietary 'query builder' that allows users to create their own<br>
reports. They can control the search criteria for the results and also set what<br>
information they wish to have returned for the matched objects.<br>
[00108] The IC 406 can track processing at three general levels: Frame, Shot, and<br>
Production.<br>
[00109] Examples of attributes tracked at a frame level include:<br>
•	render clients used to run the various processes on the frame;<br>
•	process completion status, e.g. waiting to run, completed, and running;.<br>
•	times that processes occurred or were queued for each frame; and<br>
•	frame dimensions, file type, and bit depth.<br>
[00110] Examples of attributes tracked at a shot level include:<br>
•	processes that have been or are needed to be performed on the shot;<br>
 <br>
•	shot information i.e. length, other names the shot may be referred to by<br>
external productions or companies, descriptions, and keycode on film when<br>
recorded;<br>
•	parameters set, specific to those processes;<br>
•	track and monitor changes requested for a shot by an approver;<br>
•	user identification that manipulated the shot;<br>
•	times of completion of any stages or processes to the shot;<br>
•	version control ~ signing in and out by users;<br>
•	shipping information; and<br>
<br>
•	film-recording information such as times, and recorder used.<br>
[00111] Examples of attributes tracked at a production level include:<br>
•	shipping information;<br>
•	users' assigned work;<br>
•	users' past completed work; and<br>
•	production statistics, e.g. completion percentage, estimated completion times,<br>
and frequency of multiple versions of shot.<br>
[00112] The IC 406 is also responsible for launching all the processes applied to<br>
the data. When system resources become available the IC 406 allocates the processing<br>
to the many distributed render clients 410a-n, preferably in an optimal manner based<br>
on need and resources. Each render client 410a-n, once instructed to run a job, is<br>
responsible for pulling all image data it requires from the central data storage 408,<br>
executing required operations on each frame and pushing the enhanced image data to<br>
a temporary location at controller data storage 408. For a job that was distributed to<br>
multiple render clients 410a-n, the IC 406 assembles the rendered segments from<br>
render clients 410a-n into a continuous shot. The IC 406 also checks the integrity of<br>
the assembled data for occasional missing frames or incomplete frames in the shot. If<br>
missing frames or incomplete frames are discovered, the IC 406 sends a request to the<br>
same render clients for re-rendering of those frames. The communication between the<br>
IC 410 and render clients 410a-n is crucial for render efficiency.<br>
[00113] The IC 406 tracks the current state of each render client 410a-n and<br>
constantly monitors for available processors. In the eventuality of failure of a render<br>
client 410a-n, the IC 406 raises an alert for repair. It reroutes the job to other available<br>
render clients 410a-n for processing. A diagnostics process ensures that there is no<br>
 <br>
loss of data during the transfer. If the IC 406 experiences a failure, the state of the<br>
system before malfunction is preserved. In one embodiment of the present invention,<br>
the IC 406 re-starts by killing all processes that are running on render clients 410a-n<br>
and re-assigns jobs to each render client 410a-n. In another embodiment, the IC 406<br>
polls the render clients 410a-n for their status, finds their current states and resumes<br>
the control. This is a more complicated re-start scheme, but no re-rendering of data is<br>
required.<br>
[00114] In one embodiment of the present invention, the IC 406 comprises the<br>
following software components:<br>
[00115] Scheduler - monitors for processes needing to be run on data. It manages<br>
render job distribution and assigns jobs to specific render clients 410a-n based on a<br>
pre-determined load-balancing scheme. If there are multiple available candidates, the<br>
IC 406 checks the network traffic load distribution among render client clusters and<br>
selects a render client (or render clients) 410a-n from the cluster (or clusters) with the<br>
lowest traffic load. For each job in a queue, it may assign it to a single render client<br>
410a-n, especially when there are more jobs waiting in the queue than the number of<br>
available render clients, or it may assign the job to a multiple of render clients 410a-n,<br>
especially when the job needs to be completed as quickly as possible.<br>
[00116] Auto Launch - monitors for data becoming available for processing.<br>
Usually with processing the data is assembled and a command is launched on the<br>
data. Actions or processes are set to be run on any number of shots (or all shots of a<br>
film) at the start of the production. As our processes are self-setting, that is, they<br>
intelligently analyze the data to pick optimal settings and parameters independently,<br>
no human interaction is required. The Auto Launch then, monitors the physical<br>
hardware and network for data to become available. Once all components are<br>
available it submits a request to the Scheduler to launch the processes on that<br>
particular data. This optimizes the workflow, resulting in no lost time between data<br>
being ready to process and the launching of the required processes.<br>
[00117] File Setup - monitors data that is complete and is required in another<br>
physical location or format. Often during a production, data is required to be available<br>
in different formats. The File Setup daemon monitors files and their status for any<br>
different versions or formats needed. If required it will process the data into the<br>
necessary format and also transfer it physically to another location. This has some<br>
useful applications. Primarily it optimizes the workflow by having all data available<br>
 <br>
in all required location and formats in the minimum amount of time. It can also be<br>
used to create intelligent caching on the overall system so as to improve network<br>
performance and sped up user interaction. An example of this is that the IC 406<br>
knows which data a user will be working on the next day and which front-end<br>
workstation will be used. A proxy version of the data can be transferred locally to the<br>
workstation in off-peak hours to be available immediately for the user and to<br>
eliminate the resource use during a busier time.<br>
[00118] Tape Writer - monitors for finished data that is required to be backed up<br>
to tape and writes it. The daemon is in constant contact with the other elements of the<br>
IC 406 and so is aware when data is both required and available for tape backup. It<br>
can independently write the data to a tape and report the relevant information of tape<br>
name and write times back to the central database<br>
[00119] The front-end subsystem 402 can include a number of computer<br>
workstations 412a-n and a number of QC stations 420a-n, each capable of being<br>
manned by an operator or artist. The front-end subsystem may also include a network<br>
switch 421 for controlling and otherwise assisting dataflow through the network. The<br>
workstations 412a-n are standard desktop computers and run a combination of custom<br>
software as well as commercially available applications. The computing tasks<br>
performed at the front-end subsystem 402 include inputting of EDL information in<br>
scene segmentation, quality control in both the spatial and temporal resolution<br>
enhancement processes and artifact repair.<br>
[00120] All applications running on the front-end machines are connected through<br>
a network connection to the back end control software. Any changes or updates made<br>
are reported back to the back end processes where the state of each shot and the<br>
overall production is stored. Examples of this communication include the signing out<br>
and signing back in of data, completion of a human manual task on data etc. The<br>
operators on the front-end machines can also query the IC 406 explicitly to obtain<br>
information regarding the processing and status of the production.<br>
[00121] One software application provided by the front-end workstations 412a-n<br>
primarily is a tool for artifact repair. A human operator can highlight the problem area<br>
and use a number of methods to remove the artifact. The methods include:<br>
• manual painting, either with information from the same frame or another<br>
frame from the sequences;<br>
 <br>
•	automated painting that used either, or both, spatial and temporal analysis to<br>
remove unwanted results; and<br>
•	a combination of manual painting and automatic analysis that intelligently<br>
provides suitable data to be painted in by the operator, removing the artifact.<br>
[00122] The image data flow of a typical motion picture re-mastering production<br>
between the front-end and back-end subsystems is depicted in Fig. 5. The overall<br>
workflow is as follows.<br>
[00123] Scene editorial information, an edit description list (EDL) file can be input<br>
into the IC. This enables the 'Auto Launch' to map the input digital frames from the<br>
original production to discrete individual shots that will run through the system. The<br>
digital data that is input may be, for example, a series of numbered files scanned from<br>
film (e.g. frame.0001.cin, frame.0002.cin ...) or could be arbitrarily named files from<br>
a digital color grading process, where there are many different types of file prefixes<br>
and numbering schemes.<br>
[00124] The IC for this production begins to monitor the system in preparation for<br>
performing tasks. For example, the IC may obtain whether data and system resources<br>
are available to run the re-mastering processes and data needs to be moved or<br>
somehow reformatted.<br>
[00125] After set-up processes, image data from the source production is provided<br>
to the central data storage in step 510 where it can be seen by the IC. The data can<br>
arrive in many ways such as a tape format, disk drives or arrays, directly from a<br>
scanner output, etc.<br>
[00126] In step 515, the IC notices the data is available and consults the editorial<br>
information input for this particular production. If some required grouping of data is<br>
complete on disk, (all frames from a shot), then the source data is segmented into<br>
groupings of similar frames called shots based on the editorial information. Once the<br>
original data is divided into a complete shot the IC queues the shot for re-mastering,<br>
known as a 'job', to be run on the distributed processors. The 'Scheduler' observes<br>
that there are jobs queued for processing and so splits the shot among the render<br>
clients. This means taking a shot and dividing it up to run in pieces, separately on<br>
different remote processors.<br>
[00127] In step 520, the remote processors or render clients, receive their jobs and<br>
report back that they are beginning the processing. They copy the shot data required<br>
to their local drives, for example the frames from the range of the shot they have been<br>
 <br>
assigned. The render clients can automatically analyze the data in order to decide on<br>
the best procedure to run and the optimal settings for those processes. Once complete<br>
the render client runs the required processes at the optimal settings for that data.<br>
[00128] In step 525, the render clients transfer back the finished data to the server<br>
and report to the IC that they are finished processing and that the range of frames<br>
assigned to it are ready for the next step of the workflow. They also report to the IC<br>
that they are available for more work.<br>
[00129] Alternatively, the analysis processes are separated from the executing<br>
processes. For example, an analysis job being queued with the IC is run first on the<br>
render clients and the results are passed back to the IC. From this point the actual<br>
processing is then queued with the parameters established by the analysis and the IC<br>
then splits and assigns the re-mastering processes to the remote processors. This can<br>
add efficiency and consistency in analyzing once per shot, trading off against sub-shot<br>
adjustment in parameters for possibly more accurate results if there is a great variance<br>
within a shot.<br>
[00130] In step 530, the IC transfers preview data of the re-mastered frames to a<br>
front-end quality control workstation (QC stations) for quality assurance and<br>
inspection. All work and data flow up to thus point has occurred in the back end of the<br>
System. At this point the work is passed to the front end. The IC informs the user or<br>
System Manager that there is data waiting to be viewed. A trained quality control<br>
operator views the data and has a few options based on their findings. They then must<br>
tell the IC their decision usually by setting the status of a shot within the front-end<br>
software they are evaluating the data with.<br>
[00131] In step 532, the operator can inform the IC that the data is accurately<br>
processed and complete and therefore is approved or decide that the data needs<br>
additional automated processing and the nature of the re-processing needed.<br>
[00132] If additional processing is indicated, the shot is then queued again for<br>
analysis and processing with the suggestions of the operator translated by the IC into<br>
parameter influences for the automatic analysis in step 535. The job is given a higher<br>
priority to run sooner and faster so as to not cause bottlenecks in the flow of a project<br>
through the workflow. These two decisions pass the control of the shot back to the<br>
back end of the workflow. The third decision retains the control in the front end.<br>
[00133] If no additional processing is indicated, the data is set to move to storage,<br>
such as tape backup, disk backup, film-recording, digital projection mastering or any<br>
<br>
 <br>
variety of end display manipulation in step 550. The system can then output the data,<br>
such as with a data I/O device.<br>
[00134] In step 538, the operator can decide that the shot requires some front end,<br>
user-assisted fixing of residual artifacts. The IC then transfers data to the necessary<br>
local front-end workstations for user-assisted repair of the shot The IC assigns any<br>
data marked for human manual artifact repair to available repair operators much as it<br>
would assign work to the remote processors or render clients. The same optimizations<br>
of workflow can be achieved with the human operators by either splitting shots into<br>
multiple jobs across many people to speed up the completion or to make maximum<br>
use of idle workstations.<br>
[00135] In step 540, a repair operator works on their assigned job and when<br>
complete, submits them back to the IC Server.<br>
[00136] In step 542, data can be considered 'approved' by the human operator or,<br>
in step 545, can be cycled back through to the front-end QC stations for further<br>
quality assurance and inspection.<br>
[00137] In step 550, the IC watches for completed jobs and assembles any shots<br>
that have been split across multiple workstations. Once a compete set of frames or a<br>
shot has completed all processing and is approved The IC monitors for completed<br>
data compared to required shipments and transfers any data that fulfills the criteria to<br>
the final stage. This can include processes such as tape or disk backup for shipping,<br>
film-recording, digital display or any number of display end processes.<br>
General<br>
[00138] The foregoing description of the embodiments, including preferred<br>
embodiments, of the invention has been presented only for the purpose of illustration<br>
and description and is not intended to be exhaustive or to limit the invention to the<br>
precise forms disclosed. Numerous modifications and adaptations thereof will be<br>
apparent to those skilled in the art without departing from the spirit and scope of this<br>
invention.<br>
 <br>
CLAIMS<br>
What is claimed is:<br>
1. A method for enhancing the quality of a motion picture image sequence, the<br>
method comprising:<br>
receiving an original motion picture image sequence comprising digital<br>
data of a plurality of image frames;<br>
applying a spatial resolution enhancement process and a temporal<br>
resolution enhancement process to the digital image sequence to create an enhanced<br>
image sequence;<br>
wherein the enhancement processes are automatically controlled; and<br>
wherein the enhanced image sequence has a greater frame rate than the<br>
original image sequence and the enhanced image sequence has greater image detail<br>
than the original image sequence.<br>
2.	The method of claim 1, further comprising:<br>
synchronizing the enhanced image sequence to an audio track for the<br>
original image sequence.<br>
3.	The method of claim 1, wherein the original motion picture image sequence<br>
and the enhanced image sequence are two-dimensional (2D) sequences.<br>
4.	The method of claim 1, wherein the original motion picture image sequence<br>
and the enhanced image sequence are three-dimensional (3D) sequences.<br>
5.	The method of claim 1, wherein the original motion picture image sequence is<br>
in 3D and the enhanced image sequence is in 2D.<br>
6.	The method of claim 1, further comprising:<br>
dividing the original motion picture image sequence into shots; and<br>
performing the enhancement processes on each shot<br>
7.	The method of claim 1, wherein the original motion picture image sequence is<br>
a single shot.<br>
8.	The method of claim 1, further comprising:<br>
formatting the enhanced image sequence to a display presentation<br>
format; and<br>
synchronizing the formatted enhanced image sequence to an audio<br>
track for the original image sequence.<br>
9.	The method of claim 1, wherein the spatial resolution enhancement process<br>
comprises:<br>
 <br>
a motion-based spatial resolution enhancement process; and<br>
a learning-based resolution enhancement process.<br>
10.	The method of claim 9, further comprising:<br>
applying the motion-based spatial resolution process to a 3D image<br>
sequence, wherein applying the motion-based spatial resolution process comprises:<br>
disparity estimation;<br>
display map regulation; and<br>
detail discovery.<br>
11.	The method of claim 9, wherein the learning-based spatial resolution<br>
enhancement process comprises:<br>
generating a codebook comprising codewords, each codeword being<br>
associated with a high-resolution pattern;<br>
applying a clustering analysis to reduce the size of the codebook;<br>
upsizing an original image of the original image sequence to a higher<br>
resolution, the original image comprising a plurality of pixels;<br>
matching each pixel of the upsized image to a codeword; and<br>
replacing each pixel by a central pixel of the high-resolution pattern<br>
associated with the matched codeword.<br>
12.	The method of claim 9, wherein the learning-based spatial resolution<br>
enhancement process comprises:<br>
generating a codebook comprising codewords, each codeword being<br>
associated with a high-resolution pattern;<br>
applying a clustering analysis to reduce the size of the codebook;<br>
upsizing an original image of the original image sequence to a higher<br>
resolution, the original image comprising at least one block of pixels;<br>
matching each block of pixels of the upsized image to a codeword;<br>
replacing the block of pixels by a high-resolution pattern associated<br>
with the matched codeword using a transformation process to create an enhanced<br>
block of pixels;<br>
applying a blending process to the enhanced block of pixels; and<br>
applying a temporal filtering process.<br>
13.	The method of claim 1, wherein the temporal resolution enhancement process<br>
comprises:<br>
pre-processing;<br>
 <br>
global motion estimation;<br>
local motion estimation;<br>
half-motion vector generation;<br>
frame interpolation; and<br>
artifact repair by temporal consistency check.<br>
14.	The method of claim 13, wherein pre-processing comprises:<br>
generating an edge mask map from each image frame; and<br>
generating a color segmentation map from each image frame.<br>
15.	The method of claim 13, wherein each image frame comprises a plurality of<br>
pixels, the global motion estimation comprising:<br>
a.	computing a gradient cross correlation matrix for each pixel;<br>
b.	calculating at least one feature point for each pixel based on the<br>
gradient cross correlation matrix;<br>
c.	matching at least one of the calculated feature point to a feature<br>
point of a next frame in the image sequence;<br>
d.	selecting at least four matched feature points;<br>
e.	estimating global motion based on the selected feature points;<br>
and<br>
f.	interactively repeating steps d and e by selecting different<br>
matched feature points until a global motion estimate is obtained for each pixel.<br>
16.	The method of claim 13, wherein the local motion estimation process<br>
comprises:<br>
receiving motion estimates from the spatial resolution enhancement<br>
process;<br>
using the motion estimates as initial motion estimates;<br>
receiving edge mask maps and color segmentation maps from the pre-<br>
processing process;<br>
using global motion estimates from the global motion estimation; and<br>
computing local motion vectors based on a voting-based method.<br>
17.	The method of claim 16, wherein each image frame comprises a plurality of<br>
pixels; and<br>
wherein the voting-based method comprises:<br>
applying at least one local motion estimation method to each<br>
pixel;<br>
 <br>
computing forward and backward motion vectors for each local<br>
motion estimation method; and<br>
selecting a motion vector for each pixel from the forward and<br>
backward motion vectors using a voting process.<br>
18.	The method of claim 17, wherein local motion estimation method comprises at<br>
least one of:<br>
block matching method;<br>
feature propagation method;<br>
modified optical flow method; and<br>
integral block matching method.<br>
19.	The method of claim 16, wherein the voting-based method is a pyramid<br>
voting-based method.<br>
20.	The method of claim 17, wherein the voting process comprises at least one of:<br>
selecting a motion vector by different edge values;<br>
selecting a motion vector by coherence check;<br>
selecting a motion vector by minimum matching errors;<br>
selecting a motion vector through forward and backward checks;<br>
selecting a motion vector using color segmentation maps.<br>
21.	The method of claim 13, wherein half-motion vector generation comprises:<br>
determining a time interval of a synthesized image frame between a<br>
first image frame and a second image frame;<br>
assigning half-forward motion vectors and half-backward motion<br>
vectors to each pixel of the synthesized frame; and<br>
generating a half-accuracy mask corresponding to the synthesized<br>
frame, the half-accuracy mask marking a status of each pixel.<br>
22.	The method of claim 13, wherein frame interpolation comprises:<br>
receiving half-motion vectors and half-accuracy masks generated from<br>
at least two image frames;<br>
creating a synthesized frame based, at least in part, on the half-motion<br>
vectors and half-accuracy masks;<br>
generating missing pixels of the synthesized frame by interpolation and<br>
averaging;<br>
inserting the missing pixels into the synthesized frame; and<br>
 <br>
generating the enhanced image sequence having the synthesized image<br>
frames.<br>
23.	The method of claim 22, further comprising:<br>
maintaining synchronization with an audio track of the original image<br>
sequence by adding at least one synthesized frame.<br>
24.	The method of claim 22, wherein the synthesized frame is created using<br>
pyramid representation of a pair of images.<br>
25.	The method of claim 22, wherein the synthesized frame is created using more<br>
than two images.<br>
26.	The method of claim 13, wherein artifact repair by temporal consistency<br>
check comprises:<br>
identifying artifact pixels by half accuracy masks;<br>
grouping artifact pixels by visibility;<br>
checking temporal consistency based on grouping; and<br>
automatically repairing temporal inconsistent artifact pixels.<br>
27.	The method of claim 13, further comprising a layered frame interpolation<br>
process.<br>
28.	The method of claim 1, further comprising removing artifacts by user<br>
interaction.<br>
29.	The method of claim 27, wherein removing artifacts comprises merging<br>
different versions of synthesized frames. .<br>
30.	A system for enhancing the quality of a motion picture image sequence, the<br>
system comprising:<br>
a back-end subsystem comprising:<br>
a central data storage for storing an original motion picture<br>
image sequence and an enhanced image sequence, the original motion picture image<br>
sequence comprising digital data of a plurality of image frames;<br>
a render client for configured to perform a spatial resolution<br>
enhancement process and a temporal resolution enhancement process on the original<br>
image sequence to create the enhanced image sequence; and<br>
an intelligent controller for controlling the render client and<br>
accessing the central data storage; and<br>
wherein the intelligent controller automatically controls the<br>
enhancement processes; and<br>
 <br>
wherein the enhanced image sequence has a greater frame rate than the<br>
original image sequence and the enhanced image sequence has greater detail than the<br>
original image sequence.<br>
31.	The system of claim 30, further comprising:<br>
a front-end subsystem comprising a workstation for communicating<br>
with the intelligent controller, the workstation being adapted to provide user input and<br>
interaction in repairing artifacts in the enhanced image sequence, performing a quality<br>
control check of the enhanced image sequence, and segmenting the original image<br>
sequence.<br>
32.	The system of claim 31, wherein the workstation comprises multiple<br>
workstations.<br>
33.	The system of claim 32, wherein at least one of the multiple workstations<br>
comprises a quality control workstation.<br>
34.	The system of claim 30, wherein the original motion picture image sequence<br>
and the enhanced image sequence are in 2D.<br>
35.	The system of claim 30, wherein the original motion picture image sequence<br>
and the enhanced image sequence are in 3D.<br>
36.	The system of claim 30, wherein the original motion picture image sequence is<br>
in 3D and the enhanced imaged sequence is in 2D.<br>
37.	The system of claim 30, wherein the render client is adapted to examine the<br>
quality of the enhanced image sequence.<br>
38.	The system of claim 30, wherein the intelligent controller comprises a<br>
processor and a memory comprising executable code, the executable code<br>
comprising:<br>
a scheduler;<br>
a auto launch;<br>
a file setup; and<br>
a tape writer.<br>
39.	The system of claim 30, wherein the render client comprises multiple render<br>
clients.<br>
40.	The system of claim 39, wherein the intelligent controller is adapted to detect<br>
a system failure and shut down each render client and re-assign a job to each render<br>
client.<br>
 <br>
41.	The system of claim 39, wherein the intelligent controller is adapted to<br>
monitor the render clients to prevent re-rendering of data.<br>
42.	A method for enhancing the quality of an original motion picture image<br>
sequence, the method comprising:<br>
receiving an image sequence;<br>
receiving initial motion estimates;<br>
applying a temporal resolution enhancement process comprising:<br>
pre-processing;<br>
global motion estimation;<br>
local motion estimation;<br>
half-motion vector generation;<br>
frame interpolation; and<br>
artifact repair by temporal consistency check;<br>
wherein the temporal resolution enhancement process is automatically<br>
controlled; and<br>
wherein the enhanced image sequence has a greater frame rate than the<br>
original image sequence.<br>
43.	The method of claim 42, wherein the original image sequence and enhanced<br>
image sequence is digital data.<br>
44.	The method of claim 42, wherein the original image sequence and the<br>
enhanced image sequence are in 2D.<br>
45.	The method of claim 42, wherein the original image sequence and the<br>
enhanced image sequence are in 3D.<br>
46.	The method of claim 42, wherein the original image sequence is in 3D and the<br>
enhanced imaged sequence is in 2D.<br>
47.	A method for enhancing the quality of an original motion picture image<br>
sequence, the method comprising:<br>
receiving a 3D original motion picture image sequence;<br>
applying a spatial resolution enhancement process to the 3D original<br>
image sequence to create an enhanced image sequence; and<br>
wherein the enhanced image sequence has greater image detail than the<br>
original image sequence.<br>
48.	The method of claim 47, wherein the spatial resolution enhancement process<br>
comprises<br>
 <br>
a motion-based spatial resolution enhancement process; and<br>
a learning-based resolution enhancement process.<br>
49.	The method of claim 48, further comprising:<br>
applying the motion-based spatial resolution process to a 3D image<br>
sequence, wherein applying the motion-based spatial resolution process comprises:<br>
disparity estimation;<br>
display map regulation; and<br>
detail discovery.<br>
50.	The method of claim 48, wherein the learning-based spatial resolution<br>
enhancement process comprises:<br>
generating a codebook comprising codewords, each codeword being<br>
associated with a high-resolution pattern;<br>
applying a clustering analysis to reduce the size of the codebook;<br>
upsizing an original image of the original image sequence to a higher<br>
resolution, the original image comprising a plurality of pixels;<br>
matching each pixel of the upsized image to a codeword; and<br>
replacing each pixel by a central pixel of the high-resolution pattern<br>
associated with the matched codeword.<br>
51.	The method of claim 48, wherein the learning-based spatial resolution<br>
enhancement process comprises:<br>
generating a codebook comprising codewords, each codeword being<br>
associated with a high-resolution pattern;<br>
applying a clustering analysis to reduce the size of the codebook;<br>
upsizing an original image of the original image sequence to a higher<br>
resolution, the original image comprising at least one block of pixels;<br>
matching each block of pixels of the upsized image to a codeword;<br>
replacing the block of pixels by a high-resolution pattern associated<br>
with the matched codeword using a transformation process to create an enhanced<br>
block of pixels;<br>
applying a blending process to the enhanced block of pixels; and<br>
applying a temporal filtering process.<br>
<br>
The present invention relates to methods and systems for the exhibition of a motion picture with enhanced per- ceived resolution and visual quality. The enhancement of perceived resolution is achieved both spatially and temporally. Spatial resolution enhancement creates image details using both temporal-based methods and learning-based methods. Temporal resolution enhancement creates synthesized new image frames that enable a motion picture to be displayed at a higher frame rate. The digitally<br>
enhanced motion picture is to be exhibited using a projection system or a display device that supports a higher frame rate and/or a higher display resolution than what is required for the original motion picture.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=qhtSQAw+iMMVJ7lAaYn5OA==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==" target="_blank" style="word-wrap:break-word;">http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=qhtSQAw+iMMVJ7lAaYn5OA==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==</a></p>
		<br>
		<div class="pull-left">
			<a href="280050-1-4-benzothiepin-1-1-dioxide-compounds-substituted-by-benzyl-radicals.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="280052-impact-modified-polypropylene.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>280051</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>3002/KOLNP/2008</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>06/2017</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>10-Feb-2017</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>08-Feb-2017</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>24-Jul-2008</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>IMAX CORPORATION</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>2525 SPEAKMAN DRIVE, SHERIDAN PARK MISSISSAUGA, ONTARIO L5K 1B1</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>ZHOU, SAMUEL</td>
											<td>2888 SHELFORD TERRACE, MISSISSAUGA ONTARIO L6M 6J9</td>
										</tr>
										<tr>
											<td>2</td>
											<td>YE, PING</td>
											<td>3382 ASH ROW CRESCENT, MISSISSAUGA, ONTARIO L5L 1K4</td>
										</tr>
										<tr>
											<td>3</td>
											<td>JUDKINS, PAUL</td>
											<td>27 HALTON STREET, TORONTO, ONTARIO M6J 1R5</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06T 13/00,G06T 5/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/IB2007/00188</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2007-01-29</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/762964</td>
									<td>2006-01-27</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/280051-methods-and-systems-for-digitally-re-mastering-of-2d-and-3d-motion-pictures-for-exhibition-with-enhanced-visual-quality by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 04 Apr 2024 22:34:20 GMT -->
</html>
