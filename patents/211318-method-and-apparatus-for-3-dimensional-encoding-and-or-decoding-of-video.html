<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/211318-method-and-apparatus-for-3-dimensional-encoding-and-or-decoding-of-video by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 00:29:23 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 211318:METHOD AND APPARATUS FOR 3-DIMENSIONAL ENCODING AND/OR DECODING OF VIDEO</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">METHOD AND APPARATUS FOR 3-DIMENSIONAL ENCODING AND/OR DECODING OF VIDEO</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>Provided is a method for 3-dimensional encoding of videos, which adapts to 5 temporal and spatial characteristics of the videos. The method includes performing temporal estimation on videos taken by a camera located in the center with reference to videos taken by the camera at immediately previous time, when a plurality of cameras is arranged in a row, and performing temporal-spatial estimation on videos taken by other cameras with reference to previous videos taken by cameras adjacent to the camera 10 located in the center. As described above, according to the present invention, 3-dimensional videos acquired using a number of cameras can be efficiently encoded. [Representative Drawing] FIG. 5B 1</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>FORM 2THE PATENTS ACT, 1970 (39 of 1970)&amp;The Patents Rules, 2003<br>
PROVISIONAL/ COMPLETE SPECIFICATION(See section 10 and rule 13)<br>
1. TITLE OF THE INVENTION  :         "METHOD, MEDIUM, AND APPARATUS FOR 3-DIMENSIONAL ENCODING AND/OR DECODING OF VIDEO"<br>
2. APPLICANT (S)(a)	NAME               :           DAEYANG FOUNDATION(b)	NATIONALITY:           KR(c)	ADDRESS         :           98 Kunja-dong, Kwangjin-gu, Seoul, 143-747 Republic of Korea(a)	NAME               :           SAMSUNG ELECTRONICS CO., LTD.(b)	NATIONALITY:           KR(c)	ADDRESS        :          416, Maetan-dong, Yeongton-gu, Suwon-si, Gyeonggi-do 442-742,                                                   Republic of Korea<br>
3. PREAMBLE TO THE DESCRIPTION<br>
                           PROVISIONALThe following specification describes the invention	                            COMPLETEThe following specification  particularly describes the invention and the manner in which it is to be performed.<br>
4.  DESCRIPTION (Description shall start from next page)<br>
5.   CLAIMS (not applicable for provisional specification. Claims should start with the preamble - "I/we claim" on separate page)<br>
6.    DATE AND SIGNATURE (to be given at the end of last page of specification)<br>
7.   ABSTRACT OF THE INVENTION (to be given along with complete specification on separate page)<br><br><br>
WO 2005/069630		PCT7KR2005/000182<br>
Description <br>
        METHOD,   MEDIUM,   AND   APPARATUS   FOR 	<br>
3-DIMENSIONAL   ENCODING   AND/OR   DECODING   OF<br>
                                                    VIDEO<br>
                                                       Technical  Field<br>
[1]	Embodiments of the present invention relate to video encoding and decoding, and<br>
more perticularly, to a method, medium, and apparatus for 3-dimensional encoding and/or decoding of video, which includes adapting to temporal and spatial characteristics of the video.<br>
Background   Art<br>
[2]	Video encoding in Moving picture expert group (MPEG)-4 pat 2 and H. 264<br>
(MPEG-4 advanced video encoding (AVC)) involves 2-dimensional encoding of videos and focuses on improving encoding efficiency. However in the field of real-<br>
like communication or virtual reality, 3-dimensional encoding and reproduction of videos are also required. Therefore, studies should be conducted on 3-dimesional encoding of audio video (AV) data instead of conventional 2-dimesional encoding.<br>
[3]	MPEG, which is an organization for standardizing video encoding, has made efforts<br>
to establish standards for 3-dimensional encoding of AV data. As a pat of such efforts, a 3-dimensional AV encoding ad-hoc group (AHG) has been organized and standardization is in progress.<br>
Disclosure  of Invention<br>
Technical  Solution<br>
[4]	Embodiments of the present invention include a method, medium, and apparatus for<br>
3-dimensional encoding and/or decoding of video by which video data received from a<br>
plurality of cameras and is coded/decoded 3-dimensionally.<br>
Advantageous   Effects<br>
[5]	According to embodiments of the present invention, 3-dimensional videos acquired<br>
using a number of cameras can be efficiently encoded, resulting in superior video<br>
display quality.<br>
Description  of Drawings<br>
[6]	FIG. 1 is a view illustrating encoding and reproduction of stereoscopic videos using<br>
a left view video and a right view video, according to an embodiment of the present<br>
invention;<br>
[7]	FIGS. 2A and 2B illustrate exemplary structures of a base layer video and an en-<br>
-2-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
hancement layer video;<br>
[8]	FIG. 3 is a view illustrating creation of a single video using decimation of the left<br>
view video and right view video and reconstruction of the single video into a left view<br>
video and a right view video using interpolation of the single video, according to an<br>
embodiment of the present invention;<br>
[9]	FIG. 4 is a view illustrating motion estimation/compensation of decimated video<br>
composed of a left view video and a right view video;<br>
[10]	FIG. 5A illustrates encoding of a plurality of video data received from cameras<br>
arranged in a row, according to an embodiment of the present invention;<br>
[11]	FIG. 5B illustrates video taken by a plurality of cameras over time due to scene<br>
change;<br>
[12]	FIGS. 6A and 6B are views illustrating 3-dimensional encoding of videos according<br>
to the present invention, according to embodiments of the present invention; and<br>
[ 13]	FIG. 7 illustrates camera positions and an order of encoding when the plurality of<br>
cameras exists in a 2-dimensional space, according to an embodiment of the present invention.<br>
Best  Mode<br>
[14]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a method for 3-dimensional encoding of videos, the method including performing temporal estimation on video taken by a centerly located camera with reference to video taken by the centerly located camera at at least an immediately previous time, when a plurality of other cameras are arranged in a row, with the centerly located camera being at a central position of the row, and performing temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly located camera and the video taken by the centerly located camera at the at least the immediately previous time.<br>
[15]	A result of the performed temporal estimation on video taken by the centerly<br>
	located camera may be a base layer video and a result of the performed temporal-<br>
spatial estimation on videos taken by the other cameras may be at least one en<br>
hancement layer video for the base layer video.<br>
[16]	In the performing of the temporal-spatial estimation on videos taken by the other<br>
cameras the temporal-spatial estimation may be performed on previous-in-time videos referred to by the videos taken by the other cameras with reference to a number of previous-in-time videos which is equal to a predetermined number of reference<br>
-3-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
pictures. In addition, the predetermined number of reference pictures may be 5.<br>
[ 17]	Further, in the temporal-spatial estimation on videos taken by the other cameras<br>
temporal-spatial estimation may also be performed with reference to current videos taken by cameras adjacent to the centerly located camera. The temporal-spatial estimation on videos taken by the other cameras temporal-spatial estimation may also be performed with reference to videos taken by all of a plurality of cameras that fall within a range of an angle between previous-in-time videos taken by cameras adjacent to the centerly located camera and videos to be presently estimated.<br>
[18]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a method for 3-dimensional encoding of videos, the method including referring to a previous-in-time video taken by a camera adjacent to a center of a video to be presently encoded, and performing temporal-spatial estimation with reference to as many previous-in-time videos adjacent to the camera adjacent to the center of the video according to a predetermined number of reference pictures.<br>
[19]	A result of the referring may be a base layer video and a result of the performed<br>
temporal-spatial estimation may be at least one enhancement layer video for the base layer video.<br>
[20]	In addition, an angle between the camera adjacent to the center of the video and the<br>
video to be presently encoded may vary according to an interval between adjacent cameras.<br>
[21]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a method for 3-dimensional encoding of videos, by which a plurality of videos taken by cameras arranged 2-dimensionally are encoded, the method including encoding videos taken by a camera centerly located among other cameras arranged 2-dimensionally, and sequentially encoding videos taken by the other cameras in an order based on shortest distances from the centerly  located camera.<br>
[22]	A result of the encoding of videos taken by the camera centerly  located may be a<br>
base layer video and a result of the sequential encoding may be at least one enhancement layer video for the base layer video.<br>
[23]	Further, in the sequentially encoding, if there are a plurality of cameras having a<br>
	same distance from the centerly  located camera, encoding of the plurality of cameras<br>
having the same distance may be sequentially performed in a spiral manner.<br>
[24]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a medium including computer readable code to implement a method for 3-dimensional encoding of videos, the method including performing<br>
-4-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
temporal estimation on video taken by a centerly  located camera with reference to videos taken by the centerly  located camera at at least an immediately previous time,<br>
when a plurality of other cameras are arranged in a row, with the centerly  located camera being at a central position of the row, and performing temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time.<br>
[25]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth an encoder for 3-dimensional encoding, including a first encoder to perform temporal estimation on video taken by a centerly  located camera with reference to video taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras are arranged in a row, with the centerly  located camera being at a central position of the row, a second encoder to perform temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time, and a multiplexer to multiplex an output of the first encoder and an output of the second encoder.<br>
[26]	In the second encoder the temporal-spatial estimation may be performed on<br>
previous-in-time videos referred to by the videos taken by the other cameras with reference to a number of previous-in-time videos which is equal to a predetermined number of reference pictures.<br>
[27]	In addition, an output of the first encoder may be a base layer video and an output<br>
of the second encoded may be at least one enhancement layer video for the base layer video.<br>
[28]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth an encoder for 3-dimensional encoding of videos, including a first encoder encoding present time video taken by a camera adjacent to a center of a video by referring to a previous-in-time video of the camera adjacent to the center of the video, a second encoder to perform temporal-spatial estimation with reference to as many previous-in-time videos adjacent to the camera adjacent to the center of the video according to a predetermined number of reference pictures, and a multiplexer to multiplex an output of the first encoder and an output of the second encoder.<br>
[29]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth an encoder for 3-dimensional encoding of videos, by which<br>
                                                                     -5-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
a plurality of videos taken by cameras arranged 2-dimensionally are encoded, including a first encoder to encode videos taken by a camera centerly  located among<br>
other cameras arranged 2-dimensionally, a second encoder to sequentially encode videos taken by the other cameras in an order based on shortest distances from the centerly located camera, and a multiplexer to multiplex an output of the first encoder and an output of the second encoder.<br>
[30]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth an encoding system for 3-dimensional encoding, including a plurality of cameras, with at least one camera of the plurality of cameras being centerly  located among the plurality of cameras, a first encoder to perform temporal estimation on video taken by the centerly  located camera with reference to video taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras, of the plurality of cameras, are arranged in a row, with the centerly  located camera being at a central position of the row, a second encoder to perform temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time, and a multiplexer to multiplex an output of the first encoder and an output of the second encoder.<br>
[31]		To achieve the above and/or other aspects and advantages, embodiments of the<br>
	present invention set forth a method for 3-dimensional decoding of videos, the method<br>
including demultiplexing a video bitstream into a base layer video and at least one en<br>
hancement layer video, decoding the base layer video, to decode video encoded by<br>
performed temporal estimation for video taken by a centerly  located camera with<br>
reference to video taken by the centerly  located camera at at least an immediately<br>
previous time, when a plurality of other cameras were arranged in a row, with the<br>
centerly  located camera being at a central position of the row, and decoding the at least<br>
one enhancement layer video, based on network  resources, to decode video encoded by performed temporal-spatial encoding on videos taken by the other cameras with<br>
reference to previous-in-time videos taken by cameras adjacent to the centerly  located<br>
camera and the video taken by the centerly  located camera at the at least the im-<br>
mediately previous time.<br>
[32]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a method for 3-dimensional decoding of videos, the method including demultiplexing a video bitstream into a base layer video and at least one en-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
hancement layer video, decoding the base layer video, to decode video encoded by referring to a previous-in-time video taken by a camera adjacent to a center of a video<br>
to be then presently encoded, and decoding the at least one enhancement layer video,<br>
based on network resources, to decode video encoded by performed temporal-spatial estimation with reference to as many previous-in-time videos adjacent to the camera adjacent to the center of the video according to a predetermined number of reference pictures.<br>
[33]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a method for 3-dimensional decoding of videos, by which a plurality of videos taken by cameras arranged 2-dimensionally were encoded, the method including demultiplexing a video bitstream into a base layer video and at least one enhancement layer video , decoding the base layer video, to decode video encoded by encoding videos taken by a camera centerly located among other cameras arranged 2-dimensionally, and decoding the at least one enhancement layer video, based on network resources, to decode video encoded by sequentially encoding videos taken by the other cameras in an order based on shortest distances from the centerly located camera.<br>
[34]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a computer readable medium including computer readable code to implement a method for 3-dimensional decoding of videos, the method including demultiplexing a video bitstream into a base layer video and at least one enhancement layer video , decoding the base layer video, to decode video encoded by<br>
                performed temporal estimation on videos taken by a centerly located camera with<br>
 reference to videos taken by the centerly located camera at at least an immediately<br>
 previous time, when a plurality of other cameras were arranged in a row, with the<br>
 centerly located camera being at a central position of the row, and decoding the at least<br>
 one enhancement layer video, based on network resources, to decode video encoded by<br>
 performed temporal-spatial estimation on videos taken by the other cameras with<br>
 reference to previous-in-time videos taken by cameras adjacent to the centerly located<br>
 camera and the video taken by the centerly located camera at the at least the im<br>
 mediately previous time.<br>
[35]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a decoder for 3-dimensional decoding of videos, including a demultiplexer to demultiplex a video bitstream into a base layer video and at least one enhancement layer video, a first decoder to decode the base layer video, by decoding<br>
-7-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
video encoded by performed temporal estimation for video taken by a centerly  located camera with reference to video taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras were arranged in a row, with the centerly  located camera being at a central position of the row, and a second decoder to decode the at least one enhancement layer video, based on network resources, by decoding video encoded by performed temporal-spatial encoding on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time.<br>
[36]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a decoder for 3-dimensional decoding of videos, including a demultiplexer to demultiplex a video bitstream into a base layer video and at least one enhancement layer video, a first decoder to decode the base layer video, by decoding video encoded by referring to a previous-in-time video taken by a camera adjacent to a center of a video to be then presently encoded, anda second decoder to decode the at least one enhancement layer video, based on network resources, by decoding video encoded by performed temporal-spatial estimation with reference to as many previous-in-time videos adjacent to the camera adjacent to the center of the video according to a predetermined number of reference pictures.<br>
[37]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a decoder for 3-dimensional decoding of videos, by which a plurality of videos taken by cameras arranged 2-dimensionally were encoded, including a demultiplexer to demultiplex a video bitstream into a base layer video and at least one enhancement layer video, a first decoder to decode the base layer video, by decoding video encoded by encoding videos taken by a camera centerly  located among other cameras arranged 2-dimensionally, and a second decoder to decode the at least one enhancement layer video, based on network resources, by decoding video encoded by sequentially encoding videos taken by the other cameras in an order based on shortest distances from the centerly  located camera.<br>
[38]	To achieve the above and/or other aspects and advantages, embodiments of the<br>
present invention set forth a 3-dimensional encoded signal, including a base layer video encoded through performed temporal estimation on video taken by a centerly  located camera with reference to videos taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras were arranged with the centerly  located camera being at a central position of the arranged centerly  located<br>
-8-<br><br>
WO 2005/069630<br><br><br><br>
PCT7KR2005/000182<br><br>
camera, and at least one enhancement layer video encoded through performed temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous<br>
time, wherein the base layer video and the at least one enhancement layer video are multiplexed to generate the 3-demensional encoded signal.<br>
Mode   for   Invention<br>
[39]	Reference will now be made in detail to the embodiments of the present invention,<br>
  examples of which are illustrated in the accompanying drawings, wherein like<br>
  reference numerals refer to the like elements throughout. The embodiments are<br>
 described below to explain the present invention by referring to the figures.<br>
[40]	FIG. 1 is a view illustrating encoding and reproduction of stereoscopic video using<br>
             left view video and right view video, according to an embodiment of the present<br>
             invention.<br>
[41]	As illustrating in FIG. 1, in an MPEG-2 multi-view profile (13818-2),<br>
	3-dimensional video can be coded and reproduced using a scalable codec in which a<br>
correlation between the left view video and right view video is searched and a disparity<br>
between the two videos is coded according to a condition of a corresponding network.<br>
Encoding is carried out using the left view video as base layer video and the right view<br>
video as enhancement layer video. The base layer video indicates video that can be<br>
coded as it is, while the enhancement layer video indicates video that is additionally<br>
coded and later used to improve the quality of the base layer video when the cor-<br>
responding network transporting the two video layers is in good condition, i.e., when<br>
the network conditions are not favorable only the base layer video may be reproduced.<br>
As such, encoding using both the base layer video and the enhancement layer video is<br>
referred to as scalable encoding.<br>
[42]		The left view video can be coded by a first motion compensated DCT encoder 110.<br>
A disparity between the left view video and the right view video can be calculated by a disparity estimator 122, which estimates a disparity between the left view video and the right view video, and a disparity compensator 124 and can then be coded by a second motion compensated DCT encoder 126. Assuming that the first motion compensated DCT encoder 110 that encodes the left view video is a base layer video encoder, the disparity estimator 122, the disparity compensator 124, and the second motion compensated DCT encoder 126 that involve encoding the disparity between the<br>
left view video and the right view video may be referred to as an enhancement layer<br>
-9-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
video encoder 120. The encoded base layer video and enhancement layer video can then be multiplexed by a system multiplexer 130 and transmitted to for subsequent<br>
decoding.<br>
[43]	In the decoding, multiplexed data can be decomposed into the left view video and<br>
the right view video by a system demultiplexer 140. The left view video can be decoded by a first motion compensated DCT decoder 150. Disparity video is then restored to the right view video by a disparity compensator 162, which compensates for the disparity between the left view video and the right view video, and a second motion compensated DCT decoder 164. Assuming that the first motion compensated DCT decoder 150 that decodes the left view video is a base layer video decoder, the disparity compensator 162 and the second motion compensated DCT decoder 164 that involve searching the disparity between the left view video and the right view video an d decoding the right view video can be referred to as an enhancement layer video decoder 160.<br>
[44]	FIGS. 2A and 2B illustrate exemplary structures of base layer video and en-<br>
hancement layer video.<br>
[45]	As illustrated in FIG. 2A, similar to video encoding in MPEG-2 or MPEG-4, the<br>
base layer video, which is of a left view video type, is encoded using an into picture (called an I picture) 212, a predictive picture (called a P picture) 218, and bi-directional pictures (called B pictures) 214 and 216. On the other hand, the enhancement layer video, which is of a right view video type, may include a P picture 222 encoded with reference to the I picture 212 of a left view video type, a B picture 224 encoded with reference to the P picture 222 of a right view video type and the B picture 214 of a left view video type, a B picture 226 encoded with reference to the B picture 224 of a right view video type and the B picture 216 of a left view video type, and a B picture 228 encoded with reference to the B picture 226 of a right view video type and the P picture 218 of a left view video type. In other words, the disparity can be encoded with reference to the base layer video. In the illustration of FIG. 2A, the direction of the arrows indicate encoding of respective video with reference to video identified to by the arrow point.<br>
[46]	FIG. 2B illustrates another exemplary structure of the enhancement layer video.<br>
[47]	Referring to FIG. 2B, the enhancement layer video of a right view video type can<br>
include a B picture 242 encoded with reference to a B picture 232 of a left view video type, a B picture 244 encoded with reference to the B picture 242 of a right view video type and a B picture 234 of a left view video type, and a B picture 246 encoded with<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
reference to the B picture 244 of a right view video type and a P picture 236 of a left view video type.<br>
[48]	FIG. 3 is a view illustrating creation of a single video using decimation of the left<br>
view video and right view video and reconstruction of the single video into left view video and right view video using interpolation of the single video.<br>
[49]	Referring to FIG. 3, stereo video encoding can be performed in an MPEG-2 main<br>
profile (MP) that uses motion encoding and disparity encoding. Two videos can be combined into one video by horizontally decimating the left view video and the right view video to 1/2 in stereo video encoding and then reducing the bandwidth by 1/2. The combined video can then be transmitted to a decoder. A decoder receives the combined video and restores the original videos by decomposing the combined video into the left view video and the right view video and two times interpolating the left view video and the right view video.<br>
[50]	FIG. 4 is a view illustrating motion estimation/compensation of a decimated video<br>
including the left view video and the right view video.<br>
[51]	As illustrated in FIG. 4, the enhancement layer videos RI, RB, and RP can be<br>
encoded with reference to enhancement layer videos adjacent to base layer videos LI, LB, and LP. Here, RI represents the I picture of a right view video type, RB represents the B picture of a right view video type, RP represents the P picture of a right view video type, LI represents the I picture of a left view video type, LB represents the B picture of a left view video type, and LP represents the P picture of a left view video type.<br>
[52]	However; such an encoding method has problems that disparity information is not<br>
efficiently compressed and a difference in display quality between the left view video and the right view video becomes consistently greater than 0.5 -1.5 dB. Also, if several cameos exist for one scene, it becomes difficult to receive the extra video data.<br>
[53]	FIG. 5A is a view illustrating encoding video data received from a plurality of<br>
cameras arranged in a row.<br>
[54]	Referring to FIG. 5A, the plurality of cameras can be arranged in a row, e.g., in a<br>
one-dimensional line. In embodiments of the present invention, it may be assumed that<br>
the cameras exist in a 2-dimensional space composed of i axis and j axis. However, to explain an embodiment of the present invention the case where the plurality of cameras are illustrated as existing in only a one-dimensional space, i.e., i of (i j) is equal to 0. If i is not equal to zero, a plurality of cameras will exist in a 2-dimensional space. Such an example will be described later with reference to FIG. 7.<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
[55]	FIG. 5B illustrates video taken by a plurality of cameos over time, e.g., with scene<br>
changes.<br>
[56]	With videos taken by one of the camera being identified by f (i, j, t), at a particular<br>
time t, (i, j) will identify the position of the camera, and when i is equal to 0 the corresponding camera exists in only one dimensional space, as illustrated in FIGS. 5A and 5B. For example, f (0,0,0) identifies a video taken by a center camera at the initial time. If videos taken by other cameras ax arranged along the time axis, there will also exist an angle q with respect to videos taken by adjacent cameras at the adjacent time t. The angle information q can also be used for encoding and decoding.<br>
[57]	FIGS. 6A and 6B are views illustrtate 3-dimensional encoding of video, according<br>
to an embodiment of the present invention.<br>
[58]	As illustrated in FIG. 6A, videos f (0,0,0), f (0,0,1), f (0,0,2), f (0,0,3), and f (0,<br>
0,4), respectively from cameras located at center positions (0,0, t) from a first direction, are each encoded into base layer videos, i.e., they are each temporally estimated and encoded only with reference to an immediately previous-in-time base layer videos. For example, f (0, 0, 2) is estimated with reference to f (0,0,1), and f (0, 0,3) is estimated with reference to f (0,0,2). As an example, a maximum number of five reference videos can be used. Videos f (0, -1, t) taken by cameras located in positions (0,-1, t) are encoded into first enhancement layer videos. Specifically, videos f (0, -1, t) can be estimated using temporally previous-in-time decoded videos and reference videos of f (0, -1, t-1 ~ t-5). For examples, video f (0, -1,2) can be estimated with reference to videos f (0,0,1) and f (0, -1,1), and video f (0, -1,3) can be estimated with reference to videos f (0, 0, 2) and f (0, -1, 2). Again, in this example, a maximum of five reference videos are used in motion estimation into the base layer videos. In other words, motion is temporal-spatial estimated and then encoded.<br>
[59]	Videos of other layers can be encoded in the same way as the above. In other<br>
	words, videos f (0, -2, t) taken from camera positions (0, -2, t) can be encoded into<br>
third enhancement layer videos, videos f (0,1, t) taken from cameo positions (0,1, t)<br>
can be encoded into second enhancement layer videos, and videos f (0, 2, t) taken from<br>
cameo positions f (0,2, t) can be encoded into fourth enhancement layer videos.<br>
[60]		As further illustrated in FIG. 6B, for encoding of enhancement layer videos,<br>
	adjacent layer videos can also be referred to, according to another embodiment of the<br>
present invention. In this case, since a greater number of reference videos are used,<br>
display quality of restored videos can be improved.<br>
[61]	FIG. 7 illustrates camera positions and an order of encoding when a plurality of<br>
-12-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
cameras exists in a 2-dimensional space.<br>
[62]	Referring to FIG. 7, camera positions are illustrated when cameras exist two di-<br>
mensionally and t is equal to 0. According to one order of encoding videos taken by cameras, videos taken by a camera located in a centerly position can be encoded first, and videos taken by the 8 cameras that are located closest to the centerly positioned camera, e.g., those that have a distance of 1 from the centerly positioned camera (it is assumed here that the distance from one camera to another is 1) are sequentially encoded in a spiral manner. Then, videos taken from the 16 cameras that have a distance of 2 from the centerly positioned camera are sequentially encoded in a spiral manner. Such encoding can be arranged as follows.<br>
[63]	(1) f (0,0): distance = 0<br>
[64]	(2) f (1, 0), f (1,1), f (0,1), f (-1, -1), f (-1, 0), f (-1, -1), f (0, -1), f (1, -1): distance<br>
= 1<br>
[65]	(3) f (2, 0), f (2, 1), f (2, 2), - ..: distance = 2<br>
[66]	(4) f (3,0), f (3, 1), - : distance = 3<br>
[67]	If encoding is performed in the order described above, although the bandwidth of a<br>
corresponding network may be reduced, videos from all the cameras cannot be encoded and transmitted, and thus only a portion of the videos is transmitted. Accordingly, to overcome this potential bandwidth issue, videos from N cameras can be<br>
spatially-temporally predicted and restored using bilinear interpolation or sync function type interpolation. Therefore, once 3-dimensional video information from cameras located in positions (i, j, t) is encoded and transmitted to the decoder even though only partial data is transmitted when the bandwidth of a network is poor, the decoder can still restore the original videos by performing interpolation.<br>
[68]	A method for encoding, according to an embodiment of the present invention, can<br>
be further explained using a video f (0,6,6) as an example, as follows.<br>
[69]	(1) f (0, 6, 5), f (0, 6, 4), f (0, 6, 3), f (0, 6, 2), f (0, 6, 1) : When j is equal to 6,<br>
temporal prediction, i.e., motion estimation/compensation can be performed. At this time, the number of reference pictures is 5, noting that the number of reference pictures is subject to change according to various circumstances.<br>
[70]	(2) Temporal-spatial prediction can be performed from the video f (0,6,6) towards<br>
a center picture. At this time, temporal-spatial prediction is performed using a previously defined angle<br>
θ<br>
. In other words, temporal-spatial prediction can be performed on all the pictures that<br>
-13-<br><br>
WO 2005/069630		PCT/KR2005/000182<br>
fall within a range of the angle<br>
θ<br>
.If<br>
θ<br>
is equal to 45°, prediction is performed in the following order (for example):<br>
[71]	a) f (0, 5,5), f (0,5,4), f (0,5, 3), f (0,5, 2), f (0,5,1)<br>
[72]	b) f (0,4,4), f (0,4, 3), f (0,4, 2), f (0,4, 1)<br>
[73]	c)f(0,3,3),f(0,3,2),f(0,3,l)<br>
[74]	d)f(0,2,2),f(0,2,l)<br>
[75]	e)f (0,1,1)<br>
[76]	In other words, motion estimation/compensation can be performed in units of<br>
macroblocks on the above 15 temporal-spatial reference pictures, with the reference<br>
pictures being determined using the previously defined angle<br>
θ<br>
[77]	(3) During temporal-spatial estimation encoding of (1) and (2), a macroblock that is<br>
most similar to a currently encoded macroblock can be searched for from the reference pictures and motion estimation/compensation and residual transform coding can be performed on the found macroblock.<br>
[78]	According to further embodiments of the present invention, decoding methods can<br>
be similarly performed inversely with respect to the aforementioned encoding methods, for example. As described with reference to FIGS. 6A and 6B, once the multiplexed base layer videos and enhancement layer videos are received, the multiplexed videos can be decomposed into individual layer videos and decoded.<br>
[79]	Methods for 3-dimensional encoding of videos can be implemented through<br>
	computer readable code, e.g., as computer programs. Codes and code segments making up the computer readable code can be easily construed by skilled computer<br>
programmers. Also, the computer readable code can be stored/transferred on computer<br>
readable media, with and methods for 3-dimensional encoding/decoding of videos<br>
being implemented by reading and executing the computer readable codes. The<br>
computer readable media include magnetic recording media, optical recording media,<br>
and carrier wave media, for example.<br>
[80]		While the present invention has been particularly shown and described with<br>
reference to exemplary embodiments thereof, it will be understood by those of ordinary skill in the at that various changes in form and details may be made therein<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
without departing from the spirit and scope of the present invention as defined by the following claims.<br>
-15-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
Claims<br>
[1]	LA method for 3-dimensional encoding of videos, the method comprising:<br>
performing temporal estimation on video taken by a centerly  located camera with reference to video taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras are arranged in a row, with the centerly  located camera being at a central position of the row; and performing temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time.<br>
[2]	2. The method of claim 1, wherein a result of the performed temporal estimation<br>
on video taken by the centerly  located camera is a base layer video and a result of the performed temporal-spatial estimation on videos taken by the other cameras is at least one enhancement layer video for the base layer video.<br>
[3]	3. The method of claim 1, wherein in the performing of the temporal-spatial<br>
estimation on videos taken by the other cameras the temporal-spatial estimation is performed at least on previous-in-time videos referred to by the videos taken by the other cameras with reference at least to a number of previous-in-time videos which is equal to a predetermined number of reference pictures.<br>
[4]	4. The method of claim 3, wherein the predetermined number of reference<br>
pictures is 5.<br>
[5]	5. The method of claim 3, wherein in the temporal-spatial estimation on videos<br>
taken by the other cameras temporal-spatial estimation is also performed with reference further to current videos taken bycameras adjacent to the centerly  located camera.<br>
[6]	6. The method of claim 3, wherein in the temporal-spatial estimation on videos<br>
taken by the other cameras temporal-spatial estimation is performed with reference to videos taken by all of a plurality of cameras that fall within a range of an angle between previous-in-time videos taken by cameras adjacent to the centerly  located camera and videos to be presently estimated.<br>
[7]	7. A method for 3-dimensional encoding of videos, the method comprising:<br>
referring to a previous-in-time video taken by a camera adjacent to a center of a<br>
video to be presently encoded; and<br>
performing temporal-spatial estimation with further reference to as many<br>
-16-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
previous-in-time videos adjacent to the camera adjacent to the center of the video according to a predetermined number of reference pictures.<br>
[8]	8. The method of claim 7, wherein a result of the referring is a base layer video<br>
and a result of the performed temporal-spatial estimation is at least one enhancement layer video for the base layer video.<br>
[9]	9. The method of claim 7, wherein an angle between the camera adjacent to the<br>
center of the video and the video to be presently encoded varies according to an interval between adjacent cameras.<br>
[10]	10. A method for 3-dimensional encoding of videos, by which a plurality of<br>
videos taken by cameras arranged 2-dimensionaliy are encoded, the method comprising:<br>
encoding videos taken by a camera centerly  located among other cameras arranged 2-dimensionally; and<br>
sequentially encoding videos taken by the other cameras in an order based on shortest distances from the centerly  located camera.<br>
[11]	11. The method of claim 10, wherein a result of the encoding of videos taken by<br>
the camera centerly  located is a base layer video and a result of the sequential encoding is at least one enhancement layer video for the base layer video.<br>
[12]	12. The method of claim 10, wherein in the sequentially encoding, if there are a<br>
plurality of cameras having a same distance from the centerly  located camera, encoding of the plurality of cameras having the same distance is sequentially performed in a spiral manner.<br>
[13]	13. A medium comprising computer readable code to implement a method for<br>
3-dimensional encoding of videos, the method comprising: performing temporal estimation on video taken by a centerly  located camera with reference to videos taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras are arranged in a row, with the centerly  located camera being at a central position of the row; and performing temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time.<br>
[14]	14. The medium of claim 13, wherein a result of the performed temporal<br>
estimation on video taken by the centerly  located camera is a base layer video and a result of the performed temporal-spatial estimation on videos taken by the<br>
-17-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
other cameras is at least one enhancement layer video for the base layer video.<br>
[15]	15. An encoder for 3-dimensional encoding, comprising:<br>
a first encoder to perform temporal estimation on video taken by a centerly<br>
located camera with reference to video taken by the centerly located camera at at least an immediately previous time, when a plurality of other cameras are<br>
arranged in a row, with the centerly located camera being at a central position of<br>
the row;<br>
a second encoder to perform temporal-spatial estimation on videos taken by the<br>
other cameras with reference to previous-in-time videos taken by cameras<br>
adjacent to the centerly located camera and die video taken by the centerly<br>
located camera at the at least the immediately previous time; and<br>
a multiplexer to multiplex an output of the first encoder and an output of die<br>
second encoder.<br>
[16]	16. The encoder of claim 15, wherein in the second encoder the temporal-spatial<br>
estimation is performed at least on previous-in-time videos referred to by the videos taken by the other cameras with reference at least to a number of previous-in-time videos which is equal to a predetermined number of reference pictures.<br>
[17]	17. The encoder of claim 16, wherein the predetermined number of reference<br>
pictures is 5.<br>
[18]	18. The encoder of claim 16, wherein in the second encoder temporal-spatial<br>
estimation is also performed with reference to father current videos taken by cameras adjacent to die centerly located camera.<br>
[19]	19. The encoder of claim 16, wherein in the second encoder temporal-spatial<br>
estimation is performed with reference to videos taken by all of a plurality of cameras that fall within a range of an angle between previous-in-time videos taken by cameras adjacent to the centerly located camera and videos to be presently estimated.<br>
[20]	20. The encoder of claim 16, wherein an output of the first encoder is a base<br>
layer video and an output of the second encoded is at least one enhancement layer video for the base layer video.<br>
[21]	21. An encoder for 3-dimensional encoding of videos, comprising:<br>
a first encoder encoding present time video taken by a camera adjacent to a center of a video by referring to a previous-in-time video of the camera adjacent to die center of die video;<br>
-18-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KK2005/000182<br><br>
a second encoder to perform temporal-spatial estimation with further reference to<br>
as many previous-in-time videos adjacent to the camera adjacent to the center of<br>
the video according to a predetermined number of reference pictures; and<br>
a multiplexer to multiplex an output of the first encoder and an output of the<br>
second encoder.<br>
[22]	22. The encoder of claim 21, wherein an angle between the camera adjacent to<br>
the center of the video and the video to be presently encoded varies according to<br>
an interval between adjacent cameras.<br>
[23]	23. The encoder of claim 21, wherein an output of the first encoder is a base<br>
layer video and an output of the second encoded is at least one enhancement<br>
layer video for the base layer video.<br>
[24]	24. An encoder for 3-dimensional encoding of videos, by which a plurality of<br>
videos taken by cameos arranged 2-dimensionally are encoded, comprising:<br>
a first encoder to encode videos taken by a camera centerly located among other cameras arranged 2-dimensionally;<br>
a second encoder to sequentially encode videos taken by the other cameras in an order based on shortest distances from the centerly located camera; and a multiplexer to multiplex an output of the first encoder and an output of the second encoder.<br>
[25]	25. The encoder of claim 24, wherein in the second encoder, if there are a<br>
plurality of cameras having a same distance from the centerly located cameo, encoding of the plurality of cameras having the same distance is sequentially performed in a spiral manner.<br>
[26]	26. The encoder of claim 24, wherein an output of the first encoder is a base<br>
layer video and an output of the second encoded is at least one enhancement layer video for the base layer video.<br>
[27]	27. An encoding system for 3-dimensional encoding, comprising:<br>
a plurality of cameras, with at least one camera of the plurality of cameras being<br>
centerly located among the plurality of cameras;<br>
a first encoder to perform temporal estimation on video taken by the centerly<br>
located camera with reference to video taken by the centerly located camera at at<br>
least an immediately previous time, when a plurality of other cameras, of the<br>
plurality of cameras, are arranged in a row, with the centerly located camera<br>
being at a central position of the row;<br>
a second encoder to perform temporal-spatial estimation on videos taken by the<br>
-19-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
outer cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly located camera and the video taken by the centerly <br>
located camera at the at least the immediately previous time; and<br>
a multiplexer to multiplex an output of the first encoder and an output of the<br>
second encoder.<br>
[28]	28. The encoding system of claim 27, wherein in the second encoder the<br>
	temporal-spatial estimation is performed at least on previous-in-time videos<br>
referred to by the videos taken by the other cameras with reference at least to a<br>
number of previous-in-time videos which is equal to a predetermined number of<br>
reference pictures.<br>
[29]	29. The encoding system of claim 28, wherein in the second encoder temporal-<br>
	spatial estimation is performed with reference to videos taken by all of a plurality<br>
of cameras that fall within a range of an angle between previous-in-time videos<br>
taken by cameras adjacent to the centerly  located camera and videos to be<br>
presently estimated.<br>
[30]	30. The encoding system of claim 27, wherein an output of the first encoder is a<br>
	base layer video and an output of the second encoded is at least one enhancement<br>
layer video for the base layer video.<br>
[31]	31. A method for 3-dimensional decoding of videos, the method comprising:<br>
demultiplexing a video bitstream into a base layer video and at least one enhancement layer video;<br>
decoding the base layer video, to decode video encoded by performed temporal<br>
estimation for video taken by a centerly  located camera with reference to video<br>
taken by the centerly  located camera at at least an immediately previous time,<br>
when a plurality of other cameras were arranged in a row, with the centerly <br>
located camera being at a central position of the row; and<br>
decoding the at least one enhancement layer video, based on network resources,<br>
to decode video encoded by performed temporal-spatial encoding on videos<br>
taken by the other cameras with reference to previous-in-time videos taken by<br>
cameras adjacent to the centerly  located camera and the video taken by the<br>
centerly located camera at the at least the immediately previous time.<br>
[32]	32. The method of claim 31, wherein in the encoding of the at least one en-<br>
hancement layer video, in die performed temporal-spatial estimation on videos taken by the other cameras, the temporal-spatial estimation was performed at<br>
least on previous-in-time videos referred to by the videos taken by the other<br>
-20 -<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
cameras with reference at least to a number of previous-in-time videos which is equal to a predetermined number of reference pictures.<br>
[33]	33. The method of claim 32, wherein the predetermined number of reference<br>
pictures was 5.<br>
[34]	34. The method of claim 32, wherein in the encoding of the at least one en-<br>
hancement layer video, in the performed temporal-spatial estimation on videos taken by the other cameos, the temporal-spatial estimation was also performed with reference to then further current videos taken by cameos adjacent to the centerly  located cameo.<br>
[35]	35. The method of claim 32, wherein in the encoding of the at least one en-<br>
hancement layer vide, in the performed temporal-spatial estimation on videos taken by the other cameos, the temporal-spatial estimation was performed with reference to videos taken by all of a plurality of cameos that fell within a range of an angle between previous-in-time videos taken by cameos adjacent to the centerly  located cameo and videos to then currently be estimated.<br>
[36]	36. A method for 3-dimensional decoding of videos, the method comprising:<br>
demultiplexing a video bitstream into a base layer video and at least one enhancement layer video;<br>
decoding the base layer video, to decode video encoded by referring to a previous-in-time video taken by a cameo adjacent to a center of a video to be men presently encoded; and<br>
decoding the at least one enhancement layer video, based on network resources, to decode video encoded by performed temporal-spatial estimation with father reference to as many previous-in-time videos adjacent to the cameo adjacent to the center of the video according to a predetermined number of reference pictures.<br>
[37]	37. The method of claim 36, wherein an angle between the cameo adjacent to<br>
the center of the video and the video to be then presently encoded varied according to an interval between adjacent cameos.<br>
[38]	38. A method for 3-dimensional decoding of videos, by which a plurality of<br>
videos taken by cameras arranged 2-dimensionally were encoded, the method comprising:<br>
demultiplexing a video bitstream into a base layer video and at least one enhancement layer video;<br>
decoding the base layer video, to decode video encoded by encoding videos<br>
-21-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
taken by a camera centerly  located among other cameos arranged 2-dimensionally; and<br>
decoding the at least one enhancement layer video, based on network resources, to decode video encoded by sequentially encoding videos taken by the other cameras in an order based on shortest distances from the centerly  located camera.<br>
[39]	39. The method of claim 38, wherein in the decoding of the sequentially encoded<br>
at least one enhancement layer video, if there were a plurality of cameras having a same distance from the centerly  located camera, the encoding of the plurality of cameras having the same distance was sequentially performed in a spiral manner.<br>
[40]	40. A computer readable medium comprising computer readable code to<br>
implement a method for 3-dimensional decoding of videos, the method comprising:<br>
demultiplexing a video bitstream into a base layer video and at least one enhancement layer video;<br>
decoding the base layer video, to decode video encoded by performed temporal estimation on videos taken by a centerly  located camera with reference to videos taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras were arranged in a row, with the centerly  located camera being at a central position of the row; and decoding the at least one enhancement layer video, based on network resources, to decode video encoded by performed temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time.<br>
[41]	41. A decoder for 3-dimensional decoding of videos, comprising:<br>
a demultiplexer to demultiplex a video bitstream into a base layer video and at<br>
least one enhancement layer video ;<br>
a first decoder to decode the base layer video, by decoding video encoded by<br>
performed temporal estimation for video taken by a centerly  located camera with<br>
reference to video taken by the centerly  located camera at at least an immediately<br>
previous time, when a plurality of other cameras were arranged in a row, with<br>
the centerly  located camera being at a central position of the row; and<br>
a second decoder to decode the at least one enhancement layer video, based on<br>
network resources, by decoding video encoded by performed temporal-spatial<br>
-22-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
encoding on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly located camera and the video taken by the centerly located camera at the at least the immediately previous time.<br>
[42]	42. The decoder of claim 41, wherein in the encoding of the at least one en-<br>
hancement layer video, in the performed temporal-spatial estimation on videos taken by the other cameras, the temporal-spatial estimation was performed at least on previous-in-time videos referred to by the videos taken by the other cameras with reference at least to a number of previous-in-time videos which is equal to a predetermined number of reference pictures.<br>
[43]	43. The decoder of claim 42, wherein the predetermined number of reference<br>
pictures was 5.<br>
[44]	44. The decoder of claim 42, wherein in the encoding of the at least one en-<br>
hancement layer video, in the performed temporal-spatial estimation on videos taken by the other cameras, the temporal-spatial estimation was also performed with further reference to then current videos taken by cameras adjacent to the centerly located camera.<br>
[45]	45. The decoder of claim 42, wherein in the encoding of the at least one en-<br>
hancement layer vide, in the performed temporal-spatial estimation on videos taken by the other cameras, the temporal-spatial estimation was performed with reference to videos taken by all of a plurality of cameras that fell within a range of an angle between previous-in-time videos taken by cameras adjacent to the centerly located camera and videos to then currently be estimated.<br>
[46]	46. A decoder for 3-dimensional decoding of videos, comprising:<br>
a demultiplexer to demultiplex a video bitstream into a base layer video and at least one enhancement layer video ;<br>
a first decoder to decode the base layer video, by decoding video encoded by referring to a previous-in-time video taken by a camera adjacent to a center of a<br>
video to be then presently encoded; and<br>
	a second decoder to decode the at least one enhancement layer video, based on<br>
network resources, by decoding video encoded by performed temporal-spatial<br>
estimation with further reference to as many previous-in-time videos adjacent to the camera adjacent to the center of the video according to a predetermined<br>
number of reference pictures.<br>
[47]	47. The decoder of claim 46, wherein an angle between the camera adjacent to<br>
-23-<br><br>
WO 2005/069630<br><br><br><br>
PCT/KR2005/000182<br><br>
	the center of the video and the video to be then presently encoded varied<br>
according to an interval between adjacent cameos.<br>
[48]	48. A decoder for 3-dimensional decoding of videos, by which a plurality of<br>
videos taken by cameras arranged 2-dimensionally were encoded, comprising: a demultiplexer to demultiplex a video bitstream into a base layer video and at least one enhancement layer video ;<br>
a first decoder to decode the base layer video, by decoding video encoded by encoding videos taken by a camera centerly  located among other cameras<br>
arranged 2-dimensionally; and<br>
a second decoder to decode the at least one enhancement layer video, based on<br>
network resources, by decoding video encoded by sequentially encoding videos<br>
taken by the other cameras in an order based on shortest distances from the<br>
centerly  located camera.<br>
[49]	49. The decoder of claim 48, wherein in the decoding of the sequentially<br>
encoded at least one enhancement layer video, if there were a plurality of<br>
cameras<br>
[50]	50. A 3-dimensional encoded signal, comprising:<br>
a base layer video encoded through performed temporal estimation on video<br>
taken by a centerly  located camera with reference to videos taken by the centerly  located camera at at least an immediately previous time, when a plurality of other cameras were arranged with the centerly located camera being at a central position of the arranged centerly located camera; and<br>
at least one enhancement layer video encoded through performed temporal-spatial estimation on videos taken by the other cameras with reference to previous-in-time videos taken by cameras adjacent to the centerly  located camera and the video taken by the centerly  located camera at the at least the immediately previous time,<br>
wherein the base layer video and the at least one enhancement layer video are multiplexed to generate the 3-demensional encoded signal.<br>
-24-<br><br>
51. A method for 3-dimensional encoding of videos, a medium, an encoder for 3-dimensional encoding of videos, a method for 3-dimensional decoding of videos, a computer readable medium, a decoder for 3-dimensional encoding of videos, a 3-dimensional encoded signal are substantially as herein described with reference to the accompanying drawings.<br>
Dated this 9th day of September, 2005.<br><br>
RAVI BHOLA<br>
OF K &amp; S PARTNERS<br>
AGENT FOR THE APPLICANT(S)<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
-25- <br>
ABSTRACT<br>
[Abstract of the Disclosure]<br>
Provided is a method for 3-dimensional encoding of videos, which adapts to <br>
5     	temporal and spatial characteristics of the videos.   The method includes performing temporal estimation on videos taken by a camera located in the center with reference to videos taken by the camera at immediately previous time, when a plurality of cameras is arranged in a row, and performing temporal-spatial estimation on videos taken by other cameras with reference to previous videos taken by cameras adjacent to the camera <br>
10     located in the center.   As described above, according to the present invention,<br>
3-dimensional videos acquired using a number of cameras can be efficiently encoded. [Representative Drawing]<br>
FIG. 5B<br>
1<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWFic3RyYWN0LmRvYw==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-abstract.doc</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWFic3RyYWN0LnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWFzc2lnbm1lbnQoMTAtMDktMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-assignment(10-09-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNhbmNlbGxlZCBwYWdlcygzMS0wOC0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-cancelled pages(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNsYWltcyhncmFudGVkKS0oMzEtMDgtMjAwNykuZG9j" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-claims(granted)-(31-08-2007).doc</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNsYWltcyhncmFudGVkKS0oMzEtMDgtMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-claims(granted)-(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNsYWltcy5kb2M=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-claims.doc</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRhbmNlLW90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondance-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRhbmNlLXJlY2VpdmVkLXZlci0wNzEwMDUucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondance-received-ver-071005.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRhbmNlLXJlY2VpdmVkLXZlci0wOTA5MDUucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondance-received-ver-090905.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRhbmNlLXJlY2VpdmVkLXZlci0xNTExMDUucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondance-received-ver-151105.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRhbmNlLXNlYW5kLnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondance-seand.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRlbmNlKDA1LTEwLTIwMDcpLnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondence(05-10-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWNvcnJlc3BvbmRlbmNlKGlwbyktKDA1LTEwLTIwMDcpLnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-correspondence(ipo)-(05-10-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWRlc2NyaXB0aW9uIChjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWRyYXdpbmcoMzEtMDgtMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-drawing(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWRyYXdpbmdzLnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMSgzMS0wOC0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 1(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMTgoMTUtMDktMjAwNSkucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 18(15-09-2005).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMihncmFudGVkKS0oMzEtMDgtMjAwNykuZG9j" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 2(granted)-(31-08-2007).doc</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMihncmFudGVkKS0oMzEtMDgtMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 2(granted)-(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMjYoMTAtMDktMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 26(10-09-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMjYoMTEtMDktMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 26(11-09-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMjYoMTItMDktMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 26(12-09-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMjYoMjItMDUtMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 26(22-05-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMjYoMzEtMDgtMjAwNykucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 26(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMygwOS0wOS0yMDA1KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 3(09-09-2005).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMygyMi0wNS0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 3(22-05-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMygyOS0wOC0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 3(29-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gMygzMS0wOC0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 3(31-08-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gNSgxMS0wOS0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 5(11-09-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0gNigxMC0wOS0yMDA3KS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form 6(10-09-2007).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tMS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tMTgucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tMi5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tMy5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tcGN0LWliLTMwNy5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-pct-ib-307.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tcGN0LWliLTMxMS5wZGY=" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-pct-ib-311.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tcGN0LWlzYS0yMjAucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-pct-isa-220.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LWZvcm0tcGN0LWlzYS0yMzcucGRm" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-form-pct-isa-237.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LXBjdC1pc2EtMjEwKDE1LTA5LTIwMDUpLnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-pct-isa-210(15-09-2005).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxMS1tdW1ucC0yMDA1LXBjdC1zZWFyY2ggcmVwb3J0LnBkZg==" target="_blank" style="word-wrap:break-word;">1011-mumnp-2005-pct-search report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=YWJzdHJhY3QxLmpwZw==" target="_blank" style="word-wrap:break-word;">abstract1.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="211316-waste-treatment-apparatus.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="211320-a-process-of-making-ozone-resistant-natural-rubber.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>211318</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>1011/MUMNP/2005</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>21/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>23-May-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>24-Oct-2007</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>15-Sep-2005</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>1) SEJONG INDUSTRY ACADEMY COOPERATION FOUNDATION 2) SAMSUNG ELECTRONICS CO LTD</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>98 KUNJA-DONG, KWANGJIN-GU, SEOUL, 143-747 416 MAETAN-DONG, YEONGTON-GU, SUWON-SI, GYEONGGI -DO 442-742 REPUBLIC OF KOREA</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>LEE YUNG-LYUL</td>
											<td>1-704 KIKDONG APT., 192 GARAK-DONG, SONGPA-GU SEOUL 138-160,</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H04N7/24</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/KR2005/000182</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2005-01-20</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>10-2004-0004423</td>
									<td>2004-01-20</td>
								    <td>Republic of Korea</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/211318-method-and-apparatus-for-3-dimensional-encoding-and-or-decoding-of-video by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 00:29:24 GMT -->
</html>
