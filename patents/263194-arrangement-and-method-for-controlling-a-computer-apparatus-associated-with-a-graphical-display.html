<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/263194-arrangement-and-method-for-controlling-a-computer-apparatus-associated-with-a-graphical-display by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:58:05 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 263194:ARRANGEMENT AND METHOD FOR CONTROLLING A COMPUTER APPARATUS ASSOCIATED WITH A GRAPHICAL DISPLAY</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">ARRANGEMENT AND METHOD FOR CONTROLLING A COMPUTER APPARATUS ASSOCIATED WITH A GRAPHICAL DISPLAY</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>The present invention relates to a computer based eye-tracking solution. A computer apparatus is associated with a graphical display, which in turn, represents at least one GUI-component (220a 220n) that is adapted to be manipulated based on user-generated commands. An event engine (210) is adapted to receive an eye-tracking data signal (DEYE) that describes a user&#x27;s point of regard on the display. Based on the signal (DEYE), the event engine (210) is adapted to produce a set of non-cursor controlling event output signals (D-HIi), which influence the at least one GUI-component (220a, ..., 220n). Each non-cursor controlling event output signal (D-HIi) describes a particular aspect of the user&#x27;s ocular activity in respect of the display. Initially, the proposed event engine (210) receives a control signal request (Ra,... , Rn) from each of the at least one GUI-component (220a,..., 220n). The control signal request (Ra, ... , Rn) defines a sub-set of the set of non-cursor controlling event output signals (D-HIi) which is required by the particular GUI-component (220a, .... 220n). The event engine (210) delivers non-cursor controlling event output signals (D-HIi) to the at least one GUI-component (220a,..., 220n) in accordance with each respective control signal request (Ra , Rn).</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td> <br><br>
THE BACKGROUND OF THE INVENTION AND PRIOR ART:<br>
The present invention relates generally to computer based eye-tracking systems.<br>
More particularly the invention relates to an arrangement for controlling a<br>
computer apparatus associated with a graphical display and a corresponding<br>
method which is being performed by a computer readable medium having a<br>
program recorded therein.<br>
The human computer interaction was revolutionized by the introduction of the<br>
graphical user interface (GUI). Namely, thereby, an efficient means was provided<br>
for presenting information to a user with a bandwidth which immensely<br>
exceeded any prior channels. Over the years the speed at which information can<br>
be presented has increased further through color screens, enlarged displays,<br>
intelligent graphical objects (e.g. pop-up windows), window tabs, menus,<br>
toolbars and sounds. During this time, however, the input devices have remained<br>
essentially unchanged, i.e. the keyboard and the pointing device (e.g. mouse,<br>
track ball or touch pad). In recent years, handwriting devices have been<br>
introduced (e.g. in the form of a stylus or graphical pen). Nevertheless, while the<br>
output bandwidth has multiplied several times, the input ditto has been<br>
substantially unaltered. Consequently, a severe asymmetry in the communication<br>
band-width in the human computer interaction has occurred.<br>
In order to decrease this bandwidth gap, various attempts have<br><br>
been made to use eye-tracking devices. However, in many<br>
cases these devices miss the mark in one or several respects.<br>
One problem is that the prior-art solutions fail to take a holistic<br>
view on the input interfaces to the computer. Thereby, com-<br>
paratively heavy motor tasks may be imposed on the eyes,<br>
which in fact are strictly perceptive organs. Typically, this leads<br>
to fatigue symptoms and a degree of discomfort experienced by<br>
the user. This is particularly true if an eye tracker is used to<br>
control a cursor on a graphical display, and for various reasons<br>
the eye tracker fails to track the user's point of regard suffi-<br>
ciently well, so that there is a mismatch between the user's<br>
actual point of regard and the position against which the cursor<br>
is controlled.<br>
Instead of controlling the cursor directly, an eye gaze signal may<br>
be used to select an appropriate initial cursor position. The do-<br>
cument US, 6,204,828 discloses an integrated gaze / manual<br>
cursor positioning system, which aids an operator to position a<br>
cursor by integrating an eye-gaze signal and a manual input.<br>
When a mechanical activation of an operator device is detected<br>
the cursor is placed at an initial position which is predetermined<br>
with respect to the operator's current gaze area. Thus, a user-<br>
friendly cursor function is accomplished.<br>
The document US, 6,401,050 describes a visual interaction sys-<br>
tem for a shipboard watch station. Here, an eye-tracking camera<br>
monitors an operator's visual scan, gaze location, dwell time,<br>
blink rate and pupil size to determine whether additional cueing<br>
of the operator should be made to direct the operator's attention<br>
to an important object on the screen.<br>
The document US, 5,649,061 discloses a device for estimating a<br>
mental decision to select a visual cue from a viewer's eye<br>
fixation and corresponding event evoked cerebral potential. An<br>
eye tracker registers a viewing direction, and based thereon<br>
fixation properties may be determined in terms of duration, start<br>
and end pupil sizes, saccades and blinks. A corresponding<br><br>
single event evoked cerebral potential is extracted, and an<br>
artificial neural network estimates a selection interest in the<br>
gaze point of regard. After training the artificial neural network,<br>
the device may then be used to control a computer, such that<br>
icons on a display are activated according to a user's estimated<br>
intentions without requiring any manipulation by means of the<br>
user's hands.<br>
A few attempts have also been made to abstract user-generated<br>
input data into high-level information for controlling a computer.<br>
For example, the document US 2004/0001100 describes a<br>
multimode user interface, where a flexible processing of a user<br>
input is made possible without having to switch manually<br>
between different input modes. Instead, within the data streams<br>
different information categories are distinguished depending on<br>
a context in which the data streams are generated.<br>
Although this strategy may indeed enhance the efficiency of the<br>
man-machine interaction, no multimodal solution has yet been<br>
presented according to which eye-tracking data is processed<br>
optimally. On the contrary, with only very few exceptions, to-<br>
day's eye tracking interfaces are each tailored for one specific<br>
task only. Thus, any processing of eye tracking data in respect<br>
of a first application cannot be reused by a second application,<br>
and vice versa. Hence, if multiple eye-controlled applications<br>
are used in a single computer, one particular eye-tracking data<br>
processing unit is typically required for each application. Natu-<br>
rally, in such a case, there is a high risk that the different appli-<br>
cations perform a substantial amount of overlapping eye-trac-<br>
king data processing. Moreover, each designer of eye-control-<br>
lable applications needs to have expertise in both eye tracking<br>
technology and the interpretation of eye-tracking data in order to<br>
extract the data required by the application.<br>
SUMMARY OF THE INVENTION<br>
The object of the present invention is therefore to provide a ho-<br><br>
listic means of controlling a computer apparatus based on a<br>
user's ocular activity, which alleviates the above problems and<br>
thus offers an efficient man-machine interaction with a minimal<br>
amount of double processing, i.e. wherein as much as possible<br>
of the eye-tracking data processing is performed centrally in<br>
respect of GUI-components that may belong to two or more se-<br>
parate applications.<br>
According to one aspect of the invention, the object is achieved<br>
by the arrangement as initially described, wherein the event<br>
engine is adapted to receive a control signal request from each<br>
of the at least one GUI-component. The control signal request<br>
defines a sub-set of the set of non-cursor controlling event<br>
output signals, which is required by the GUI-component in<br>
question. The event engine is also adapted to deliver non-cursor<br>
controlling event output signals to the at least one GUI-compo-<br>
nent in accordance with each respective control signal request.<br>
This arrangement is advantageous because thereby a very<br>
flexible interface is attained towards any applications which are<br>
controllable by means of eye tracking signals. This, in turn,<br>
enables software developers without an in-depth knowledge of<br>
eye-tracking technology to design eye-controllable applications.<br>
Therefore, the invention is believed to stimulate the develop-<br>
ment of new such applications, and consequently render further<br>
improvements of the human computer interaction possible.<br>
Moreover, the computer's processing resources are freed for<br>
alternative purposes, since a high-level eye-tracking data signal<br>
derived in respect of one application may be reused by one or<br>
more additional applications. In other words, the event engine is<br>
adapted to perform a centralized eye-tracking data processing<br>
for GUI-components (or potential "subscribers") on-demand.<br>
Thus, each of these components and their requested control<br>
signals need not be explicitly known at the point in time when<br>
the event engine is implemented. Instead, according to the<br>
invention, one or more GUI-components may request relevant<br>
control signals in connection with the implementation of a later<br><br>
added eye-controllable application. Of course, the opposite may<br>
also be true, namely that a set of control signal requests is<br>
presented once and for all at start-up of the proposed arrange-<br>
ment.<br>
According to a preferred embodiment of this aspect of the inven-<br>
tion, the computer apparatus is also adapted to receive a cursor<br>
control signal, and in response to the cursor control signal,<br>
control a graphical pointer on the display. Thus, for instance, the<br>
event engine and the cursor control signal may interact jointly<br>
with GUI-components represented on the display, such that a<br>
very intuitive man-machine interaction is accomplished.<br>
According to another preferred embodiment of this aspect of the<br>
invention, at least one GUI-component is adapted to generate at<br>
least one respective output control signal upon a user mani-<br>
pulation of the component. This means that by manipulating a<br>
GUI-component, the user may cause the computer apparatus to<br>
generate outgoing signals to one or more external units, such as<br>
a printer, a camera etc. Naturally, this is a highly desirable<br>
feature. Moreover, the fact that one or more GUI-components<br>
may generate an output control signal does not preclude that<br>
with respect to one or more other GUI-components, the non-<br>
cursor controlling event output signals may exclusively affect the<br>
component internally.<br>
According to a preferred embodiment of this aspect of the inven-<br>
tion, the event engine is adapted to produce at least a first<br>
signal of the non-cursor controlling event output signals based<br>
on a dynamic development of the eye-tracking data signal.<br>
Thereby, a time parameter of the user's ocular activity may be<br>
used to control functions and processes of the computer appa-<br>
ratus. For instance, the time parameter may reflect a dwell time<br>
of the user's gaze within a particular region on the display,<br>
identify a certain gaze pattern etc. Many types of advanced eye-<br>
controllable functions can thereby be realized.<br><br>
According to yet another preferred embodiment of this aspect of<br>
the invention, at least one GUI-component is adapted to<br>
interpret a non-cursor controlling event output signal as a rep-<br>
resentation of the user's intention. In response to this estimated<br>
intention, a user manipulation of the component is triggered. For<br>
example this may involve activating one or more computer<br>
functions based on a command history. This is advantageous<br>
because thereby the command input procedure may be simpli-<br>
fied.<br>
According to still another preferred embodiment of this aspect of<br>
the invention, at least one GUI-component is adapted to<br>
interpret a non-cursor controlling event output signal as an esti-<br>
mated attention level of the user. In response to this estimated<br>
attention level, a user manipulation of the component is<br>
triggered. One advantage attainable thereby is that the com-<br>
puter's behavior may be adapted to match a current attention<br>
level of the user.<br>
According to yet another preferred embodiment of this aspect of<br>
the invention, at least one GUI-component is adapted to<br>
interpret a non-cursor controlling event signal as a state-of-mind<br>
parameter of the user, and in response thereto trigger a user<br>
manipulation of the component. This feature is desirable<br>
because it allows the computer to behave differently depending<br>
on whether the user appears to be focused/concentrated,<br>
distracted, tired/unfocused or confused etc.<br>
According to still another preferred embodiment of this aspect of<br>
the invention, the event engine is adapted to receive at least<br>
one auxiliary input signal, such as a signal from a button or a<br>
switch, a speech signal, a movement pattern of an input mem-<br>
ber, a camera registered gesture pattern or facial expression, or<br>
an EEG (electroencephalogram)-signal. On the further basis of<br>
this auxiliary signal the event engine produces the set of non-<br>
cursor controlling event output signals. Thereby, a highly<br>
efficient combination of user input signals may be used to<br><br>
control the computer. Generally, these combinations can be<br>
made very intuitive and easy to learn.<br>
According to another aspect of the invention the object is<br>
achieved by the method as initially described, wherein a control<br>
signal request is received from each of the at least one GUI-<br>
component. The control signal request defines a sub-set of the<br>
set of non-cursor controlling event output signals, which is<br>
required by the particular GUI-component. Non-cursor con-<br>
trolling event output signals are then delivered to the at least<br>
one GUI-component in accordance with each respective control<br>
signal request.<br>
The advantages of this method, as well as the preferred embodi-<br>
ments thereof, are apparent from the discussion hereinabove<br>
with reference to the proposed arrangement.<br>
According to a further aspect of the invention the object is<br>
achieved by a computer program, which is directly loadable into<br>
the internal memory of a computer, and includes software for<br>
controlling the above proposed method when said program is<br>
run on a computer.<br>
According to another aspect of the invention the object is<br>
achieved by a computer readable medium, having a program<br>
recorded thereon, where the program is to control a computer to<br>
perform the above proposed method.<br>
The invention dramatically increases the available bandwidth for<br>
transferring information from a user to a computer apparatus,<br>
i.e. essentially generating commands, however not necessarily<br>
perceived as such by the user. Therefore, this increase of the<br>
bandwidth places no additional cognitive workload on the user.<br>
On the contrary, by means of the invention, the cognitive<br>
workload may, in fact, be reduced. At the same time, the<br>
increased bandwidth vouches for an improved efficiency of the<br>
man-machine interaction.<br><br>
Moreover, by means of the invention, commands which traditio-<br>
nally have required hand and/or finger manipulations, can be<br>
efficiently and effortlessly effected based on the user's eye<br>
activity. Naturally, this is desirable in a broad range of appli-<br>
cations, from disabled computer users, support operators in a<br>
call-center environment (e.g. when entering/editing data in a<br>
customer relationship management application), users of<br>
advanced computer aided design (CAD) tools, surgeons, to<br>
drivers and pilots who, for various reasons, cannot effectively<br>
produce hand- and/or finger-based commands. Even in<br>
situations where the users have their hands and fingers readily<br>
accessible, the invention may be useful to improve the ergono-<br>
mics and the reduce risk of e.g. repetitive strain injuries.<br>
Alternatively, the environment in which the computer apparatus<br>
is placed may be so clean or dirty that either the environment<br>
has to be protected from possible emissions from the computer<br>
apparatus, or reverse this apparatus must be protected against<br>
hazardous substances in the environment and therefore has to<br>
be encapsulated to such a degree that a traditional entry of<br>
input of commands is made impossible, or at least impractical.<br>
The invention offers an excellent basis for developing new<br>
software and computer applications, which are controllable by<br>
the eyes of a user. Thereby, in the long term, the invention<br>
vouches for a deep and unconstrained integration of eye inter-<br>
action applications into the standard computer environment.<br>
Further advantages, advantageous features and applications of<br>
the present invention will be apparent from the following des-<br>
cription and the dependent claims.<br>
BRIEF DESCRIPTION OF THE DRAWINGS<br>
The present invention is now to be explained more closely by<br>
means - of preferred embodiments, which are disblosed as<br>
examples, and with reference to the attached drawings.<br><br><br>
DESCRIPTION OF PREFERRED EMBODIMENTS OF THE<br>
INVENTION<br>
Figure 1 shows an overview picture of a typical: use-case<br>
according to the invention. Here, a user 110 controls a computer<br>
apparatus 130, at least partly based on an eye-tracking data<br>
signal DEYE, which describes the user's 110 point of regard x, y<br>
on a display 120. Thus, by watching a representation of a GUI-<br>
component 220 on the display 120, the user 110 may generate<br>
commands to the computer apparatus 130. This manipulation is<br>
enabled, since the GUI-component 220 is adapted to be, at leasl<br>
indirectly, influenced by the eye-tracking data signal,DEYE. The<br>
invention presumes that the eye-tracking data signal DEYE may<br>
result in events, related to any task performable by the computer<br>
apparatus, apart from affecting a cursor/pointer on the display<br>
120. It should be noted that according to the invention, any type<br>
of known computer screen or monitor, as well as combinations<br>
of two or more separate displays may represent the display 120.<br>
For example, the display 120 may constitute a pair of<br>
stereoscopic screens, a heads-up display (HUD), a head-<br>
mounted display (HMD) and a presentation means for a virtual<br>
environment, such as the eyepieces of a pair of 3D-glasses or a<br>
room where the walls include projection screens for presenting a<br>
virtual environment.<br>
Naturally, in order to produce the eye-tracking data signal DEYE,<br>
the display 120 is associated with, or includes, an eye-tracker.<br>
This unit is not a subject of the present patent application, and<br>
therefore will not be described in detail here. However, the eye-<br>
tracker is preferably embodied by the solution described in the<br>
Swedish patent application 0203457-7, filed on 21 November<br>
2002 in the name of the applicant.<br>
Preferably, a graphics control signal C-GR is generated by the<br>
computer apparatus 130 to accomplish visual feedback infor-<br>
mation on the display 120. The visual feedback information is<br>
generated in response to any user-commands received by the<br>
computer apparatus 130, so as to confirm to the user 110 any<br><br>
commands, which are based on the eye-tracking data signal<br>
DEYE- Such a confirmation is especially desirable when the com-<br>
mands are produced by means of perceptive organs; for exam-<br>
ple the human eyes.<br>
According to a preferred embodiment of the invention, besides<br>
the eye-tracking data signal DEYE. the computer apparatus 130<br>
is adapted to receive a cursor control signal K, which controls<br>
the position of a graphical pointer on the display 120. Of course,<br>
the graphics control signal C-GR may be based also on the<br>
cursor control signal K.<br>
Figure 2 shows an arrangement according to an embodiment of<br>
the invention, which may be realized by means of the computer<br>
apparatus 130 described above with reference to the figure 1.<br>
The arrangement includes an event engine 210 and at least one<br>
GUI-component 220, which is adapted to be manipulated based<br>
on user-generated commands, at least partly expressed by the<br>
eye-tracking data signal DEYE- The event engine 210 is adapted<br>
to receive the eye-tracking data signal DEYE. and based thereon<br>
produce a set of non-cursor controlling event output signals D-<br>
Hlj that influence the at least one GUI-component 220. Each<br>
non-cursor controlling event output signal D-Hlj, in turn, descri-<br>
bes a particular aspect of the user's 110 ocular activity in<br>
respect of the display 120.<br>
For example, a first signal may indicate whether the user's 110<br>
gaze is directed towards the display at all (i.e. a "gaze-on-<br>
display" signal), a second non-cursor controlling event output<br>
signal may reflect a dwell time of the user's 110 gaze within a-<br>
certain area on the display 120, a third signal may designate a<br>
gaze fixation (at a specific point), a fourth signal may indicate<br>
whether the gaze saccades, a fifth signal may indicate whether<br>
the gaze follows a smooth path, a sixth signal may reflect that<br>
the user 110 reads a text, and a seventh signal may be triggered<br>
if the user 110 appears to be distracted, based on the particular<br><br>
eye-tracking data signal DEYE that he/she produces.<br>
According to the invention, the event engine 210 receives a<br>
respective control signal request Ra, ..., Rn from each of the at<br>
least one GUI-component 220a, ..., 220n. The control signal<br>
request, say Ra, defines a sub-set of the set of non-cursor con-<br>
trolling event output signals D-Hli which are required by the par-<br>
ticular GUI-component, say 220a, to operate as intended. The<br>
event engine 210 then delivers non-cursor controlling event<br>
output signals D-Hli to each of the at least one GUI-component<br>
220a, ..., 220n in accordance with the respective control signal<br>
request Ra, ..., Rn.<br>
A most efficient processing is accomplished if the event engine<br>
210 exclusively produces those event output signals D-Hli which<br>
are actually requested by at least one GUI-component.<br>
However, according to the invention, it is also conceivable that<br>
all non-cursor controlling event output signals D-Hli that are<br>
possible to produce are always generated by the event engine<br>
210, irrespective of whether a corresponding control signal<br>
request has been received or not. Namely, this simplifies the<br>
procedure, and depending on the application, this strategy may<br>
not require an overly extensive processing.<br>
According to a preferred embodiment of the invention, each<br>
GUI-component 220a, .... 220n is adapted to generate at least<br>
one respective output control signal Ca, ..., Ca upon a user<br>
manipulation of the component 220a, ..., 220n. Thus, in addition<br>
to generating the above-mentioned visual feedback information,<br>
one or more internal or peripheral devices may be influenced by<br>
means of the output control signals Ca, ..., Ca. For example a<br>
print job may be initiated, a computing task may be executed, an<br>
e-mail may be sent out, a camera may be triggered to take a<br>
picture etc.<br>
As mentioned above, the non-cursor controlling event output<br>
signals D-HIi may describe many different aspects of the eye-<br><br>
tracking data signal DEYE- According to one embodiment of the<br>
invention, at least one output signal D-Hli is based on a dynamic<br>
development of the eye-tracking data signal DEYE- Thereby, the<br>
signal can represent a particular gaze pattern over the display<br>
120. The gaze pattern, in turn, may be determined, for instance<br>
to constitute saccades, a smooth pursuit, periods of fixation or<br>
reading.<br>
A non-cursor controlling event output signal D-Hli may also<br>
indicate gaze-enter/gaze-leave data. This data is a parameter<br>
which reflects the time instances when the eye-tracking data<br>
signal DEYE indicates that the user's point of regard falls within a<br>
GUI component's representation on the display. Hence, the<br>
gaze-enter data is generated when the user's gaze falls onto a<br>
GUI component's representation on the display, and the gaze-<br>
leave data is generated when the gaze is directed outside this<br>
representation.<br>
The above-mentioned dwell time for a GUI-component is typi-<br>
cally defined as the period between the gaze-enter data and the<br>
gaze-leave data with respect to a particular GUI component. It is<br>
normally preferable to link an activation signal to the dwell time,<br>
such that for instance an "eye-button" is activated when a<br>
certain dwell time is reached. Alternatively, a button, a switch, a<br>
speech signal, a movement pattern of an input member, a<br>
camera registered gesture pattern or facial expression may<br>
constitute the activation signal. Moreover, an eye blink or a pre-<br>
defined EEG-signal may cause the activation signal. However,<br>
the latter types of signals are usually relatively difficult for the<br>
user to control with a sufficient accuracy.<br>
According to one embodiment of the invention, the event engine<br>
210 is adapted to receive at least one auxiliary input signal DJ,<br>
and produce the set of non-cursor controlling event output<br>
signals D-Hli on the further basis of this signal. The auxiliary<br>
input signal DJ may originate from a button, a switch, a speech<br>
signal, a movement pattern of an input member, a camera<br><br>
registered gesture pattern or a facial expression, or an EEG-<br>
signal.<br>
Thereby, based on combinations of the eye-tracking data signal<br>
DEYE and one or more auxiliary input signals DJ composite user<br>
commands may be created, which are very intuitive and easy to<br>
learn. Hence, a highly efficient man-machine interaction with the<br>
computer apparatus 130 may be accomplished. For instance,<br>
watching an eye-button for the document and uttering the<br>
control word "open" can open a text document. If a more<br>
complex speech recognition is available, an Internet search may<br>
be effected by focusing the gaze towards a relevant text input<br>
box while uttering the desired search terms, and so on.<br>
According to one embodiment of the invention, at least one GUI-<br>
component 220a, ..., 220n is adapted to'interpret a non-cursor<br>
controlling event output signal D-Hli from the event engine 210<br>
as an estimated intention of the user 110. Then, in response to<br>
the estimated intention, a user manipulation of the component<br>
220a, ..., 220n is triggered. Preferably, the event engine 210<br>
estimates a user intention based on multiple input sources<br>
received as auxiliary input signals DJ, which may include a key-<br>
board signal, a mouse signal, voice data and camera images.<br>
However, naturally, the eye-tracking data signal DEYE may also<br>
constitute a basis for the estimated user intention. For example,<br>
important information can be drawn from different gaze patterns<br>
and fixation times.<br>
According to another embodiment of the invention, at least one<br>
GUI-component 220a, ..., 220n is adapted to interpret a non-<br>
cursor controlling event output signal D-Hli from the event<br>
engine 210 as an estimated attention level of the user 110.<br>
Correspondingly, a user manipulation of the component 220a,<br>
..., 220n is triggered in response to the estimated attention<br>
level. Also the attention level may be estimated based on the<br>
auxiliary input signals DJ, for instance originating from the<br>
keyboard, the mouse, voice data and camera images, and the<br><br>
eye-tracking data signal DEYE- Particularly, gaze patterns,<br>
fixation points and fixation times constitute an important basis<br>
for determining the user's 110 attention level. Preferably, the<br>
GUI-components 220a, ..., 220n vary their behavior depending<br>
on the estimated attention level, such that the components'<br>
characteristics match the user's 110 current performance.<br>
Furthermore, according to one embodiment of the invention, at<br>
least one GUI-component 220a, .... 220n is adapted to interpret<br>
a non-cursor controlling event output signal D-Hli from the event<br>
engine 210 as a state-of-mind parameter of the user 110. The<br>
state-of-mind parameter reflects a general user 110 status, for<br>
example whether he/she appears to be focused/concentrated,<br>
distracted, tired/unfocused or confused. For example, the state-<br>
of-mind parameter may indicate an approximated 20% degree of<br>
tiredness and an approximated 50% degree of attention. Then,<br>
based on the estimated state-of-mind, a user manipulation of the<br>
component 220a, ..., 220n is triggered. Typically, the number of<br>
and the contents of any help menus and pop-up windows may<br>
be adapted in response to the estimated state-of-mind. How-<br>
ever, in operator environments, such as radar watch stations where the attention level as well as the state-of-mind may be<br>
truly critical, the security may be improved by pointing out<br>
targets etc. that the operator has not yet observed.<br>
According to one embodiment of the invention, the event engine<br>
210 is associated with a template library 230, which contains<br>
generic GUI-components, such as eye-buttons, scroll bars,<br>
multiview toolbars (see below with reference to figure 4), text<br>
input fields (see below with reference to figure 5), expandable<br>
text input fields (see below with reference to figure 6) and scroll<br>
windows (see below with reference to figure 7). Thereby, by<br>
means of the template library 230, a software designer may<br>
conveniently create functions and controls which can be<br>
manipulated based on a user's ocular activity in respect of a<br>
display. Of course, after completing the design of a particular<br>
eye-controllable application, however, the template library 230<br><br>
has no actual function in respect of the GUI-components that<br>
may originate from its generic components. Nevertheless, the<br>
template library 230 may again become useful in case of a<br>
future upgrade or a redesign of the application.<br>
According to the invention, it is not technically necessary that<br>
any GUI-component manipulations be graphically confirmed on<br>
the display. However, this is generally preferable from a user-<br>
friendliness point-of-view. Figure 3a shows a schematic symbol<br>
310, which represents an eye-controllable GUI-component on a<br>
display that is set in a non-observed mode (i.e. the eye-tracking<br>
data signal DEYE indicates that the user's point of regard lies<br>
outside the symbol 310). Figure 3b shows the symbol 310 in an<br>
observed mode, which is, set when the eye-tracking data signal<br>
DEYE indicates that the user's point of regard falls within the<br>
display area represented by the symbol 310. In the observed<br>
mode, the symbol 310 contains a centrally located object 311.<br>
This object 311 confirms to the user that the computer<br>
apparatus has registered that his/her gaze presently is directed<br>
towards the symbol 310. Thus, any manipulations in respect of<br>
the GUI-component associated with the symbol 310 can be<br>
performed. An important advantage attained by means of the<br>
centrally located object 311 is that this object assists the user in<br>
focusing his/her gaze to the center of the symbol 310. Thereby,<br>
a more reliable eye-tracking function is accomplished, and for a<br>
given eye-tracker, the symbols 310 can be made smaller than<br>
otherwise. Of course, the symbol 310 and the centrally located<br>
object 311 may have any other outline than the square repre-<br>
sentation of figures 3a and 3b. Moreover, to further improve the<br>
visual cueing, the object 311 may be animated and or have a<br>
particularly interesting color or shape.<br>
Figure 4 illustrates a first embodiment according to the inven-<br>
tion, where a proposed multiview toolbar 401 is used to control<br>
applications in a frame 400. The multiview toolbar 401 here<br>
includes four different eye-buttons, which each may contain a<br>
thumbnail image (not shown) of a respective application with<br><br><br>
instance as shown in the figure 6b. Hence, more information<br>
than what was initially visible in the field 620 may be shown.<br>
The thus expanded text field 620 may even cover graphical<br>
objects which otherwise are shown in the frame 400. This is ad-<br>
vantageous, because thereby the information may be presented<br>
on-demand, so that the frame 400 and the sub-frame 520<br>
contain more data than what actually can be fitted therein. For<br>
instance, a text scrolling, which otherwise would have been<br>
necessary can be avoided.<br>
Figure 7 illustrates a fourth embodiment according to the<br>
invention, which realizes a scrolling function based on a user's<br>
ocular activity. Here, a third application, e.g. a map viewer, is<br>
associated with a third button 710. Thus, by watching the third<br>
button 710, and activating its associated GUI-component, either<br>
based on a gaze dwell time or by means of a separate activation<br>
signal, the computer apparatus opens up a map sub-frame 720<br>
within the frame 400. This sub-frame, in turn, presents a digi-<br>
tized map. It is here presumed that the map is larger than the<br>
available presentation area in the map sub-frame 720, so that<br>
only a part of the map can be presented at the same time.<br>
A scrolling with- respect to the map is achieved based on the<br>
user's point or regard. Preferably, no scrolling occurs as long as<br>
the eye-tracking data signal indicates that the point of regard<br>
lies within a central area delimited by a first dashed line a, a<br>
second dashed line b, a third dashed line c and a fourth dashed<br>
line d, neither of which preferably are visible on the display. If<br>
however, the user's point of regard is placed outside any of the<br>
lines a, b, c or d, the map scrolls in a particular direction given<br>
by the point of regard. Specifically, this means that a point of<br>
regard below the line c results in a downward scroll along the<br>
arrow S; a point of regard above the line d results in a upward<br>
scroll along the arrow N; a point of regard to the right of the line<br>
b results in a rightward scroll along the arrow E; and a point of<br>
regard to the left of the line a results in a leftward scroll along<br>
the arrow W. Moreover, a point of regard, which lies below the<br><br>
line c and to the right of the line b results in a diagonal scroll<br>
along the arrow SE; a point of regard, which lies above the line<br>
d and to the right of the line b results in a diagonal scroll along<br>
the arrow NE; a point of regard, which lies above the line d and<br>
to the left of the line a results in a diagonal scroll along the<br>
arrow NW; and a point of regard, which lies below the line c and<br>
to the left of the line a results in a diagonal scroll along the<br>
arrow SW. This scroll function may either be activated based on<br>
a dwell time, or a separate activation signal may be required,<br>
such as a clicking a key/button or holding down a key/button.<br>
Furthermore, the scroll speed may depend on the distance<br>
between the point of regard and the respective lines a, b, c and<br>
d, such that a relatively long distance corresponds to a compa-<br>
ratively high speed, and vice versa. The scroll speed may also<br>
depend on the scroll time, and/or a length of the latest saccades<br>
registered in the eye-tracking data signal. Preferably, a maxi-<br>
mum scroll speed is set to such a value that the scrolled infor-<br>
mation is visible to the user at all possible speeds.<br>
According to one embodiment of the invention, the scroll<br>
function is stopped by pressing a key/button, releasing a<br>
key/button, the length of the latest saccades exceeding a<br>
particular value, the point of regard moving outside the map<br>
sub-frame 720, the point of regard moving towards the center of,<br>
the map sub-frame 720, or towards an opposite scroll activation<br>
line a, b, c, or d.<br>
It should be noted that, eyetrack-driven scrolling solutions as<br>
such are described in the prior art, for instance in the document<br>
US, 5,850,221. Here, a-page oriented or continuous information<br>
scrolling function is initiated or controlled based on where a<br>
viewer's eyes are looking.<br>
To sum up, the general method of controlling a computer appa-<br>
ratus according to the invention will now be described with<br>
reference to the flow diagram in figure 8.<br><br>
An initial step 810 receives a control signal request from each of<br>
at least one GUI-component, which is adapted to be manipu-<br>
lated based on user-generated, eye commands. The control<br>
signal request defines a sub-set of the set of non-cursor con-<br>
trolling event output signals, which is required by the particular<br>
GUI-component in order to operate as intended.<br>
A step 820 then receives an eye-tracking data signal, which des-<br>
cribes a user's point of regard on a display associated with a<br>
computer that embodies the at least one GUI-component.<br>
Subsequently, a step 830 produces a set of non-cursor con-<br>
trolling event output signals based on the eye-tracking data<br>
signal, plus any auxiliary input signal.<br>
Subsequently, a step 840 delivers the non-cursor controlling<br>
event output signals to the at least one GUI-component so that a<br>
relevant influence thereof may occur (i.e. according to each<br>
respective control signal request). It is presumed that each non-<br>
cursor controlling event output signal describes a particular<br>
aspect of the user's ocular activity in respect of the display.<br>
Thus, the non-cursor controlling event output signals express<br>
user-generated eye commands. After the step .840, the pro-<br>
cedure loops back to the step 820 for receiving an updated eye<br>
tracking data signal.<br>
All of the process steps, as well as any sub-sequence of steps,<br>
described with reference to the figure 8 above may be controlled<br>
by means of a programmed computer apparatus. Moreover,<br>
although the embodiments of the invention described above with<br>
reference to the drawings comprise computer apparatus and<br>
processes performed in computer apparatus, the invention thus<br>
also extends to computer programs, particularly computer<br>
programs on or in a carrier, adapted for putting the invention<br>
into practice. The program may be in the form of source code,<br>
object code, a code intermediate source and object code such<br>
as in partially compiled form, or in any other form suitable for<br>
use in the implementation of the process according to the<br><br>
invention. The program may either be a part of an operating<br>
system, or be a separate application. The carrier may be any<br>
entity or device capable of carrying the program. For example,<br>
the carrier may comprise a storage medium, such as a Flash<br>
memory, a ROM (Read Only Memory), for example a CD<br>
(Compact Disc) or a semiconductor ROM, an EPROM (Erasable<br>
Programmable Read-Only Memory), an EEPROM (Electrically<br>
Erasable Programmable Read-Only Memory), or a magnetic recording medium, for example a floppy disc or hard disc.<br>
Further, the carrier may be a transmissible carrier such as an<br>
electrical or optical signal which may be conveyed via electrical<br>
or optical cable or by radio or by other means. When the<br>
program is embodied in a signal which may be conveyed directly<br>
by a cable or other device or means, the carrier may be<br>
constituted by such cable or device or means. Alternatively, the<br>
carrier may be an integrated circuit in which the program is<br>
embedded, the integrated circuit being adapted for performing,<br>
or for use in the performance of, the relevant processes.<br>
The term "comprises/comprising" when used in this specification<br>
is taken to specify the presence of stated features, integers,<br>
steps or components. However, the term does not preclude the<br>
presence or addition of one or more additional features,<br>
integers, steps or components or groups thereof.<br>
The invention is not restricted to the described embodiments in<br>
the figures, but may be varied freely within the scope of the<br>
claims.<br><br>
We Claim:<br>
1. An arrangement for controlling a computer apparatus (130) associated<br>
with a graphical display (120), the display (120) presenting a representation<br>
of at least one GUI-component (220) which is adapted to be manipulated<br>
based on user-generated commands, and at least one of the at least one<br>
GUI-component (220a,... 220n) is adapted to generate at least one respective<br>
output control signal (Ca ...Ca) upon a user manipulation of the component<br>
(220a,... 220n), the arrangement comprising an event engine (210) adapted<br>
to receive an eye-tracking data signal (DEYE) describing a user's (110) point of<br>
regard (x, y) on the display (120), and at least based on the eye-tracking<br>
data signal (DEYE) produce a set of non-cursor controlling event output signals<br>
(D-HIi) influencing the at least one GUI-component (220), each non-cursor<br>
controlling event output signal (D-HIi) describing a particular aspect of the<br>
user's (110) ocular activity in respect of the display (120), characterized in<br>
that the event engine (210) is adapted to:<br>
receive a control signal request (Ra, ..., Rn) from each of the at least one GUI-<br>
component (220a, ..., 220n), the control signal request (Ra, ..., Rn) defining a<br>
sub-set of the set of non-cursor controlling event output signals (D-HIi) which<br>
is required by the GUI-component (220a, ..., 220n) to operate as intended to<br><br>
produce the event output signals (D-HIi) which are requested by the at least<br>
one GUI-component (220a,...,220n) in the control signal request (Ra,...,Rn),<br>
and<br>
deliver non-cursor controlling event output signals (D-HIi) to the at least one<br>
GUI-component (220a, ..., 220n) in accordance with each respective control<br>
signal request (Ra, ..., Rn).<br>
2.	An arrangement as claimed in claim 1, wherein the computer apparatus<br>
(130) is adapted to:<br>
receive a cursor control signal (K), and<br>
control a graphical pointer on the display (120) in response to the cursor<br>
control signal (K).<br>
3.	An arrangement as claimed in any one of the preceding claims, wherein<br>
the event engine (210) is adapted to produce at least a first signal of the<br>
non-cursor controlling event output signals (D-HIi) based on a dynamic<br>
development of the eye-tracking data signal (DEYE).<br>
4.	An arrangement as claimed in claim 3, wherein the first signal represents<br>
a particular gaze pattern over the display (120).<br><br>
5.	An arrangement as claimed in claim 4, wherein at least one GUI-<br>
component (220a, ..., 220n) is adapted to interpret the first signal as an<br>
estimated intention of the user (110), and trigger a user manipulation of<br>
the component (220a,..., 220n) in response to the estimated intention.<br>
6.	An arrangement as claimed in any one of the claims 4 or 5, wherein at<br>
least one GUI-component (220a, ..., 220n) is adapted to interpret the first<br>
signal as an estimated attention level of the user (110), and trigger a user<br>
manipulation of the component (220a, ..., 220n) in response to the<br>
estimated attention level.<br>
7.	An arrangement as claimed in any one of the claims 4 to 6, wherein at<br>
least one GUI-component (220a, ..., 220n) is adapted to interpret the first<br>
signal as a state-of-mind parameter of the user (110), and trigger a user<br>
manipulation of the component (220a, ..., 220n) in response to the state-<br>
of-mind parameter.<br>
8.	An arrangement as claimed in any one of the preceding claims, wherein<br>
the event engine (210) is adapted to receive at least one auxiliary input<br>
signal (DJ) and produce the set of non-cursor controlling event output<br>
signals (D-HIi) on the further basis of the at least one auxiliary input<br>
signal (DJ).<br><br>
9.	An arrangement as claimed in any one of the preceding claims, wherein<br>
the at least one auxiliary input signal (DJ) is originated from at least one<br>
of a button, a switch, a speech signal, a movement pattern of an input<br>
member, a gesture pattern, a facial expression and an EEG-signal.<br>
10.	A method of controlling a computer apparatus (130) associated with<br>
graphical display (120), the display (120) representing at least one GUI-<br>
component (220) which is adapted to be manipulated based on user-<br>
generated commands, the method comprising:<br>
receiving an eye-tracking data signal (DEYE) which describes a user's (110)<br>
point of regard (x, y) on the display (120), and<br>
producing a set of non-cursor controlling event output signals (D-HIi) at least<br>
based on the eye-tracking data signal (DEYE), the set of non-cursor controlling<br>
event output signals (D-HIi) influencing the at least one GUI-component<br>
(220), and each non-cursor controlling event output signal (D-HIi) describing<br>
a particular aspect of the user's (110) ocular activity in respect of the display<br>
(120), the method characterized by<br>
receiving a control signal request (Ra, ... , Rn) from each of the at least one<br>
GUI-component (220a, ..., 220n), the control signal request (Ra, ..., Rn)<br>
defining a sub-set of the set of non-cursor controlling event output signals<br>
(D-HIi) which is required by the GUI-component (220a, ..., 220n) to operate<br>
as intended<br><br>
producing the event output signals (D-HIi) which are requested by the at<br>
least one GUI-component (220a,...,220n) in the control signal request<br>
(Ra,...,Rn), and<br>
delivering non-cursor controlling event output signals (D-HIi) to the at least<br>
one GUI-component (220a, ..., 220n) in accordance with each respective<br>
control signal request (Ra, ..., Rn).<br>
11.	A method as claimed in claim 10, comprising:<br>
receiving a cursor control signal (K), and<br>
controlling a graphical pointer on the display (120) in response to the cursor<br>
control signal (K).<br>
12.	A method as claimed in any one of the claims 10 or 11, comprising<br>
producing at least a first signal of the non-cursor controlling event output<br>
signals (D-HIi) based on a dynamic development of the eye-tracking data<br>
signal (DEYE).<br>
13.	A method as claimed in claim 12, comprising the first signal representing a<br>
particular gaze pattern over the display (120).<br>
14.	A method as claimed in claim 13, comprising at least one GUI-component<br>
(220a, ..., 220n) being adapted to interpret the first signal as an<br>
estimated intention of the user (110), and trigger a user manipulation of<br>
the component (220a,..., 220n) in response to the estimated intention.<br><br>
15.	A method as claimed in any one of the claims 13 or 14, comprising at<br>
least one GUI-component (220a, ..., 220n) being adapted to interpret the<br>
first signal as an estimated attention level of the user (110), and trigger a<br>
user manipulation of the component (220a, ..., 220n) in response to the<br>
estimated attention level.<br>
16.	A method as claimed in any one of the claims 13 to 15,comprising at least<br>
one GUI-component (220a, ... , 220n) being adapted to interpret the first<br>
signal as a state-of-mind parameter of the user (110), and trigger a user<br>
manipulation of the component (220a, ..., 220n) in response to the state-<br>
of-mind parameter.<br>
17.	A method as claimed in any one of the claims 11 to 18,<br>
comprising:<br>
receiving at least one auxiliary input signal (DJ), and<br>
producing the set of non-cursor controlling event output signals (D-HIi) on<br>
the further basis of the at least one auxiliary input signal (DJ).<br><br>
18. A method as claimed in any one of the claims 10 to 17,comprising the at<br>
least one auxiliary input signal (DJ) being originated from at least one of a<br>
button, a switch, a speech signal, a movement pattern of an input<br>
member, a gesture pattern, a facial expression and an EEG-signal.<br><br><br><br>
(57) Abstract: The present invention relates to a computer based eye-tracking solution. A computer apparatus is associated with<br>
a graphical display, which in turn, represents at least one GUI-component (220a	220n) that is adapted to be manipulated based<br>
on user-generated commands. An event engine (210) is adapted to receive an eye-tracking data signal (DEYE) that describes a user's<br>
point of regard on the display. Based on the signal (DEYE), the event engine (210) is adapted to produce a set of non-cursor controlling<br>
event output signals (D-HIi), which influence the at least one GUI-component (220a, ..., 220n). Each non-cursor controlling event<br>
output signal (D-HIi) describes a particular aspect of the user's ocular activity in respect of the display. Initially, the proposed event<br>
engine (210) receives a control signal request (Ra,... , Rn) from each of the at least one GUI-component (220a,..., 220n). The control<br>
signal request (Ra, ... , Rn) defines a sub-set of the set of non-cursor controlling event output signals (D-HIi) which is required by the<br>
particular GUI-component (220a, .... 220n). The event engine (210) delivers non-cursor controlling event output signals (D-HIi) to <br>
the at least one GUI-component (220a,..., 220n) in accordance with each respective control signal request (Ra , 	Rn).	</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBhYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBjbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBjb3JyZXNwb25kZW5jZSBvdGhlcnMucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 correspondence others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBkZXNjcmlwdGlvbihjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBkcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBmb3JtLTEucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBmb3JtLTIucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBmb3JtLTMucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBmb3JtLTUucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBpbnRlcm5hdGlvbmFsIHB1YmxpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 international publication.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNiBpbnRlcm5hdGlvbmFsIHNlYXJjaCBhdXRob3JpdHkgcmVwb3J0LnBkZg==" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006 international search authority report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNi1jb3JyZXNwb25kZW5jZSBvdGhlcnMtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006-correspondence others-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNi1jb3JyZXNwb25kZW5jZS0xLjIucGRm" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006-correspondence-1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNi1mb3JtLTI2LnBkZg==" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006-form-26.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNi1pbnRlcm5hdGlvbmFsIHNlYXJjaCBhdXRob3JpdHkgcmVwb3J0LTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006-international search authority report-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDM0MjIta29sbnAtMjAwNi1wY3QgcmVxdWVzdC5wZGY=" target="_blank" style="word-wrap:break-word;">03422-kolnp-2006-pct request.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1BTk5FWFVSRSBUTyBGT1JNIDMucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-ANNEXURE TO FORM 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1DTEFJTVMucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1ERVNDUklQVElPTiAoQ09NUExFVEUpLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1GT1JNLTEucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-FORM-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1GT1JNLTIucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-FORM-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1GT1JNLTUucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-FORM-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1PVEhFUlMucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1QRVRJVElPTiBVTkRFUiBSVUxFIDEzNy0xLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-PETITION UNDER RULE 137-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgwNy0wNi0yMDEzKS1QRVRJVElPTiBVTkRFUiBSVUxFIDEzNy5wZGY=" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(07-06-2013)-PETITION UNDER RULE 137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgyNy0wMS0yMDE0KS1DTEFJTVMucGRm" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(27-01-2014)-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LSgyNy0wMS0yMDE0KS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-(27-01-2014)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUNBTkNFTExFRCBQQUdFUy5wZGY=" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-CANCELLED PAGES.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1LT0xOUC0yMDA2LUNPUlJFU1BPTkRFTkNFLjEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">3422-KOLNP-2006-CORRESPONDENCE.1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUNPUlJFU1BPTkRFTkNFLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LURFQ0lTSU9OLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-DECISION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUVYQU1JTkFUSU9OIFJFUE9SVC5wZGY=" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LWZvcm0gMTgucGRm" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-form 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUZPUk0gMjYucGRm" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-FORM 26.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtQUJTVFJBQ1QucGRm" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtQ0xBSU1TLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtREVTQ1JJUFRJT04gKENPTVBMRVRFKS5wZGY=" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtRFJBV0lOR1MucGRm" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-DRAWINGS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtRk9STSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtRk9STSAyLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtRk9STSAzLnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-FORM 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtRk9STSA1LnBkZg==" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-FORM 5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUdSQU5URUQtU1BFQ0lGSUNBVElPTi1DT01QTEVURS5wZGY=" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-GRANTED-SPECIFICATION-COMPLETE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LUlOVEVSTkFUSU9OQUwgUFVCTElDQVRJT04ucGRm" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-INTERNATIONAL PUBLICATION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LU9USEVSUy5wZGY=" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzQyMi1rb2xucC0yMDA2LVJFUExZIFRPIEVYQU1JTkFUSU9OIFJFUE9SVC5wZGY=" target="_blank" style="word-wrap:break-word;">3422-kolnp-2006-REPLY TO EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=YWJzdHJhY3QtMDM0MjIta29sbnAtMjAwNi5qcGc=" target="_blank" style="word-wrap:break-word;">abstract-03422-kolnp-2006.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="263193-decellularization-of-organs-and-tissues.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="263195-device-strap-and-garment-for-treating-skin-and-subcutaneous-tissue-disorders-and-for-repairing-sports-injury-and-method-for-making-same.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>263194</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>3422/KOLNP/2006</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>42/2014</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>17-Oct-2014</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>13-Oct-2014</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>20-Nov-2006</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>TOBII TECHNOLOGY AB</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>KARLSROVAGEN 2D,SE-18253 DANDERYD.</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>BJORKLUND,CHRISTOFFER</td>
											<td>ANGSKARSGATAN 3,SE-11529 STOCKHOLM.</td>
										</tr>
										<tr>
											<td>2</td>
											<td>JACOBSON,MAGNUS</td>
											<td>FREDSGATAN 15,SE-17233 SUNDBYBERG.</td>
										</tr>
										<tr>
											<td>3</td>
											<td>SKOGO,MARTEN</td>
											<td>SVARTVIKSSLINGAN 25,SE-16738 BROMMA</td>
										</tr>
										<tr>
											<td>4</td>
											<td>ESKILSSON,HENRIK</td>
											<td>KVARNVAGEN 53,SE-18239 DANDERYD.</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06F3/00; G06F3/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/SE2005/000775</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2005-05-24</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>04445071.6</td>
									<td>2004-06-18</td>
								    <td>EUROPEAN UNION</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/263194-arrangement-and-method-for-controlling-a-computer-apparatus-associated-with-a-graphical-display by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:58:06 GMT -->
</html>
