<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/254797-an-apparatus-for-automated-production-of-a-foreign-language-speech-model-for-a-speech-recognition-program by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:17:07 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 254797:&quot; AN APPARATUS FOR AUTOMATED PRODUCTION OF A FOREIGN LANGUAGE SPEECH MODEL FOR A SPEECH RECOGNITION PROGRAM&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot; AN APPARATUS FOR AUTOMATED PRODUCTION OF A FOREIGN LANGUAGE SPEECH MODEL FOR A SPEECH RECOGNITION PROGRAM&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>The invention relates to an apparatus for automated production of a foreign language speech model for a speech recognition program product, wherein said foreign language speech model provides a sufficient set of words to teach the voice dictation recording based upon a transcribed file produced by a human transcriptionist and a written text produced by the speech recognition program product, wherein said written text is at least temporarily synchronized to said voice dictation recording, said apparatus is configured to sequentially compare a copy of said written text with said transcribed file resulting in a sequential list of unmatched words culled from said copy of said written text, said sequential list having a beginning, an end, and a current list of unmatched word, said current unmatched words being successively advanced from said beginning to said end; incrementally search for said current unmatched word contemporaneously within a first buffer associated with the speech recognition program product containing said written text and a second buffer associated with said sequential list; and correct said current unmatched word in said second buffer, display said current unmatched word in a manner substantially visually isolated from other text in said copy of said written text, and play a portion of said synchronized voice dictation recording from said first buffer associated with said current unmatched word.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>1.	Field of the Invention<br>
The present invention relates in general to computer speech recognition systems<br>
and, in particular, to a system and method for automating the text transcription of voice<br>
dictation by various end users.<br>
2.	Background Art<br>
Speech recognition programs are well known in the art. While these programs<br>
are ultimately useful in automatically converting speech into text, many users are<br>
dissuaded from using these programs because they require each user to spend a<br>
significant amount of time training the system. Usually this training begins by having<br>
each user read a series of pre-selected materials for approximately 20 minutes. Then, as<br>
the user continues to use the program, as words are improperly transcribed the user is<br>
expected to stop and train the program as to the intended word thus advancing the<br>
ultimate accuracy of the acoustic model. Unfortunately, most professionals (doctors,<br>
dentists, veterinarians, lawyers) and business executive are unwilling to spend the time<br>
developing the necessary acoustic model to truly benefit from the automated<br>
transcription.<br>
Accordingly, it is an object of the present invention to provide a system that<br>
offers transparent training of the speech recognition program to the end-users.<br>
There are systems for using computers for routing transcription from a group of<br>
end users. Most often these systems are used in large multi-user settings such as<br>
hospitals. In those systems, a voice user dictates into a general-purpose computer or<br>
other recording device and the resulting file is transferred automatically to a human<br>
transcriptionist The human transcriptionist transcribes the file, which is then returned to<br>
the original "author" for review. These systems have the perpetual overhead of<br>
employing a sufficient number of human transcriptionist to transcribe all of the diction<br>
files.<br><br>
Accordingly it is another object of the present invention to provide an automated<br>
means of translating speech into text where ever suitable so as to minimize the number of<br>
human transcriptionist necessary to transcribe audio files coming into the system.<br>
It is an associated object to provide a simplified means for providing verbatim<br>
text files for training a user's acoustic model for the speech recognition portion of the<br>
system.<br>
It is another associated object of the present invention to automate a preexisting<br>
speech recognition program toward further minimizing the number operators necessary<br>
to operate the system.<br>
These and other objects will be apparent to those of ordinary skill in the art<br>
having the present drawings, specification and claims before them.<br>
Summary of the Invention<br>
The present invention comprises, in part, a system for substantially automating<br>
transcription services for one or more voice users. The system includes means for<br>
creating a uniquely identified voice dictation file from a current user and an audio player<br>
used to audibly reproduce said uniquely identified voice dictation file. Both of these<br>
system elements can be implemented on the same or different general-purpose<br>
computers. Additionally, the voice dictation file creating means includes a system for<br>
assigning unique file handles to audio files and an audio recorder, and further comprise<br>
means for operably connecting to a separata digital recording device and/or means for<br>
reading audio files from removable magnetic and other computer media.<br>
Each of the general purpose computers implementing the system may be<br>
remotely located from the other computers but in operable connection to each other by<br>
way of a computer network, direct telephone connection, via email or other Internet<br>
based transfer.<br>
The system further includes means for manually inputting and creating a<br>
transcribed file based on humanly perceived contents of the uniquely identified voice<br>
dictation file. Thus, for certain voice dictation files, a human transcriptionist manually<br>
transcribes a textual version of the audio — using a text editor or word processor — based<br>
on the output of the output of the audio player.<br><br>
The system also includes means for automatically converting the voice dictation<br>
file into written text. The automatic speech converting means may be a preexisting<br>
speech recognition program, such as Dragon Systems' Naturally Speaking, IBM's Via<br>
Voice or Philips Corporation's Magic Speech. In such a case, the automatic speech<br>
converting means includes means for automating responses to a series of interactive<br>
inquiries from the preexisting speech recognition program. In one embodiment, the<br>
system also includes means for manually selecting a specialized language model.<br>
The system further includes means for manually editing the resulting written text<br>
to create a verbatim text of the voice dictation file. At the outset of a users use of the<br>
system, this verbatim text will have to be created completely manually. However, after<br>
the automatic speech converting means has begun to sufficiently develop that user!s<br>
acoustic model a more automated means can be used.<br>
In a preferred embodiment, that manual editing means includes means for<br>
sequentially comparing a copy of the written text with the transcribed file resulting in a<br>
sequential list of unmatched words culled from the copy of said written text. The manual<br>
editing means further includes means for incrementally searching for the current<br>
unmatched word contemporaneously within a first buffer associated with the speech<br>
recognition program containing the written text and a second buffer associated with the<br>
sequential list. Finally, the preferred manual editing means includes means for<br>
correcting the current unmatched word in the second buffer, which includes means for<br>
displaying the current unmatched word in a manner substantially visually isolated from<br>
other text in the written text and means for playing a portion of the voice dictation<br>
recording from said first buffer associated with said current unmatched word. In one<br>
*<br>
embodiment, the manual input means further includes means for alternatively viewing<br>
the current unmatched word in context within the written text. For instance, the operator<br>
may wish to view the unmatched within the sentence in which it appears or perhaps with<br>
only is immediately adjacent words. Thus, the manner substantially visual isolation can<br>
be manually selected from the group containing word-by-word display, sentence-by-<br>
sentence display, and said current unmatched word display. The manual editing means<br>
portion of the complete system may also be utilized as a separate apparatus.<br>
The system may also include means for determining the skill of a human<br>
traasenptionist. In one approach, this accuracy determination can be made by<br><br>
determining the ratio of the number of words in the sequential list of unmatched words to<br>
the number of words in the written text.<br>
The system additionally includes means for training the automatic speech<br>
converting means to achieve higher accuracy for the current user. In particular, the<br>
training means uses the verbatim text created by the manual editing means and the voice<br>
dictation file. The training means may also comprise a preexisting training portion of the<br>
preexisting speech recognition program. Thus, the training means would also include<br>
means for automating responses to a series of interactive inquiries from the preexisting<br>
training portion of the speech recognition program. This functionality can be used, for<br>
instance, to establish a new language model (i.e. foreign language).<br>
The system finally includes means for controlling the flow of the voice dictation<br>
file based upon the training status of the current user using the unique identification. The<br>
control means reads and modifies a user's training status such that it is an appropriate<br>
selection from the group of pre-enrollment, enrollment, training, automation and stop<br>
automation. During a user's pre-enrollment phase the control means further includes<br>
means for creating a user identification and acoustic model within the automatic speech<br>
converting means. During this phase, the control means routes the voice dictation file to<br>
the automatic speech converting means and the manual input means, routes the written<br>
text and the transcribed file to the manual editing means, routes the verbatim text to the<br>
training means and routes the transcribed file back to the current user as a finished text.<br>
' During the training phase, the control means routes (1) the voice dictation file to<br>
the automatic speech concerting means and the manual input means, (2) routes the<br>
written text and the transcribed file to the manual editing means, (3) routes the verbatim<br>
text to the training means and (4) routes the transcribed file back to the current user as a<br>
finished text.<br>
During the automation stage, the control means routes (1) the voice dictation file<br>
only to the automatic speech converting means and (2) the written text back to the<br>
current user as a finished text.<br>
The present application also discloses a method for automating transcription<br>
services for one or more voice users in a system including a manual transcription station<br><br>
and a speech recognition program. The method comprising the steps of: (1) establishing<br>
a profile for each of the voice users, the profile containing a training status; (2) creating a<br>
uniquely identified voice dictation file from a current voice user; (3) choosing the<br>
training status of the current voice user from the group of enrollment, training, automated<br>
and stop automation; (4) routing the voice dictation file to at least one of the manual<br>
transcription station and the speech recognition program based on the training status; (5)<br>
receiving the voice dictation file in at least one of the manual transcription station and<br>
the speech recognition program; (6) creating a transcribed file at the manual transcription<br>
station for each received voice dictation file; (7) automatically creating a written text<br>
with the speech recognition program for each received voice dictation file if the training<br>
status of the current user is training or automated; (8) manually establishing a verbatim<br>
file if the training status of the current user is enrollment or training; (9) training the<br>
speech recognition program with an acoustic model for the current user using the<br>
verbatim file and the voice dictation file if the training status of the current user is<br>
enrollment or training; (10) returning the transcribed file to the current user if the<br>
training status of the current user is enrollment or training; and (11) returning the written<br>
text to the current user if the training status of the current user is automated.<br>
Brief Description of the Acompanying Drawings <br>
Fig. 1 of the drawings is a block diagram of one potential embodiment of the<br>
present system for substantially automating transcription services for one or more voice<br>
users;<br>
Fig. lb of the drawings is a block diagram of a general-purpose computer which<br>
may be used as a dictation station, a transcription station and the control means within<br>
the present system;<br>
Fig. 2a of the drawings is a flow diagram of the main loop of the control means<br>
of the present system;<br>
Fig. 2b of the drawings is a flow diagram of the enrollment stage portion of the<br>
control means of the present system;<br>
Fig. 2c of the drawings is a flow diagram of the training stage portion of the<br>
control means of the present system;<br><br>
Fig. 2d of the drawings is a flow diagram of the automation stage portion of the<br>
control means of the present system;<br>
Fig. 3 of the drawings is a directory structure used by the control means in the<br>
present system;<br>
Fig. 4 of the drawings is a block diagram of a portion of a preferred embodiment<br>
of the manual editing means; and<br>
Fig. 5 of the drawings is an elevation view of the remainder of a preferred<br>
embodiment of the manual editing means.<br>
Best Modes of Practicing the Invention<br>
While the present invention may be embodied in many different forms, there is<br>
shown in the drawings and discussed herein a few specific embodiments with the<br>
understanding that the present disclosure is to be considered only as an exemplification<br>
of the principles of the invention and is not intended to limit the invention to the<br>
embodiments illustrated.<br>
Fig. 1 of the drawings generally shows one potential embodiment of the present<br>
system for substantially automating transcription services for one or more voice users.<br>
The present system must include some means for receiving a voice dictation file from a<br>
current user. This voice dictation file receiving means can be a digital audio recorder, an<br>
analog audio recorder, or standard means for receiving computer files on magnetic media<br>
or via a data connection.<br>
As shown, in one embodiment, the system 100 includes multiple digital recording<br>
stations 10, 11,12 and 13. Each digital recording station has at least a digital audio<br>
recorder and means for identifying the current voice user.<br>
Preferably, each of these digital recording stations is implemented on a general-<br>
purpose computer (such as computer 20), although a specialized computer could be<br>
developed for this specific purpose. The general-purpose computer, though has the<br>
added advantage of being adaptable to varying uses in addition to operating within the<br>
present system 100. In general, the general-purpose computer should have, among other<br>
elements, a microprocessor (such as the Intel Corporation PENTIUM, Cyrix K6 or<br><br>
Motorola 68000 series); voiatue ana non-voiauie memory; one or more mass storage<br>
devices (i.e. HDD (not shown), floppy drive 21, and other removable media devices 22<br>
such as a CD-ROM drive, DITTO, ZIP or JAZ drive (from Iomega Corporation) and the<br>
like); various user input devices, such as a mouse 23, a keyboard 24, or a microphone 25;<br>
and a video display system 26. In one embodiment, the general-purpose computer is<br>
controlled by the WINDOWS 9.X operating system. It is contemplated, however, that<br>
the present system would work equally well using a MACINTOSH computer or even<br>
another operating system such as a WINDOWS CE, UNIX or a JAVA based operating<br>
system, to name a few.<br>
Regardless of the particular computer platform used, in an embodiment utilizing<br>
an analog audio input (via microphone 25) the general-purpose computer must include a<br>
sound-card (not shown). Of course, in an embodiment with a digital input no sound card<br>
would be necessary.<br>
In the embodiment shown in Fig. 1, digital audio recording stations 10, 11, 12<br>
and 13 are loaded and configured to run digital audio recording software on a<br>
PENTIUM-based computer system operating under WINDOWS 9.x. Such digital<br>
recording software is available as a utility in the WINDOWS 9.x operating system or<br>
from various third party vendor such as The Programmers' Consortium, Inc. of Oakton,<br>
Virginia (VOICEDOC), Syntrillium Corporation of Phoenix, Arizona (COOL EDIT) or<br>
Dragon Systems Corporation (Dragon Naturally Speaking Professional Edition). These<br>
various software programs produce a voice dictation file in the form of a "WAV" file.<br>
However, as would be known to those skilled in the art, other audio file formats, such as<br>
MP3 or DSS, could also be used to format the voice dictation file, without departing<br>
from the spirit of the present invention. In one embodiment where VOICEDOC software<br>
is used that software also automatically assigns a file handle to the WAV file, however, it<br>
would be known to those of ordinary skill in the art to save an audio file on a computer<br>
system using standard operating system file management methods.<br>
Another means for receiving a voice dictation file is dedicated digital recorder 14,<br>
such as the Olympus Digital Voice Recorder D-1000 manufactured by the Olympus<br>
Corporation. Thus, if the current voice user is more comfortable with a more<br>
conventional type of dictation device, they can continue to use a dedicated digital<br>
recorder 14. In order to harvest the digital audio text file, upon completion of a<br><br>
recording, dedicated digital recorder 14 would be operably connected to one of the.<br>
digital audio recording stations, such as 13, toward downloading the digital audio file<br>
into that general-purpose computer. With this approach, for instance, no audio card<br>
would be required.<br>
Another alternative for receiving the voice dictation file may consist of using one<br>
form or another of removable magnetic media containing a pre-recorded audio file. With<br>
this alternative an operator would input the removable magnetic media into one of the<br>
digital audio recording stations toward uploading the audio file into the system.<br>
In some cases it may be necessary to pre-process the audio files to make them<br>
acceptable for processing by the speech recognition software. For instance, a DSS file<br>
format may have to be changed to a WAV file format, or the sampling rate of a digital<br>
audio file may have to be upsampled or downsampled. For instance, in use the Olympus<br>
Digital Voice Recorder with Dragon Naturally Speaking, Olympus' 8MHz rate needs to<br>
be upsampled to 11 MHz. Software to accomplish such pre-processing is available from<br>
a variety of sources including Syntrillium Corporation and Olympus Corporation.<br>
The other aspect of the digital audio recording stations is some means for<br>
identifying the current voice user. The identifying means may include keyboard 24 upon<br>
which the user (or a separate operator) can input the current user's unique identification<br>
code. Of course, the user identification can be input using a myriad of computer input<br>
devices such as pointing devices (e.g. mouse 23), a touch screen (not shown), a light pen<br>
(not shown), bar-code reader (not shown) or audio cues via microphone 25, to name a<br>
few.<br>
In the case of a first time user the identifying means may also assign that user an<br>
identification number after receiving potentially identifying information from that user,<br>
including: (1) name; (2) address; (3) occupation; (4) vocal dialect or accent; etc. As<br>
discussed in association with the control means, based upon this input information, a<br>
voice user profile and a sub-directory within the control means are established. Thus,<br>
regardless of the particular identification means used, a user identification must be<br>
established for each voice user and subsequently provided with a corresponding digital<br>
audio file for each use such that the control means can appropriately route and the system<br>
ultimately transcribe the audio.<br><br>
For each new user (as indicated by the existence of a ".pro" file in the "current"<br>
subdirectory), a new subdirectory is established, step 204 (such as the "usern"<br>
subdirectory (shown in Fig. 3)). This subdirectory is used to store all of the audio files<br>
("xxxx.wav"), written text ("xxxx.wrt"), verbatim text ("xxxx.vb"), transcription text<br>
("xxxx.txt") and user profile ("usern.pro") for that particular user. Each particular job is<br>
assigned a unique number "xxxx" such that all of the files associated with a job can be<br>
associated by that number. With this directory structure, the number of users is<br>
practically limited only by storage space within general-purpose computer 40.<br>
Now that the user subdirectory has been established, the user profile is moved to<br>
the subdirectory, step 205. The contents of this user profile may vary between systems.<br>
The contents of one potential user profile is shown in Fig. 3 as containing: the user name,<br>
address, occupation and training status. Aside from the training status variable, which is<br>
necessary, the other data is useful in routing and transcribing the audio files.<br>
The control means, having selected one set of files by the handle, determines the<br>
identity of the current user by comparing the ".id" file with its "user.tbl," step 206. Now<br>
that the user is known the user profile may be parsed from that user's subdirectory and<br>
the current training status determined, step 207. Steps 208-211 are the triage of the<br>
current training status is one of: enrollment, training, automate, and stop automation.<br>
Enrollment is the first stage in automating transcription services. As shown in<br>
Fig. 2b, the audio file is sent to transcription, step 301. In particular, the "xxxx.wav" file<br>
is transferred to transcriptionist stations 50 and 51. In a preferred embodiment, both<br>
stations are general-purpose computers which run both an audio player and manual input<br>
means. The audio player is likely to be a digital audio player, although it is possible that<br>
an analog audio file could be transferred to the stations. Various audio players are<br>
commonly available including a utility in the WINDOWS 9.x operating system and<br>
various other third parties such from The Programmers' Consortium, Inc. of Oakton,<br>
Virginia ( VOICESCRIBE). Regardless of the audio player used to play the audio file,<br>
manual input means is running on the computer at the same time. This manual input<br>
means may comprise any of text editor or word processor (such as MS WORD,<br>
Word perfect. AmiPro or Word Pad) in combination with a keyboard, mouse, or other<br>
user-interface device .In embodiment of the present invention, this manual input<br><br>
Dragon Systems of Newton, Massachusetts, Via Voice from IBM Corporation of<br>
Armonk, New York, or Speech Magic from Philips Corporation of Atlanta, Georgia.<br>
Human transcriptionist 6 listens to the audio file created by current user 5 and as is<br>
known, manually inputs the perceived contents of that recorded text, thus establishing<br>
the transcribed file, step 302. Being human, human transcriptionist 6 is likely to impose<br>
experience, education and biases on the text and thus not input a verbatim transcript of<br>
the audio file. Upon completion of the human transcription, the human transcriptionist 6<br>
saves the file and indicates that it is ready for transfer to the current users subdirectory as<br>
"xxxx.txt", step 303.<br>
Inasmuch as this current user is only at the enrollment stage, a human operator<br>
will have to listen to the audio file and manually compare it to the transcribed file and<br>
create a verbatim file, step 304. That verbatim file "xxxx.vb" is also transferred to the<br>
current user's subdirectory, step 305. Now that verbatim text is available, control means<br>
200 starts the automatic speech conversion means, step 306. This automatic speech<br>
conversion means may be a preexisting program, such as Dragon System's Naturally<br>
Speaking, IBM's Via Voice or Philips' Speech Magic, to name a few. Alternatively, it<br>
could be a unique program that is designed to specifically perform automated speech<br>
recognition.<br>
In a preferred embodiment, Dragon Systems' Naturally Speaking has been used<br>
by running an executable simultaneously with Naturally Speaking that feeds phantom<br>
keystrokes and mousing operations through the WIN32API, such that Naturally<br>
Speaking believes that it is interacting with a human being, when in fact it is being<br>
controlled by control means 200. Such techniques are well known in the computer<br>
software testing art and, thus, will not be discussed in detail. It should suffice to say that<br>
by watching the application flow of any speech recognition program, an executable to<br>
mimic the interactive manual steps can be created.<br>
If the current user is anew user, the speech recognition program will need to<br>
establish the new user, step 307. Control means provides the necessary information from<br>
the user profile found in the current user's subdirectory. AH speech recognition require<br>
significant training to establish an acoustic model of a particular user. In the case of<br>
Dragon, initially the program seeks approximately 20 minutes of audio usually obtained<br>
by the user reading a canned text provided by Dragon Systems. There is also<br><br>
functionality built into Dragon that allows "mobile training." Using this feature, the<br>
verbatim file and audio file are fed into the speech recognition program to beginning<br>
training the acoustic model for that user, step 308. Regardless of the length of that audio<br>
file, control means 200 closes the speech recognition program at the completion of the<br>
file, step 309.<br>
As the enrollment step is too soon to use the automatically created text, a copy of<br>
the transcribed file is sent to the current user using the address information contained in<br>
the user profile, step 310. This address can be a street address or an e-mail address.<br>
Following that transmission, the program returns to the main loop on Fig. 2a.<br>
After a certain number of minutes of training have been conducted for a<br>
particular user, that user's training status may be changed from enrollment to training.<br>
The border for this change is subjective, but perhaps a good rule of thumb is once<br>
Dragon appears to be creating written text with 80% accuracy or more, the switch<br>
between states can be made. Thus, for such a user the next transcription event will<br>
prompt control means 200 into the training state. As shown in Fig. 2c, steps 401-403 are<br>
the same human transcription steps as steps 301-303 in the enrollment phase. Once the<br>
transcribed file is established, control means 200 starts the automatic speech conversion<br>
means (or speech recognition program) and selects the current user, step 404. The audio<br>
file is fed into the speech recognition program and a written text is established within the<br>
program buffer, step 405. In the case of Dragon, this buffer is given the same file handle<br>
on very instance of the program. Thus, that buffer can be easily copied using standard<br>
operating system commands and manual editing can begin, step 406.<br>
In one particular embodiment utilizing the VOICEWARE system from The<br>
Programmers' Consortium, Inc. of Oakton, Virginia, the user inputs audio into the<br>
VOICEWARE system's VOICEDOC program, thus, creating a "wav" file. In addition,<br>
before releasing this ".wav" file to the VOICEWARE server, the user selects a<br>
"transcriptionist." This "transcriptionist" may be a particular human transcriptionist or<br>
may be the "computerized transcriptionist." If the user selects a "computerized<br>
transcriptionist" they may also select whether that transcription is handled locally or<br>
remotely. This file is assigned a job number by the VOICEWARE server, which routes<br>
the job to the VOICESCRIBE portion of the system. Normally, VOICESCRIBE is used<br>
by the human transcriptionist to receive and playback the job's audio (".wav") file. In<br><br>
addition, the audio file is grabbed by the automatic speech conversion means. In this<br>
VOICEWARE system embodiment, by placing VOICESCRIBE in "auto mode" new<br>
jobs (i.e. an audio file newly created by VOICEDOC) are automatically downloaded<br>
from the VOICEWARE server and a VOICESCRIBE window having a window title<br>
formed by the job number of the current ".wav" file. An executable file, running in the<br>
background "sees" the VOICESCRIBE window open and using the WIN32 API<br>
determines the job number from the VOICESCRIBE window title. The executable file<br>
then launches the automatic speech conversion means. In Dragon System's Naturally<br>
Speaking, for instance, there is a built in function for performing speech recognition on a<br>
preexisting ".wav" file. The executable program feeds phantom keystrokes to Naturally<br>
Speaking to open the ".wav" file from the "current" directory (see Fig. 3) having the job<br>
number of the current job.<br>
In this embodiment, after Naturally Speaking has completed automatically<br>
transcribing the contexts of the ".wav" file, the executable file resumes operation by<br>
selecting all of the text in the open Naturally Speaking window and copying it to the<br>
WINDOWS 9.x operating system clipboard. Then, using the clipboard utility, save the<br>
clipboard as a text file using the current job number with a "dmt" suffix. The executable<br>
file then "clicks" the "complete" button in VOICESCRIBE to return the "dmt" file to the<br>
VOICEWARE server. As would be understood by those of ordinary skill in the art, the<br>
foregoing procedure can be done utilizing other digital recording software and other<br>
automatic speech conversion means. Additionally, functionality analogous to the<br>
WINDOWS clipboard exists in other operating systems. It is also possible to require<br>
human intervention to activate or prompt one or more of the foregoing steps. Further,<br>
although, the various programs executing various steps of this could be running on a<br>
number of interconnected computers (via a LAN, WAN, internet connectivity, email and<br>
the like), it is also contemplated that all of the necessary software can be running on a<br>
single computer.<br>
Another alternative approach is also contemplated wherein the user dictates<br>
directly into the automatic speech conversion means and the VOICEWARE server picks<br>
up a copy in the reverse direction. This approach works as follows; without actually<br>
recording any voice, the user clicks on the "complete" button in VOICEDOC, thus,<br>
creating an empty ".wav" file. This empty file is nevertheless assigned a unique job<br><br>
number by the VOICEWARE server. The user (or an executable file running in the.<br>
background) then launches the automatic speech conversion means and the user dictates<br>
directly into that program, in the same mannerpreviously used in association with such<br>
automatic speech conversion means. Upon completion of the dictation, the user presses<br>
a button labeled "return" (generated by a background executable file), which executable<br>
then commences a macro that gets the current job number from VOICEWARE (in the<br>
manner describe above), selects all of the text in the document and copies it to the<br>
clipboard. The clipboard is then saved to the file "<jobnumber>.dmt," as discussed<br>
above. The executable then "clicks" the "complete" button (via the WIN32API) in<br>
VOICESCRIBE, which effectively returns the automatically transcribed text file back to<br>
the VOICEWARE server, which, in turn, returns the completed transcription to the<br>
VOICESCRIBE user. Notably, although, the various programs executing various steps<br>
of this could be running on a number of interconnected computers (via a LAN, WAN,<br>
internet connectivity, email and the like), it is also contemplated that all of the necessary<br>
software can be running on a single computer.. As would be understood by those of<br>
ordinary skill in the art, the foregoing procedure can be done utilizing other digital<br>
recording software and other automatic speech conversion means. Additionally,<br>
functionality analogous to the WINDOWS clipboard exists in other operating systems. It<br>
is also possible to require human intervention to activate or prompt one or more of the<br>
foregoing steps.<br>
Manual editing is not an easy task. Human beings are prone to errors. Thus, the<br>
present invention also includes means for improving on that task. As shown in Fig. 4,<br>
the transcribed file ("3333.txt") and the copy of the written text ("3333.wrt") are<br>
sequentially compared word by word 406a toward establishing sequential list of<br>
unmatched words 406b that are culled from the copy of the written text. This list has a<br>
beginning and an end and pointer 406c to the current unmatched word. Underlying the<br>
sequential list is another list of objects which contains the original unmatched words, as<br>
well as the words immediately before and after that unmatched word, the starting<br>
location in memory of each unmatched word in the sequential list of unmatched words<br>
406b and the length of the unmatched word.<br>
As shown in Fig. 5, the unmatched word pointed at by pointer 406c from list<br>
406b is displayed in substantial visual isolation from the other text in the copy of the<br><br>
written text on a standard computer monitor 500 in an active window 501. As shown in<br>
Fig. 5, the context of the unmatched word can be selected by the operator to be shown<br>
within the sentence it resides, word by word or in phrase context, by clicking on buttons<br>
514, 515, and 516, respectively.<br>
Associated with active window 501 is background window 502, which contains<br>
the copy of the written text file. As shown in background window 502, a incremental<br>
search has located (see pointer 503) the next occurrence of the current unmatched word<br>
"cash." Contemporaneously therewith, within window 505 containing the buffer from<br>
the speech recognition program, the same incremental search has located (see pointer<br>
506) the next occurrence of the current unmatched word. A human user will likely only<br>
being viewing active window 501 activate the audio replay from the speech recognition<br>
program by clicking on "play" button 510, which plays the audio synchronized to the<br>
text at pointer 506. Based on that snippet of speech, which can be played over and over<br>
by clicking on the play button, the human user can manually input the correction to the<br>
current unmatched word via keyboard, mousing actions, or possibly even audible cues to<br>
another speech recognition program running within this window.<br>
In the present example, even if the choice of isolated context offered by buttons<br>
514, 515 and 516, it may still be difficult to determine the correct verbatim word out-of-<br>
context, accordingly there is a switch window button 513 that will move background<br>
window 502 to the foreground with visible pointer 503 indicating the current location<br>
within the copy of the written text. The user can then return to the active window and<br>
input the correct word, "trash." This change will only effect the copy of the written text<br>
displayed in background window 502.<br>
When the operator is ready for the next unmatched word, the operator clicks on<br>
the advance button 511, which advances pointer 406c down the list of unmatched words<br>
and activates the incremental search in both window 502 and 505. This unmatched word<br>
is now displayed in isolation and the operator can play the synchronized speech from the<br>
speech recognition program and correct this word as well. If at any point in the<br>
operation, the operator would like to return to a previous unmatched word, the operator<br>
clicks on the reverse button 512, which moves pointer 406c back a word in the list and<br>
causes a backward incremental search to occur. This is accomplished by using the<br>
underlying list of objects which contains the original unmatched words. This list is<br><br>
traversed in object by object fashion, but alternatively each of the records could be .<br>
padded such that each item has the same word size to assist in bi-directional traversing of<br>
the list. As the unmatched words in this underlying list are read only it is possible to<br>
return to the original unmatched word such that the operator can determine if a different<br>
correction should have been made.<br>
Ultimately, the copy of the written text is finally corrected resulting in a verbatim<br>
 copy, which is saved to the user's subdirectory. The verbatim file is also passed to the<br>
 speech recognition program for training, step 407. The new (and improved) acoustic<br>
model is saved, step 408, and the speech recognition program is closed, step 409. As the<br>
 system is still in training, the transcribed file is returned to the user, as in step 310 from<br>
 the enrollment phase.<br>
As shown in Fig. 4, the system may also include means for determining the<br>
accuracy rate from the output of the sequential comparing means. Specifically, by<br>
counting the number of words in the written text and the number of words in list 406b<br>
the ratio of words in said sequential list to words in said written text can be determined,<br>
thus providing an accuracy percentage. As before, it is a matter of choice when to<br>
advance users from one stage to another. Once that goal is reached, the user's profile is<br>
changed to the next stage, step 211.<br>
One potential enhancement or derivative functionality is provided by the<br>
determination of the accuracy percentage. In one embodiment, this percentage could be<br>
used to evaluate a human transcriptionist's skills. In particular, by using either a known<br>
verbatim file or a well-established user, the associated ".wav" file would be played for<br>
the human transcriptionist and the foregoing comparison would be performed on the<br>
transcribed text versus the verbatim file created by the foregoing process. In this<br>
manner, additional functionality can be provided by the present system.<br>
As understood, currently, manufacturers of speech recognition programs use<br>
recording of foreign languages, dictions, etc. with manually established verbatim files to<br>
program speech models. It should be readily apparent that the foregoing manner of<br>
establishing verbatim text could be used in the initial development of these speech files<br>
simplifying this process greatly.<br><br>
Once the user has reached the automation stage, the greatest benefits of the<br>
present system can be achieved. The speech recognition software is started, step 600,<br>
and the current user selected, step 601. If desired, a particularized vocabulary may be<br>
selected, step 602. Then automatic conversion of the digital audio file recorded by the<br>
current user may commence, step 603. When completed, the written file is transmitted to<br>
the user based on the information contained in the user profile, step 604 and the program<br>
is returned to the main loop.<br>
Unfortunately, there may be instances where the voice users cannot use<br>
automated transcription for a period of time (during an illness, after dental work, etc.)<br>
because their acoustic model has been temporarily (or even permanently) altered. In that<br>
case, the system administrator may set the training status variable to a stop automation<br>
state in which steps 301, 302, 303, 305 and 310 (see Fig. 2b) are the only steps<br>
performed.<br>
The foregoing description and drawings merely explain and illustrate the<br>
invention and the invention is not limited thereto. Those of the skill in the art who have<br>
the disclosure before them will be able to make modifications and variations therein<br>
without departing from the scope of the present invention. For instance, it is possible to<br>
implement all of the elements of the present system on a single general-purpose<br>
computer by essentially time sharing the machine between the voice user, transcriptionist<br>
and the speech recognition program. The resulting cost saving makes this system<br>
accessible to more types of office situations not simply large medical clinics, hospital,<br>
law firms or other large entities.	<br><br>
We Claim:<br>
1. An apparatus for automated production of a foreign language speech model<br>
for a speech recognition program product, wherein said foreign language speech<br>
model provides a sufficient set of words to teach the voice dictation recording<br>
based upon a transcribed file produced by a human transcriptionist and a written<br>
text produced by the speech recognition program product, wherein said written<br>
text is at least temporarily synchronized to said voice dictation recording, said<br>
apparatus is configured to:<br>
sequentially compare a copy of said written<br>
text with said transcribed file resulting in a sequential<br>
list of unmatched words culled from said copy of said<br>
written text, said sequential list having a beginning, an<br>
end, and a current list of unmatched word, said current unmatched<br>
words being successively advanced from said beginning to<br>
said end;<br><br>
incrementally search for said current unmatched<br>
word contemporaneously within a first buffer associated<br>
with the speech recognition program product containing said written<br>
text and a second buffer associated with said sequential<br>
list; and<br>
correct said current unmatched word in said<br>
second buffer, display said current unmatched word in a manner<br>
substantially visually isolated from other text in said<br>
copy of said written text, and play a portion of said synchronized voice<br>
dictation recording from said first buffer associated with said current<br>
unmatched word.<br>
2. The apparatus as claimed in claim 1, wherein said correcting device is<br>
enabled to alternatively view said current unmatched word in context within<br>
said copy of said written text.<br><br>
3. The apparatus as claimed in claim 2 wherein said<br>
isolation device can be manually selected from the group containing word-by-<br>
word display, sentence-by-sentence display, and said current unmatched<br>
word display.<br><br><br>
ABSTRACT<br><br>
TITLE: AN APPARATUS FOR AUTOMATED PRODUCTION OF A FOREIGN<br>
LANGUAGE SPEECH MODEL FOR A SPEECH RECOGNITION PROGRAM<br>
The invention relates to an apparatus for automated production of a foreign<br>
language speech model for a speech recognition program product, wherein said<br>
foreign language speech model provides a sufficient set of words to teach the<br>
voice dictation recording based upon a transcribed file produced by a human<br>
transcriptionist and a written text produced by the speech recognition program<br>
product, wherein said written text is at least temporarily synchronized to said<br>
voice dictation recording, said apparatus is configured to sequentially compare a<br>
copy of said written text with said transcribed file resulting in a sequential<br>
list of unmatched words culled from said copy of said written text, said<br>
sequential list having a beginning, an end, and a current list of unmatched word,<br>
said current unmatched words being successively advanced from said beginning<br>
to said end; incrementally search for said current unmatched word<br>
contemporaneously within a first buffer associated with the speech recognition<br>
program product containing said written text and a second buffer associated with<br>
said sequential list; and correct said current unmatched word in said<br>
second buffer, display said current unmatched word in a manner substantially<br>
visually isolated from other text in said copy of said written text, and play a<br>
portion of said synchronized voice dictation recording from said first buffer<br>
associated with said current unmatched word.</jobnumber></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDEwMTgta29sLTIwMDUtY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">01018-kol-2005-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDEwMTgta29sLTIwMDUtZGVzY3JpcHRpb24gY29tcGxldGUucGRm" target="_blank" style="word-wrap:break-word;">01018-kol-2005-description complete.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDEwMTgta29sLTIwMDUtZHJhd2luZ3MucGRm" target="_blank" style="word-wrap:break-word;">01018-kol-2005-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDEwMTgta29sLTIwMDUtZm9ybSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">01018-kol-2005-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDEwMTgta29sLTIwMDUtZm9ybSAyLnBkZg==" target="_blank" style="word-wrap:break-word;">01018-kol-2005-form 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDEwMTgta29sLTIwMDUtZm9ybSAzLnBkZg==" target="_blank" style="word-wrap:break-word;">01018-kol-2005-form 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDItMDctMjAxMiktQ09SUkVTUE9OREVOQ0UucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(02-07-2012)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDItMDctMjAxMiktUEEucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(02-07-2012)-PA.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDUtMDktMjAxMiktQ09SUkVTUE9OREVOQ0UucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(05-09-2012)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDUtMDktMjAxMiktRk9STS0xMy5wZGY=" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(05-09-2012)-FORM-13.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDUtMDktMjAxMiktT1RIRVJTLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(05-09-2012)-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDctMDktMjAxMSktQUJTVFJBQ1QucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(07-09-2011)-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDctMDktMjAxMSktQU1BTkRFRCBDTEFJTVMucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(07-09-2011)-AMANDED CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDctMDktMjAxMSktQ09SUkVTUE9OREVOQ0UucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(07-09-2011)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDctMDktMjAxMSktRk9STSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(07-09-2011)-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDctMDktMjAxMSktRk9STSAyLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(07-09-2011)-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS0oMDctMDktMjAxMSktT1RIRVJTLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-(07-09-2011)-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1hYnN0cmFjdC0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-abstract-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1hYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">1018-kol-2005-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1hbWFuZGVkIGNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">1018-kol-2005-amanded claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1jbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1DT1JSRVNQT05ERU5DRSAxLjEucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-CORRESPONDENCE 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1kZXNjcmlwdGlvbiAoY29tcGxldGUpLTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1018-kol-2005-description (complete)-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1kZXNjcmlwdGlvbiAoY29tcGxldGUpLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1kcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">1018-kol-2005-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1FWEFNSU5BVElPTiBSRVBPUlQucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDEtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 1-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDEucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1GT1JNIDE4IDEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-FORM 18 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDE4LnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDItMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 2-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDIucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1GT1JNIDMgMS4yLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-FORM 3 1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDMtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 3-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1mb3JtIDMucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-form 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELUFCU1RSQUNULnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELUNMQUlNUy5wZGY=" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELURFU0NSSVBUSU9OIChDT01QTEVURSkucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELURSQVdJTkdTLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-DRAWINGS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELUZPUk0gMS5wZGY=" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELUZPUk0gMi5wZGY=" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1HUkFOVEVELVNQRUNJRklDQVRJT04ucGRm" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-GRANTED-SPECIFICATION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1PVEhFUlMgMS4yLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-OTHERS 1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1vdGhlcnMtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-others-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1wZXRpdGlvbiB1bmRlciBydWxlIDEzNy0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-petition under rule 137-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1wZXRpdGlvbiB1bmRlciBydWxlIDEzNy5wZGY=" target="_blank" style="word-wrap:break-word;">1018-kol-2005-petition under rule 137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1LT0wtMjAwNS1SRVBMWSBUTyBFWEFNSU5BVElPTiBSRVBPUlQgMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-KOL-2005-REPLY TO EXAMINATION REPORT 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1yZXBseSB0byBleGFtaW5hdGlvbiByZXBvcnQucGRm" target="_blank" style="word-wrap:break-word;">1018-kol-2005-reply to examination report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAxOC1rb2wtMjAwNS1zcGVjaWZpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">1018-kol-2005-specification.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="254796-cutting-tool-made-of-surface-coated-cubic-boron-nitride-based-ultrahigh-pressure-sintered-material.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="254798-a-herbicidal-composition-comprising-4-aminopicolinate-of-formula-1.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>254797</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>1018/KOL/2005</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>51/2012</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>21-Dec-2012</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>19-Dec-2012</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>08-Nov-2005</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>CUSTOM SPEECH USA, INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>SUIT B365, 3 NORTH COURT STREET, CROWN POINT, IN</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>QIN, CHARLES</td>
											<td>23461 NORTH GARDEN LANE, LAKE ZURICH, IL 60047</td>
										</tr>
										<tr>
											<td>2</td>
											<td>TIPPE, ROBERT, J</td>
											<td>3818 W. 214TH STREET, MATTERSON , IL 60443</td>
										</tr>
										<tr>
											<td>3</td>
											<td>KAHN, JONATHAN</td>
											<td>1108 CHEYENNE DRIVE, CROWN POINT, IN 46307</td>
										</tr>
										<tr>
											<td>4</td>
											<td>FLYNN, THOMAS P</td>
											<td>562 RIDGELAWM ROAD, CROWN POINT, IN 46307</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 15/26</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>N/A</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/118,949</td>
									<td>1999-02-05</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/254797-an-apparatus-for-automated-production-of-a-foreign-language-speech-model-for-a-speech-recognition-program by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:17:08 GMT -->
</html>
