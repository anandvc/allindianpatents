<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/220099-a-microprocessor-controlled-intuitive-gesture-based-graphical-user-interfaced-electronic-communication-device by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:07:26 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 220099:&quot;A MICROPROCESSOR CONTROLLED INTUITIVE GESTURE-BASED GRAPHICAL USER INTERFACED ELECTRONIC COMMUNICATION DEVICE&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;A MICROPROCESSOR CONTROLLED INTUITIVE GESTURE-BASED GRAPHICAL USER INTERFACED ELECTRONIC COMMUNICATION DEVICE&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>This invention concerns a microprocessor-controlled intuitive gesture-based graphical user interfaced electronic communication device comprising: (A) gesture-based graphical user interface enabled touch-sensitive screen for displaying at least one gesture-supported screen object; and (B) receiving the user input corresponding to a selection of the said screen object and evaluating the said user input as either corresponding to gesture selection; and (C) providing at least one user feedback acknowledging the said gesture selection; and determining if the said user input is said function call; and if the user input is the said function call performing a function; and if the user input is not the said function call, returning to the step of automatically presenting on the screen a said directional palette (step C) 1)).</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>RAPfflCA<br>
Field of the Invention<br>
This invention relates generally to user interfaces, and more particularly to graphical user interfaces that allow gestures.<br>
Background of the Invention<br>
Generally, a graphical user interface uses linear menus, screen buttons, and a pointing device, such as a mouse, joystick, or touch-pad, to control an electronic machine such as a computer or computer-controlled appliance. The pointing device selects an area of the screen to indicate a desired function and then a selection button, such as a mouse button, commands implementation of that function. In order for a user to know what functions are available, the linear menus and screen buttons explicitly state or iconically show their functionality on a continuous basis, which reduces the amount of display space available to present other information to the user. To minimize the screen space taken by the linear menus and screen buttons, pull-down or other sub-menus may list collateral choices and present additional functions upon selection of a main menu item.<br>
Several pen- or stylus-based electronic products have adopted "gesture" user interfaces as shortcuts for performing frequently-used functions. Gestures, however, are available only as an addition to conventional linear menu and screen button graphical user interfaces, thus the problem of reduced screen space remains in both types of user interfaces. Through simple, learned motions, gesture user interfaces allow quick access to functions. For example, instead of using a pointing device to select characters to be deleted and depressing a "delete" button, a user could simply strike through words (from left to right) using the stylus and they would be deleted Unfortunately, most of the gestures available are not nearly as intuitive as this deletion example. Also, the gesture short-<br><br>
cuts are radically different from the linear menu and screen burton methods of achieving the same function.<br>
Thus, these gesture user interfaces are meant to be used only by expert users. Novice users have no way of easily ascertaining what gestures are available. The novice user must first go through a process of learning available gestures and their functionality from user documentation or on-screen "help" documentation and then memorizing these gestures. If a gesture is forgotten, the user must make a concerted effort to return to the documentation to relearn the gesture and its function. Consequently, there is a need for an easy-to-learn gesture-based user interface that allows users to quickly access frequently-used functions on an electronic machine and yet avoid the inconvenience of documentation. Also there is a demand for a gesture-hased user interface that reduces the need for linear menus and screen buttons which consume display space.Accordingly, the present invention relates to a wireless electronic communication device characterised by:<br>
a touch-sensitive screen having at least one screen object; and<br>
a pointing device for selecting a first screen object, wherein the pointing device selects the first screen object using either a manual selection or a gesture selection.<br>
BRIEF DESCRIPTION OF THE ACCOMPANYING DRAWINGS<br>
FIG. 1 shows a diagram of an intuitive gesture-based graphical user interface according to a preferred embodiment as implemented on an electronic device.<br>
FIG. 2 shows details of the screen of the electronic device shown in FIG. 1 according to a preferred embodiment.<br>
FIG. 3 shows details of the screen shown in FIG. 2 when a user performs a manual selection of a screen object according to a preferred embodiment.<br>
FIG. 4 shows details of the screen shown in FIG. 2 when a<br>
r<br>
directional palette appears according to a preferred embodiment.<br>
FIG. 5 shows details of the screen shown in FIG. 2 when a user performs a manual selection of a palette button on the directional palette shown in FIG. 4 according to a preferred embodiment.<br>
FIG. 6 shows details of the screen shown in FIG. 2 when a user performs a gesture selection of a screen object and a function according to a preferred embodiment.<br>
FIG. 7 shows details of the screen shown in FIG. 2 when a user performs a manual selection of a palette button on the directional palette shown in FIG. 4 according to another preferred embodiment.<br>
FIG. 8 shows details of the screen shown in FIG. 2 when a user performs a gesture selection of a screen object according to another preferred embodiment.<br>
FIG. 9 shows a flow chart diagram of the operation of an intuitive gesture-based graphical user interface according to a preferred embodiment.<br>
Detailed Description of the Preferred Embodiment<br>
An intuitive gesture-based graphical user interface allows users to learn quickly gestures as a method of accessing frequently-used functions of a computer or computer-based appliance. The intuitive gesture-based graphical user interface also reduces the need for screen buttons that decrease available display space. Preferably, the intuitive gesture-based graphical user interface is implemented using a pen- or stylus-based device, and the user performs a manual selection of a screen item by tapping, or drawing a point, on a touch-sensitive screen using the stylus. After a single tap, the device would present a directional palette with palette buttons having different compass directions relative to the center of the directional palette. Each palette button explicitly displays a unique identifier representing a function or other item of information. By making a manual section of a desired palette button, such as a second tap, the novice user learns available functions of the device and their corresponding gestures. As the user grows more familiar with the electronic device, a user may tap twice in succession on a screen object or draw a line in an appropriate compass direction originating from a selected screen object,<br>
and the device would process this gesture selection appropriately without the directional palette appearing.<br>
FIG. 1 shows a diagram of an intuitive gesture-based graphical user interface according to a preferred embodiment as implemented on an electronic device. Electronic device 100 is preferably a computer or other microprocessor-based appliance. In this diagram, electronic device 100 is shown as an integrated wireless communication device with radiotelephone, electronic mail, and facsimile features; however, electronic device 100 could be a desktop computer or a portable computer with a MODEM (modulator/demodulator), a tele vision/VCR combination, a facsimile machine, a photocopy machine, a personal digital assistant, or the like. Electronic device 100 has a touch-sensitive screen 150 and push buttons 120. A pointing device 190 such as a pen or stylus interacts with the screen 150 to select areas of the screen. Of course, another pointing device, such as a mouse, joystick, touch-pad, or even human finger, could be substituted for a pen or stylus.<br>
FIG. 2 shows details of the screen of the electronic device shown in FIG. 1 according to a preferred embodiment. This screen displays one or more screen objects 210, 220, 230, such as a list of received messages in an electronic mailbox, as directed by the microprocessor. A screen object is a graphical representation of data. /Although an electronic mail software<br>
———"		   ' '	**-	'<br>
program is used in this example, many other programs may be substituted with other types of screen objects, such as an address book program with an alphabet index as screen objects, a scheduling program with calendar dates as screen objects, a memo or to-do program with list items as screen objects, an electronic game with directional buttons as screen objects, or electronic programs suitable for functions such as photocopying or facsimile transmission.<br>
There are two methods of selecting a screen object and an associated function. A manual selection of a screen object, such as a tap or a drawing of a point, automatically brings up a directional palette with palette buttons having explicit identifiers to indicate functions corresponding to the manually-selected screen object. A manual selection of a palette<br>
button, such as a second tap on the desired palette button, directs execution of the identified function. On the other hand, a gesture selection allows the designation of both a screen object and a function simultaneously. A gesture selection, such as drawing a line starting on a screen object and going in a certain direction, indicates the desired screen object from the starting point of the line and the desired function from the direction the line is drawn.<br>
In order to manually select a screen object, a novice user would intuitively tap, or draw a point, on a screen object to select an electronic mail message. FIG. 3 shows details of the screen shown in FIG. 2 when a user performs a manual selection of a screen object according to a preferred embodiment. The electronic device preferably provides visual feedback to the user by highlighting the tapped area 310. Audio feedback may optionally be provided to the user. After a manual selection such as a single tap or a drawing of a point, the electronic device would automatically present the user with a directional palette preferably centering on the same area of the screen where the tap or point occurred.<br>
FIG. 4 shows details of the screen shown in FIG. 2 when a directional palette appears according to a preferred embodiment. Directional palette 450 presents the user with a number of palette buttons. Although the directional palette 450 shown presents five palette buttons to the user, directional palettes may present more or less buttons as needed. The shape, size, and configuration of a directional palette depends on which screen object is selected. Each palette button has a unique direction relative to the center of the directional palette, which is preferably also the approximate location of the first tap; these directions may be referred to like compass directions. Each palette button also displays a function identifier stating or showing what function can be accessed by activating the palette button.<br>
In this electronic mail example, a screen object represents a received mail message. Thus the directional palette 450 that appears when a received mail message screen object is manually selected has a palette button 451 to the north which accesses a cancel function. An alternate method of accessing this cancel function is to tap on a portion of the screen<br>
outside of the palette. The cancel function removes the present palette from the screen or otherwise undoes the most recent action. To the east, the user may select palette button 452 to forward the message to another recipient. To the south, palette button 453 directs deletion of the message, and to the west, palette button 454 prompts the user to reply to the sender of the message. Finally, center palette button 455 allows the user to read the message. Tapping on any of the palette buttons calls the function that is identified on the palette button.<br>
In order to aid the user in learning and memorizing the available function options and their related gestures, the directional palette preferably presents function options as logically as possible. In this example, "reply" is to the west while "forward" is to the east. Thus, the user can intuit that a "reply" somehow travels backwards and a "forward" somehow travels forward.<br>
FIG. 5 shows details of the screen shown in FIG. 2 when a user performs a manual selection of a palette button on the directional palette shown in FIG. 4 according to a preferred embodiment. A manual selection of a palette button is implemented using a second tap. After the center palette button 455 is tapped, the microprocessor temporarily highlights the palette button, as if it blinks, and the previous screen as shown in FIG. 2 is replaced by the selected electronic message screen. Thus, the second tap allows the user to read the message selected by the first tap. In this manner, the directional palette 450 teaches the user to "double-tap" to read a selected message.<br>
Continuing with the directional palette shown in FIG. 5, a second tap to the left of the first tap, on the "reply" palette button, will bring up a reply screen. The corresponding gesture would be a line drawn from right to left as shown in FIG. 6. FIG. 6 shows details of the screen shown in FIG. 2 when a user performs a gesture selection of a screen object and a function according to a preferred embodiment. The gesture, a line drawn from right to left on screen object 220, is tracked using line 620 on the screen. Once the reply gesture is recognized by the microprocessor, the reply palette button 454 appears, blinks to provide visual feedback, and a reply screen appears<br>
exactly as the reply screen would appear if the user had manually selected the reply palette button from a directional palette. Note that the mental model for accessing a function is the same both for novice users and expert users. Thus, there is no longer a need for separate, redundant screen buttons for novice users.<br>
Once a user has learned a gesture, the user no longer has to wait for a directional palette to appear in order to access available functions. Thus, the intuitive gesture-based graphical user interface teaches a novice user to gesture as a primary method for interacting with the device.  By combining the directional palette with a gesture-based graphical user interface, the intuitive gesture-based graphical user interface explicitly depicts program functionality. Because, however, the user may interact with the device before a directional palette is presented, the intuitive gesture-based graphical user interface promotes more efficient use of the screen by providing the user with explicit function options only if they are needed. Furthermore, by providing the same mental model to both novice and expert users, the novice can more easily graduate to expert user status.<br>
FIG. 7 shows details of the screen shown in FIG. 2 when a user performs a manual selection of a palette button on the directional palette shown in FIG. 4 according to another preferred embodiment. Directional palettes may be "stacked" to create any number of gestures and accessible functions. A tap on the west palette button 454, which is labeled "reply," brings up a sub-palette 750. Sub-palette 750 provides only two palette buttons, a north-west palette button 751 to reply to the sender of the message and a south-west palette button 753 to select from a list of possible recipients. Sub-palettes 750 may be stacked indefinitely and contain any number of palette buttons.<br>
FIG. 8 shows details of the screen shown in FIG. 2 when a user performs a gesture selection of a screen object according to another preferred embodiment. In a stacked palette embodiment, a gesture selection may bring a user to a subpalette or straight to accessing a function. In this embodiment, gesture line 620 is made by a user and subpalette 750 appears. If, however, the gesture had continued with a<br>
north-west bend, the reply palette button 454 would first appear and blink, and then palette button 751 would appear and blink, and a reply screen addressed to the sender of the message, John, would replace the present screen. Thus, both a palette 450 and a subpalette 750 may be bypassed through the use of gestures.<br>
FIG. 9 shows a flow chart diagram of the operation of an intuitive gesture-based graphical user interface according to a preferred embodiment. After the start step 901, the electronic device displays one or more objects as shown in step 910. The device then waits for a user input as shown in step 920. When a user input is received, a portion of the screen is highlighted to provide visual feedback to the user. The machine could also provide audio feedback to the user. The device evaluates whether the received user input was a manual selection or a gesture selection of a screen object as shown in step 930. If the user input is determined to be a manual selection (e.g., a tap), shown by branch 933, the device displays a directional palette as shown in step 940. Again, the devices waits for a user input and highlights user input as shown in step 950 and evaluates whether the received user input was a tap or a gesture as shown in step 955.<br>
If the next user input is a tap, the microprocessor evaluates whether the user input was a function call as shown in step 960. If the next user input was a gesture, the selected palette button is displayed as visual feedback as shown in step 970 and then the microprocessor evaluates whether the user input was a function call as shown in step 960. Again, audio feedback may also be used to acknowledge the gesture selection.<br>
If the user input was not a function call as evaluated in step 960, the device returns to step 940 and displays a next directional palette (i.e., subpalette). Loop 963 allows a stacked palette effect and may be performed as many times as necessary until a user input is a function call as shown by branch 965. Once a function call is selected, the device blinks the selected palette button and the microprocessor performs the function as shown in step 980.<br>
Returning to step 930, if the initial user input is determined to be a gesture selection, as shown in branch 935, the device displays the palette button selected by the gesture selection in step 970 to provide visual feedback to the user. Then, the device goes directly to step 960 to determine whether the gesture was a function call. Once a selected function is performed, the end step 990 occurs and the device may return to the start step 901.<br>
As an option, the timing of the appearance of the directional palette may vary to motivate a novice user to use gesture selections rather than simply wait for the directional palette to appear before making a second tap. For instance, when a device is first used, the directional palette appears quickly in reaction to a single tap. The quick appearance of the directional palette allows the user to learn the available functions and their corresponding gestures at the outset. As the user learns to make a second tap in certain directions to activate certain functions, the directional palette takes longer and longer to appear after the first tap has occurred. Thus, an impatient user will be inclined to make a second tap before the directional palette appears. Once the user either taps twice or draws a line before a directional palette appears, the user has employed a gesture selection but without making any concerted effort to memorize the gesture. If a user forgets a gesture, the user merely needs to wait until the directional palette appears on the screen after the first tap to relearn the gesture and related function.<br>
Thus, an intuitive gesture-based graphical user interface quickly and easily teaches users gestures for interacting with a microprocessor-controlled electronic device. While specific components and functions of the intuitive gesture-based graphical user interface are described above, fewer or additional functions could be employed by one skilled in the art within the true spirit and scope of the present invention. The invention should be limited only by the appended claims.<br><br><br><br>
WE CLAIM:<br>
1.       A microprocessor-controlled intuitive gesture-based graphical user interfaced electronic communication device comprising:<br>
(A)	gesture-based graphical user interface enabled touch-sensitive<br>
screen for displaying at least one gesture-supported screen object; and<br>
(B)	receiving the user input corresponding to a selection of the said<br>
screen object and evaluating the said user input corresponding to gesture<br>
selection; and<br>
(C)      providing at least one user feedback acknowledging the said gesture selection;<br>
and determining if the said user input is said function call;<br>
and if the user input is the said function call performing a function;<br>
and if the user input is not the said function call, returning to the step of automatically presenting on the screen a said directional palette (step C) 1)).<br>
2.	The  electronic  device  as  claimed  in  claim   1,  wherein  the  user<br>
feedback is a visual feedback.<br>
3.	The electronic device as claimed in claim 1, wherein user feedback is<br>
a audio feedback.<br>
4.	The electronic device as claimed in claim 1, wherein the said gesture<br>
selection is drawing of a line.<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1hYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">2311-del-1997-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1ERUwtMTk5Ny1Bc3NpZ25tZW50LSgyMy0xMi0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">2311-DEL-1997-Assignment-(23-12-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1jbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1ERUwtMTk5Ny1Db3JyZXNwb25kZW5jZSBPdGhlcnMtKDIzLTEyLTIwMTEpLnBkZg==" target="_blank" style="word-wrap:break-word;">2311-DEL-1997-Correspondence Others-(23-12-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1ERUwtMTk5Ny1Db3JyZXNwb25kZW5jZS1PdGhlcnMtKDE2LTAzLTIwMTEpLnBkZg==" target="_blank" style="word-wrap:break-word;">2311-DEL-1997-Correspondence-Others-(16-03-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1jb3JyZXNwb25kZW5jZS1vdGhlcnMucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1jb3JyZXNwb25kZW5jZS1wby5wZGY=" target="_blank" style="word-wrap:break-word;">2311-del-1997-correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1kZXNjcmlwdGlvbiAoY29tcGxldGUpLnBkZg==" target="_blank" style="word-wrap:break-word;">2311-del-1997-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1kcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">2311-del-1997-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1mb3JtLTEucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1mb3JtLTEzLnBkZg==" target="_blank" style="word-wrap:break-word;">2311-del-1997-form-13.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1ERUwtMTk5Ny1Gb3JtLTE2LSgyMy0xMi0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">2311-DEL-1997-Form-16-(23-12-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1mb3JtLTE5LnBkZg==" target="_blank" style="word-wrap:break-word;">2311-del-1997-form-19.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1mb3JtLTIucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1ERUwtMTk5Ny1Gb3JtLTI3LSgxNi0wMy0yMDExKS5wZGY=" target="_blank" style="word-wrap:break-word;">2311-DEL-1997-Form-27-(16-03-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1mb3JtLTQucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-form-4.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1mb3JtLTYucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-form-6.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1ERUwtMTk5Ny1HUEEtKDIzLTEyLTIwMTEpLnBkZg==" target="_blank" style="word-wrap:break-word;">2311-DEL-1997-GPA-(23-12-2011).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1ncGEucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-gpa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1wZXRpdGlvbi0xMzcucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-petition-137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMxMS1kZWwtMTk5Ny1wZXRpdGlvbi0xMzgucGRm" target="_blank" style="word-wrap:break-word;">2311-del-1997-petition-138.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="220098-a-solar-based-semi-underground-greenhouse-structure.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="220100-superabsorbent-composite-sheet-and-method-for-preparing-the-same.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>220099</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>2311/DEL/1997</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>28/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>11-Jul-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>15-May-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>18-Aug-1997</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>MOTOROLA, INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>SUTHIRUG NUM PISUTHA-ARNOND</td>
											<td></td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06F 15/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>N/A</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>08/711,236</td>
									<td>1996-09-09</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/220099-a-microprocessor-controlled-intuitive-gesture-based-graphical-user-interfaced-electronic-communication-device by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:07:27 GMT -->
</html>
