<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/217308-method-and-system-for-translingual-visual-speech-synthesis by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:36:41 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 217308:&quot;METHOD AND SYSTEM FOR TRANSLINGUAL VISUAL SPEECH SYNTHESIS.&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;METHOD AND SYSTEM FOR TRANSLINGUAL VISUAL SPEECH SYNTHESIS.&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A computer implemented method in a language independent system generates audio-driven facial animation given the speech recognition means for just one language. The method is based on the recognition that once alignment is generated, the mapping and the animation hardly have any [language dependency in mem. Translingual visual speech synthesis can be achieved if the first step of alignment generation can be made speech independent. Given a speech recognition system for a base language, the method synthesizes video with speech of any novel language as the input. The present invention also provides a system for carrying out the above method.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Field of the Invention:<br>
The present invention generally relates to visual speech synthesis and more particularly for a system for translingual synthesis of visual speech from a given audio signal in a first language with the help of speech recognition means in a second language and a method of implementing audio driven facial animation system in any language using a speech recognition system and*visemes of-1 different language.<br>
Background Description:<br>
Audio-driven facial animation is an interesting and evolving technique in the field of human-computer interaction. The realization of a natural and friendly interface is very important in human-computer interface, Speech recognition and computer lip-reading has been developed as a means of input for information interaction with the machine. It is also important to provide a natural and friendly means to render the information. Visual speech synthesis is very important in mis respect as it can provide various kinds of animated computer agents, which look very realistic. Furthermore, it can also be used for distance learning applications where it can obviate the transmission of video. It can also be a useful tool for hearing impaired people to compensate for lack of auditory information.<br>
Techniques exist for synthesizing the speech given the text as input to the system. These text to speech synthesizers work by producing a phonetic alignment of the text to be<br>
pronounced and then by generating the smooth transitions in corresponding phones to get the desired sentence. See R. E. Donovan and E. M. Eide, "The IBM Trainable Speech Synthesis System", International Conference on Speech and Language Processing", 1998. Recent work in bimodal speech recognition uses the fact that the audio and corresponding video signals have dependencies which can be exploited to improve the speech recognition accuracy. See T. Chen and R. R. Rao, "Audio-Visual Integration in Multimodal Communication", Proceedings of the IEEE, vol. 86, no. 5, May 1998, pp. 837-852, and E. D. Petajan, B. Bischolf, D. Bodolf, and N. M. Brooke, "An Improved Automatic Lipreading System to Enhance Speech Recognition", Proc. OHI, 1988, pp. 19-25. A viseme-to-phoneme mapping is required to convert the score from video space to the audio spacer- (Jskig-sach a mappingand the text-to-speech synthesis, a text-to-video synthesizer can be built. This synthesis or facial animation can be driven by text or speech audio, as the application may desire. In the later case, the phonetic alignment is generated from the audio with the help of the true word string representing the spoken word.<br>
Researchers have tried various ways of synthesizing visual speech from a given audio signal. In the simplest method, vector quantization is used to divide the acoustic vector space into a number of subspaces (generally equal to the number of phones) and the centroid of each subspace is mapped to a distinct viseme. During the synthesis time, the nearest centroid is found for the incoming audio vector and the corresponding viseme is chosen as the output.<br>
In F. Lavagetto, Arzarello and M. Caranzano, "Iipreadable Frame Automation Driven by Speech Parameters", International Symposium on Speech, Image Processing and Neural Networks, 1994, ISSIPNN, the authors have used Hidden Markov Models (HMMs) which are trained using both audio and video features as follows. During the training period, viterbi alignment is used to get the most likely HMM state sequence for a given speech. Now, for a given HMM state, all the corresponding image frames are chosen and an average of their visual parameters is assigned to the HMM state. At the time of synthesis, input speech is aligned to the most likely HMM sequence using the viterbi decoding. Image parameters corresponding to the most likely HMM state sequence are retrieved, and this visual parameter sequence is animated with proper smoothing.<br>
Recently, co-pending patent application Serial No. 09/384,763 describes a novel way of<br>
generating the visemic alignments from an audio signal, which makes use of viseme based HMM. In this approach, all the audio vectors corresponding to a given viseme are merged into a single class. Now, this viseme based audio data is used to train viseme based audio HMMs. During the synthesis time, input speech is aligned with the viseme based HMM state sequence. Now, the image parameters corresponding to these viseme based HMM state sequences are animated with the required smoothing. See also T. Ezzai and T. Poggio, "Miketalk: A Talking Facial Display Based on Morphing Visemes", Proceedings of IEEE Computer Animation '98, Philadelphia, PA, June 1998, pp. 96-102.<br>
All of the above approaches require training of a speech recognition system, which is used for generating alignment of the input speech needed for synthesis. Further, these approaches require a speech recognition system in the language in which audio is provided to get the time alignment for the phonetic sequence of the audio signal. However, building a speech recognition system is a very tedious and time-consuming task.<br>
OBJECT AND SUMMARY OF THE INVENTION:<br>
It is therefore an object of the present invention to provide a novel scheme to implement a language independent system for audio-driven facial animation given the speech recognition system for just one language; e.g., English. The same method can also be used for text to audiovisual speech synthesis.<br>
The invention is based on the recognition that once the alignment is generated, the mapping and the animation hardly have any language dependency in them. Translingual visual speech synthesis can be achieved if the first step of alignment generation can be made speech independent. In the following, we propose a method.to perform tt^uislinguaLvisual speech-synthesis; that is, given a speech recognition system for one language (the base language), the invention provides a method of synthesizing video with speech of any other language (the novel language) as the input<br>
The invention further provides a system for translingual synthesis of visual speech from a given audio signal in a first language with the help of speech recognition means in a second language, comprising:<br>
means for receiving input audio and text of the first language; means for generating a phonetic alignment based on best phone boundaries using the speech recognition system of the second language and its own set of phones and means for mapping to convert the phones from the second language to the phones in the first language so as to get an effective alignment in the phone set of the first language;<br>
means for executing a phone to viseme jnapping to get a corresponding visemic alignment which generates a sequence of visemes which are to be animatedlo get a desired video; and<br>
means for animating the sequence qf viseme images to get a desired video synthesized output aligned with the input audio signals of the first language. The said means for executing phone to viseme mapping is a viseme database in the<br>
second language.<br>
The said means for executing phone to viseme mapping is a viseme database in the first<br>
language.<br>
The instant invention further provides a method of translingual synthesis of visual<br>
speech from a given audio signal in a first language with the help of a speech recognition<br>
means in a second language, comprising the steps of:<br>
receiving input audio and text of the first language;<br>
generating a phonetic alignment based on best phone boundaries using the<br>
speech recognition system of the second language and its own set of phones and<br>
mapping to convert the phones from the second language to the phones in the<br>
first language so as to get an effective alignment in the phone set of the first<br>
language;<br>
executing a phone to viseme mapping to get a corresponding visemic alignment<br>
which generates a sequence of visemes which are to be animated to get a<br>
desired video; and<br>
animating the sequence of viseme images to get a desired video synthesized<br>
output aligned with the input audio signals of the first language.<br>
The step of executing phone to viseme mapping is performed using a viseme database in the second language.<br>
The step of executing phone to viseme mapping is performed using a viseme database-in the first language.<br>
The present invention further provides a computer implemented system for implementing audio driven facial animation device in a first language, referred to as the novel language, using a speech recognition means of a second language, referred to as the base language, the system comprising:<br>
means for determining whether a correspondence exists between an audio speech signal of the novel language and a phone of the base language; and means for writing a word of the novel language into a base language database and means for adding it to a new vocabulary of a speech recognition system of the base language.<br>
If mere is no correspondence between audio data of the novel language and a phoneme of the base language, further comprising means for finding a closest phone of the base language which best matches that of the novel language.<br>
The above computer implemented system of implementing audio driven facial animation device further comprises means for using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding phonetic word of the base language vocabulary.<br>
The above computer implemented system of implementing audio driven facial animation device further comprises means for using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary.<br>
The above computer implemented system of implementing audio driven facial animation device further comprises means for using the time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary to drive images in video animation for generating an animated video in the facial animation system in the first language.<br>
The instant invention also provides a computer implemented method of implementing audio driven facial animation system in a first language, referred to as the novel language, using a speech recognition system of a second language, referred to as the base language, the method comprising the steps of:<br>
determining whether a correspondence exists between an audio speech signal of the novel language and a phone of the base language; and writing a word of the novel language into a base language database and adding it to a new vocabulary of a speech recognition system of the base language.<br>
If mere is no correspondence between audio data of the novel language and a phoneme of the base language, further comprising the step of finding a closest phone of the base language which best matches that of the novel language.<br>
The phonetically closest phone is chosen.<br>
The visemicalry closest phone is chosen.<br>
The above computer implemented method of implementing audio driven facial animation system further comprises the step of using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding phonetic word of the base language vocabulary.<br>
The above computer implemented method of implementing audio driven facial animation system further comprises the step of using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary.<br>
The above computer implemented method of implementing audio driven facial animation system further comprises the step of using the time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary to drive images in video animation for generating an animated video in the facial animation system in the first language.<br>
BRIEF DESCRIPTION OF THE ACCOMPANYING DRAWINGS<br>
The foregoing and other objects, aspects and advantages will be better understood from the following detailed description of a preferred embodiment of the invention with reference to the drawings, in which:<br>
Figure 1 is a block diagram showing the animation system, which has the viseme database of the basic language;<br>
Figure 2 is a block diagram showing the animation system which has the viseme database of the novel language; and<br>
Figure 3 is a flow diagram showing the process of creating the vocabulary, which has the words in the novel language using the base forms of the base language.<br>
DETAILED    DESCRIPTION   OF    PREFERRED    EMBODIMENTS    OF    THE INVENTION:<br>
In order to understand the translingual synthesis of the present invention, the steps required to animate the sequence are first presented:<br>
1.	From the given input audio and the text truth, we generate the phonetic alignment. This requires a speech recognition engine which could understand the phonetic base forms of the text. This would work fine if the input audio is in the same language as was the language used for training the recognition system.<br>
2.	If the language in which the video is to be synthesized is a different language, then the phone set of the different language may be other than the trained language. But the alignment generation system generates the alignments based on the best phone boundaries using its own set of phonemes. Then a mapping is required which can convert the phones from one language to the phones in the other language so as to get an effective alignment in the phone set of the novel language.<br>
3.	A phone to viseme mapping can then be used to get the corresponding visemic alignment which generates the sequence of visemes which are to be animated to get the desired video.<br>
4.        Animating the sequence of viseme images to get the desired video synthesized output aligned with the input audio signals.<br>
The present invention provides a new approach to synthesize visual speech from a given audio signal in any language with the help of a speech recognition system in one language. From here onwards, we refer the language of training the speech recognition system as the base language and the language in which the video is to be synthesized as the novel language. In the illustrations, Hindi has been chosen as the novel language and English, as base language.<br>
If a word in the novel language is presented to the alignment generator, then the alignment generator will not be able to generate the alignments for such a word as the word is not in the phonetic vocabulary of the training system. Moreover, the phonetic spelling of a word in novel language may not be represented completely by the phonetic set of the base language. We present below the technique to overcome these problems to finally have a language independent alignment generation system. This system will have the trained alignment generation system and the viseme images for the base language but it can be made to work to generate the animation for audio input in any language.<br>
Vocabulary Adaptation Layer<br>
In order to generate alignments for words in the novel language, first a phonetic vocabulary of this language is created wherein words are represented in the phonetic base forms using the phone set of the novel language. Since the recognition system is trained on the phone set of the base language, the vocabulary needs to be modified so that the words now represent the base forms in the base language phone set. Such a modification is made possible by the Vocabulary Modification Layer. This layer works by using a mapping from the phone set of one language to the other language. For illustration, a mapping from the Hindi phones to the English phones is as shown in Table 1.<br>
Table 1. Phoneme Mapping from Hindi to English<br><br>
(Table Removed)<br>
In Table 1 is presented an example of mapping phones of the Hindi language to the English language phone set. As is seen, not all the English phones are used by the novel language. Also, there exists an exact mapping<br>
for a large number of phones. These arc shown by triple asterisks (***) on that row. A double asterisks (**) on the row implies that the mapping is not exact but it is the acoustically closest map. A single asterisk (*) in the mapping shows that the novel language phone has been approximated by a string of more than one phone (or phoneme) from the English language for acoustic similarity.<br>
There are three possible cases:<br>
1.	The word in the novel language can be represented by the phones in the base language;<br>
for such words, the base forms can be simply written using the base language phone<br>
set.<br>
2.	The word in novel language cannot be represented by the base language phone set; then the word is written using the novel language phone set and the mapping as in Table 1 is used to convert the base form, in the base language.<br>
3.	A phone in the base language never appears by the words in the novel language; in such a case, that particular phone in the base language is redundant and is left as "don't care".<br>
Since the aim of mapping the phone set is to generate the best phone boundaries through acoustic alignment, the mapping is based on similar-sounding phones; i.e., if there is no exactly similar phone in the base language which can be associated with the phone in the novel language, then that base language phone is chosen which is acoustically similar. Bom, however, may map to a different viseme.<br>
The above vocabulary modification layer helps in generating the base language alignments of the novel language audio. Next, we describe how we extract the base language visemic alignments.<br>
Generation of Visemic Alignments<br>
Since the system has to work for any novel language using the trained alignment generator, phone to viseme mapping and the viseme set in the base language, visemic alignment cannot be simply generated from the phonetic alignment as generated previously. As was shown above, the vocabulary modification layer was built on the mapping based on acoustically similar phones. However, this mapping may distort the visemic alignment as it did not take into consideration the visemes corresponding to each such phone. So an additional vocabulary which represents the words of the novel language in phone set of base language is created. This does not use the mapping in Table 1. It uses a mapping based on the visemic similarity of the two phones in the same row. Using this additional vocabulary, the base language alignments and the base language phone-to-viseme mapping, we get the visemic alignments. This visemic alignment is used to generate the animated video sequence. The mapping is not one-to-one. So<br>
a single phone in base language may represent more than one phone in the novel language. This, however, creates no confusion as the Vocabulary Modification Layer outputs the alignment in the novel language after taking into account the many-to-one mapping.<br>
Also, since the application uses the visemic alignment for the purpose of animation, a morphing is done from one viseme to another. So due to non-accurate mapping of phones which are represented by ** and * in Table 1, the generated alignment may not represent the exact phone boundaries. This however is not observed in tile animated video as the viseme is always in transition during these boundaries. A smooth and continuous video is thus generated which does not reflect any inaccurate phone boundaries.<br>
Description of the Drawings<br>
Referring now to the drawings, and more particularly to Figure 1, there is shown a block diagram of the animation system, which has the viseme database in the base language. The phonetic alignment is first generated in the base language using the base language modified vocabulary The audio or text in the novel language is input to the phonetic alignment generator<br>
101, which receives the corresponding phonetic word VocabPB in the base language from the phonetic vocabulary modifier 102. The output of the phonetic alignment generator 101 is<br>
Alignment * which<br>
is men converted in the base language visemic alignment by using the<br>
visemic alignment generator 103. This visemic alignment generator uses the base language<br>
visemic vocabulary Vocab B which is formed by incorporating the corrections using the visemic vocabulary modifier 104. This visemic vocabulary modifier uses a visemically similar<br>
mapping from base language to the novel language to generate the  * ocab   B    Then the<br>
the video animation 105 for generating the animated video.<br>
In an alternative embodiment of the invention, if the viseme set is available for the novel language, then the lower layer can be modified to directly give the visemic alignment using the phone-to-viseme mapping in that language. Here the phonetic alignment generated in<br>
the base language is converted to the novel language by using the corresponding vocabulary entries in the two languages. Then the phoneme to viseme mapping of the novel language is applied. Note that the visemic alignment so generated is in the novel language and mis was desired as the visemes are available in that language and not in the base language.<br>
Figure 2 is a block diagram of the animation system, which has the viseme database in the novel language. As in Figure 1, the phonetic alignment is first generated in the base language using the base language modified vocabulary. The audio or text in the novel language is input to the phonetic alignment generator 201, which receives the corresponding phonetic<br>
word v ocab B m the base language from the phonetic vocabulary modifier 202. The output of<br>
the phonetic alignment generator 201 is AiWraeni B which is then converted in the visemic alignment generator 203. The base language phonetic alignment is converted to the novel<br>
language visemic alignment by using the novel language vocabulary Vocab N m addition to the base language visemic vocabulary   Vocab    B     xhe novel language visemic  alignment<br>
A%nment ^ ^ ^<br>
to drive the images (representing the novel language visemes) in video<br>
animation 204 for generating the animated video.<br>
Figure 3 is a flow diagram of the process used to create the vocabulary which has the<br>
novel language words being represented in the transforms suing the base language phoneme<br>
set. Such a vocabulary is used to generate the phonetic alignments. For every word in the novel<br>
language, langjv, a determination is made in decision block 301 to determine if it exists in the<br>
base language, \angs- If it exits, the particular base forms are chosen in function block 302 that<br>
the word is in. The word is then copied in the base language vocabulary in function block 303.<br>
On the other hand, if the word does not exist in the base language, as determined in decision<br>
block 301, the base form representation is required before it can be written to the vocabulary.<br>
A determination is made in decision block 304 as to whether the vocabulary is to be based on<br>
the phonetic or the visemic similarity of the two languages. If phonetic similarity is to be used, a<br>
corresponding transform using the phonetic similar mapping is chosen in function block 305<br>
from the base language, langs. On the other hand, if visemic similarity is to be used, a visemic<br>
similar phoneme set is then chosen in function block 306 from the base language to make a transition of the novel language word to the base language word. This approximated representation of the word in the base language is written to form the base language vocabulary in function block 303. These phonetic/visemic vocabularies so generated are then used for animation as shown in Figures 1 and 2.<br>
An advantage of using the invention is that one does not need to build a speech recognition engine for the same language in which the visual speech is to be synthesized. Given a speech recognition system for any given language, one can easily and quickly customize the two layers to get a synthesized video in any other language. Moreover, the viseme images can also be of only the language of which the alignment generation system is built, thus obviating the need for every time generating new visemes images for each language. The system also works if the novel language has visemes that are totally different from the visemes of the base language as is shown in the alternative approach. Similarly, for text to audiovisual speech synthesis one does not need text to speech synthesizer in the same language in which tfie synthesis has to be performed.<br>
While the invention has been described in terms of preferred embodiments, those skilled in the art will recognize that the invention can be practiced with modification within the spirit and scope of the appended claims.<br><br><br><br><br><br><br><br>
CLAIM:<br>
1.	A system for translingual synthesis of visual speech from a given audio signal in a first<br>
language with the help of speech recognition means in a second language, comprising:<br>
means for receiving input audio and text of the first language;<br>
means for generating a phonetic alignment based on best phone boundaries<br>
using the speech recognition system of the second language and its own set of<br>
phones and means for mapping to convert the phones from the second language<br>
to the phones in the first language so as to get an effective alignment in the<br>
phone set of the first language;<br>
means for executing a phone to viseme mapping to get a corresponding visemic<br>
alignment which generates a sequence of visemes which are to be animated to<br>
get a desired video; and<br>
means for animating the sequence of viseme images to get a desired video<br>
synthesized output aligned with the input audio signals of the first language.<br>
2.	The system for translingual synthesis of visual speech as claimed in claim 1, wherein the means for executing phone to viseme mapping is a viseme database in the second language.<br>
3.	The system for translingual synthesis of visual speech as claimed in claim 1, wherein the means for executing phone to viseme mapping is a viseme database in the first language.<br>
4.	A method of translingual synthesis of visual speech from a given audio signal in a first language with the help of a speech recognition means in a second language, comprising the steps of:<br>
receiving input audio and text of the first language;<br>
generating a phonetic alignment based on best phone boundaries using the<br>
speech recognition system of the second language and its own set of phones and<br>
mapping to convert the phones from the second language to the phones in me<br>
first language so as to get an effective alignment in the phone set of the first<br>
language;<br>
executing a phone to viseme mapping to get a corresponding visemic alignment<br>
which generates a sequence of visemes which are to be animated to get a<br>
desired video; and<br>
animating the sequence of viseme images to get a desired video synthesized<br>
output aligned with the input audio signals of the first language.<br>
5.	The method of translingual synthesis of visual speech as claimed in claim_4, wherein the step of executing phone to viseme mapping is performed using a viseme database in the second language.<br>
6.	The method of translingual synthesis of visual speech as claimed in claim 4, wherein the step of executing phone to viseme mapping is performed using a viseme database in the first language.<br>
7.	A computer implemented system for implementing audio driven facial animation device in a first language, referred to as the novel language, using a speech recognition means of a second language, referred to as the base language, the system comprising:<br>
means for determining whether a correspondence exists between an audio speech signal of the novel language and a phone of the base language; and means for writing a word of the novel language into a base language database and means for adding it to a new vocabulary of a speech recognition system of the base language.<br>
8.	The computer implemented system as claimed in claim 7, further comprising means for<br>
finding a closest phone of the base language which best matches that of the novel<br>
language.<br>
Y0999-621<br>
9.	The computer implemented system of implementing audio driven facial animation device as claimed in claim 8, wherein the phonetically closest phone is chosen.<br>
10.	The computer implemented system of implementing audio driven facial animation device as claimed in claim 8, wherein the visemicalry closest phone is chosen.<br>
11.	The computer implemented system of implementing audio driven facial animation device as claimedlin claM 8j'furm1erS	forcing the new vocabulary to generate a time alignment of the audio speech signal with a corresponding phonetic word of the base language vocabulary.<br>
12.	The computer implemented system of implementing audio driven facial animation device as claimed in claim 11, further comprising means for using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary.<br><br>
13.	The computer implemented system of implementing audio driven facial animation device as claimed in claim 12, further comprising means for using the time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary to drive images in video animation for generating an animated video in the facial animation system in the first language.<br>
14.	A computer implemented method of implementing audio driven facial animation device in a first language, referred to as the novel language, using a speech recognition means of a second language, referred to as the base language, the method comprising the steps of:<br>
detenriining whether a correspondence exists between an audio speech signal of the novel language and a phone of the base language; and<br>
writing a word of the novel language into a base language database and adding it to a new vocabulary of a speech recognition system of the base language.<br>
15.	The computer implemented method of implementing audio driven facial animation system as claimed in claim 14, wherein if there is no correspondence between audio data of the novel language and a phoneme of the base language, further comprising the step of finding a closest phone of the base language which best matches that of the novel language.<br>
16.	The computer implemented method of implementing audio driven facial animation system as claimed in claim 15, wherein the phonetically closest phone is chosen.<br>
17.	The computer implemented method of implementing audio driven facial animation system as claimed in claim 15, wherein the visemicalry closest phone is chosen.<br>
18.	The computer implemented method of implementing audio driven facial animation system as claimed in claim 15, further comprising the step of using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding phonetic word of the base language vocabulary.<br>
19.	The computer implemented method of implementing audio driven facial animation system as claimed in claim 18, further comprising the step of using the new vocabulary to generate a time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary.<br>
20.	The computer implemented method of implementing audio driven facial animation system as claimed in claim 19, further comprising the step of using the time alignment of the audio speech signal with a corresponding visemic word of the base language vocabulary to drive images in video animation for generating an animated video in the facial animation system in the first language.<br>
21.	A system for translingual synthesis of visual speech from a given audio signal in a first language with the help of speech recognition means in a second language substantially as herein described with reference to and as illustrated in the accompanying drawings.<br>
22.	A method for translingual synthesis of visual speech from a given audio signal in a first language with the help of speech recognition means in a second language substantially as herein described with reference to and as illustrated in the accompanying drawings.<br>
23.	A computer implemented system for implementing audio driven facial animation device in a first language, referred to as the novel language, using a speech recognition means of a second language, referred to as the base language substantially as herein described with reference to and as illustrated in the accompanying drawings.<br>
24.	A computer implemented method for implementing audio driven facial animation device in a first language, referred to as the novel language, using a speech recognition means of a second language, referred to as the base language substantially as herein described with reference to and as illustrated in the accompanying drawings.<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtYWJzdHJhY3QucGRm" target="_blank" style="word-wrap:break-word;">58-del-2001-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtYXNzaWdubWVudC5wZGY=" target="_blank" style="word-wrap:break-word;">58-del-2001-assignment.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtY29tcGxldGUgc3BlY2lmaWNhdGlvbiAoZ3JhbmRlZCkucGRm" target="_blank" style="word-wrap:break-word;">58-del-2001-complete specification (granded).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtY29ycmVzcG9uZGVuY2Utb3RoZXJzLnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtY29ycmVzcG9uZGVuY2UtcG8ucGRm" target="_blank" style="word-wrap:break-word;">58-del-2001-correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZGVzY3JpcHRpb24gKGNvbXBsZXRlKS5wZGY=" target="_blank" style="word-wrap:break-word;">58-del-2001-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZHJhd2luZ3MucGRm" target="_blank" style="word-wrap:break-word;">58-del-2001-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZm9ybS0xLnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZm9ybS0xOS5wZGY=" target="_blank" style="word-wrap:break-word;">58-del-2001-form-19.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZm9ybS0yLnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZm9ybS0zLnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZm9ybS01LnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NTgtZGVsLTIwMDEtZ3BhLnBkZg==" target="_blank" style="word-wrap:break-word;">58-del-2001-gpa.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="217307-method-of-updating-encryption-keys-in-data-communications-system.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="217309-method-of-eliminating-malodours-from-gases.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>217308</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>58/DEL/2001</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>37/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>12-Sep-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>26-Mar-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>23-Jan-2001</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>INTERNATIONAL BUSINESS MACHINE CORPORATION</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>ARMONK, NEW YORK 10504, U.S.A.</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>RAJPUT NITENDRA</td>
											<td>G-9, NARAINA VIHAR, NEW DELHI-110028, INDIA.</td>
										</tr>
										<tr>
											<td>2</td>
											<td>VENKATASUBRAMANIAM L.</td>
											<td>I-344, SAROJINI NAGAR, NEW DELHI, INDIA.</td>
										</tr>
										<tr>
											<td>3</td>
											<td>VERMA ASHISH</td>
											<td>C-4/74, SAFDARJUNG DEVELOPMENT AREA, HAUZ KHAS, NEW DELHI-110016, INDIA.</td>
										</tr>
										<tr>
											<td>4</td>
											<td>FARUQUIE TANVEE AFZAL</td>
											<td>BG 6D, DDA FLATS, MUNIRKA, NEW DELHI, INDIA.</td>
										</tr>
										<tr>
											<td>5</td>
											<td>NETI CHALAPATHY</td>
											<td>235 HIGH RIDGE COURT, YORKTOWN HEIGHTS, NY 1059, U.S.A.</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 21/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>N/A</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>09/494,582</td>
									<td>2000-01-31</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/217308-method-and-system-for-translingual-visual-speech-synthesis by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:36:42 GMT -->
</html>
