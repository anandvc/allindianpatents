<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/218424-method-of-streaming-and-decoding-of-streamed-data by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 12:50:00 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 218424:METHOD OF STREAMING AND DECODING OF STREAMED DATA</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">METHOD OF STREAMING AND DECODING OF STREAMED DATA</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>The present invention relates to an Optimal resilience to errors in packetized streaming 3-D wireframe animation is achieved by partitioning the stream into layers and applying unequal error correction coding to each layer independently to maintain the same overall bitrate. The unequal error protection scheme for each of the layers combined with error concealment at the receiver achieves graceful degradation of streamed animation at higher packet loss rates than approaches that do not account for subjective parameters such as visual smoothness.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>PRIORITY CLAIM<br>
[0001] The present application claims priority to U.S. Provisional Patent Application No. 60/404,410, filed August 20, 2002, the contents of which are incorporated herein by reference.<br>
RELATED APPLICATION<br>
[0002] Non-Provisional Application, entitled "Coding of Animated 3-D Wirefi"ame Models For Internet Streaming Applications: Methods, Systems and Program Products", Serial Number 10/198129, filed July 19, 2002, assigned to the same assignee as that of the present applications and fully incorporated herein by reference.<br>
BACKGROUND OF THE INVENTION<br>
1.	Field of the Invention<br>
[0003] The present invention relates to streaming data and more specifically relates to a system and method of streaming 3-D wirefi-ame animations.<br>
2.	Introduction<br>
[0004] The Internet has rapidly evolved during the past few years from a low-bandwidth, text-only collaboration medium, to a rich, interactive, real-time, audio-visual virtual world. It involves many users, environments and applications, where 3-D animations constitute a driving force. Animated 3-D models enable intuitive and realistic interaction with displayed objects and allow for effects that cannot be achieved with conventional audio-visual animations. Consequently, the current challenge is to integrate animated 3-D geometry as a new data stream in the existing evolving infrastructure of the Internet, in a way that both enhances the existing networked environment and respects its limited resources. Although static 3-D mesh geometry compression has been actively researched in the past decade, very little research has been<br><br>
conducted in compressing dynamic 3-D geometry, which is an extension of static 3-D meshes to the temporal domain.<br>
[0005] The most prevalent representations fyr 3-D static models are polygonal or triangle meshes. These representations allow for approximate models of arbitrary shape and topology within some desired precision or quality. Efficient algorithms and data structures exist to generate, modify, compress, transmit and store such static meshes. Future, non-static, stream types that introduce the time dimension, would require scalable solutions to survive with respect to the network"s limited resources (bandwidth) and characteristics (channel errors).<br>
[0006] The problem of 3-D wireframe animation streaming addressed herein can be stated as follows: Assume (i) a time-dependent 3-D mesh has been scalably compressed in a sequence of wireframe animation frames, (ii) the available transmission rate R is known (or determined with respect to the corresponding TCP-friendly rate), (iii) the channel error characteristics are knovm, and (iv) a fraction C of the available transmission rate (C 
[0007] Most animation coding approaches use objective metrics to achieve a hierarchical coding of static 3-D meshes. What is needed is an animation approach that utilizes a subjective quantity, such as visual smoothness, to provide an improved appearance of animation. Described herein is a 3-D wireframe animation codec and its bitstream content, along with the associated forward error correction (FEC) codes.  The visual distortion metric as well as<br><br>
the unequal error protection (UEP) method and receiver-based concealment method are further explained.<br>
SUMMARY OF THE INVENTION<br>
[0008] The present invention focuses on source and channel coding techniques for error resilient time-dependent 3-D mesh streaming over the Internet that respects network bandwidth and considers the bursty loss nature of the channel. [0009] An exemplary embodiment of the invention is a method of streaming data comprising computing a visual smoothness value for each node in a wireframe mesh and layering data associated with the wireframe mesh into a plurality of layers such that an average visual smoothness value associated with each layer reflects the respective layer"s importance in an animation sequence. Other embodiments of the invention may include a bitsfream generated according to a process similar to the above method and an apparatus of generating and fransmitting a bitstream or receiving a bitstream.<br>
[0010] Additional features and advantages of the invention will be set forth in the description which follows, and in part will be obvious from the description, or may be learned by practice of the invention. The features and advantages of the invention may be realized and obtained by means of the instruments and combinations particularly pointed out in the appended claims. These and other features of the present invention will become more fully apparent from the following description and appended claims, or may be learned by the practice of the invention as set forth herein.<br>
BRIEF DESCRIPTION OF THE DRAWINGS<br><br>
[0011] In order to describe the manner in which the above-recited and other<br>
advantages and features of the invention can be obtained, a more particular<br>
description of the invention briefly described above will be rendered by<br>
reference to specific embodiments thereof which are illustrated in the appended<br>
drawings. Understanding that these drawings depict only typical embodiments<br>
of the invention and are not therefore to be considered to be limiting of its<br>
scope, the invention will be described and explained with additional specificity<br>
and detail through the use of the accompanying drawings in which:<br>
[0012] Figure lA is a block diagram of a 3-D Animation codec;<br>
[0013] Figure IB is a block diagram of a decoder;<br>
[0014] Figure 2 is a comparative plot of distortion metrics including PSNR,<br>
Hausdorff Distance and Visual Smoothness;<br>
[0015] Figure 3 A represents a flowchart of method of error resilient wireframe<br>
streaming;<br>
[0016] Figure 3B illustrates a flowchart according to an aspect of the invention;<br>
[0017] Figure 4 is a comparative plot of three error concealment methods for<br>
sequence for wireframe animation TELLY;<br>
[0018] Figure 5 is a comparative plot of Visual smoothness (VS) transmitted<br>
and decoded frames of 3 layers of the wireframe animation TELLY; and<br>
[0019] Figure   6   is   a  comparative  plot  of Visual   Smoothness  between<br>
fransmitted   and   decoded   frames   of 2   layers   of wireframe   animation<br>
BOUNCEBALL.<br>
DETAILED DESCRIPTION OF THE INVENTION<br>
[0020] Much research has been undertaken to study streaming video across computer networks in general and over the Internet in particular. Relatively little has been undertaken in the field of streaming 3-D wireframe animation.<br><br>
Although both processes may have some similarities, the two are significantly different. Different data passes across the network, so loss affects signal reconstruction differently. The perceptual effects of such loss have been poorly addressed in the art. Much of the work in this area relied on objective measures such as PSNR in lieu of those that take subject effects into account. [0021] The present invention brings together concepts from a number of fields to address the problem of how to achieve optimal resilience to errors in terms of the perceptual effect at the receiver. In this regard, the invention relates to a subjective quality of an animation, for example, the mesh surface smoothness. To achieve an improved coding scheme taking subjective factors into account, an aspect of the invention comprises partitioning the animation stream into a number of layers and applying Reed-Solomon (RS) forward error correction (FEC) codes to each layer independently and in such a way as to maintain the same overall bitrate whilst minimizing the perceptual effects of error, as measured by a distortion metric related to static 3-D mesh compression. Graceful degradation of streamed animations at higher packet loss rates than other approaches can be achieved by the unequal error protection (UEP) approach combined with error concealment (EC) and an efficient packetization scheme.<br>
[0022] The present disclosure first provides an overview of the 3D-Animation codec that introduces the related notation, followed by an overview of the error correcting RS codes follows together with derivation of the channel model, as well as an exemplary description of UEP packetization for the encoded bitstream.<br>
[0023] The vertices mj of a time-dependent 3-D mesh form the indexed set M, = {mjt; j = 1, 2, ..., n}, at time t, where n is the number of vertices in the mesh. Since a vertex has three space components (Xj, yj, Zj), and assuming that no connectivity changes occur in time (constant n), we can represent the indexed set"s data at time t by the position matrix Mt, as:<br><br><br>
[0024] The indexed set of vertices are partitioned into intuitively natural partitions, called nodes. The term "node" is used to highlight the correspondence of such nodes with the nodes as defend in VRML. The position matrix corresponding to the i"" node is denoted by Njt. Note that without loss of generality, the vertex matrix can now be expressed as:<br><br>
for k such nodes. For notational convenience, the terms Ni_, (i = 1,2, ..., k) are used both for representing a matrix and to refer to the i* node as well. The objective of the 3D-Animation compression algorithm is to compress the sequence of matrices Mt that form the synthetic animation, for transmission over a communications charmel. Obviously, for free-form animations of a 3-D mesh the coordinates of the mesh may exhibit high variance, which makes the Mt matrices unsuitable for compression. Hence, the signal can be defined as the set of non-zero displacements of all vertices in all nodes at time t:<br><br><br><br>
where Fit the displacement matrix of node i, for 1 such nodes (i= 1, 2, ..., 1). Note that Dt"s dimension is reduced to p 
 <br>
in the range [0..1], where F is the number of animation frames and k the number of nodes in the reference model. For p —• n and 1 —♦ k, then df —+ 1, therefore a complete animation.<br>
[0025] The concept described and summarized in equation (1) above, is suited to a DPCM coder, as detailed below. The coding process assumes that the initial wireframe model Mo, here termed as the reference model, is already present at the receiver. The reference model can be compressed and streamed with an existing method for static 3-D mesh transmission, along with error protection if it is assumed the transmission is done over the same lossy channel as the time-dependent mesh. Such existing methods can accommodate and interoperate with static mesh transmissions and will not be discussed further here.<br><br>
[0026] In the 3D-Animation codec"s context, an I-frame describes changes from the reference model Mo to the model at the current time instant t. A P-frame describes the changes of a model from the previous time instant t - 1 to the current time instant t. The corresponding position and displacement matrices for I and P frames are denoted respectively by A//, Ml", Dj, D^.<br>
[0027] Figure lA shows an exemplary block diagram 100 of a coding process according to an aspect of the invention. The diagram illustrates a DPCM encoder that takes advantage of the temporal correlation of the displacement of each vertex along every axis in the 3-D space. To encode a P-frame, the decoded set (animation frame or displacement matrix) of the previous instance is used as the predicted value 108, 106 D,"^,.  (Equivalently for encoding an I-<br>
frame the predicted matrix is ^/_, where at t = 0 is the displacement matrix for<br>
the reference model.) Then, the prediction error Et, i.e. the difference between the current displacement matrix and the predicted one 106 is computed 102 and quantized \OA\E,). Finally, the quantized samples are entropy coded (Ct) using an adaptive arithmetic coding algorithm 110 to handle the unknown data statistics. This predictive scheme prevents quantization error accumulation. [0028] A DPCM decoder 120 is shown in Figure IB. The decoder 120 first decodes arithmetically the received samples 122 (C"t) and computes the decoded samples 124, 126 \p\].     The quantization range of each node is<br>
determined by their bounding box. The quantization step size can be assumed to be the same for all nodes, or can vary in order to shape the encoded bitstream rate. Allowing different quantization step sizes for different nodes may result in artifacts such as mesh cracks, especially in the boundaries between nodes.<br>
[0029] The discloser mentions above that Dt"s dimension is reduced to p 
 <br>
BIFS-Animation, which does not allow for reduced animation frames. For sparse Dt matrices it may also be the case that a whole node is not animated thus allowing great animation flexibility and generating a scalable bitstream.<br>
Furthermore, in the case where Fj t = 0, V/ e [U], the displacement matrix Dt is zero, leading to an "empty" frame. This property resembles the silence period inherent in speech audio streams and can be exploited in the application layer of RTP-based receivers to absorb network jitter. Inter-stream synchronization can also be achieved, which is paramount for many applications (e.g. lip synchronization of a 3-D animated virtual salesman with packet speech).<br>
[0030] Next is described a channel model and error correction codes. The idea of Forward Error Correction (FEC) is to transmit additional redundant packets which can be used at the receiver to reconstruct lost packets. In the FEC process according to a preferred embodiment of the present invention, Reed-Solomon (RS) codes are used across packets. RS codes are the only non-trivial maximum distance separable codes known, hence they are suitable for protection against packet losses over bursty loss channels. An RS(n, k) code of length n and dimension k is defined over the Galois Field GF (2"") and encodes k q-bit information symbols into a codeword of n such symbols, i.e. n 
 <br>
[0031] In reality, the underlying bursty loss process of the Internet is quite complex, but it can be closely approximated by a 2-state Markov model. The two states are state G (good), where packets are timely and correctly received, and B (bad), where packets are either lost or delayed to the point that they that can be considered lost. The state transition probabilities poe and PBG fully describe the model, but since they are not sufficiently intuitive, the model can be expressed using the average loss probability PB, and the average burst length LB, as:<br><br>
[0032] For the selection of the RS code parameters the probability needs to be known that a BOP cannot be reconstructed by the erasure decoder as a function of the channel and the RS code parameters. For an RS(n, k) code, this is the probability that more than n - k packets are lost within a BOP, and it is called the block error rate, PBER. Let P (m, n) be the probability of m lost packets within a block of n packets, also called the block error density function. Then, the calculation is:<br><br>
[0033] The average loss probability PB and the average loss burst LB corresponding to the 2-state Markov model described above, relate to block error density function P (m, n). The exact nature of their relationship has been extensively studied and derived in the literature. Here we adapt the derivation for bit error channels to a packet loss channel.<br><br>
[0034] The Markov model as described before is a renewal model, i.e. a loss event resets the loss process. Such a model is determined by the distribution of error-free intervals (gaps).   If there occurs an event of gap length v such that<br>
V	- 1 packets are received between two lost packets, then the gap density<br>
function g(v) gives the probability of a gap length v, i.e. g(v) = PrCO""" 11). The<br>
gap distribution function G(v) gives the probability of a gap length greater than<br>
V	-1, i.e. G(v) = PrCO""" 11). In state B of our model all packets are lost, while<br>
in state G all packets are received, yielding:<br><br><br><br>
where PB is the average error probability.<br>
[0036] From Eq. 5, it is noted that P (m, n) determines the performance of the FEC scheme, and can be expressed as a function of PB, LB using Eq. 3 and 4. As explained below, the expression of P (m, n) can be used in a RS(m, n) FEC scheme for optimized source/channel rate allocation that minimizes the visual distortion.<br>
[0037] Next is described a bitstream format and packetization process. The output bitstream of the 3D-Animation codec needs to be appropriately packetized for streaming with an application-level transport protocol, e.g. RTP. This process for a single layer bit- stream is known and its main features are summarized by the following three concepts:<br>
[0038] (1) In order to describe which nodes of the model are to be animated the animation masks, NodeMask and VertexMasks are defined in a similar way to BIFS-Anim. The NodeMask is essentially a bit-mask where each bit, if set, denotes that the corresponding node in the Node Table will be animated. The Node Table (an ordered list of all nodes in the scene) is either known a priori at the receiver since the reference wireframe model exists there already, or is downloaded by other means. In a similar way, the VertexMasks are defined, one per axis, for the vertices to be animated.<br>
[0039] (2) In its simplest form, one frame (which represents one Application Data Unit (ADU)), is contained in one RTP packet. In this sense, the 3D-Animation codec"s output bit-stream is "naturally packetizable" according to the known Application Level Framing (ALF) principle. An RTP packet payload format is considered starting with the NodeMask and VertexMasks, followed by the encoded samples along each axis.<br><br>
[0040] (3) The M bit in the RTP header must be set for the first of a series of "empty" frames, which (if they exist) can be grouped together. [0041] This simple format suffices for light animations with a modest number of vertices. However, sequences with high scene complexity or high-resolution meshes may generate a large number of coded data after compression, resulting in frames that potentially exceed the path MTU. In such cases, raw packetization in a single layer would require the definition of fragmentation rules for the RTP payload, which may not always be straightforward in the ALF sense. Furthermore, frames directly packetized in RTP as described above generate a variable bitrate stream due to their varying lengths. [0042] A more efficient packetization scheme is sought that satisfies the requirements set out above: (a) to accommodate layered bitstreams, and (b) to produce a constant bitrate stream. This efficiency can be achieved by appropriately adapting the block structure known as Block-Of-Packets (BOP). In this method, encoded frames of a single layer are placed sequentially in line order of an n-Iine by Sp -column grid structure and then RS codes are generated vertically across the grid. For data frames protected by an RS (n, k) erasure code, error resilience information is appended so that the length of the grid is n for k frames of source data. This method is most appropriate for packet networks with burst packet errors, and can be fiilly described by the sequence frame rate FR, the packet size Sp, the data frame rate in a BOP FBOP , and the RS code (n, k).<br>
[0043] Intuitively, for a BOP consisting of FBOP data frames, with Sp bytes long packets, at FR frame rate, the total source and channel bitrate R is given by:<br><br><br>
[0044] This equation serves as a guide to the design of efficient packetization schemes by appropriately balancing the parameters FBOP, n and Sp. It also encompasses the trade-off between delay and resilience. For a layered bitstream, a design is needed for one BOP structure per layer. By varying the parameters in Eq. 6, different RS code rates can be allocated to each layer, thus providing unequal level of error protection to each layer. The way these parameters are adjusted in practice for the application of 3-D animation streaming, considering a measure of visual error, is explained next.<br>
[0045] In order to measure the visual loss resulting from a non-perfect reconstruction of the animated mesh at the receiver, a metric is required that is able to capture the visual difference between the original mesh Mt at time t and its decoded equivalent M,. The simplest measure is the RMS geometric distance between corresponding vertices. Alternatively, the Hausdorff Distance has been commonly used as an error metric. The Hausdorff distance is defined in the present case as the maximum minimum distance between the vertices of two sets, Mt and M, in such a way that every point Mt lies within the distance H (Mt , M,) of every point in M, and vice versa. This can be expressed as:<br><br><br>
such SNR and PSNR, but they are tailored to the statistical properties of the specific signal they encode, failing to give a uniform measure of user perceived distortion across a number of signals and encoding methods over different media. Moreover, especially for 3-D meshes, all these metrics give only objective indications of geometric closeness, or signal to noise ratios, and they fail to capture the more subtle visual properties the human eye appreciates, such as surface smoothness.<br>
[0046] Figure 2 illustrates a comparative plot 200 of distortion metrics: PSNR, Hausdorff Distance, and Visual Smoothness for 150 frames of the animated sequence BOUNCEBALL with I-frame frequency at 8 Hz. The two upper plots (PSNR-Hausdorff) show the expected correlation between the corresponding metrics of geometric distance and Hausdorff Distance (eq. 7) they represent. The two lower plots indicate that the visual distortion (eq. 8) might be low in cases where the geometric distance is high and vice-versa. [0047] One attempt that was made in the direction of using surface smoothness was reported by Kami and Gotsman as being undertaken whilst evaluating their spectral compression algorithm for 3-D mesh geometries. See, Zachi Kami and Craig Gotsman, "Spectral compression for mesh geometry," in Siggraph 2000, Computer Graphics Proceedings, Kurt Akeley, Ed. 2000, pp. 279 - 286, ACM Press/ACM SIGGRAPH/Addison Wesley Longman, incorporated herein by reference. In this, the suggested 3-D mesh distortion metric normalizes the objective error computed as the Euclidean Distance between two vertices, by each vertex"s distance to its adjacent vertices. This type of error metric captures the surface smoothness of the 3-D mesh. This may be achieved by a Laplacian operator, which takes into account both topology and geometry. The value of this geometric Laplacian at vertex Vi is:<br><br><br>
where n(i) is the set of indices of the neighbors of vertex i, and ly is the geometric distance between vertices i and j. Hence, the new metric is defined as the average of the norm of the geometric distance between meshes and the norm of the Laplacian difference (mt   w, are the vertex sets of meshes<br>
M,M,respectively, and n the set size of M,M,):<br><br>
[0048] This metric in Eq. 8 is preferably used in the present invention, and will be referred to hereafter as the Visual Smoothness metric (VS). Other equations that also relate to the visual smoothness of the mesh may also be used. [0049] The VS metric requires cormectivity information such as the adjacent vertices of every vertex mt. For the case of the 3D- Animation codec, where it is assumed that no connectivity changes during the animation, the vertex adjacencies can be precomputed.<br>
[0050] The BOP structure described above is suitable for the design of an efficient packetization scheme that employs redundancy information based on RS erasure codes. The relation of its design parameters was also shown in Eq. 6. This equation, though, does not reflect any information about layering. An exemplary layering design approach is described next, followed by the proposed error resilient method for 3-D wireframe streaming. [0051] The layering is performed in a way that the average VS value of each layer reflects its importance in the animation sequence. To achieve this, the VS from Eq. 8 is computed for every node in the mesh independently and the nodes are ordered according to their average VS in the sequence. A node, or group of nodes, with the highest average VS forms the first and most important layer visually, LQ. This is the layer that should be more resilient to packet errors than other layers. Subsequent importance layers Li, ..., LM are created by correspondingly subsequent nodes, or group of nodes, in the VS order.<br><br>
[0052] If a 3-D mesh has more nodes than the desirable number of layers, then the number of nodes to be grouped in the same layer is a design choice, and dictates the output bitrate of the layer. For meshes with only a few nodes but large number of vertices per node, node partitioning might be desirable. The partitioning would restructure the 3-D mesh"s vertices in a new mesh with more nodes than originally. This process will affect connectivity, but not the overall rendered model. Mesh partitioning into nodes, if it is possible, should not be arbitrary, but should rather reflect the natural objects these new nodes will represent in the 3-D scene and their corresponding motion. If partitioning is not possible in the above sense, one could partition the mesh in arbitrary sized sub-meshes (nodes) that will be allocated to the same layer. Mesh partitioning may require complex pre-processing steps that would be imderstood by one of skill in the art. Recall, however, that the 3D-Animation codec assumes static connectivity.<br>
[0053] It is common practice in "natural video" to build layers with a cumulative effect. That is, layer L j data add detail to the data of layer Lj.i and improve the overall quality of video. But, one can decode only up to layer Lj.i and forget about the refinement layers. This approach may be taken in an adaptive streaming scenario, where a sender may choose to send only j-1 layers during congested network conditions, and j or more layers when the network conditions improve, i.e., more bandwidth becomes available. [0054] The nature of 3-D animation layers disclosed herein is not always cumulative in the same sense. Decoding layer Lj (which has been built with appropriate node grouping or node partitioning) does not necessarily only refine the quality of data contained in previous layers L o-.L j.i, but adds animation details to the animated model by, for example, adding animation to more vertices in the model.<br>
[0055] As an example, consider the sequence TELLY (discussed more fully below), which is a head-and-shoulders talking avatar. TELLY always faces the<br><br>
camera (static camera). Since the camera does not move, it is a waste of bandwidth to animate the back side of the hair. However, one can easily detect the visible and invisible parts (set of vertices) of the hair and with appropriate partitioning of node "hair" to allocate the visible part to layer L j.i and the invisible part to layer L j. In the case of a static camera (and where no interactivity is allowed) layer L j is not transmitted. Thus when a user views the wireframe mesh or animation in a static mode, only the visible portions of the animation can be seen since the animation does not rotate or move. In the case where the user should be able to examine the animation by rotating or zooming in on the avatar (or other model), or look at the back side of it, layer L j is sent. In this case, the user views the animation in an interactive mode that enables the user to view portions of the animation that were invisible in the static mode, due to the lack of motion of the animation. But, layer Lj does not refine the animation of the visible node of the hair in layer Lj.i. It contains additional animation data for the invisible vertices. This provides an example result of the partitioning method.<br>
[0056] Further, the "interactive mode" does not necessarily require user interaction with the animation. The interactive mode refers to any viewing mode wherein the animation can move or rotate to expose a portion of the animation previously hidden. Thus, in some cases where the viewer is simply looking at the animation, the animation may move and rotate in a more human or natural way while speaking. In this case, the L j layer or other invisible layers may be sent to provide the additional animation data to complete the viewing experience. In this regard, the static or interactive mode may depend on bandwidth available. I.e., if enough bandwidth is available to transmit both visible and invisible layers of the animation, then the animation can be viewed in an interactive mode instead of a static mode. In another aspect of the invention, the user may select the static or interactive mode and thus control what layers are transmitted.<br><br>
[0057] Figure 3A illustrates an example set of steps according to an aspect of the invention. The method comprises partitioning the 3-D wireframe mesh (302), computing the VS value for each node in the mesh (304) and layering data associated with the wireframe mesh into a plurality of layers such that an average VS value associated with each layer reflects the respective layer"s importance in an animation sequence (306). The same overall bitrate is maintained when transmitting the plurality of layers by applying the error correction code to each layer where the error correction code is unequal in the layer according to the layer"s importance (308).<br>
[0058] The terms "partition" as used herein can mean a preprocessing step such as partitioning the mesh into arbitrary or non-arbitrary sub-meshes that will be allocated to the same layer. Further, the term may also have other applications, such as the process of generating the various layers comprising one or more nodes.<br>
[0059] Figure 3B illustrates a flowchart of another aspect of the invention. The method comprises allocating more redundancy to a layer of the plurality of layers that exhibits the greatest visual distortion (320). This may be, for example, a layer comprising visually coarse information. Next, the redundancy is gradually reduced on layers having less contribution to visual smoothness (322). Interpolation-based concealment is applied to each layer at the receiver where an irrecoverable loss of packets occurs only within the respective layer (324) from the standpoint of the receiver. As packets belonging to a particular layer travel through the communications network, they may take different paths from the sender to the receiver, thus suffering variable delays and losses. When the receiver sees an overall packet loss rate, the receiver will try to reduce the loss rate by using the redundant information (FEC) provided separately in each layer. The amount of FEC may not be enough to recover all missing packets (residual packets). The interpolation-based concealment can be applied to each layer independently to reduce the distortion introduced by<br><br>
residual packet loss. In general, steps 320 and 322 are performed on the coding/transmitter end and step 324 is performed at the receiver over a communications network, such as a peer-to-peer network.<br>
[0060] The expected distortion of the animation at the receiver at time t is the sum of the product quantities Pjt • Djt, where j is the layer index, Djt is the visual distortion incurred by missing information in layer j at time t, and Pjt is the probability of having an irrecoverable packet loss in layer j. By the way we constructed the layers, the probabilities Pjt are independent, and a burst packet loss in a layer contributes its own visual distortion Djt in the decoded sequence. Formally, the expected visual smoothness VS(t) of an animation at the decoder at time t can be expressed as:<br><br><br>
[0061] Equation 11 estimates in a statistical sense the expected visual smoothness experienced per frame at the decoder. The objective is to minimize this distortion with respect to the values of kjt"s in Eq. 11. From the way the bitstream is split into layers it is expected that the optimization process allocates more redundancy to the layer that exhibits the greatest visual distortion (coarse layer), and gradually reduces the redundancy rate on layers with finest contribution to the overall smoothness. There are L values of kjt that need to be calculated at every time t, that follow the conditions 0 
and "^~Q(n-kj,) = R^/q where Re the redundancy bits, and q is the symbol<br>
size. The above problem formulation yields a non-linear constraint optimization problem that can be solved numerically.<br>
[0062] The anticipated behavior of the model for PB = 0 is to produce equal values for kjt"s, whereas in high PB"S unequally varying kjt"s would be obtained. Note that for the calculation of smoothness distortions in Eq. 11, it is assumed that no error concealment takes place at the receiver.<br>
[0063] It has been shown that techniques based on vertex linear interpolation are a sufficient and efficient method of error concealment for 3D-Animation frames. This relies on the "locality of reference principle", according to which high-frame rate animations are unlikely to exhibit vertex trajectories other than linear or piece-wise linear. If higher complexity can be accommodated, higher order interpolation can be employed by using information from the neighboring frames. Some known interpolation and other concealment methods are generic in that they can be used by any other decoder.<br>
[0064] Figure 4 is a graph 400 illustrating the relative performances of three error concealment methods adapted to the experimental parameters of this work, namely Pe = [0..30] and LB = 4. It is evident that linear interpolation outperforms Frame Repetition or Motion Vector-based methods. The plot shows average values for 8 iterations with different loss patterns. It is clear on the plot (as seen by the error bars) that the interpolation concealment method<br><br>
exhibits very low variance, verifying the locality of reference principle (the average loss burst length LB = 4 is much lower than the sequence frame rate of 30 Hz.) Therefore, the present invention preferably uses interpolation-based error concealment at the receiver in the case where the channel decoder receives less than n - kjt BOP packets. In fact, the kjt"s that provide a solution to the optimization problem, will also give minimum distortion if combined with concealment techniques. The expected distortion in such cases will be lower than the distortion without error concealment.<br>
[0065] The following explains the experimental procedure and how to tune the values and the optimization process for a real-world case of 3-D wireframe animation, along with discussion of experimental results using the present invention.<br>
[0066] The following experiments demonstrate through simulation the efficiency of the proposed Unequal Error Protection (UEP) scheme combined with Error Concealment (EC) for streaming 3-D wireframe animations. In particular, using UEP with EC is compared to simple UEP, to Equal Error Protection (EEP) and to No Protection (NP). The comparison is based on the Visual Smoothness metric, which is known to yield a distortion measure that captures the surface smoothness of the time-dependent mesh during the animation. For the calculation of the parameters kjt, the constrained minimization problem of Eq. 11 is numerically solved, given the channel rate Re. Furthermore, n is calculated from Eq. 6 such that the rate characteristics of the original source signal are met for the particular design of a BOP. The other parameters used in Eq. 6 are given below for the two sequences in the experiments, and are also summarized in Table 1.<br><br><br>
[0067] For the EEP case, a constant k is considered that can be derived directly from the selection of the channel rate, which is set to 15%. For the NP case, all available channel rates to the source are allocated. Finally, an EC scheme was used based on interpolation for the case of UEP with residual losses. In all experiments used LB = 4.<br>
[0068] The sequences TELLY and BOUNCEBALL were used with density factors of dfrELLY = 0-75 and dfasALL = LO given by Eq. 2. TELLY consists of 9 nodes (out of which 3 are relatively sparse, and the remaining 6 are complete) and totals 780 frames at 30 Hz as shown in Table L Its average source bitrate is RS.TELLY - 220 Kbps. BOUNCEBALL only has 1 complete node and 528 frames at 24 Hz, forming 1 layer of source rate RS,BALL= 61 Kbps average. Both sequences have been coded with I-frames at every 15 frames. Roughly 15% of channel coding redundancy was allowed, resulting in total source and channel coding redundancy, resulting in total source and channel rates of RJELLY = 253 Kbps and RBBALL = 70.15 Kbps. Choosing n = 32 the parameters, from Eq. 6 the calculations for each layer"s packetization are tabulated in Table I.   The value    nf n is chosen as a comnromise between<br><br>
latency and efficiency, since higher n makes the RS codes more resilient, by sacrificing delay and buffer space.<br>
[0069] Sequence TELLY was split into 3 layers according to the suggested layering method presented in Section V, each consisting of the nodes shown in Table L Each layer"s fraction of the total number of animated vertices in the 3-D mesh is (LQ, LI, L2) = (0.48, 0.42, 0.10) on average. This splitting is expected to reflect the source bitrates of each layer proportionally. It was noticed that the suggested layering scheme allocated 2 out of 3 sparse nodes to the same layer, Li. The total number of vertices of these two sparse nodes represents 65% of the vertices in the reference mesh. The third sparse node, Nostril, was allocated to layer L2, but its individual motion relates to a very small fraction of the model"s total number of vertices (« 1.3%). This fact may bear some significance if one desires to relate the node-to-layer allocation (using the VS metric) to the density factor dfL, calculated per layer"* (Eq. 2), and to the output bitrates. If such relation exists, a dynamic layering scheme may be developed for applications with such needs.<br>
[0070] Sequence BOUNCES ALL initially contains only one node. The sequence represents a soft ball with inherent symmetry around a center point as its shape implies. The ball also deforms slightly as it bounces. Given the shape symmetry, it was decided to partition the mesh into 2 nodes of equal number of vertices without respect to the VS metric for each node. The logic behind this partitioning is to attempt to verify the effect the VS metric has on the proposed UEP resilience scheme. All other source coding parameters are constant between the two layers, most importantly the quantization step size. It is anticipated that both layers will receive roughly equal average protection bits, so that UEP performance will approach that of EEP. [0071] Figure 5 depicts a first diagram 502 illustrating VS as a function of the average packet loss rate, PB, for TELLY. The four curves on the plot represent each suggested resilience method, for the code (31, 22).     The average<br><br>
calculated codes for the UEP are as follows (rounded to nearest integer): (nJo) = (31,19),(nJi) = (31,23),(nJ2) = (3U8).     It  is  clear that UEP,   and<br>
UEP+EC outperform NP and EEP for medium to high loss rates of PB &gt; 9%.<br>
Recall that the layering is performed in such a way that the lowest layer<br>
exhibited high average visual distortion. Since the UEP method allocates<br>
higher codes to the lower layer (LQ), better resilience is expected for LQ at high<br>
loss rates. This factor dominates in the average distortion, resulting in better<br>
performance. At low loss rates it was noticed that EEP and UEP behave in<br>
approximately the same way, as the RS codes are more than sufficient to<br>
recover all or most errors. It is also noted that the NP method under conditions<br>
of no loss is much better than any other. This is an intuitive result, since source<br>
information takes all available channel rate, thus better encoding the signal. It<br>
is also worth noticing the effect of EC: the distortion of the UEP+EC scheme is<br>
slightly improved over the simple UEP case. This is also expected.<br>
[0072] The results for the (31,27) RS code on sequence TELLY, shown in plot<br>
504 of Figure 5, are similar. Here, the threshold where the UEP methods (with<br>
or without EC) take over EEP or NP is around PB =7%. Note how the initial<br>
NP performance (low PB"S) is steep compared to the (31, 22), highlighting<br>
again the fact that channel coding bits are actually "wasted" since they do<br>
not contribute much resilience in this low loss region, at the expense of source<br>
rate.	The      corresponding     average     codes      per      layer     are:<br>
(n, k0) = (31,26), (n, k,) = (31,28), (n, ^2) = (31,30). There is an improvement again in the UEP method"s performance resulting from the error concealment"s interpolation algorithm. As this quantity has not been accounted for in the optimization problem it is expected to contribute a small reduction to the visual error.<br>
[0073] Figure 6 shows the results 602 achieved for the same experiment repeated over the BOUNCEBALL sequence, which was "symmetrically" layered as described earlier in this section.   The same (31, 22) EEP code was<br><br>
used as before for comparison. The graph 602 shows the same trends and relative performances as in TELLY, with UEP+EC being the one giving the best overall performance. It is noted, however, that the distance of the UEP curves from the EEP ones decreased considerably compared to the TELLY sequence at high PB"S. The average integer calculated RS codes for the UEP case are: {n,kg) = (31,22),(«,^,) = (31,22), i.e. equivalent to the EEP case.  This<br>
may be a surprising result at the first glance, but careful reasoning suggests that equally balanced layers in terms of the amount of animation they contain (same number of vertices, nodes, very similar motion in the scene, and same encoding parameters) correspond to visually balanced distortions. This is exactly the expected result when layering for the BOUNCEBALL sequence described above. In fact, the real values of kot, ku computed as the solution to the optimization problem, vary around the average integer value of 22. Furthermore, recall that the original symmetric BOUNCEBALL mesh was partitioned into two arbitrary nodes without consideration to their individual visual distortions, which were assumed to be similar. In fact, the Softball"s deformation at the bouncing points reduces the symmetry of the original shape. These facts reasonably explain why the UEP and EEP curves are not accurately fit at higher PB"S as one would normally expect. Finally, it is noted that the UEP+EC method provides a slight, but hardly noticeable, improvement to the visual distortion as in the previous experiment.<br>
[0074] The present invention addresses the fundamental problem of how best to utilize the available channel capacity for streaming 3-D wireframe animation in such a way as to achieve optimal subjective resilience to error. In short, the invention links channel coding, packetization, and layering with a subjective parameter that measures visual smoothness in the reconstructed image. On this basis, it is believed that the result may help open the way for 3-D animation to become a serious networked media type. The disclosed methods attempt to optimize the distribution of the bit budget allocation reserved for channel<br><br>
coding amongst different layers, using a metric that reflects the human eye"s visual property of detecting surface smoothness on time-dependent meshes. Using this metric, the encoded bitstream is initially partitioned into layers of visual importance, and experimental results show that UEP combined with EC yields good protection against burst packet errors occurring on the Internet. [0075] Embodiments within the scope of the present invention may also include computer-readable media for carrying or having computer-executable instructions or data structures stored thereon. Such computer-readable media can be any available media that can be accessed by a general purpose or special purpose computer. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to carry or store desired program code means in the form of computer-executable instructions or data structures. When information is transferred or provided over a network or another communications connection (either hardwired, wireless, or combination thereof) to a computer, the computer properly views the connection, either wired or wireless, as a computer-readable medium. Thus, any such connection is properly termed a computer-readable medium. Combinations of the above should also be included within the scope of the computer-readable media. [0076] Computer-executable instructions include, for example, instructions and data which cause a general purpose computer, special purpose computer, or special purpose processing device to perform a certain flinction or group of functions. Computer-executable instructions also include program modules that are executed by computers in stand-alone or network environments. Generally, program modules include routines, programs, objects, components, and data structures, etc. that perform particular tasks or implement particular abstract data types. Computer-executable instructions, associated data structures, and program modules represent examples of the program code<br><br>
means for executing steps of the methods disclosed herein. The particular sequence of such executable instructions or associated data structures represents examples of corresponding acts for implementing the functions described in such steps.<br>
[0077] Those of skill in the art will appreciate that other embodiments of the invention may be practiced in network computing environments with many types of computer system configurations, including personal computers, hand¬held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, and the like. Embodiments may also be practiced in distributed computing environments where tasks are performed by local and remote processing devices that are linked (either by hardwired links, wireless links, or by a combination thereof) through a communications network. For example, peer-to-peer distributed environments provide an ideal communications network wherein the principles of the present invention would apply and be beneficial. In a distributed computing environment, program modules may be located in both local and remote memory storage devices.<br>
[0078] Although the above description may contain specific details, they should not be construed as limiting the claims in any way. Other configurations of the described embodiments of the invention are part of the scope of this invention. Accordingly, the appended claims and their legal equivalents should only define the invention, rather than any specific examples given.<br><br><br>
WE CLAIM:<br>
1.	A method of streaming data, the method comprising the steps of:<br>
a)	computing a visual smoothness value for each node in a wireframe mesh;<br>
and<br>
b)	layering data associated with the wireframe mesh into a plurality of layers<br>
such that an average visual smoothness value associated with each layer reflects<br>
the respective layer"s importance in an animation sequence.<br>
2.	The method as claimed in claim 1, comprising maintaining a same overall<br>
bit rate when transmitting the plurality of layers.<br>
3.	The method of streaming data as claimed in claim 1, wherein each layer of<br>
                            the plurality of layers comprises a node or a group of nodes within the wireframe<br>
	mesh.<br>
	4.        The method of streaming data as claimed in claim 1, wherein a most<br>
important layer contains the highest average visual smoothness value and is the<br>
	most resilient to packet errors.<br><br>
	5.       The method of streaming data as claimed in claim 1, wherein the number of<br>
	nodes contained within a particular layer is associated with an output bit rate of the<br>
respective layer.<br>
6.	The method as claimed in claim 1, comprising producing a 3-D packetized<br>
streaming signal representative of a scene including animation.<br>
7.	The method as claimed in claim 1, wherein maintaining the same overall<br>
bit rate occurs by applying an error correcting code to each layer where the error<br>
protection code is unequal in the layer according to the respective layer"s<br>
importance in the animation sequence.<br><br>
WE CLAIM:<br>
1.	A method of streaming data, the method comprising the steps of:<br>
a)	computing a visual smoothness value for each node in a wireframe mesh; and<br>
b)	layering data associated with the wireframe mesh into a plurality of layers such that an average visual smoothness value associated with each layer reflects the respective layer"s importance in an animation sequence.<br><br>
2.	The method as claimed in claim 1, comprising maintaining a same overall bit rate when transmitting the plurality of layers.<br>
3.	The method of streaming data as claimed in claim 1, wherein each layer of the plurality of layers comprises a node or a group of nodes within the wireframe mesh.<br>
4.	The method of streaming data as claimed in claim 1, wherein a most important layer contains the highest average visual smoothness value and is the most resilient to packet errors.<br>
5.	The method of sfreaming data as claimed in claim 1, wherein the number of nodes contained within a particular layer is associated with an output bit rate of the respective layer.<br>
6.	The method as claimed in claim 1, comprising producing a 3-D packetized streaming signal representative of a scene including animation.<br>
7.	The method as claimed in claim 1, wherein maintaining the same overall bit rate occurs by applying an error correcting code to each layer where the error protection code is unequal in the layer according to the respective layer"s importance in the animation sequence.<br><br>
8.	The method as claimed in claim 1, comprising, prior to layering the data:<br>
partitioning the wireframe mesh to produce a partitioned mesh having more<br>
nodes than the wireframe mesh.<br>
9.	The method as claimed in claim 8, wherein the step of partitioning further comprises partitioning the wireframe mesh according to objects within a scene represented by the nodes and a corresponding motion of the objects.<br>
10.	The method as claimed in claim 8, wherein the step of partitioning further comprises partitioning the wireframe mesh into arbitrary sized sub-meshes that will be allocated to the same layer.<br>
11.	A method of receiving streamed data, wherein an unequal error protection scheme to a wireframe mesh to partition the wireframe mesh into a plurality of layers is applied to the streamed data, the method comprising the steps of:<br>
applying an error concealment scheme to the plurality of layers, wherein graceful degradation of streamed animation at a high packet loss rate is achieved at a receiver.<br>
12.	The method as claimed in claim 11, wherein the unequal error protection scheme comprises optimizing a distribution of a bit budget allocation amongst the plurality of layers.<br>
13.	The method as claimed in claim 12, wherein the unequal error protection scheme utilizes a visual smoothness metric.<br>
14.	The method as claimed in claim 11, wherein the plurality of layers are partitioned according to visual importance.<br><br>
15.	The method as claimed in claim 11, wherein interpolation-based<br>
concealment is applied to residual packets not recovered from error-concealment.<br>
16.	A method of decoding streaming data associated with a wireframe mesh at<br>
a receiver, the streaming data comprising a plurality of layers organized according<br>
to visual importance of the wireframe mesh, where a sender allocates more<br>
redundancy to a layer of the plurality of layers that exhibits a greater visual<br>
distortion, the method comprising the steps of:<br>
using redundant information within a respective layer in the streaming data to recover missing packets; and<br>
applying interpolation-based error concealment to reduce distortion due to residual packet loss.<br>
17.	The method as claimed in claim 16, wherein the layer exhibiting the<br>
greatest visual distortion is a coarse layer.<br>
18.	The method as claimed in claim 16, wherein the interpolation-based<br>
concealment is applied independently to each layer.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGFic3RyYWN0LWR1cGxpY2F0ZS5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 abstract-duplicate.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGFic3RyYWN0LnBkZg==" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGNsYWltcy1kdXBsaWNhdGUucGRm" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 claims-duplicate.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGNvcnJlc3BvbmRlbmNlLW90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGNvcnJlc3BvbmRlbmNlLXBvLnBkZg==" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGRlc2NyaXB0aW9uKGNvbXBsZXRlKS1kdXBsaWNhdGUucGRm" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 description(complete)-duplicate.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGRlc2NyaXB0aW9uKGNvbXBsZXRlKS5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGRyYXdpbmdzLnBkZg==" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGZvcm0tMS5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGZvcm0tMTgucGRm" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 form-18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGZvcm0tMjYucGRm" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 form-26.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGZvcm0tMy5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IGZvcm0tNS5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IG90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IHBjdCBzZWFyY2ggcmVwb3J0LnBkZg==" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 pct search report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IHBjdC5wZGY=" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 pct.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDQwMS1jaGVucC0yMDA1IHBldGl0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">0401-chenp-2005 petition.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="218423-a-method-for-the-remote-and-dynamic-configuration-of-a-server.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="218425-conjugates-comprised-of-polymer-and-hiv-gp41-derived-peptides.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>218424</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>401/CHENP/2005</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>21/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>23-May-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>01-Apr-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>16-Mar-2005</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>AT &amp; T CORP.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>32 Avenue of the Americas, New York, New York 10013 - 2412,</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>JOERN, Ostermann</td>
											<td>86 Queensbury Ct, Mountainside, NJ 07092,</td>
										</tr>
										<tr>
											<td>2</td>
											<td>SOCRATES, Varakliotis</td>
											<td>140 Salisbury Walk, London N19 5DU,</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H04N 7/26</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/US03/25761</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2003-08-15</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/404,410</td>
									<td>2002-08-20</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/218424-method-of-streaming-and-decoding-of-streamed-data by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 12:50:01 GMT -->
</html>
