<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/225855-a-system-a-method-and-a-framework-for-managing-objects-on-a-network by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:47:48 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 225855:A SYSTEM, A METHOD AND A FRAMEWORK FOR MANAGING OBJECTS ON A NETWORK</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">A SYSTEM, A METHOD AND A FRAMEWORK FOR MANAGING OBJECTS ON A NETWORK</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A system for managing objects in a clustered network includes a file system (212) containing at least one copy of a data object (208). The system can include several clustered servers in communication with the file system (212). A lead server is selected, which contains a distributed consensus algorithm for selecting a host server (206),&#x27; and which utilizes multicasting while executing rounds of the algorithm. The selected host server (206) can contain a copy of the data object (208), such as in local cache, providing access to the local copy (208) to any other server in the cluster. Any change made to an item hosted by the host server (206) can also be updated in the file system (212). If the host server (206) becomes unable to host the object, a new host can be chosen using the distributed consensus a algorithm. The other severs (216, 218) are then notified to the new host by multicast messaging.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td> <br><br>
EXACTLY ONCE CACHE FRAMEWORK<br>
CLAM OF PRIORITY<br>
[0001J	This application claims priority to the following applications which<br>
are incorporated herein:<br>
[0002]	U.S. Provisional Patent Application No. 60/317,718 entitled<br>
"EXACTLY ONCE CACHE FRAMEWORK," by Dean Bernard Jacobs and Eric<br>
Halpem, filed September 6, 2001.<br>
[0003]	U.S. Patent Application entitled "EXACTLY ONCE CACHE<br>
FRAMEWORK," by Dean Bernard Jacobs and Eric Halpem, filed September 4,<br>
2002.<br>
[0004J	U. S. Provisional Patent Application No. 60/317,566 entitled<br>
"EXACTLY ONCE JMS COMMUNICATION," BY Dean Bernard Jacobs and<br>
Eric Halpem, filed September 6, 2001.<br>
[0005]	U. S. Patent Application entitled "EXACTLY ONCE JMS<br>
COMMUNICATION," by Dean Bernard Jacobs and Eric Halpem, filed September<br>
4, 2002.<br>
COPYRIGHT NOTICE<br>
[0006]	A portion of the disclosure of this patent document contains material<br>
which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document of the patent<br><br>
disclosure, as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever.<br>
CROSS-REFERENCED CASES:<br>
[0007]	The following U.S. Patent Application is cross-referenced and<br>
incorporated herein by reference:<br>
[0008]	U.S.   Patent   Application   No.   60/305,986   entitled  "DATA<br>
REPLICATION PROTOCOL," by Dean Bernard Jacobs, Reto Kramer, and Ananthan Bala Srinivasan, filed July 16,2001.<br>
TECHNICAL FIELD<br>
[0009]	The present invention is related to technology for distributing<br>
objects among servers in a network cluster.<br>
BACKGROUND<br>
[0010]	In distributed computer systems, it is often the case that several<br>
servers and/or networking nodes need to work together. These servers and nodes have to be coordinated, as there is typically networking information that needs to be shared among the machines in order to allow them to function as a single entity. Typical approaches to machine coordination can be very expensive in terms of resources and efficiency.<br>
[0011]	In general, some synchronization is required for the nodes to agree,<br>
as there may be several messages passing between the nodes. This requirement for synchronization  may,  however,  be  undesirable  in a clustered networking<br><br>
environment. Many clustered environments simply avoid imposing any such synchronization requirement- There are applications, however, where agreement is necessary,<br>
[0012]	In one case where agreement is needed, a device can exist to which<br>
a cluster may want exclusive access. One such device is a transaction log on a file system. Whenever a transaction is in progress, there are certain objects that need to be saved in a persistent way, such that if a failure occurs those persistently-saved objects can be recovered.<br>
[0013]	For these objects that need to be saved in one place, there is<br>
typically a transaction monitor that runs on each server in that cluster or domain, which then uses a local file system to access the object. Each server can have its own transaction manager such that there is little to no problem with persistence. There is then also no need for coordination, as each server has its own transaction manager.<br>
[0014]	For example, there can be a cluster including three servers, each<br>
server having a transaction manager. One of those servers can experience a failure or other problem causing the server to be unavailable to the cluster. Because the failed server is the only server having access to a particular transaction log, none of the transactions in that particular log can be recovered until the server is again available to the cluster. Recovery of the log can be difficult or at least inefficient, as a problem with the server can take a significant amount of time to fix. Significant server problems can include such occurrences as the shorting out of a motherboard on the server or a power supply being burnt out.<br><br>
BRIEF SUMMARY<br>
[0015]	The present invention includes a system for managing objects, such<br>
as can be stored in servers on a network or in a cluster. The system includes a data source, application, or service, such as a file system or Java Message Service component, which can be located inside or outside of a cluster. The system can include several servers in communication with the file system or application, such as through a high-speed network connection.<br>
[0016]	The system includes a lead server, such as can be agreed upon by the<br>
other servers. The lead server can be contained in a hardware cluster or in a software cluster. The system can include an algorithm for selecting a lead server from among the servers, such as an algorithm built into a hardware cluster machine. The lead server in turn will contain a distributed consensus algorithm for selecting a host server, such as a Paxos algorithm. The algorithm used for selecting the lead server can be different from, or the same as, the algorithm used to select the host server.<br>
[0017]	The host server can contain a copy of the item or object, such as can<br>
be stored in local cache. The host server can provide local copy access to any<br>
server on the network or in a cluster. The host server can also provide the only<br>
access point to an object stored in a file system, or the only access point to an<br>
application or service. Any change made to an item cached, hosted, or owned by<br>
the host server can also be updated in the file system, application, or service.<br>
[0018]	If the host server becomes unable to host the object, a new host can<br>
be chosen using a distributed consensus algorithm. The new host can then pull the necessary data for the object from the file system or service. The other servers in the cluster can be notified that a new server is hosting the object. The servers can<br><br>
be notified by any appropriate means, such as by point-to-point connections or by multicasting.<br>
BRIEF DESCRIPTION OF THE DRAWINGS<br>
[0019]	Figure 1 is a diagram of a distributed object system in accordance<br>
with one embodiment of the present invention.<br>
[0020]	Figure 2 is a diagram of another distributed object system in<br>
accordance with one embodiment of the present invention.<br>
[0021]	Figure 3 is a flowchart of a method for selecting a host server in<br>
accordance with the present invention.<br>
[0022]	Figure 4 is a flowchart of a method for selecting a new host server<br>
in accordance with the present invention.<br>
[0023]	Figure 5 is a flowchart of a method for utilizing a lead server in<br>
accordance with the present invention.<br>
[0024]	Figure 6 is a diagram of JMS message store system in accordance<br>
with one embodiment of the present invention.<br>
[0025]	Figure 7 is a block diagram depicting components of a computing<br>
system that can be used in accordance with the present invention.<br>
DETAILED DESCRIPTION<br>
[0026]	Systems in accordance with the present invention can provide<br>
solutions to availability issues, such as when a server owning a data object becomes unavailable to a server cluster. One such solution allows for another server in the cluster to take over the ownership of the data object. A problem arises, however, in making the data object accessible to both servers without having to replicate the<br><br>
data object on both.<br>
[0027]	If a file system, data store, or database (all collectively referred to<br>
hereinafter as "file system") is used by the cluster to persistently store data, and the file system is accessible from more than one server, the second server can automatically take over the task of data object access if the first server owning that object encounters a problem. Alternatively, there can be an algorithm utilized by the cluster or a server in the cluster to instruct a server to take ownership of the item. Another fundamental problem, however, involves getting the cluster to agree on which server now owns the resource or object, or achieving a "consensus" amongst the servers.<br>
[0028]	Figure 1 shows one example of a cluster system 100 in accordance<br>
with the present invention, where an object such as a transaction log 114 is stored in a file system 112, The file system 112 is accessible to all servers 106,116,118 in the cluster 110, but only one of these servers can access the log 114 at a time. A host server 106 among the servers in the cluster 110 will "own" or "host" the log 114, such as by storing a copy 108 of the log 114 or by providing all access to the log 114 in the file system 112. Any other server 116,118 in the cluster 110 can access the copy 108 of the log, and/or can access the log 114 through the hosting server 106. For example, a client or browser 102 can make a request to a network 104 that is directed to server 116 in cluster 110. That server can access the copy 108 of the transaction log on the host server 106 through the network 104. If the transaction log needs to be updated, the copy 108 can be updated along with the original log 114 on the file system 112.<br>
[0029]	A server can "own" or "host" a data object when, for example, it<br>
acts as a repository for the object, such as by storing a copy of the data object in<br><br>
local cache and making that copy available to other servers in the cluster, or by being the sole server having direct access to an object in a file system, service, or application, such that all other servers in the cluster must access that object through the hosting server. This ensures that an object exists "exactly once" in the server cluster.<br>
[0030]	Figure 3 shows one process 300 that can be used to establish the<br>
hosting of an object A host server can be selected using a distributed consensus<br>
algorithm 302, such as a Paxos algorithm. Such an algorithm is referred to as a<br>
"distributed consensus" algorithm because servers in a cluster must generally agree,<br>
or come to a consensus, as to how to distribute objects amongst the cluster servers.<br>
[0031]	If a hosted object is, for example, to be cached on the hosting server,<br>
a copy of the data object can be pulled from a file system to the host server and stored as an object in local cache 304. The other servers on the network or in the appropriate cluster are then notified, such as by the hosting server, that a local copy of the object exists on the hosting server, and that the local copy should be used in processing future network requests 306.<br>
[0032]	In a Paxos algorithm, one example of a distributed consensus<br>
algorithm, a server can be selected to act as a host or lead server by a network server, the network server leading a series of "consensus rounds." In each of these consensus rounds, a new host or lead server is proposed. Rounds continue until one of the proposed servers is accepted by a majority or quorum of the servers. Any server can propose a host or lead server by initiating a round, although a system can be configured such that a lead server always initiates a round for a host server selection. Rounds for different selections can be carried out at the same time. Therefore, a round selection can be identified by a round number or pair of<br><br>
values, such as a pair with one value referring to the round and one value referring to the server leading the round.<br>
[0033]	The steps for one such round are as follows, although other steps<br>
and/or approaches may be appropriate for certain situations or applications. First, a round can be initiated by a leader sending a "collect" message to other servers in the cluster. A collect message collects information from servers in the cluster regarding previously conducted rounds in which those servers participated. If there have been previous consensus rounds for this particular selection process, the collect message also informs the servers not to commit selections from previous rounds. Once the leader has gathered responses from at least half of the cluster servers, for example, the leader can decide the value to propose for the next round and send this proposal to the cluster servers as a "begin" message. In order for the leader to choose a value to propose in this approach, it is necessary to receive the initial value information from the servers.<br>
[0034]	Once a server receives a begin message from the leader, it can<br>
respond by sending an "accept" message, stating that the server accepts the<br>
proposed host/lead server. If the leader receives accept messages from a majority<br>
or quorum of servers, the leader sets its output value to the value proposed in the<br>
round. If the leader does not receive majority or quorum acceptance ("consensus")<br>
within a specified period of time, the leader can begin a new round. If the leader<br>
receives consensus, the leader can notify the cluster or network servers that the<br>
servers should commit to the chosen server. This notification can be broadcast to<br>
the network servers by any appropriate broadcasting technology, such as through<br>
point-to-point connections or multicasting.<br>
[0035]	The agreement condition of the consensus approach can be<br><br>
guaranteed by proposing selections that utilize information about previous rounds. This information can be required to come from at least a majority of the network servers, so that for any two rounds there is at least one server that participated in both rounds.<br>
[0036]	The leader can choose a value for the new round by asking each<br>
server for the number of the latest round in which the server accepted a value,<br>
possibly also asking for the accepted value. Once the leader gets this information<br>
from a majority or quorum of the servers, it can choose a value for the new round<br>
that is equal to the value of the latest round among the responses. The leader can<br>
also choose an initial value if none of the servers were involved in a previous<br>
round. If the leader receives a response that the last accepted round is x, for<br>
example, and the current round is y, the server can imply that no round between x<br>
and y would be accepted, in order to maintain consistency.<br>
[0037]	A sample interaction between a round leader and a network server<br>
involves the following messages:<br>
[0038]	(1) "Collect" - a message is sent to the servers that a new round * V"<br>
is starting. The message can take the form of m=("Collect", r).<br>
[0039]	(2) "Last" - a message is sent to the leader from a network server<br>
providing the last round accepted, V, and the value of that round,<br>
"v". The message can take the form of m=("Last", r, a, v).<br>
[0040]	(3) "Begin" - a message is sent to the servers announcing the value<br>
for round r. The message can take the form of m=("Begin'\ r, v).<br>
[0041]	(4) "Accept" - a message is sent to the leader from the servers<br>
accepting the value for round r. The message can take the form of<br>
w=("Accept", r).<br><br>
[0042]	(5) "Success" - a message is sent to the servers announcing the<br>
selection of value v for round r. The message can take the form of<br>
/w^'Success", r, v).<br>
[0043]	(6) "Ack" - a message is sent to the leader from a server<br>
acknowledging that the server received the decision for round r.<br>
The message can take the form of m=(uAck", r).<br>
[0044]	There can be a file system that is separated from the servers, located<br>
either inside or outside of a hardware or software cluster. This file system can<br>
persistently store the transaction log, such as by storing the log on a first disk and<br>
replicating the log to a second disk within the file system. If the first disk crashes,<br>
the file system can hide the crash from the cluster and/or server and get the log<br>
information from the second disk. The file system can also choose to replicate the<br>
log to a third disk, which can serve as a backup to the second disk.<br>
[0045]	From the perspective of a server in the cluster, the file system can<br>
be a single resource. In one embodiment, the server may only care that a single server owns the file system at any time.<br>
[0046]	Another example of a system in accordance with the present<br>
invention involves caching in a server cluster. It may be desirable in a clustered<br>
environment, such as for reasons of network performance, to have a single cache<br>
represent a data object to servers in the cluster. Keeping items in a single cache can<br>
be advantageous, as servers in the cluster can access the cache without needing to<br>
continually return to persistent storage. Being able to pull an item already in<br>
memory can greatly increase the efficiency of such a system, as hits to a database<br>
or file system can be relatively time intensive.<br>
[0047]	One problem with a single cache, however, is that it may be<br><br>
necessary to ensure that the object stored in memory is the same as that which is stored on a disk of the file system. One reason for requiring such consistency is to ensure that any operations or calculations done on a cached item produce the correct result. Another reason is that it can be necessary to restore the cache from the file system in the event that the cache crashes or becomes otherwise tainted or unavailable.<br>
[0048]	There can be at least two primary ways to handle this type of<br>
caching in a cluster, although other ways may work at least as well for certain<br>
applications. One way is to replicate the cache in multiple places. This approach<br>
can be problematic, as any change to an item being cached requires that all servers<br>
replicating the cache agree to, or are at least aware of, the change. This can prove<br>
to be very expensive in terms of resources and performance.<br>
[0049]	An alternative approach in accordance with the present invention<br>
assigns a particular server to be the owner of a cache in the cluster, and all access to the cache goes through that particular server. Any server in a cluster can host such a cache. Each server can host one, several, or no caches. The caches can be hosted on a single server, or spread out among some or all of the servers in the cluster. The cluster itself can be any appropriate cluster, such as a hardware cluster or a group of servers designated by a software application to be in a given "software" cluster.<br>
[0050]	It may be possible to think of either example, a transaction log<br>
and/or a cache, as a type of object that sits somewhere on a system. It may be desirable to ensure that any such object exists only once in a cluster, and that the object is always available. It may also be desirable to ensure that the object can be recovered on another server if the server hosting the object fails, and that the object<br><br>
will be available to the cluster.<br>
[0051]	One method 400 for recovery is shown in Figure 4. In this method,<br>
a determination is made whether the host server can continue to host an object 402,<br>
such as whether the server is still available to the network. If not, a new host is<br>
selected using a distributed consensus algorithm. This selection maybe performed<br>
according to the method used to select the original host 404. A copy of the data<br>
object is pulled from a file system to the new host, and can be stored in a local<br>
cache 406. The other servers on the network or in the appropriate cluster are<br>
notified that the new host server contains a local copy of the object, and that the<br>
local copy should be used in processing any future network requests 408.<br>
[0052]	Systems and methods in accordance with the present invention can<br>
define objects that exist in exactly one place in a cluster, and can ensure that those objects always exist. From a server's perspective, it may not matter whether an object such as a transaction log is mirrored or replicated, such as by a file system. From the server's perspective, there is always one persistent storage accessible by any server in the cluster. The system can periodically check for the existence of an object, or may assign ownership of objects for short periods of time such that an object will be reassigned frequently to ensure existence on some machine on the network or in the cluster.<br>
[0053]	A hardware cluster can comprise a bank of machines, each machine<br>
being capable of running multiple servers. There can also be a file system behind each machine. Servers in a hardware cluster are typically hardwired, such that they are able to more quickly make decisions and deal with server faults within the hardware cluster. Hardware clusters can be limited in size to the physical hardware of the machine containing the servers. Servers in a hardware cluster can be used<br><br>
the necessary information from persistent storage.<br>
[0057]	An exactly-once framework can act as a memory buffer for use by<br>
the cluster. The framework can provide a single cache representing data in the<br>
system that is backed by a reliable, persistent storage. Whenever data is read from<br>
the cache, the read can be done without needing to access persistent storage. When<br>
an update is written to cache, however, it can be necessary to write back through<br>
the persistent storage, such that the system can recover if there is a failure.<br>
[0058]	One important aspect of an exactly-once framework involves the<br>
way in which the approach is abstracted, which can vary depending upon the application and/or implementation. A new type of distributed object is created, â€¢ such as can be referred to as an "exactly-once object." An exactly-once object can be, for example, a locally-cached copy of a data item in a file system, or the sole access point to such a data item for servers in a cluster. Underlying techniques for implementing this abstraction can also be important.<br>
[0059]	Systems of the present invention can utilize any of a number of<br>
methods useful for distributed consensus, such as a method using the aforementioned Paxos algorithm. Such an algorithm can be selected which provides an efficient way for multiple nodes and/or distributed nodes to agree on one value of an object. The algorithm can be chosen to work even if nodes fail and/or return during an agreement process.<br>
[0060]	A  typical  approach  to  network   clustering  utilizes  reliable<br>
broadcasting, where every message is guaranteed to be delivered to its intended recipient, or at least delivered to every intended functioning server. This approach can make it very difficult to parallelize a system, as reliable broadcasting requires a recipient to acknowledge a message before moving on to the next message or<br><br>
recipient. A distributed algorithm utilizing multicasting may reduce the number of guarantees, as multicasting does not guarantee that all servers receive a message. Multicasting does simplify the approach such that the system can engage in parallel processing, however, as a single message can be multicast to all the cluster servers concurrently without waiting for a response from each server. A server that does not receive a multicast message can pull the information from the lead server, or another cluster server or network server, at a later time. As used herein, a network server can refer to any server on the network, whether in a hardware cluster, in a software cluster, or outside of any cluster.<br>
[0061J	An important aspect of an exactly-once architecture is that<br>
consensus difficulties are reduced. In accordance with the present invention, the performance of a distributed consensus implementation can be improved by using multicast messaging with a distributed consensus algorithm. This approach can allow for minimizing the message exchange and/or network traffic required for all the servers to agree.<br>
[0062]	When multicasting, one of several approaches can be taken. In a<br>
first approach, which maybe referred to as "one-phase distribution," a lead server can multicast a message to all other servers on the network, such as may be used in a round of a Paxos algorithm, or used to state that a new host has been selected for an object. In this approach, the lead server only needs to send one message, which can be passed to any server available on the network. If a server is temporarily off the network, the server can request the identification of the new host after coming back onto the network.<br>
[0063]	Using another multicast approach, which may be referred to as a<br>
"two-phase distribution," the lead server can pre-select a host server using an<br><br>
appropriate algorithm. Before assigning an object to that host, however, the lead server can contact every other server in the cluster to determine whether the servers agree with the choice of the new host server. The lead server can contact each server by a point-to-point connection, or can send out a multicast request and then wait for each server to respond. If the servers do not agree on the selection of the host, the lead server can pre-select a new host using the algorithm. The lead server would then send out another multicast request with the identity of the newly preselected host in another round.<br>
[0064]	If every server agrees to the pre-selected host, the lead server can<br>
assign the object to the host server. The lead server can then multicast a commit message, informing the servers that the new change has taken effect and the servers should update their information accordingly.<br>
[0065]	An exactly-once framework can also utilize a "leasing" mechanism.<br>
In using such a mechanism, an algorithm can be used to get the cluster servers to agree on a lead server, such as by using distributed consensus. Once selected, that lead server can be responsible for assigning exactly-once objects to various servers in the cluster. The system can be set up such that the cluster servers will always agree on a new leader if an existing lead server fails.<br>
[0066]	While the lead server is active, the lead server can be aware of all<br>
the exactly-once objects that need to exist in the system. The lead server can decide which server should host each object, and can then "lease" that object to the selected server. When an object is leased to a server, that server can own or host the object for a certain period of time, such as for the duration of a lease period. The lead server can be configured to periodically renew these leases. This approach can provide a way to ensure that a server will not get its lease renewed<br><br>
if it fails or becomes disconnected in any way, or is otherwise not operating properly within the cluster.<br>
[0067]	Much of the problem with distributed systems under failure is that<br>
it is difficult to tell the difference between a server that has failed and one that is<br>
simply not responding. Any server that has been somehow cut off the network can<br>
no longer host an object. That server will still know, even though it is not available<br>
to the cluster, that it can drop its hosting of any object after the lease period. As the<br>
server is not available to the cluster, it will not get its lease renewed.<br>
[0068]	The lead server also knows that, if the lead server is unable to reach<br>
the host server within a certain amount of time, the host server will relinquish its ownership of the object. The lease period can be for any appropriate time, such as for a matter of seconds. The lease period can be the same for all objects in the cluster, or can vary between objects.<br>
[0069]	A system using an exactly-once architecture can also be tightened<br>
down. Operating systems often provide special machinery that is built closer to the hardware and can offer more control. One problem with this approach, however, is that it can be limited by the hardware available. For example, a hardware cluster of servers can have on the order of 16 servers. Because these systems require some tight hardware coupling, there can be limitations on the number of servers that can be included in the cluster.<br>
[0070]	An exactly-once framework, on the other hand, may be able to<br>
handle clusters much larger than these proprietary hardware clusters can handle. A framework can allow for some leveraging of the qualities of service that are available from one of the proprietary clusters, thereby allowing for a larger cluster. Differing qualities of service may include, for example, whether messages are sent<br><br>
by a reliable protocol, such as by point-to-point connections, or are sent by a less reliable but more resource-friendly protocol, such as multicasting. An advantage to using an exactly-once framework is the ability to balance scalability with fault tolerance, such that a user can adapt the system to the needs of a particular application.<br>
[0071]	Prior art systems such as hardware cluster machines can attempt<br>
high availability solutions by having (what appears to the cluster to be) a single machine backed up by a second machine. If the first machine goes down, there is a "buddy" that takes over, and any software that was running on the first machine is brought up on the second machine.<br>
[0072]	An exactly-once framework in accordance with the present<br>
invention can assign the lead server to a server in one of these hardware clusters,<br>
such that dealing with leader failure can become faster than dealing with it in a<br>
software cluster. This lead server can, however, dole out leases to servers whether<br>
those servers are in the hardware cluster or the software cluster. This arrangement<br>
may provide for faster lead server recovery, while allowing for a software cluster<br>
that is larger than, but still includes, the hardware cluster.<br>
[0073]	One such system 200 is shown in Figure 2. A hardware cluster 218<br>
can comprise a single machine containing multiple servers 220, 222, 224. The hardware cluster can be used to choose a lead server 220 from among the servers on that machine, such as may improve efficiency. Once a lead server 220 is selected, the lead server can select a host 206 for an object 214 in a file system 212, which can be located inside or outside of the software cluster 210. The file system 214 itself can replicate the object 214 to a second object 216 on another disk of the file system, such as may provide persistence. The object 214 can be pulled from<br><br>
the file system 212 by the new host 206 with a copy 208 of the object cached on the host 206. When a request is received from a browser or client 202 to a server through the network 204, such as servers 206,216, and 220, that server will know to contact host server 206 if the server needs access to the cached copy of the object 208.<br>
[0074]	One method 500 for using such a system is shown in Figure 5. The<br>
lead server is selected using an algorithm of a hardware cluster 502. This algorithm may be, for example, a proprietary algorithm of the hardware cluster machine, or may be a distributed consensus algorithm requiring consensus over the hardware cluster servers only. A host server can then be pre-selected using a distributed consensus algorithm with the lead server 504, such as a Paxos algorithm. The identity of the pre-selected host can then be multicast to the other servers in a software cluster containing the hardware cluster 506. The lead server can receive approval or disapproval from each server that is presently operational and connected to the cluster 508. If the servers approve the pre-selected host server, a commit message is multicast to the cluster servers informing the servers that the pre-selected host now hosts the item; otherwise, if the servers do not approve a new host is pre-selected and the process begins again 510.<br>
[0075]	An exactly-once framework can be used, for example, to handle<br>
transaction logs or caching. Such a framework can also be used, for example, to<br>
define  an administration server as an  exactly-once object and  lease the<br>
administration server such that the administration server never goes down.<br>
[0076]	Figure 6 shows another example of a cluster system 600 in<br>
accordance with the present invention, where an object 608 acts as a message store for Java Message Service (JMS) 612. All servers 606, 614, 616 in the cluster 610<br><br>
can use JMS, but they must send messages to the message store 608 and pick up<br>
any messages from the message store 608 through the network 604. A host server<br>
606 of the servers in the cluster 610 will "own" or "host" the message store 608.<br>
A client or browser 602 can make a request to a network 604 that is directed to<br>
server 616 in cluster 610. That server 616 can access JMS only by sending a<br>
message to the message store 608 on the host server 606 through the network 604.<br>
[0077]	Figure 7 illustrates a block diagram 700 of a computer system<br>
which can be used for components of the present invention or to implement methods of the present invention. The computer system of Figure 7 includes a processor unit 704 and main memory 702. Processor unit 704 may contain a single microprocessor, or may contain a plurality of microprocessors for configuring the computer system as a multi-processor system. Main memory 702 stores, in part, instructions and data for execution by processor unit 704. If the present invention is wholly or partially implemented in software, main memory 702 can store the executable code when in operation. Main memory 702 may include banks of dynamic random access memory (DRAM), high speed cache memory, as well as other types of memory known in the art.<br>
[0078]	The system of Figure 7 further includes a mass storage device 706,<br>
peripheral devices 708, user input devices 712, portable storage medium drives 714, a graphics subsystem 718, and an output display 716. For purposes of simplicity, the components shown in Figure 7 are depicted as being connected via a single bus 720. However, as will be apparent to those skilled in the art, the components may be connected through one or more data transport means. For example, processor unit 704 and main memory 702 may be connected via a local microprocessor bus, and the mass storage device 706, peripheral devices 708,<br><br>
portable storage medium drives 714, and graphics subsystem 718 may be connected via one or more input/output (I/O) buses. Mass storage device 706, which may be implemented with a magnetic disk drive, optical disk drive, as well as other drives known in the art, is a non-volatile storage device for storing data and instructions for use by processor unit 704. In one embodiment, mass storage device 706 stores software for implementing the present invention for purposes of loading to main memory 702.<br>
[0079]	Portable storage medium drive 714 operates in conjunction with a<br>
portable non-volatile storage medium, such as a floppy disk, to input and output data and code to and from the computer system of Figure 7. In one embodiment, the system software for implementing the present invention is stored on such a portable medium, and is input to the computer system via the portable storage medium drive 714. Peripheral devices 708 may include any type of computer support device, such as an input/output (I/O) interface, to add additional functionality to the computer system. For example, peripheral devices 708 may include a network interface for connecting the computer system to a network, as well as other networking hardware such as modems, routers, or other hardware known in the art.<br>
[0080]	User input devices 712 provide a portion of a user interface. User<br>
input devices 712 may include an alpha-numeric keypad for inputting alphanumeric and other information, or a pointing device, such as a mouse, a trackball, stylus, or cursor direction keys. In order to display textual and graphical information, the computer system of Figure 7 includes graphics subsystem 718 and output display 716. Output display 716 may include a cathode ray tube (CRT) display, liquid crystal display (LCD) or other suitable display device. Graphics<br><br>
subsystem 718 receives textual and graphical information, and processes the<br>
information for output to display 716. Additionally, the system of Figure 7<br>
includes output devices 710. Examples of suitable output devices include speakers,<br>
printers, network interfaces, monitors, and other output devices known in the art.<br>
[0081]	The components contained in the computer system of Figure 7 are<br>
those typically found in computer systems suitable for use with certain embodiments of the present invention, and are intended to represent a broad category of such computer components known in the art. Thus, the computer system of Figure 7 can be a personal computer, workstation, server, minicomputer, mainframe computer, or any other computing device. Computer system 700 can also incorporate different bus configurations, networked platforms, multi-processor platforms, etc. Various operating systems can be used including Unix, Linux, Windows, Macintosh OS, Palm OS, and other suitable operating systems.<br>
[0082]	The foregoing description of preferred embodiments of the present<br>
invention has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Obviously, many modifications and variations will be apparent to-the practitioner skilled in the art. The embodiments were chosen and described in order to best explain the principles of the invention and its practical application, thereby enabling others skilled in the art to understand the invention for various embodiments and with various modifications that are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalence.<br><br><br><br>
CLAIMS What is claimed is:<br>
1.	A system for managing objects on a network, comprising:<br>
a plurality of network servers, each network server adapted to communicate with a network data source; and<br>
a lead server in said plurality of network servers, the lead server containing a distributed consensus algorithm for selecting a host server from said plurality of network servers, the host server containing an object related to a data item in the network data source such that any of said plurality of network servers needing to access the data item can access the object on the host server.<br>
2.	A system according to claim 1, wherein said network servers are selected from the group consisting of hardware cluster servers and software cluster servers.<br>
3.	A system according to claim 1, wherein said distributed consensus algorithm comprises rounds of messages between said lead server and said plurality of servers, the rounds continuing until a majority of said plurality of network servers agrees on the host server.<br>
4.	A system according to claim 1, wherein the host server contains a data object comprising a copy of data from the network data source.<br>
5.	A system according to claim 1, wherein the host server contains a data object serving as the sole access point for the data item in the network data source.<br><br>
6.	A system according to claim 1, wherein said data item is a transaction log.<br>
7.	A system according to claim 1, wherein said distributed consensus algorithm is a Paxos algorithm.<br>
8.	A system for managing objects on a network, comprising:<br>
a plurality of network servers, each network server adapted to communicate with a network data source; and<br>
a lead server in said plurality of network servers, the lead server containing a distributed consensus algorithm for selecting a host server from said plurality of network servers, the host server containing a copy of a data item located in the network data source such that any of said plurality of network servers needing to access the data item can access the copy on the host server.<br>
9.	A system for managing objects on a network, comprising:<br>
a plurality of network servers, each network server adapted to communicate with a network data source; and<br>
a lead server in said plurality of network servers, the lead server contaiipng a distributed consensus algorithm for selecting a host server from said plurality of network servers, the host server containing the sole access point to a data item located in the network data source such that any of said plurality of network servers needing to access the data item must access the data item through the host server.<br>
10.	A system for managing objects on a network, comprising:<br>
a file system containing at least one copy of a data item;<br><br>
a plurality of servers in communication with the file system;<br>
a lead server in said plurality of servers, the lead server containing a distributed consensus algorithm for selecting a host server from said plurality of servers; and<br>
a host server in said plurality of servers, said host server containing a local copy of the data item, said host server adapted to provide access to the local copy to any of said plurality of servers and update the copy of the data item in the file system any time an update is made to the local copy.<br>
11.	A system according to claim 10, wherein said host server is further adapted to store the local copy in a local cache.<br>
12.	A system according to claim 10, wherein said plurality of servers comprise a cluster.<br>
13.	A system according to claim 10, wherein said file system replicates the data item over multiple disks.<br>
14.	A system for managing objects on a network, comprising:<br>
a file system containing at least one copy of a data item;<br>
a plurality of servers in communication with the file system;<br>
a hardware cluster containing hardware cluster servers located in said plurality of servers, said hardware cluster containing a distributed consensus algorithm for selecting a lead server from among said hardware cluster servers;<br>
a lead server in said hardware cluster servers, the lead server containing an<br><br>
algorithm for selecting a host server from said plurality of servers; and<br>
a host server in said plurality of servers, said host server containing a local copy of the data item, said host server adapted to provide access to the local copy to any of said plurality of servers and update the copy of the data item in the file system any time an update is made to the local copy.<br>
15.	A system according to claim 14, wherein said host server is in said hardware cluster.<br>
16.	A method for managing objects on a network, comprising:<br>
selecting a host server from among a plurality of network servers using a<br>
distributed consensus algorithm;<br>
pulling a copy of a data item from a file system to the host server; and notifying other network servers that the host server contains a copy of the<br>
iata item to be used in processing network requests.<br>
[ 7. A method according to claim 16, further comprising the step of:<br>
updating the data item in the file system when the copy on the host server s modified.<br>
[8. A method according to claim 16, further comprising the step of:<br>
restricting other network servers to pass through the host server to access he file system.<br>
9. A method according to claim 16, further comprising the step of:<br><br>
ensuring that only one copy of the data item exists outside the file system.<br>
20.	A method according to claim 16, further comprising the step of:<br>
ensuring that one copy of the data item always exists outside the file system.<br>
21.	A method according to claim 16, further comprising the step of:<br>
selecting a new host server from among a plurality of network servers using a distributed consensus algorithm if the host server is no longer able to host the object.<br>
22.	A method according to claim 16, further comprising the step of:<br>
pulling a copy of a data item from a file system to the a new host server if the host server is no longer able to host the object, the host server selected using the distributed consensus algorithm.<br>
23.	A method according to claim 16, further comprising the step of:<br>
notifying other network servers that a new host server contains a copy of the data item to be used in processing network requests if the host server is no longer able to host the object.<br>
24.	A framework for managing objects on a network, comprising:<br>
a plurality of servers, each server capable of caching a data object; a file system containing at least one copy of a data item; a distributed consensus algorithm for selecting a host server from among said plurality of servers, the host server to cache a copy of the data object; and<br><br>
a distribution system for notifying servers on the network that the host computer contains a copy of the data object.<br>
25.	A method for leasing an object to a server on a network, comprising:<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
assigning a data object to the host server, the host server assigned to provide sole access to a data item for a specific period of time;<br>
pulling a copy of a data item from a file system to the host server; and<br>
notifying other network servers that the host server contains a copy of the data item to be used in processing network requests.<br>
26.	A method according to claim 25, further comprising the step of:<br>
assigning the data object to the host server for another period of time once the specific period of time expires.<br>
27.	A method according to claim 25, further comprising the step of:<br>
assigning the data object to a new host server for another specific period of time once the specific period of time expires on the host server.<br>
28.	A method for leasing an object to a server on a network, comprising:<br>
selecting a lead server from among a plurality of hardware cluster servers in a hardware cluster;<br>
selecting a host server from among a plurality of network servers using a<br><br>
distributed consensus algorithm on said lead server;<br>
assigning a data object to the host server, the host server assigned to provide<br>
sole access to a data item for a specific period of time;<br>
pulling a copy of a data item from a file system to the host server; and notifying other network servers that the host server contains a copy of the<br>
data item to be used in processing network requests.<br>
29.	A method for assigning ownership of an object on a network, comprising:<br>
selecting a lead server from among a plurality of hardware cluster servers in a hardware cluster;<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm on said lead server;<br>
assigning a data object to the host server, the host server assigned to provide sole access to a data object on the network;<br>
pulling a copy of a data object from a file system to the host server; and<br>
notifying other network servers that the host server contains a copy of the data object to be used in processing network requests.<br>
30.	A method for hosting Java Messenger Service (JMS) on a network, comprising:<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
assigning a JMS object the host server, the JMS object comprising a JMS message store providing the sole access point and message queue for JMS over the network; and<br>
notifying the network servers that the host server is hosting the sole JMS<br><br>
message store.<br>
31.	A method according to claim 30, further comprising the step of:<br>
checking the JMS message store for messages intended for one of the plurality of network servers.<br>
32.	A method according to claim 30, further comprising the step of:<br>
sending a JMS message from a network server to the JMS message store on the host server.<br>
33.	A method according to claim 30, further comprising the step of:<br>
sending messages in the JMS message store on the host server to a JMS component.<br>
34.	A method for ensuring the existence of an object in a cluster, comprising:<br>
providing access to a data object using a host server in a plurality of servers;<br>
selecting a new host server from among the plurality of servers using a distributed consensus algorithm if the host server is unable to provide access to the data object;<br>
pulling information to the new host server needed to provide access to the data object; and<br>
notifying other servers in said plurality of servers that a new host server is providing access to the data object.<br>
35.	A method for ensuring the availability of an administration server in a cluster,<br><br>
comprising:<br>
selecting a lead server from among a plurality of servers;<br>
selecting an administration server from among a plurality of servers using a distributed consensus algorithm on the lead server;<br>
pulling administration information from a data source to the administration server and updating administration information in the data source in order to coordinate information in the data source and on the administration server; and<br>
notifying other servers in the cluster of the identity of the administration server.<br>
36.	A method according to claim 35, wherein the step of pulling administration information from a data source comprises pulling administration information from a file system.<br>
37.	A method for distributing objects in a cluster, comprising:<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
assigning a data object to the host server, the host server assigned to provide sole access to a data item; and<br>
multicasting a notification to other servers in the cluster that the host server contains a copy of the data item to be used in processing network requests.<br>
38.	A method for distributing objects in a cluster, comprising:<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br><br>
contacting each server in the cluster to determine whether the selected host is acceptable to that server; and<br>
multicasting a notification to other servers in the cluster to commit the selection of a new host server if all servers in the cluster agree that the selected host is acceptable.<br>
39.	A computer-readable medium, comprising:<br>
means for selecting a host server from among a plurality of network servers using a distributed consensus algorithm;-<br>
means for assigning a data object to the host server, the host server assigned to provide sole access to a data item;<br>
means for pulling a copy of a data item from a file system to the host server; and<br>
means for notifying other network servers that the host server contains a copy of the data item to be used in processing network requests.<br>
40.	A computer program product for execution by a server computer for managing<br>
objects on a network, comprising:<br>
computer code for selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
computer code for pulling a copy of a data item from a file system to the<br>
host server; and<br>
computer code for notifying other network servers that the host server contains a copy of the data item to be used in processing network requests.<br><br>
41.	A system for distributing objects in a cluster, comprising:<br>
means for selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
means for pulling a copy of a data item from a file system to the host server; and<br>
means for notifying other network servers that the host server contains a copy of the data item to be used in processing network requests.<br>
42.	A computer system comprising:<br>
a processor;<br>
object code executed by said processor, said object code configured to:<br>
select a host server from among a plurality of network servers using<br>
a distributed consensus algorithm; pull a copy of a data item from a file system to the host server; and notify other network servers that the host server contains a copy of the data item to be used in processing network requests.<br>
43.	A method for managing objects on a network, comprising:<br>
selecting a host server from among a plurality of network servers in a software cluster using a Paxos algorithm; and<br>
assigning a data object to the host server, the data object existing only on the host server in the network.<br>
44.	A method according to claim 43, further comprising the step of:<br>
pulling data for the data object from a file system.<br><br>
45.	A method according to claim 43, further comprising the step of:<br>
notifying other network servers that the host server contains an object for the data item to be used in processing network requests.<br>
46.	A method according to claim 43, further comprising the step of:<br>
multicasting the identification of the new host server to the other network servers.<br>
47.	A method according to claim 43, wherein said step of selecting a host server<br>
from among a plurality of network servers in a software cluster using a Paxos<br>
algorithm comprises multicasting rounds of information to the other network<br>
servers.<br>
48.	A method for hosting Java Messenger Service (JMS) on a network,<br>
comprising:<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
assigning a JMS object the host server, the JMS object comprising a JMS message store providing the sole access point and message queue for JMS over the network; and<br>
notifying the network servers that the host server is hosting the sole JMS message store.<br>
49.	A method according to claim 48, further comprising the step of:<br><br>
checking the JMS message store for messages intended for one of the plurality of network servers.<br>
50.	A method according to claim 48, further comprising the step of:<br>
sending a JMS message from a network server to the JMS message store on the host server.<br>
51.	A method according to claim 48, further comprising the step of:<br>
sending messages in the JMS message store on the host server to a JMS component.<br>
52.	A method for ensuring the existence of a JMS object in a cluster, comprising:<br>
providing access to a JMS object using a host server in a plurality of servers, the JMS object comprising a JMS message store providing the sole access point and message queue for JMS over the network;<br>
selecting a new host server from among the plurality of servers using a distributed consensus algorithm if the host server is unable to provide access to the JMS object;<br>
pulling information to the new host server needed to provide access to the JMS object; and<br>
notifying other servers in said plurality of servers that a new host server is providing access to the JMS object.<br>
53.	A method for assigning a JMS object to a server in a cluster, comprising:<br>
selecting a host server from among a plurality of network servers using a<br><br>
distributed consensus algorithm;<br>
contacting each server in the cluster to determine whether the selected host is acceptable to that server; and<br>
multicasting a notification to other servers in the cluster to commit the selection of a new host server if all servers in the cluster agree that the selected host is acceptable.<br>
54.	A computer-readable medium, comprising:<br>
means for selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
means for assigning a JMS object to the host server, the host server assigned to provide sole access to JMS; and<br>
means for notifying other network servers that the host server provides sole access to JMS.<br>
55.	A computer program product for execution by a server computer for managing<br>
objects on a network, comprising:<br>
computer code for selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
computer code for assigning a JMS object to the host server, the host server assigned to provide sole access to JMS; and<br>
computer code for notifying other network servers that the host server provides sole access to JMS.<br>
56.	A system for distributing objects in a cluster, comprising:<br><br>
means for selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
means for assigning a JMS object to the host server, the host server assigned to provide sole access to JMS; and<br>
means for notifying other network servers that the host server provides sole access to JMS.<br>
57.	A computer system comprising:<br>
a processor;<br>
object code executed by said processor, said object code configured to: select a host server from among a plurality of network servers<br>
using a distributed consensus algorithm; assign a JMS object to the host server, the host server assigned to<br>
provide sole access to JMS notify other network servers that the host server provides sole access to JMS,<br>
58.	A method for managing a JMS message store on a network, comprising: .<br>
selecting a host server from among a plurality of network servers in a software cluster using a Paxos algorithm; and<br>
assigning a JMS message store to the host server, the JMS message store existing only on the host server.<br>
59.	A method according to claim 58, further comprising the step of:<br>
notifying other network servers that the host server is hosting a JMS<br><br>
message store.<br>
60.	A method according to claim 58, further comprising the step of:<br>
multicasting the identification of the new host server to the other network servers.<br>
61.	A method according to claim 58, wherein said step of selecting a host server from among a plurality of network servers in a software cluster using a Paxos algorithm comprises multicasting rounds of information to the other network servers.<br>
62.	A system for managing a JMS object on a network, comprising:<br>
a plurality of network servers; and<br>
a lead server in said plurality of network servers, the lead server containing a distributed consensus algorithm for selecting a host server from said plurality of network servers, the host server containing a JMS object such that any of said plurality of network servers needing to access JMS must access the JMS object on the host server.<br>
63.	A system according to claim 62, wherein said network servers are selected from the group consisting of hardware cluster servers and software cluster servers.<br>
64.	A system according to claim 62, wherein said distributed consensus algorithm comprises rounds of messages between said lead server and said plurality of servers, the rounds continuing until a majority of said plurality of network servers<br><br>
agrees on the host server.<br>
65.	A system according to claim 48, wherein said distributed consensus algorithm is a Paxos algorithm.<br>
66.	A system for managing JMS on a network, comprising;<br>
a JMS component;<br>
a plurality of servers in communication with the JMS component;<br>
a lead server in said plurality of servers, the lead server containing a distributed consensus algorithm for selecting a host server from said plurality of servers; and<br>
a host server in said plurality of servers, said host server containing a JMS message store, said host server adapted to provide access to the JMS component to any of said plurality of servers.<br>
67.	A system according to claim 66, wherein said plurality of servers comprise a cluster.<br>
68.	A system for managing JMS on a network, comprising:<br>
a JMS component;<br>
a plurality of servers in communication with the JMS component;<br>
a hardware cluster containing hardware cluster servers located in said plurality of servers, said hardware cluster containing an algorithm for selecting a lead server from among said hardware cluster servers;<br>
a lead server in said hardware cluster servers, the lead server containing a<br><br>
distributed consensus algorithm for selecting a host server from said plurality of servers; and<br>
a host server in said plurality of servers, said host server containing a JMS message store, said host server adapted to provide access to JMS to any of said plurality of servers.<br>
69.	A system according to claim 68, wherein said host server is in said hardware cluster.<br>
70.	A framework for managing JMS on a network, comprising:<br>
a plurality of servers, each server capable of hosting a JMS message store;<br>
a distributed consensus algorithm for selecting a host server from among said plurality of servers, the host server to host a JMS message store; and<br>
a distribution system for notifying servers on the network that the host computer is hosting the JMS message store.<br>
71.	A method for leasing a JMS object to a server on a network, comprising;<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm;<br>
assigning a JMS object to the host server, the host server assigned to provide sole access to JMS for a specific period of time; and<br>
notifying other network servers that the host server is hosting the JMS object.<br>
72.	A method according to claim 71, further comprising the step of:<br><br>
assigning the JMS object to the host server for another period of time once the specific period of time expires.<br>
73.	A method according to claim 72, further comprising the step of:<br>
assigning the JMS object to a new host server for another specific period of time once the specific period of time expires on the host server.<br>
74.	A method for leasing a JMS object to a server on a network, comprising:<br>
selecting a lead server from among a plurality of hardware cluster servers in a hardware cluster;<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm on said lead server;<br>
assigning a JMS object to the host server, the host server assigned to provide sole access to JMS for a specific period of time; and<br>
notifying other network servers that the host server is hosting the JMS object.<br>
75.	A method for assigning ownership of an object on a network, comprising:<br>
selecting a lead server from among a plurality of hardware cluster servers in a hardware cluster;<br>
selecting a host server from among a plurality of network servers using a distributed consensus algorithm on said lead server;<br>
assigning a JMS object to the host server, the host server assigned to provide sole access to JMS; and<br>
notifying other network servers that the host server is hosting the JMS object.<br><br>
76. A system for managing objects on a network substantially as herein described with reference to the accompanying drawings.<br><br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDcxNy1jaGVucC0yMDA0IGFic3RyYWN0IGdyYW50ZWQucGRm" target="_blank" style="word-wrap:break-word;">0717-chenp-2004 abstract granted.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDcxNy1jaGVucC0yMDA0IGNsYWltcyBncmFudGVkLnBkZg==" target="_blank" style="word-wrap:break-word;">0717-chenp-2004 claims granted.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDcxNy1jaGVucC0yMDA0IGRlc2NyaXB0aW9uKGNvbXBsZXRlKSBncmFudGVkLnBkZg==" target="_blank" style="word-wrap:break-word;">0717-chenp-2004 description(complete) granted.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDcxNy1jaGVucC0yMDA0IGRyYXdpbmdzIGdyYW50ZWQucGRm" target="_blank" style="word-wrap:break-word;">0717-chenp-2004 drawings granted.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtY29ycmVzcG9uZG5lY2Utb3RoZXJzLnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-correspondnece-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtY29ycmVzcG9uZG5lY2UtcG8ucGRm" target="_blank" style="word-wrap:break-word;">717-chenp-2004-correspondnece-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtZGVzY3JpcHRpb24oY29tcGxldGUpLnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtZHJhd2luZ3MucGRm" target="_blank" style="word-wrap:break-word;">717-chenp-2004-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtZm9ybSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtZm9ybSAxOC5wZGY=" target="_blank" style="word-wrap:break-word;">717-chenp-2004-form 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtZm9ybSAzLnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-form 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtZm9ybSA1LnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-form 5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NzE3LWNoZW5wLTIwMDQtcGN0LnBkZg==" target="_blank" style="word-wrap:break-word;">717-chenp-2004-pct.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="225854-coiler-for-metal-strip-especially-steel-strip.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="225856-process-for-the-preparation-of-oxazolidinones-and-the-oxazolidinones.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>225855</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>717/CHENP/2004</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>02/2009</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>09-Jan-2009</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>01-Dec-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>06-Apr-2004</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>BEA SYSTEMS, INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>2315 NORTH FIRST STREET, SAN JOSE, CALIFORNIA 95131</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>JACOBS, DEAN, BERNARD</td>
											<td>1747 MADERA STREET, BERKELY, CALIFORNIA 94707</td>
										</tr>
										<tr>
											<td>2</td>
											<td>HALPERN, ERIC</td>
											<td>1747 MADERA STREET, BERKELEY, CALIFORNIA 94117,</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06F 15/16</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/US02/28199</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2002-09-05</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/317,566</td>
									<td>2001-09-06</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>10/234,693</td>
									<td>2002-09-04</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>3</td>
									<td>10/234,597</td>
									<td>2002-09-04</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>4</td>
									<td>60/317,718</td>
									<td>2001-09-06</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/225855-a-system-a-method-and-a-framework-for-managing-objects-on-a-network by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 01:47:49 GMT -->
</html>
