<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/191633-data-memory-and-processor-buse by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 02:09:19 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 191633:&quot;DATA MEMORY AND PROCESSOR BUSE&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;DATA MEMORY AND PROCESSOR BUSE&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A data processing system has a CPU 36 linked via a unidirectional read bus 42 and a unidirectional write and address bus 44 to a data memory (e.g cache, RAM or disc), in the form of a cache memory 40. Since the read bus and the write and address bus are only driven in one direction, lost time through reversing the direction of signal travel along a bus is avoided. Read-data words RD and instruction-data words I are transferred from the cache memory to a core 38 of the CPU via the read bus. Instruction-address PC, read-address RA, write-addresses WA and write-data words WD are time division multiplexed on the write and address bus to pass from the core to the cache memory. The system supports burst mode transfer thereby reducing the number of addresses that need to be transferred on the write and address bus thereby releasing bandwidth on this bus for use by write-data words. [Figure 5]</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>This invention relates to an improved data processing apparatus. More particularly, this invention relates to data processing systems having a processor and a data memory between which instruction-data words, read-data words and write-data words are to be transferred.<br>
It is known to provide data processing systems having a processor coupled to a data memory, such as a cache memory, where the data memory stores instruction-data words, read-data words and write-data words. It will be appreciated by those skilled in the art, that the distinction between the different data words depends upon the access operation one is currently performing with that data word, e.g. a data word one is writing to the data memory can be considered a write-data word at that time and that same data word subsequently being read from the data memory can be considered to be a read-data word.<br>
The transfer of data words between a data memory and an associated processor is often a performance critical factor in the operation of such a data processing system. In order to improve the overall performance of the system, considerable effort is expanded in the design of the mechanism for transferring such data words.<br>
Accordingly, there is provided an improved data processing apparatus, said apparatus comprising:<br>
a data memory means;<br>
a processor means, responsive to instruction-data words read from instruction addresses in said data memory means, to read read-data words from read addresses in said data memory means and to write-data words to write addresses in said data memory means;<br>
a unidirectional read bus means between said data memory means and said processor means for transferring instruction-data words and read-data words from said data memory means to said processor means; and<br>
a unidirectional write and address bus means between said processor means and said data memory means for transferring write-data words, instruction-address words, read-address words and write-address words from said processor means to said data memory means.<br>
Figures 1 to 4 of the accompanying drawings illustrate a typical arrangement (similar to the ARM6OO integrated circuit of Advanced RISC Machines Limited) for transferring data between a central processing unit 2 (CPU) and a cache memory 4. The system has a data bus 6 and an address bus 8 extending between the CPU 2 and the cache memory k. The CPU 2 includes a core 10 which is responsive to instruction-data words supplied to it via an instruction-data word bus 12. The core 10 outputs write data words (WD) via driver 30 onto the data bus 6 and write addresses (WA) on to a read and write address bus 14. The write address WA, when received by the cache memory 4, controls the location where the write-data word WD is stored within the cache memory 4. In an Â£inalogous manner, the data bus 6 and the read and write address bus 14 can be used to read read-data words (RD) from the cache memory 4 into the core 10 from locations within the cache memory 4 specified by a read address (RA) on the read and write address bus 14.<br>
Instruction-data words to be recovered from the cache memory 4 are passed via the data bus 6 to a prefetch unit I6 (the ARM6OO does<br>
not have a prefetch unit, although some other processors of this class do have a prefetch unit). The prefetch unit l6 is intended to aid the smooth feeding of instruction-data words to the core 10 in a timely manner and perform functions such as branch instruction identification. The addresses within the cache memory 4 from which the instruction-data words are recovered is specified by a program counter address (PC) generated by the core 10 and passed via a program counter bus l8, the prefetch unit 16 and a multiplexer 20 to the address bus 8. The multiplexer 20 serves to selectively couple either the program counter bus 18 or the read and write address bus l4 to the address bus 8 in dependence upon an instruction not data flag signal (I/D) generated by the core 10.<br>
An alternative would be for the prefetch unit 16 to contain the program counter register PC and pass the program counter value back to the core 10 together with the associated instruction. The prefetch unit l6 would thus be responsible for most prefetching and would be required to drive an address flag AF. The core 10 could retain the capability of updating the prefetch unit program unit value to effectively control its own prefetching.<br>
Within the cache memory 'l, an array of storage cells 22 is provided within which the instruction-data words, the read-data words and the write-data words are stored. The address bus 8|is supplied to an address decoder 24 which serves to select an individual storage cell or row of storage cells from within the array of storage cells 22 in dependence upon the address word read from the address bus 8. Read-data words to be recovered from the cache 4 are supplied via a read data bus 26 to the data bus 6. Write-data words to be stored within the cache 4 are coupled from the data bus 6 via a write data bus 28 to the array of storage cells 22.<br>
In order to load data into the cache other than through the core 10 requires another mechanism. This mechanism would require its own drivers to force the signal lines to the appropriate values. Such a mechanism would, for example, operate upon a cache miss. This mechanism would typically operate at a much slower rate than the core 10, but is not so critical to system performance.<br>
An important consideration within the design of such system as illustrated in Figure 1, is that the data bus 6 and the address bus 8<br>
tend to have long track lengths and so relatively high associated capacitances. In order to cope with this, words to be placed upon these buses must be actively driven onto the buses so as to speed the time taken for the individual bus lines to reach the correct signal level values. To this end, a driver circuit 30 within the CPU 2 drives write-data words onto the data bus 6. In an analogous manner, a driver circuit 32 within the cache 4 drives read-data words onto the data bus 6. It will be appreciated that only one of these two drivers for the data bus 6 should be active at any one time. A read flag signal (R) and a write flag signal (W) generated by the core 10 serve to activate and de-activate these two drivers 30, 32. A driver 3^ within the CPU 2 serves to drive address words onto the address bus 8.<br>
Figure 2 illustrates the operation of the system of Figure 1 when fetching instruction-data words. In this mode, the multiplexer 20 switches the PC value onto the address bus 8 to select the instruction address location from within the cache memory 4. The instruction-data word I accessed in this way is passed to the core 10 via the read bus 26, the driver 32, the data bus 6, the prefetch unit 16 and the instruction bus 12.<br>
Figure 3 illustrates the operation of the system of Figure 1 in transferring read-data words. In this case, the multiplexer 20 selects a read address RA from the core 10 for supply via the address bus 8 to the address decoder 2k. The read-data word thus accessed is passed back to the core 10 via the read bus 26, the driver 32 and the data bus 6.<br>
Finally, Figure k illustrates the operation of the system of Figure 1 in transferring a write-data word. In this case, a write address WA is passed from the core 10 via the multiplexer 20 to the address bus 8. One cycle later, the core 10 generates a write-data word WD and passes this via the driver 30, the data bus 6 and the write bus 28 to the array of storage cells 22.<br>
It will bo noticed that during transfer of a read-data word, the driver 32 within the cache memory 4 drives the data bus 6. Conversely, during transfer of a write-data word, the driver 30 within the CPU 2 drives the data bus 6. In order to avoid any conflicts between the two drivers 30, 32, it is important that they should never attempt to drive the data bus 6 simultaneously. Such a conflict may result in damage to<br>
the circuit and will consume a disadvantageously large funount of power. In order to be certain of avoiding such conflicts, a delay period has to be allowed between switching off one of the drivers 30, 32 and switching on the other of the drivers 30, 32. This requires two separate control signals or careful timing design in the buffers (i.e. slow turn-on and fast turn-off).<br>
In order to increase the overall performance of systems such as that illustrated in Figures 1 to 4, various approaches may be adopted. One approach is to increase the clock speed with which data is transferred, e.g. if a clock frequency of f is used for the core 10, then a clock speed of 2f could be used for the transfers. However, with such an approach problems arise in maintaining and adequately synchronising a sufficient delay between the driving of the data bus 6 with the driver 30 and the driving of the data bus 6 with the driver 32. In addition, there is a loss of bandwidth due to the synchronisation requirements.<br>
Another approach to increasing performance might be simply to increase the bus widths. Wider buses allow more data to be transferred in a given time at a given clock speed. This approach suffers from the disadvantage that it increases the physical size of the buses. In many circumstances, an increase in physical size is disadvantageous from a manufacturing point of view since larger integrated circuits with consequent lower yields are required.<br>
Viewed from one aspect this invention provides apparatus for processing data, said apparatus comprising:<br>
a data memory;<br>
a processor, responsive to instruction-data words read from instruction addresses in said data memory, to read read-data words from read addresses in said data memory and to write write-data words to write addresses in said data memory;<br>
a unidirectional read bus between said data memory and said processor for transferring instruction-data words and read-data words from said data memory to said processor; and<br>
a unidirectional write and address bus between said processor and said data memory for transferring write-data words, instruction-address words, read-address words and write-address words from said processor to said data memory.<br>
l&gt;rovI.dlMg uiiidirecLionui buses for data flow in oitlior direction has the advantage that a time gap necessary for the safe reversing of bus direction need not be provided. Thus, bus speeds can be increased without causing synchronization problems. Furthermore, the invention exploits the realisation that address data passed via the address bus changes relatively infrequently compared with the data on the data bus. Thus, multiplexing write data onto this bus with the address data to form a write and address bus takes fuller advantage of the bandwidth available.<br>
It will be appreciated that the data memory could take a number of forms, e.g. a RAM or even a magnetic storage device. However, the invention is particularly suited for applications in which the data memory is a cache data memory. Such cache data memory applications are particularly speed-critical.<br>
As previously discussed, driver circuits can be provided for the data buses to increase the speed with which signal values are reliably obtained. Whilst the invention has the advantage of making increased use of the bandwidth of the bus passing the address data even without such drivers, such drivers are well suited for use with the invention since they can remain permanently active for a given bus.<br>
In preferred embodiments of the invention said processor comprises an instruction prefetch unit for receiving said instruction-data words and a read-data word receiving circuit for receiving said read data words, said instruction prefetch unit and said read-data word receiving circuit being connected in parallel to said read data bus.<br>
The provision of a prefetch unit speeds the operation of instruction processing. The parallel connection of the read-data word receiving circuit and the prefetch unit allows the read bus to pass either instruction-data words or read-data words without having to route via a multiplexer, the processor selectively activating either the prefetch unit or the read-data word receiving circuit as appropriate.<br>
In preferred embodiments of the invention said data memory comprises an address receiver and decoder for receiving and decoding said instruction-address words, said read-address words and said write-address words and a write circuit for writing said write-data words, and wherein an address flag signal line runs between said processor and<br>
said address recoivcr and decoder for activating said address receiver and decoder.<br>
The multiplexing of address-data words and write-data words onto the address bus raises the problem of how this different data can be identified and processed accordingly by the data memory. To this end, advantageous flexibility is achieved by providing an address flag signal in order to control the appropriate processing within the data memory.<br>
In an complementary manner, it is preferred that said processor comprises a processor multiplexer for selecting either instruction-address words or read-address words and write-address words or write-data words for connection to said write and address bus.<br>
In this way, a mechanism is provided for placing the appropriate data onto the write and address data bus.<br>
Data can be recovered from a data memory in a number of different ways. An address can be provided for each data word to be recovered. However, in preferred embodiments of this invention said data memory is operable in a burst access mode whereby an address word transferred to said data memory specifies a start address for a sequence of access operations for consecutive addresses within said data memory.<br>
Such burst mode access is particularly suited to the invention since an address-data word need only be provided at the start of a sequence of instruction-data words or read-data words to be recovered or write-data words to be written. A single address-data word starts the access process which then proceeds sequentially through the following addresses until terminated. In this way, -the write and address data bus is required to carry fewer address-data words so releasing bandwidth for the passing of write-data words.<br>
A preferred manner of controlling the operation of the data memory is to provide a read flag signal line running between said processor and said data memory for transferring a read flag signal, a write flag signal line running between said processor and said data memory for transferring a write flag signal and an instruction flag signal line running between said processor and said data memory for transferring an instruction flag signal, wherein said read flag signal, said write flag signal and said instruction signal select an access mode to said   data memory and said road flag sorvos to override said<br>
instruction flag signal such that transfer of a read-data word over said read bus has higher priority than transfer of an instruction-data word.<br>
In this way, the data memory can be placed in the appropriate mode for recovering read-data words or instruction-data words or storing write-data words. Furthermore, average processing speed is improved by providing the transfer of a read-data word to have a higher priority than that of an instruction-data word since instruction-data words are typically prefetched and buffered within a system, whereas, the recovery of read-data words tends to occur spasmodically as individual decoded instructions so require.<br>
Viewed from another aspect this invention provides a method of processing data, said method comprising the steps of:<br>
storing instruction-data words, read-data words and write-data words in a data memory;<br>
transferring instruction-data words and read-data words from said data memory to a processor via a unidirectional read bus between said data memory and said processor; and<br>
transferring write-data words, instruction-address words, read-address words and write-address words from said processor to said data memory via a unidirectional write and address bus between said processor and said data memory.<br>
An embodiment of the invention will now be described, by way of example only, with reference to the accompanying drawings in which:<br>
Figures 1 to 4 illustrate a typical known processor and data memory system;<br>
Figure 5 illustrates the arrangement of a processor and data memory system having unidirectional buses in accordance with one embodiment of this invention; and<br>
Figures 6 to 8 illustrate the operation of the embodiment of Figure 5 in different modes.<br>
Figure 5 shows a CPU 36 having a core 38 that receives read-data words RD and instruction-data words I and generates write-data words WD. These data words are transferred between the CPU 36 and a cache memory 40 via a unidirectional read bus 42 and a unidirectional write and address bus 44. The read bus 42 is connected in parallel to a prefetch unit 46 and a read data receiving circuit within the core 38.<br>
Instruction-data words from the prefetch unit 46 are passed to the core 38 via an instruction bus 48.<br>
The write and address bus 44 is driven by a driver 50 within the CPU 36, the driver 50 receiving its input from a three-way multiplexer 52. The three-way multiplexer 52 selects from among a program counter value PC from the prefetch unit 46, a read address RA or write address WA from the core 38 or write data WD from the core 38. The signal selected by the three-way multiplexer 52 is selected in dependence upon an address flag signal AF generated by the core 38 and an instruction flag signal IF generated by the prefetch unit 46.<br>
Within the cache memory 40, a driver 54 serves to drive signal values onto the read data bus 42. Signals from the write and address bus 44 are directed to both an address decoder circuit 58 (via an address latch 56 controlled by the address flag signal AF) and a write data receiving circuit.<br>
It will be appreciated by that, whilst the above described embodiment uses an address decoder circuit 58, embodiments based upon an associative cache (i.e. decode-TAG lookup) are also possible.<br>
The recovery of either read-data words or instruction-data words by the cache memory 40 is controlled via a read flag signal RF generated by the core 38 and the instruction flag signal IF. Furthermore, a write flag signal WF generated by the core 38 serves to indicate that write-data words should be received and stored by the cache memory 40 from the write and address bus 44.<br>
In the circuit of Figure 5. as contrasted with the circuits of Figures 1 to 4, the read bus 42 is unidirectional with a driver 54 provided only at one end, this bus having no need to reverse its direction of transfer. Accordingly, no time is wasted in such reversals and the potential damage and power wastage due to driver circuit conflicts is avoided. The address data and wri,te data is time division multiplexed onto the write and address bus 44 by the three-way multiplexer 52.<br>
Figure 6 illustrates the system operating in an instruction fetch mode. In this mode, the program counter address value PC is passed to the address decoder 58 via the prefetch unit 46, the three-way multiplexer 52, the driver 50, the write and address bus 44 and the address latch 56. The address flag signal AF is ON and the instruction<br>
flag signal IF is ON. This combination of signals controls the three-way multiplexer 52 to select the program counter address value PC from the prefetch unit 46 for application via the driver 50 to the write and address bus 44. The instruction flag IF passing to the cache memory 40 also indicates that an instruction-data word should be recovered from the address specified on the write and address bus 44. This instruction-data word is returned to the CPU 36 via the driver 54, the read bus 42, the prefetch unit 46 and the instruction bus 48.<br>
The system operates in a burst mode whereby a single start program counter address value PC specifies an address from which sequential instruction fetches precede until a further address is supplied. The address latch 56 holds the input to the address decoder circuit 58, the address decoder circuit 58 incorporating a counter to increment the address during burst mode operation.<br>
Figure 7 illustrates a data read operation for the system of Figure 5- In this case, read-data words are routed from the cache memory 40 to the core 38 via the driver 54 and the read-data bus 42. The read address RA is selected by the three-way multiplexer 52 in response to the address flag signal AF being asserted ON and the instruction flag IF being asserted OFF. Accordingly, the read address is applied to the cache 40 via the driver 5O, the write and address bus 44, the address latch 56 and the address decoder 58' The address flag being ON, the read flag being ON and the write flag being OFF causes the cache 40 to treat the signals on the write and address bus 44 as a read address.<br>
A read data operation takes priority over an instruction fetch operation. Thus, if the prefetch unit 46 is asserting the instruction flag signal IF ON, indicating that it is ready to receive more instruction-data words, and the core 38 asserts the read flag signal RF ON, then logic within the cache memory 40 takes the read operation as having the higher priority and returns the requested read-data words via the read bus 42 rather than any instruction-data words.<br>
Figure 8 illustrates the system of Figure 5 in a mode storing write-data words in the cache memory 40. During this mode, the address flag signal AF alternates between being asserted ON and OFF depending upon whether a write-address WA or a write-data word WD is currently being output by the core 38.   The instruction flag signal IF is<br>
asserted OFF and the address flag signal AF serves to control the three-way multiplexer 52 to select the appropriate write-address WA or write-data word and apply this to the write and address bus kk via the driver 50. The write-data words and write-address words are thus effectively time division multiplexed onto the write and address bus kk. The provision of a burst mode write transfer has the result that a write address need only be provided at infrequent intervals, e.g. at page boundaries. If a write address had to be provided for each write data word, then this would effectively halve the write data bandwidth. Within the cache memory kO, the address latch 56 responds to the value of the address flag signal AF to capture address words on the write and address bus 44. The write flag signal WF is asserted OFF and ON (in anti-phase to the address flag AF) indicating to the cache kO that write data words from the write and address bus 44 are to be stored.<br><br><br><br><br>
WE CLAIM:-<br>
i.        An improved data processing apparatus, said apparatus comprising:<br>
a data memory means(40);<br>
a processor means (36), responsive to instruction-data words read from instruction addresses in said data memory means (40), to read read-data words from read addresses in said data memory means (40) and to write-data words to write addresses in said data memory means(40);<br>
a unidirectional read bus means (42) between said data memory means (40) and said processor means (36) for transferring instruction-data words and read-data words from said data memory means (40) to said processor means (36); and<br>
a unidirectional write and address bus means (44) between said processor means (36) and said data memory means (40) for transferring write-data words, instruction-address words, read-address words and write-address words from said processor means (36) to said data memory means (40).<br>
2.	An improved data processing apparatus as claimed in claim 1, wherein said data memory means (40) is a cache data memory means.<br>
3.	An improved data processing apparatus as claimed in any one of claims 1 and 2, wherein said data memory means (40) is provided with a read bus driver circuit means (54) for driving signal values to be transferred on said read data bus means (42).<br>
4.	An improved data processing apparatus as claimed in any one of claims 1, 2 and 3, wherein said processor means (36) is provided with a write and address bus driver circuit means (50) for driving signal values to be transferred on said write and address bus means (44).<br>
5.	An improved data processing apparatus as claimed in any one of the preceding claims, wherein said processor means (36) is provided with an instruction prefetch unit (46) for receiving said instruction-data words and a read-data word receiving circuit means for receiving said read data words, said instruction prefetch unit (46) and said read-data word receiving circuit means being connected in parallel to said read bus means (42).<br>
6.	An improved data processing apparatus as claimed in any one of the preceding claims, wherein said data memory means (40) is provided with an address receiver means (56) and decoder means (58) for receiving and decoding said instruction-address words, said read-address words and said write-address words and a write connection means (57) for writing said write-data words, and wherein an address flag signal line runs between said processor means (36) and said address receiver and decoder means (58) for activating said address receiver and decoder means (58).<br>
7.	An improved data processing apparatus as claimed in any one of the preceding claims, wherein said processor means (36) comprises a processor multiplexer (52) for selecting either instruction-address words or read-address words and write-address words or write-data words for connection to said write and address bus     means (44).<br>
8.	An improved data processing apparatus as claimed in any one of the proceeding claims wherein said processor means (36) and said data memory means (40) have a read flag signal line (62) and write flag signal line (64) and instruction flag signal line (66) running therebetween for transferring a read flag signal, write flag signal and instruction flag signal respectively.<br>
9.	An improved data processing apparatus substantially as herein described and with reference to the and illustrated in the accompanying drawings.<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtYWJzdHJhY3QucGRm" target="_blank" style="word-wrap:break-word;">10-del-1995-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtY2xhaW1zLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtY29ycmVzcG9uZGVuY2Utb3RoZXJzLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtY29ycmVzcG9uZGVuY2UtcG8ucGRm" target="_blank" style="word-wrap:break-word;">10-del-1995-correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZGVzY3JpcHRpb24gKGNvbXBsZXRlKS5wZGY=" target="_blank" style="word-wrap:break-word;">10-del-1995-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZHJhd2luZ3MucGRm" target="_blank" style="word-wrap:break-word;">10-del-1995-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS0xLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS0xMy5wZGY=" target="_blank" style="word-wrap:break-word;">10-del-1995-form-13.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS0yLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS0yOS5wZGY=" target="_blank" style="word-wrap:break-word;">10-del-1995-form-29.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS0zLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS00LnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-form-4.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS02LnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-form-6.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZm9ybS05LnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-form-9.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtZ3BhLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-gpa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTAtZGVsLTE5OTUtcGV0aXRpb24tb3RoZXJzLnBkZg==" target="_blank" style="word-wrap:break-word;">10-del-1995-petition-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=YWJzdHJhY3QuanBn" target="_blank" style="word-wrap:break-word;">abstract.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="191602-intrauterine-chemical-necrosing-method-composition-and-apparatus.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="191636-a-dry-moisture-barrier-film-coating-composition-and-method-of-coating-the-substrates.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>191633</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>10/DEL/1995</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>49/2003</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>06-Dec-2003</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>30-Jul-2004</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>06-Jan-1995</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>ARM LIMITED</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>110 FULBOURN ROAD, CHERRY HINTON, CAMBRIDGE CB1 9NJ, ENGLAND</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>STEPHEN BYRAM FURBER</td>
											<td>1A GORSEY ROAD, WILMSLOW, CHESHIRE, SK9 5DU, U.K</td>
										</tr>
										<tr>
											<td>2</td>
											<td>WILLIAM HENRY OLDFIELD</td>
											<td>2A SUTTON ROAD, MEPAL, ELY, CAMBRIDGESHIRE, CB6 2AQ</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06F 15/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>N/A</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>9400381.1</td>
									<td>1994-01-11</td>
								    <td>U.K.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/191633-data-memory-and-processor-buse by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 02:09:20 GMT -->
</html>
