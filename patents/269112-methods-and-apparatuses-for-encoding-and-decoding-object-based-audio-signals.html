<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/269112-methods-and-apparatuses-for-encoding-and-decoding-object-based-audio-signals by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 08:17:50 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 269112:METHODS AND APPARATUSES FOR ENCODING AND DECODING OBJECT-BASED AUDIO SIGNALS</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">METHODS AND APPARATUSES FOR ENCODING AND DECODING OBJECT-BASED AUDIO SIGNALS</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>An audio decoding method and apparatus and an audio encoding method and apparatus which can efficiently process object-based audio signals are provided. The audio decoding method includes receiving a downmix signal and object-based side information, the downmix signal comprising at least two downmix channel signals; extracting gain information from the object-based side information and generating modification information for modifying the downmix channel signals on a channel-by-channel basis based on the gain information; and modifying the downmix channel signals by applying the modification information to the downmix channel signals.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Description<br>
METHODS AND APPARATUSES FOR ENCODING AND<br>
DECODING OBJECT-BASED AUDIO SIGNALS<br>
Technical Field<br>
[1]	The present invention relates to an audio encoding method and apparatus and an<br>
audio decoding method and apparatus in which object-based audio signals can be ef-<br>
fectively processed by performing encoding and decoding operations.<br>
[2]<br>
Background Art<br>
[3]	In general, in multi-channel audio encoding and decoding techniques, a number of<br>
channel signals of a multi-channel signal are downmixed into fewer channel signals,<br>
side information regarding the original channel signals is transmitted, and a multichannel<br>
signal having as many channels as the original multi-channel signal is<br>
restored.<br>
[4]	Object-based audio encoding and decoding techniques are basically similar to<br>
multi-channel audio encoding and decoding techniques in terms of downmixing<br>
several sound sources into fewer sound source signals and transmitting side information<br>
regarding the original sound sources. However, in object-based audio<br>
encoding and decoding techniques, object signals, which are basic elements (e.g., the<br>
sound of a musical instrument or a human voice) of a channel signal, are treated the<br>
same as channel signals in multi-channel audio encoding and decoding techniques and<br>
can thus be coded.<br>
[5]	In other words, in object-based audio encoding and decoding techniques, object<br>
signals are deemed entities to be coded. In this regard, object-based audio encoding<br>
and decoding techniques are different from multi-channel audio encoding and<br>
decoding techniques in which a multi-channel audio coding operation is performed<br>
simply based on inter-channel information regardless of the number of elements of a<br>
channel signal to be coded.<br>
[6]<br>
[7]<br>
Disclosure of Invention<br>
Technical Problem<br>
[8]	The present invention provides an audio encoding method and apparatus and an<br>
audio decoding method and apparatus in which audio signals can be encoded or<br>
decoded so that the audio signals can be applied to various environments.<br>
[9]<br><br>
Technical Solution<br>
[10]	According to an aspect of the present invention, there is provided an audio decoding<br>
method including receiving a downmix signal and object-based side information, the<br>
downmix signal including at least two downmix channel signals; extracting gain information<br>
from the object-based side information and generating modification information<br>
for modifying the downmix channel signals on a channel-by-channel basis<br>
based on the gain information; and modifying the downmix channel signals by<br>
applying the modification information to the downmix channel signals.<br>
[11]	According to another aspect of the present invention, there is provided an audio<br>
encoding method including: generating a downmix signal by downmixing an object<br>
signal, the downmix signal including at least two downmix channel signals; extracting<br>
object-related information regarding the object signal and generating object-based side<br>
information based on the object-related information; and inserting gain information for<br>
modifying the downmix channel signals on a channel-by-channel basis into the object-<br>
based side information.<br>
[ 12]	According to another aspect of the present invention, there is provided an audio<br>
decoding apparatus including: a demultiplexer configured to extract a downmix signal<br>
and object-based side information from an input audio signal, the downmix signal<br>
including at least two downmix channel signals; and a transcoder configured to<br>
generate modification information for modifying the downmix channel signals on a<br>
channel-by-channel basis based on gain information extracted from the object-based<br>
side information and modifies the downmix channel signals by applying the modification<br>
information to the downmix channel signals.<br>
[13]	According to another aspect of the present invention, there is provided a computer-<br>
readable recording medium having recorded thereon a computer program for executing<br>
an audio decoding method, the audio decoding method including receiving a downmix<br>
signal and object-based side information, the downmix signal including at least two<br>
downmix channel signals; extracting gain information from the object-based side information<br>
and generating modification information for modifying the downmix<br>
channel signals on a channel-by-channel basis based on the gain information; and<br>
modifying the downmix channel signals by applying the modification information to<br>
the downmix channel signals.<br>
[ 14]	According to another aspect of the present invention, there is provided a computer-<br>
readable recording medium having recorded thereon a computer program for executing<br>
an audio encoding method, the audio encoding method including: generating a<br>
downmix signal by downmixing an object signal, the downmix signal including at least<br>
two downmix channel signals; extracting object-related information regarding the<br><br>
object signal and generating object-based side information based on the object-related<br>
information; and inserting gain information for modifying the downmix channel<br>
signals on a channel-by-channel basis into the object-based side information<br>
[15]<br>
Brief Description of the Drawings<br>
[16]	FIG. 1 illustrates a block diagram of a typical object-based audio encoding/<br>
decoding system;<br>
[17]	FIG. 2 illustrates a block diagram of an audio decoding apparatus according to a<br>
first embodiment of the present invention;<br>
[18]	FIG. 3 illustrates a block diagram of an audio decoding apparatus according to a<br>
second embodiment of the present invention;<br>
[19]	FIG. 4 illustrates a block diagram of an audio decoding apparatus according to a<br>
third embodiment of the present invention;<br>
[20]	FIG. 5 illustrates a block diagram of an arbitrary downmix gain (ADG) module that<br>
can be used in the audio decoding apparatus illustrated in FIG. 4;<br>
[21]	FIG. 6 illustrates a block diagram of an audio decoding apparatus according to a<br>
fourth embodiment of the present invention;<br>
[22]	FIG. 7 illustrates a block diagram of an audio decoding apparatus according to a<br>
fifth embodiment of the present invention;<br>
[23]	FIG. 8 illustrates a block diagram of an audio decoding apparatus according to a<br>
sixth embodiment of the present invention<br>
[24]	FIG. 9 illustrates a block diagram of an audio decoding apparatus according to a<br>
seventh embodiment of the present invention<br>
[25]	FIG. 10 illustrates a block diagram of an audio decoding apparatus according to an<br>
eighth embodiment of the present invention<br>
[26]<br>
[27]	* FIGS. 11 and 12 illustrate diagrams for explaining a transcoder operation;<br>
[28]	FIGS. 13 through 16 illustrate diagrams for explaining the configuration of object-<br>
based side information;<br>
[29]	FIGS. 17 through 22 illustrate diagrams for explaining the incorporation of a<br>
plurality of pieces of object-based side information into a single piece of side in-<br>
formation;<br>
[30]	FIGS. 23 through 27 illustrate diagrams for explaining a preprocessing operation;<br>
and<br>
[31]	FIGS. 28 to 33 are diagrams illustrating a case of combining a plurality of<br>
bitstreams decoded with object-based signals into one bitstream.<br>
Best Mode for Carrying Out the Invention<br><br>
[32]	The present invention will hereinafter be described in detail with reference to the<br>
accompanying drawings in which exemplary embodiments of the invention are shown.<br>
[33]	An audio encoding method and apparatus and an audio decoding method and<br>
apparatus according to the present invention may be applied to object-based audio<br>
processing operations, but the present invention is not restricted to this. In other words,<br>
the audio encoding method and apparatus and the audio decoding method and<br>
apparatus may be applied to various signal processing operations other than object-<br>
based audio processing operations.<br>
[34]	FIG. 1 illustrates a block diagram of a typical object-based audio encoding/<br>
decoding system. In general, audio signals input to an object-based audio encoding<br>
apparatus do not correspond to channels of a multi-channel signal but are independent<br>
object signals. In this regard, an object-based audio encoding apparatus is differentiated<br>
from a multi-channel audio encoding apparatus to which channel signals of<br>
a multi-channel signal are input.<br>
[35]	For example, channel signals such as a front left channel signal and a front right<br>
channel signal of a 5.1-channel signal may be input to a multi-channel audio signal,<br>
whereas object signals such as a human voice or the sound of a musical instrument<br>
(e.g., the sound of a violin or a piano) which are smaller entities than channel signals<br>
may be input to an object-based audio encoding apparatus.<br>
[36]	Referring to FIG. 1, the object-based audio encoding/decoding system includes an<br>
object-based audio encoding apparatus and an object-based audio decoding apparatus.<br>
The object-based audio encoding apparatus includes an object encoder 100, and the<br>
object-based audio decoding apparatus includes an object decoder 111 and a mixer/<br>
renderer 113.<br>
[37]	The object encoder 100 receives N object signals, and generates an object-based<br>
downmix signal with one or more channels and side information including a number of<br>
pieces of information extracted from the N object signals such as energy difference information,<br>
phase difference information, and correlation information. The side information<br>
and the object-based downmix signal are incorporated into a single<br>
bitstream, and the bitstream is transmitted to the object-based decoding apparatus.<br>
[38]	The side information may include a flag indicating whether to perform channel-<br>
based audio coding or object-based audio coding, and thus, it may be determined<br>
whether to perform channel-based audio coding or object-based audio coding based on<br>
the flag of the side information. The side information may also include energy information,<br>
grouping information, silent period information, downmix gain information<br>
and delay information regarding object signals.<br>
[39]	The side information and the object-based downmix signal may be incorporated<br>
into a single bitstream, and the single bitstream may be transmitted to the object-based<br><br>
audio decoding apparatus.<br>
[40]	The object decoder 111 receives the object-based downmix signal and the side information<br>
from the object-based audio encoding apparatus, and restores object signals<br>
having similar properties to those of the N object signals based on the object-based<br>
downmix signal and the side information. The object signals generated by the object<br>
decoder 111 have not yet been allocated to any position in a multi-channel space. Thus,<br>
the mixer/renderer 113 allocates each of the object signals generated by the object<br>
decoder 111 to a predetermined position in a multi-channel space and determines the<br>
levels of the object signals so that the object signals so that the object signals can be<br>
reproduced from respective corresponding positions designated by the mixer/renderer<br>
113 with respective corresponding levels determined by the mixer/renderer 113.<br>
Control information regarding each of the object signals generated by the object<br>
decoder 111 may vary over time, and thus, the spatial positions and the levels of the<br>
object signals generated by the object decoder 111 may vary according to the control<br>
information.<br>
[41]	FIG. 2 illustrates a block diagram of an audio decoding apparatus 120 according to<br>
a first embodiment of the present invention. Referring to FIG. 2, the audio decoding<br>
apparatus 120 may be able to perform adaptive decoding by analyzing control in-<br>
formation.<br>
[42]	Referring to FIG. 2, the audio decoding apparatus 120 includes an object decoder<br>
121, a mixer/renderer 123, and a parameter converter 125. The audio decoding<br>
apparatus 120 may also include a demultiplexer (not shown) which extracts a downmix<br>
signal and side information from a bitstream input thereto, and this will apply to all<br>
audio decoding apparatuses according to other embodiments of the present invention.<br>
[43]	The object decoder 121 generates a number of object signals based on a downmix<br>
signal and modified side information provided by the parameter converter 125. The<br>
mixer/renderer 123 allocates each of the object signals generated by the object decoder<br>
121 to a predetermined position in a multi-channel space and determines the levels of<br>
the object signals generated by the object decoder 121 according to control information.<br>
The parameter converter 125 generates the modified side information by<br>
combining the side information and the control information. Then, the parameter<br>
converter 125 transmits the modified side information to the object decoder 121.<br>
[44]	The object decoder 121 may be able to perform adaptive decoding by analyzing the<br>
control information in the modified side information.<br>
[45]	For example, if the control information indicates that a first object signal and a<br>
second object signal are allocated to the same position in a multi-channel space and<br>
have the same level, a typical audio decoding apparatus may decode the first and<br>
second object signals separately, and then arrange them in a multi-channel space<br><br>
through a mixing/rendering operation.<br>
[46]	On the other hand, the object decoder 121 of the audio decoding apparatus 120<br>
learns from the control information in the modified side information that the first and<br>
second object signals are allocated to the same position in a multi-channel space and<br>
have the same level as if they were a single sound source. Accordingly, the object<br>
decoder 121 decodes the first and second object signals by treating them as a single<br>
sound source without decoding them separately. As a result, the complexity of<br>
decoding decreases. In addition, due to a decrease in the number of sound sources that<br>
need to be processed, the complexity of mixing/rendering also decreases.<br>
[47]	The audio decoding apparatus 120 may be effectively used when the number of<br>
object signals is greater than the number of output channels because a plurality of<br>
object signals are highly likely to be allocated to the same spatial position.<br>
[48]	Alternatively, the audio decoding apparatus 120 may be used when the first object<br>
signal and the second object signal are allocated to the same position in a multichannel<br>
space but have different levels. In this case, the audio decoding apparatus 120<br>
decode the first and second object signals by treating the first and second object signals<br>
as a single signal, instead of decoding the first and second object signals separately and<br>
transmitting the decoded first and second object signals to the mixer/renderer 123.<br>
More specifically, the object decoder 121 may obtain information regarding the<br>
difference between the levels of the first and second object signals from the control information<br>
in the modified side information, and decode the first and second object<br>
signals based on the obtained information. As a result, even if the first and second<br>
object signals have different levels, the first and second object signals can be decoded<br>
as if they were a single sound source.<br>
[49]	Still alternatively, the object decoder 121 may adjust the levels of the object signals<br>
generated by the object decoder 121 according to the control information. Then, the<br>
object decoder 121 may decode the object signals whose levels are adjusted. Accordingly,<br>
the mixer/renderer 123 does not need to adjust the levels of the decoded<br>
object signals provided by the object decoder 121 but simply arranges the decoded<br>
object signals provided by the object decoder 121 in a multi-channel space. In short,<br>
since the object decoder 121 adjusts the levels of the object signals generated by the<br>
object decoder 121 according to the control information, the mixer/renderer 123 can<br>
readily arrange the object signals generated by the object decoder 121 in a multichannel<br>
space without the need to additionally adjust the levels of the object signals<br>
generated by the object decoder 121. Therefore, it is possible to reduce the complexity<br>
of mixing/rendering.<br>
[50]	According to the embodiment of FIG. 2, the object decoder of the audio decoding<br>
apparatus 120 can adaptively perform a decoding operation through the analysis of the<br><br>
control information, thereby reducing the complexity of decoding and the complexity<br>
of mixing/rendering. A combination of the above-described methods performed by the<br>
audio decoding apparatus 120 may be used.<br>
[51]	FIG. 3 illustrates a block diagram of an audio decoding apparatus 130 according to<br>
a second embodiment of the present invention. Referring to FIG. 3, the audio decoding<br>
apparatus 130 includes an object decoder 131 and a mixer/renderer 133. The audio<br>
decoding apparatus 130 is characterized by providing side information not only to the<br>
object decoder 131 but also to the mixer/renderer 133.<br>
[52]	The audio decoding apparatus 130 may effectively perform a decoding operation<br>
even when there is an object signal corresponding to a silent period. For example,<br>
second through fourth object signals may correspond to a music play period during<br>
which a musical instrument is played, and a first object signal may correspond to a<br>
mute period during which only background music is played, and a first object signal<br>
may correspond to a silent period during which an accompaniment is played. In this<br>
case, information indicating which of a plurality of object signals corresponds to a<br>
silent period may be included in side information, and the side information may be<br>
provided to the mixer/renderer 133 as well as to the object decoder 131.<br>
[53]	The object decoder 131 may minimize the complexity of decoding by not decoding<br>
an object signal corresponding to a silent period. The object decoder 131 sets an object<br>
signal corresponding to a value of 0 and transmits the level of the object signal to the<br>
mixer/renderer 133. In general, object signals having a value of 0 are treated the same<br>
as object signals having a value, other than 0, and are thus subjected to a mixing/<br>
rendering operation.<br>
[54]	On the other hand, the audio decoding apparatus 130 transmits side information<br>
including information indicating which of a plurality of object signals corresponds to a<br>
silent period to the mixer/renderer 133 and can thus prevent an object signal cor-<br>
responding to a silent period from being subjected to a mixing/rendering operation<br>
performed by the mixer/renderer 133. Therefore, the audio decoding apparatus 130 can<br>
prevent an unnecessary increase in the complexity of mixing/rendering.<br>
[55]	FIG. 4 illustrates a block diagram of an audio decoding apparatus 140 according to<br>
a third embodiment of the present invention. Referring to FIG. 4, the audio decoding<br>
apparatus 140 uses a multi-channel decoder 141, instead of an object decoder and a<br>
mixer/renderer, and decodes a number of object signals after the object signals are appropriately<br>
arranged in a multi-channel space.<br>
[56]	More specifically, the audio decoding apparatus 140 includes the multi-channel<br>
decoder 141 and a parameter converter 145. The multi-channel decoder 141 generates<br>
a multi-channel signal whose object signals have already been arranged in a multichannel<br>
space based on a down-mix signal and spatial parameter information, which is<br><br>
channel-based parameter information provided by the parameter converter 145. The<br>
parameter converter 145 analyzes side information and control information transmitted<br>
by an audio encoding apparatus (not shown), and generates the spatial parameter information<br>
based on the result of the analysis. More specifically, the parameter<br>
converter 145 generates the spatial parameter information by combining the side information<br>
and the control information which includes playback setup information and<br>
mixing information. That is, the parameter conversion 145 performs the conversion of<br>
the combination of the side information and the control information to spatial data corresponding<br>
to a One-To-Two (OTT) box or a Two-To-Three (TTT) box.<br>
[57]	The audio decoding apparatus 140 may perform a multi-channel decoding<br>
operation into which an object-based decoding operation and a mixing/rendering<br>
operation are incorporated and may thus skip the decoding of each object signal.<br>
Therefore, it is possible to reduce the complexity of decoding and/or mixing/rendering.<br>
[58]	For example, when there are 10 object signals and a multi-channel signal obtained<br>
based on the 10 object signals is to be reproduced by a 5.1 channel speaker system, a<br>
typical object-based audio decoding apparatus generates decoded signals respectively<br>
corresponding the 10 object signals based on a down-mix signal and side information<br>
and then generates a 5.1 channel signal by appropriately arranging the 10 object signals<br>
in a multi-channel space so that the object signals can become suitable for a 5.1<br>
channel speaker environment. However, it is inefficient to generate 10 object signals<br>
during the generation of a 5.1 channel signal, and this problem becomes more severe as<br>
the difference between the number of object signals and the number of channels of a<br>
multi-channel signal to be generated increases.<br>
[59]	On the other hand, in the embodiment of FIG. 4, the audio decoding apparatus 140<br>
generates spatial parameter information suitable for a 5.1-channel signal based on side<br>
information and control information, and provides the spatial parameter information<br>
and a downmix signal to the multi-channel decoder 141. Then, the multi-channel<br>
decoder 141 generates a 5.1 channel signal based on the spatial parameter information<br>
and the downmix signal. In other words, when the number of channels to be output is<br>
5.1 channels, the audio decoding apparatus 140 can readily generate a 5.1-channel<br>
signal based on a downmix signal without the need to generate 10 object signals and is<br>
thus more efficient than a conventional audio decoding apparatus in terms of<br>
complexity.<br>
[60]	The audio decoding apparatus 140 is deemed efficient when the amount of<br>
computation required to calculates spatial parameter information corresponding to each<br>
of an OTT box and a TTT box through the analysis of side information and control information<br>
transmitted by an audio encoding apparatus is less than the amount of<br>
computation required to perform a mixing/rendering operation after the decoding of<br><br>
each object signal.<br>
[61]	The audio decoding apparatus 140 may be obtained simply by adding a module for<br>
generating spatial parameter information through the analysis of side information and<br>
control information to a typical multi-channel audio decoding apparatus, and may thus<br>
maintain the compatibility with a typical multi-channel audio decoding apparatus.<br>
Also, the audio decoding apparatus 140 can improve the quality of sound using<br>
existing tools of a typical multi-channel audio decoding apparatus such as an envelope<br>
shaper, a a sub-band temporal processing (STP) tool, and a decorrelator. Given all this,<br>
it is concluded that all the advantages of a typical multi-channel audio decoding<br>
method can be readily applied to an object-audio decoding method.<br>
[62]	Spatial parameter information transmitted to the multi-channel decoder 141 by the<br>
parameter converter 145 may have been compressed so as to be suitable for being<br>
transmitted. Alternatively, the spatial parameter information may have the same format<br>
as that of data transmitted by a typical multi-channel encoding apparatus. That is, the<br>
spatial parameter information may have been subjected to a Huffman decoding<br>
operation or a pilot decoding operation and may thus be transmitted to each module as<br>
uncompressed spatial cue data. The former is suitable for transmitting the spatial<br>
parameter information to a multi-channel audio decoding apparatus in a remote place,<br>
and the later is convenient because there is no need for a multi-channel audio decoding<br>
apparatus to convert compressed spatial cue data into uncompressed spatial cue data<br>
that can readily be used in a decoding operation.<br>
[63]	The configuration of spatial parameter information based on the analysis of side information<br>
and control information may cause a delay. In order to compensate for such<br>
delay, an additional buffer may be provided for a downmix signal so that a delay<br>
between the downmix signal and a bitstream can be compensated for. Alternatively, an<br>
additional buffer may be provided for spatial parameter information obtained from<br>
control information so that a delay between the spatial parameter information and a<br>
bitstream can be compensated for. These methods, however, are inconvenient because<br>
of the requirement to provide an additional buffer. Alternatively, side information may<br>
be transmitted ahead of a downmix signal in consideration of the possibility of<br>
occurrence of a delay between a downmix signal and spatial parameter information. In<br>
this case, spatial parameter information obtained by combining the side information<br>
and control information does not need to be adjusted but can readily be used.<br>
[64]	If a plurality of object signals of a downmix signal have different levels, an<br>
arbitrary downmix gain (ADG) module which can directly compensate for the<br>
downmix signal may determine the relative levels of the object signals, and each of the<br>
object signals may be allocated to a predetermined position in a multi-channel space<br>
using spatial cue data such as channel level difference (CLD) information, inter-<br><br>
channel correlation (ICC) information, and channel prediction coefficient (CPC) in-<br>
formation.<br>
[65]	For example, if control information indicates that a predetermined object signal is<br>
to be allocated to a predetermined position in a multi-channel space and has a higher<br>
level than other object signals, a typical multi-channel decoder may calculate the<br>
difference between the energies of channels of a downmix signal, and divide the<br>
downmix signal into a number of output channels based on the results of the<br>
calculation. However, a typical multi-channel decoder cannot increase or reduce the<br>
volume of a certain sound in a downmix signal. In other words, a typical multi-channel<br>
decoder simply distributes a downmix signal to a number of output channels and thus<br>
cannot increase or reduce the volume of a sound in the downmix signal.<br>
[66]	It is relatively easy to allocate each of a number of object signals of a downmix<br>
signal generated by an object encoder to a predetermined position in a multi-channel<br>
space according to control information. However, special techniques are required to<br>
increase or reduce the amplitude of a predetermined object signal. In other words, if a<br>
downmix signal generated by an object encoder is used as is, it is difficult to reduce the<br>
amplitude of each object signal of the downmix signal.<br>
[67]	Therefore, according to an embodiment of the present invention, the relative<br>
amplitudes of object signals may be varied according to control information by using<br>
an ADG module 147 illustrated in FIG. 5. The ADG module 147 may be installed in<br>
the multi-channel decoder 141 or may be separate from the multi-channel decoder 141.<br>
[68]	If the relative amplitudes of object signals of a downmix signal are appropriately<br>
adjusted using the ADG module 147, it is possible to perform object decoding using a<br>
typical multi-channel decoder. If a downmix signal generated by an object encoder is a<br>
mono or stereo signal or a multi-channel signal with three or more channels, the<br>
downmix signal may be processed by the ADG module 147. If a downmix signal<br>
generated by an object encoder has two or more channels and a predetermined object<br>
signal that needs to be adjusted by the ADG module 147 only exists in one of the<br>
channels of the downmix signal, the ADG module 147 may be applied only to the<br>
channel including the predetermined object signal, instead of being applied to all the<br>
channels of the downmix signal. A downmix signal processed by the ADG module 147<br>
in the above-described manner may be readily processed using a typical multi-channel<br>
decoder without the need to modify the structure of the multi-channel decoder.<br>
[69]	Even when a final output signal is not a multi-channel signal that can be<br>
reproduced by a multi-channel speaker but is a binaural signal, the ADG module 147<br>
may be used to adjust the relative amplitudes of object signals of the final output<br>
signal.<br>
[70]	Alternatively to the use of the ADG module 147, gain information specifying a<br><br>
gain value to be applied to each object signal may be included in control information<br>
during the generation of a number of object signals. For this, the structure of a typical<br>
multi-channel decoder may be modified. Even though requiring a modification to the<br>
structure of an existing multi- channel decoder, this method is convenient in terms of<br>
reducing the complexity of decoding by applying a gain value to each object signal<br>
during a decoding operation without the need to calculate ADG and to compensate for<br>
each object signal.<br>
[71]	The ADG module 147 may be used not only for adjusting the levels of object<br>
signals but also for modifying spectrum information of a certain object signal. More<br>
specifically, the ADG module 147 may be used not only to increase or lower the level<br>
of a certain object signal and but also to modify spectrum information of the certain<br>
object signal such as amplifying a high- or low-pitch portion of the certain object<br>
signal. It is impossible to modify spectrum information without the use of the ADG<br>
module 147.<br>
[72]	FIG. 6 illustrates a block diagram of an audio decoding apparatus 150 according to<br>
a fourth embodiment of the present invention. Referring to FIG. 6, the audio decoding<br>
apparatus 150 includes a multi-channel binaural decoder 151, a first parameter<br>
converter 157, and a second parameter converter 159.<br>
[73]	The second parameter converter 159 analyzes side information and control information,<br>
which is provided by an audio encoding apparatus, and configures spatial<br>
parameter information based on the result of the analysis. The first parameter converter<br>
157 configures virtual three-dimensional (3D) parameter information, which can be<br>
used by the multi-channel binaural decoder 151, by adding three-dimensional (3D) information<br>
such as head-related transfer function (HRTF) parameters to the spatial<br>
parameter information. The multi-channel binaural decoder 151 generates a binaural<br>
signal by applying the binaural parameter information to a downmix signal.<br>
[74]	The first parameter converter 157 and the second parameter converter 159 may be<br>
replaced by a single module, i.e., a parameter conversion module 155 which receives<br>
the side information, the control information, and 3D information and configures the<br>
binaural parameter information based on the side information, the control information,<br>
and the HRTF parameters.<br>
[75]	Conventionally, in order to generate a binaural signal for the playback of a<br>
downmix signal including 10 object signals with a headphone, an object signal must<br>
generate 10 decoded signals respectively corresponding to the 10 object signals based<br>
on the downmix signal and side information. Thereafter, a mixer/renderer allocates<br>
each of the 10 object signals to a predetermined position in a multi-channel space with<br>
reference to control information so as to suit a 5-channel speaker environment.<br>
Thereafter, the mixer/renderer generates a 5-channel signal that can be reproduced by a<br><br>
5-channel speaker. Thereafter, the mixer/renderer applies 3D information to the<br>
5-channel signal, thereby generating a 2-channel signal. In short, the above-mentioned<br>
conventional audio decoding method includes reproducing 10 object signals,<br>
converting the 10 object signals into a 5-channel signal, and generating a 2-channel<br>
signal based on the 5-channel signal, and is thus inefficient.<br>
[76]	On the other hand, the audio decoding apparatus 150 can readily generate a<br>
binaural signal that can be reproduced using a headphone based on object signals. In<br>
addition, the audio decoding apparatus 150 configures spatial parameter information<br>
through the analysis of side information and control information, and can thus generate<br>
a binaural signal using a typical multi-channel binaural decoder. Moreover, the audio<br>
decoding apparatus 150 still can use a typical multi-channel binaural decoder even<br>
when being equipped with an incorporated parameter converter which receives side in-<br>
formation, control information, and HRTF parameters and configures binaural<br>
parameter information based on the side information, the control information, and the<br>
HRTF parameters.<br>
[77]	FIG. 7 illustrates a block diagram of an audio decoding apparatus 160 according to<br>
a fifth embodiment of the present invention. Referring to FIG. 7, the audio decoding<br>
apparatus 160 includes a preprocessor 161, a multi-channel decoder 163, and a<br>
parameter converter 165.<br>
[78]	The parameter converter 165 generates spatial parameter information, which can be<br>
used by the multi-channel decoder 163, and parameter information, which can be used<br>
by the preprocessor 161. The preprocessor 161 performs a pre-processing operation on<br>
a downmix signal, and transmits a downmix signal resulting from the pre-processing<br>
operation to the multi-channel decoder 163. The multi-channel decoder 163 performs a<br>
decoding operation on the downmix signal transmitted by the preprocessor 161,<br>
thereby outputting a stereo signal, a binaural stereo signal or a multi-channel signal.<br>
Examples of the pre-processing operation performed by the preprocessor 161 include<br>
the modification or conversion of a downmix signal in a time domain or a frequency<br>
domain using filtering.<br>
[79]	If a downmix signal input to the audio decoding apparatus 160 is a stereo signal,<br>
the downmix signal may have be subjected to downmix preprocessing performed by<br>
the preprocessor 161 before being input to the multi-channel decoder 163 because the<br>
multi-channel decoder 163 cannot map an object signal corresponding to a left channel<br>
of a stereo downmix signal to a right channel of a multi-channel signal through<br>
decoding. Therefore, in order to shift an object signal belonging to a left channel of a<br>
stereo downmix signal to a right channel, the stereo downmix signal may need to be<br>
preprocessed by the preprocessor 161, and the preprocessed downmix signal may be<br>
input to the multi-channel decoder 163.<br><br>
[80]	The preprocessing of a stereo downmix signal may be performed based on preprocessing<br>
information obtained from side information and from control information.<br>
[81]	FIG. 8 illustrates a block diagram of an audio decoding apparatus 170 according to<br>
a sixth embodiment of the present invention. Referring to FIG. 8, the audio decoding<br>
apparatus 170 includes a multi-channel decoder 171, a postprocessor 173, and a<br>
parameter converter 175.<br>
[82]	The parameter converter 175 generates spatial parameter information, which can be<br>
used by the multi-channel decoder 163, and parameter information, which can be used<br>
by the postprocessor 173. The postprocessor 173 performs a post-processing operation<br>
on a signal output by the multi-channel decoder 173. Examples of the signal output by<br>
the multi-channel decoder 173 include a stereo signal, a binaural stereo signal and a<br>
multi-channel signal.<br>
[83]	Examples of the post-processing operation performed by the post processor 173<br>
include the modification and conversion of each channel or all channels of an output<br>
signal. For example, if side information includes fundamental frequency information<br>
regarding a predetermined object signal, the postprocessor 173 may remove harmonic<br>
components from the predetermined object signal with reference to the fundamental<br>
frequency information. A multi-channel audio decoding method may not be efficient<br>
enough to be used in a karaoke system. However, if fundamental frequency in-<br>
formation regarding vocal object signals is included in side information and harmonic<br>
components of the vocal object signals are removed during a post-processing<br>
operation, it is possible to realize a high-performance karaoke system by using the<br>
embodiment of FIG. 8. The embodiment of FIG. 8 may also be applied to object<br>
signals, other than vocal object signals. For example, it is possible to remove the sound<br>
of a predetermined musical instrument by using the embodiment of FIG. 8. Also, it is<br>
possible to amplify predetermined harmonic components using fundamental frequency<br>
information regarding object signals by using the embodiment of FIG. 8. In short, post-<br>
processing parameters may enable the application of various effects such as the<br>
insertion of a reverberation effect, the addition of noise, and the amplification of a low-<br>
pitch portion that cannot be performed by the multi-channel decoder 171.<br>
[84]	The postprocessor 173 may directly apply an additional effect to a downmix signal<br>
or add a downmix signal to which an effect has already been applied the output of the<br>
multi-channel decoder 171. The postprocessor 173 may change the spectrum of an<br>
object or modify a downmix signal whenever necessary. If it is not appropriate to<br>
directly perform an effect processing operation such as reverberation on a downmix<br>
signal and to transmit a signal obtained by the effect processing operation to the multichannel<br>
decoder 171, the preprocessor 173 may simply add the signal obtained by the<br>
effect processing operation to the output of the multi-channel decoder 171, instead of<br><br>
directly performing effect processing on the downmix signal and transmitting the result<br>
of effect processing to the multi-channel decoder 171.<br>
[85]	FIG. 9 illustrates a block diagram of an audio decoding apparatus 180 according to<br>
a seventh embodiment of the present invention. Referring to FIG. 9, the audio<br>
decoding apparatus 180 includes a preprocessor 181, a multi-channel decoder 183, a<br>
postprocessor 185, and a parameter converter 187.<br>
[86]	The description of the preprocessor 161 directly applies to the preprocessor 181.<br>
The postprocessor 185 may be used to add the output of the preprocessor 181 and the<br>
output of the multi-channel decoder 185 and thus to provide a final signal. In this case,<br>
the postprocessor 185 simply serves an adder for adding signals. An effect parameter<br>
may be provided to whichever of the preprocessor 181 and the postprocessor 185<br>
performs the application of an effect. In addition, the addition of a signal obtained by<br>
applying an effect to a downmix signal to the output of the multi-channel decoder 183<br>
and the application of an effect to the output of the multi-channel decoder 185 may be<br>
performed at the same time.<br>
[87]	The preprocessors 161 and 181 of FIGS. 7 and 9 may perform rendering on a<br>
downmix signal according to control information provided by a user. In addition, the<br>
preprocessors 161 and 181 of FIGS. 7 and 9 may increase or reduce the levels of object<br>
signals and alter the spectra of object signals. In this case, the preprocessors 161 and<br>
181 of FIGS. 7 and 9 may perform the functions of an ADG module.<br>
[88]	The rendering of an object signal according to direction information of the object<br>
signal, the adjustment of the level of the object signal and the alteration of the<br>
spectrum of the object signal may be performed at the same time. In addition, some of<br>
the rendering of an object signal according to direction information of the object<br>
signal, the adjustment of the level of the object signal and the alteration of the<br>
spectrum of the object signal may be performed by using the preprocessor 161 or 181,<br>
and whichever of the rendering of an object signal according to direction information<br>
of the object signal, the adjustment of the level of the object signal and the alteration of<br>
the spectrum of the object signal is not performed by the preprocessor 161 or 181 may<br>
be performed by using an ADG module. For example, it is not efficient to alter the<br>
spectrum of an object signal by using an ADG module, which uses a quantization level<br>
interval and a parameter band interval. In this case, the preprocessor 161 or 181 may<br>
be used to minutely alter the spectrum of an object signal on a frequency-by-frequency<br>
basis, and an ADG module may be used to adjust the level of the object signal.<br>
[89]	FIG. 10 illustrates a block diagram of an audio decoding apparatus according to an<br>
eight embodiment of the present invention. Referring to FIG. 10, the audio decoding<br>
apparatus 200 includes a rendering matrix generator 201, a transcoder 203, a multichannel<br>
decoder 205, a preprocessor 207, an effect processor 208, and an adder 209.<br><br>
[90]	The rendering matrix generator 201 generates a rendering matrix, which represents<br>
object position information regarding the positions of object signals and playback configuration<br>
information regarding the levels of the object signals, and provides the<br>
rendering matrix to the transcoder 203. The rendering matrix generator 201 generates<br>
3D information such as an HRTF coefficient based on the object position information.<br>
An HRTF is a transfer function which describes the transmission of sound waves<br>
between a sound source at an arbitrary position and the eardrum, and returns a value<br>
that varies according to the direction and altitude of the sound source. If a signal with<br>
no directivity is filtered using the HRTF, the signal may be heard as if it were<br>
reproduced from a certain direction.<br>
[91]	The object position information and the playback configuration information, which<br>
is received by the rendering matrix generator 201, may vary over time and may be<br>
provided by an end user.<br>
[92]	The transcoder 203 generates channel-based side information based on object-<br>
based side information, the rendering matrix and 3D information, and provides the<br>
multi-channel decoder 209 with the channel-based side information and 3D in-<br>
formation necessary for the multi-channel decoder 209. That is, the transcoder 203<br>
transmits channel-based side information regarding M channels, which is obtained<br>
from object-based parameter information regarding N object signals, and 3D information<br>
of each of the N object signals to the multi-channel decoder 205.<br>
[93]	The multi-channel decoder 205 generates a multi-channel audio signal based on a<br>
downmix signal and the channel-based side information provided by the transcoder<br>
203, and performs 3D rendering on the multi-channel audio signal according to 3D information,<br>
thereby generating a 3D multi-channel signal. The rendering matrix<br>
generator 201 may include a 3D information database (not shown).<br>
[94]	If there is the need to preprocess a downmix signal before the input of the<br>
downmix signal to the multi-channel decoder 205, the transcoder 203 transmits information<br>
regarding preprocessing to the preprocessor 207. The object-based side information<br>
includes information regarding all object signals, and the rendering matrix<br>
includes the object position information and the playback configuration information.<br>
The transcoder 203 may generate channel-based side information based on the object-<br>
based side information and the rendering matrix, and then generates the channel-based<br>
side information necessary for mixing and reproducing the object signals according to<br>
the channel information. Thereafter, the transcoder 203 transmits the channel-based<br>
side information to the multi-channel decoder 205.<br>
[95]	The channel-based side information and the 3D information provided by the<br>
transcoder 205 may include frame indexes. Thus, the multi-channel decoder 205 may<br>
synchronize the channel-based side information and the 3D information by using the<br><br>
frame indexes, and may thus be able to apply the 3D information only to certain frames<br>
of a bitstream. In addition, even if the 3D information is updated, it is possible to easily<br>
synchronize the channel-based side information and the updated 3D information by<br>
using the frame indexes. That is, the frame indexes may be included in the channel-<br>
based side information and the 3D information, respectively, in order for the multichannel<br>
decoder 205 to synchronize the channel-based side information and the 3D information.<br>
[96]	The preprocessor 207 may perform preprocessing on an input downmix signal, if<br>
necessary, before the input downmix signal is input to the multi-channel decoder 205.<br>
As described above, if the input downmix signal is a stereo signal and there is the need<br>
to play back an object signal belonging to a left channel from a right channel, the<br>
downmix signal may have be subjected to preprocessing performed by the preprocessor<br>
207 before being input to the multi-channel decoder 205 because the multichannel<br>
decoder 205 cannot shift an object signal from one channel to another. Information<br>
necessary for preprocessing the input downmix signal may be provided to<br>
the preprocessor 207 by the transcoder 205. A downmix signal obtained by preprocessing<br>
performed by the preprocessor 207 may be transmitted to the multi-channel<br>
decoder 205.<br>
[97]	The effect processor 208 and the adder 209 may directly apply an additional effect<br>
to a downmix signal or add a downmix signal to which an effect has already been<br>
applied to the output of the multi-channel decoder 205. The effect processor 208 may<br>
change the spectrum of an object or modify a downmix signal whenever necessary. If<br>
it is not appropriate to directly perform an effect processing operation such as reverberation<br>
on a downmix signal and to transmit a signal obtained by the effect<br>
processing operation to the multi-channel decoder 205, the effect processor 208 may<br>
simply add the signal obtained by the effect processing operation to the output of the<br>
multi-channel decoder 205, instead of directly performing effect processing on the<br>
downmix signal and transmitting the result of effect processing to the multi-channel<br>
decoder 205.<br>
[98]	A rendering matrix generated by the rendering matrix generator 201 will<br>
hereinafter be described in detail.<br>
[99]	A rendering matrix is a matrix that represents the positions and the playback configuration<br>
of object signals. That is, if there are N object signals and M channels, a<br>
rendering matrix may indicate how the N object signals are mapped to the M channels<br>
in various manners.<br>
[100]	More specifically, when N object signals are mapped to M channels, an N*M<br>
rendering matrix may be established. In this case, the rendering matrix includes N<br>
rows, which respectively represent the N object signals, and M columns, which re-<br><br>
spectively represent M channels. Each of M coefficients in each of the N rows may be<br>
a real number or an integer indicating the ratio of part of an object signal allocated to a<br>
corresponding channel to the whole object signal.<br>
[101]	More specifically, the M coefficients in each of the N rows of the N*M rendering<br>
matrix may be real numbers. Then, if the sum of M coefficients in a row of the N*M<br>
rendering matrix is equal to a predefined reference value, for example, 1, it may be<br>
determined that the level of an object signal has not been varied. If the sum of the M<br>
coefficients is less than 1, it is determined that the level of the object signal has been<br>
reduced. If the sum of the M coefficients is greater than 1, it is determined that the<br>
level of the object signal has been increased. The predefined reference value may be a<br>
numerical value, other than 1. The amount by which the level of the object signal is<br>
varied may be restricted to the range of 12dB. For example, if the predefined reference<br>
value is 1 and the sum of the M coefficients is 1.5, it may be determined that the level<br>
of the object signal has been increased by 12dB. If the predefined reference value is 1<br>
and the sum of the M coefficients is 0.5, it is determined that that the level of the object<br>
signal has been reduced by 12 dB. If the predefined reference value is 1 and the sum of<br>
the M coefficients is 0.5 to 1.5, it is determined that the object signal has been varied<br>
by a predetermined amount between -12 dB and +12dB, and the predetermined amount<br>
may be linearly determined according to the sum of the M coefficients.<br>
[ 102]	The M coefficients in each of the N rows of the N*M rendering matrix may be<br>
integers. Then, if the sum of M coefficients in a row of the N*M rendering matrix is<br>
equal to a predefined reference value, for example, 10, 20, 30 or 100, it may be<br>
determined that the level of an object signal has not been varied. If the sum of the M<br>
coefficients is less than the predefined reference value, it may be determined that the<br>
level of the object signal has not been reduced. If the sum of the M coefficients is<br>
greater than the predefined reference value, it may be determined that the level of the<br>
object signal has not been increased. The amount by which the level of the object<br>
signal is varied may be restricted to the range of, for example, 12dB. The amount by<br>
which the sum of the M coefficients is discrepant from the predefined reference value<br>
may represent the amount (unit: dB) by which the level of the object signal has been<br>
varied. For example, if the sum of the M coefficients is one greater than the predefined<br>
reference value, it may be determined that the level of the object signal has been<br>
increased by 2 dB. Therefore, if the predefined reference value is 20 and the sum of the<br>
M coefficients is 23, it may be determined that the level of the object signal has been<br>
increased by 6 dB. If the predefined reference value is 20 and the sum of the M co-<br>
efficients is 15, it may be determined that the level of the object signal has been<br>
reduced by 10 dB.<br>
[103]	For example, if there are six object signals and five channels (i.e., front left (FL),<br><br>
front right (FR), center (C), rear left (RL) and rear right (RR) channels), a 6*5<br>
rendering matrix having six rows respectively corresponding to the six object signals<br>
and five columns respectively corresponding to the five channels may be established.<br>
The coefficients of the 6*5 rendering matrix may be integers indicating the ratio at<br>
which each of the six object signals is distributed among the five channels. The 6*5<br>
rendering matrix may have a reference value of 10. Thus, if the sum of five coefficients<br>
in any one of the six rows of the 6*5 rendering matrix is equal to 10, it may<br>
be determined that the level of a corresponding object signal has not been varied. The<br>
amount by which the sum of the five coefficients in any one of the six rows of the 6*5<br>
rendering matrix is discrepant from the reference value represents the amount by which<br>
the level of a corresponding object signal has been varied. For example, if the sum of<br>
the five coefficients in any one of the six rows of the 6*5 rendering matrix is<br>
discrepant from the reference value by 1, it may be determined that the level of a corresponding<br>
object signal has been varied by 2 dB. The 6*5 rendering matrix may be<br>
represented by Equation (1):<br><br>
[107]	Referring to the 6*5 rendering matrix of Equation (1), the first row corresponds to<br>
the first object signal and represents the ratio at which the first object signal is<br>
distributed among FL, FR, C, RL and RR channels. Since the first coefficient of the<br>
first row has a greatest integer value of 3 and the sum of the coefficients of the first<br>
row is 10, it is determined that the first object signal is mainly distributed to the FL<br>
channel, and that the level of the first object signal has not been varied. Since the<br>
second coefficient of the second row, which corresponds to the second object signal,<br>
has a greatest integer value of 4 and the sum of the coefficients of the second row is<br>
12, it is determined that the second object signal is mainly distributed to the FR<br>
channel, and that the level of the second object signal has been increased by 4 dB.<br>
Since the third coefficient of the third row, which corresponds to the third object<br>
signal, has a greatest integer value of 12 and the sum of the coefficients of the third<br><br>
row is 12, it is determined that the third object signal is distributed only to the C<br>
channel, and that the level of the third object signal has been increased by 4 dB. Since<br>
all the coefficients of the fifth row, which corresponds to the fifth object signal, has the<br>
same integer value of 2 and the sum of the coefficients of the fifth row is 10, it is<br>
determined that the fifth object signal is evenly distributed among the FL, FR, C, RL<br>
and RR channels, and that the level of the fifth object signal has not been varied.<br>
[108]	Alternatively, when N object signals are mapped to M channels, an N*(M+1)<br>
rendering matrix may be established. An N*(M+1) rendering matrix is very similar to<br>
an N*M rendering matrix. More specifically, in an (N*(M+1) rendering matrix, like in<br>
an N*M rendering matrix, first through M-th coefficients in each of N rows represent<br>
the ratio at which a corresponding object signal distributed among FL, FR, C, RL and<br>
RR channels. However, an (N*(M+1) rendering matrix, unlike an N*M rendering<br>
matrix, has an additional column (i.e., an (M+l)-th column) for representing the levels<br>
of object signals.<br>
[109]	An N*(M+1) rendering matrix, unlike an N*M rendering matrix, indicates how an<br>
object signal is distributed among M channels and whether the level of the object<br>
signal has been varied separately. Thus, by using an N* (M+1) rendering matrix, it is<br>
possible to easily obtain information regarding a variation, if any, in the level of an<br>
object signal without a requirement of additional computation. Since an N*(M+1)<br>
rendering matrix is almost the same as an N*M rendering matrix, an N*(M+1)<br>
rendering matrix can be easily converted into an N*M rendering matrix or vice versa<br>
without a requirement of additional information.<br>
[110]	Still alternatively, when N object signals are mapped to M channels, an N*2<br>
rendering matrix may be established. The N*2 rendering matrix has a first column<br>
indicating the angular positions of object signals and a second column indicating a<br>
variation, if any, in the level of each of the object signals. The N*2 rendering matrix<br>
may represent the angular positions of object signals at regular intervals of 1 or 3<br>
degrees within the range of 0-360 degrees. An object signal that is evenly distributed<br>
among all directions may be represented by a predefined value, rather than by an angle.<br>
[Ill]	An N*2 rendering matrix may be converted into an N*3 rendering matrix which<br>
can indicate not only the 2D directions of object signals but also the 3D directions of<br>
the object signals. More specifically, a second column of an N*3 rendering matrix may<br>
be used to indicate the 3D directions of object signals. A third column of an N*3<br>
rendering matrix indicates a variation, if any, in the level of each object signal using<br>
the same method used by an N*M rendering matrix. If a final playback mode of an<br>
object decoder is binaural stereo, the rendering matrix generator 201 may transmit 3D<br>
information indicating the position of each object signal or an index corresponding to<br>
the 3D information. In the latter case, the transcoder 203 may need to have 3D in-<br><br>
formation corresponding to an index transmitted by the rendering matrix generator<br>
201. In addition, if 3D information indicating the position of each object signal is<br>
received from the rendering matrix generator 201, the transcoder 203 may be able to<br>
calculate 3D information that can be used by the multi-channel decoder 205 based on<br>
the received 3D information, a rendering matrix, and object-based side information.<br>
[112]	A rendering matrix and 3D information may adaptively vary in real time according<br>
to a modification made to object position information and playback configuration information<br>
by an end user. Therefore, information regarding whether the rendering<br>
matrix and the 3D information is updated and updates, if any, in the rendering matrix<br>
and the 3D information may be transmitted to the transcoder 203 at regular intervals of<br>
time, for example, at intervals of 0.5 sec. Then, if updates in the rendering matrix and<br>
the 3D information are detected, the transcoder 203 may perform linear conversion on<br>
the received updates and an existing rendering matrix and existing 3D information,<br>
assuming that the rendering matrix and the 3D information linearly vary over time.<br>
[113]	If object position information and playback configuration information has not been<br>
modified by an end user since the transmission of a rendering matrix and 3D information<br>
to the transcoder 203, information indicating that the rendering matrix and<br>
the 3D information has not been varied may be transmitted to the transcoder 203. On<br>
the other hand, if the object position information and the playback configuration information<br>
has been modified by an end user since the transmission of the rendering<br>
matrix and the 3D information to the transcoder 203, information indicating that the<br>
rendering matrix and the 3D information has been varied and updates in the rendering<br>
matrix and the 3D information may be transmitted to the transcoder 203. More<br>
specifically, updates in the rendering matrix and updates in the 3D information may be<br>
separately transmitted to the transcoder 203. Alternatively, updates in the rendering<br>
matrix and/or updates in the 3D information may be collectively represented by a<br>
predefined representative value. Then, the predefined representative value may be<br>
transmitted to the transcoder 203 along with information indicating that the predefined<br>
representative value corresponds to updates in the rendering matrix or updates in the<br>
3D information. In this manner, it is possible to easily notify the transcoder 203<br>
whether or not a rendering matrix and 3D information have been updated.<br>
[114]	An N*M rendering matrix, like the one indicated by Equation (1), may also include<br>
an additional column for representing 3D direction information of object signals. In<br>
this case, the additional column may represent 3D direction information of object<br>
signals as angles in the range of -90 to +90 degrees. The additional column may be<br>
provided not only to an N+M matrix but also to an N*(M+1) rendering matrix and an<br>
N*2 matrix. 3D direction information of object signals may not be necessary for use in<br>
a normal decoding mode of a multi-channel decoder. Instead, 3D direction information<br><br>
of object signals may be necessary for use in a binaural mode of a multi-channel<br>
decoder. 3D direction information of object signals may be transmitted along with a<br>
rendering matrix. Alternatively, 3D direction information of object signals may be<br>
transmitted along with 3D information. 3D direction information of object signals dose<br>
not affect channel-based side information but affects 3D information during a binaural-<br>
mode decoding operation.<br>
[115]	Information regarding the spatial positions and the levels of object signals may be<br>
provided as a rendering matrix. Alternatively, information regarding the spatial<br>
positions and the levels of object signals may be represented as modifications to the<br>
spectra of the object signal such as intensifying low-pitch parts or high-pitch parts of<br>
the object signals. In this case, information regarding the modifications to the spectra<br>
of the object signals may be transmitted as level variations in each parameter band,<br>
which is used in a multi-channel codec. If an end user controls modifications to the<br>
spectra of object signals, information regarding the modifications to the spectra of the<br>
object signals may be transmitted as a spectrum matrix separately from a rendering<br>
matrix. The spectrum matrix may have as many rows as there are object signals and<br>
have as many columns as there are parameters. Each coefficient of the spectrum matrix<br>
indicates information regarding the adjustment of the level of each parameter band.<br>
[116]	Thereafter, the operation of the transcoder 203 will hereinafter be described in<br>
detail. The transcoder 203 generates channel-based side information for the multichannel<br>
decoder 205 based on object-based side information, rendering matrix information<br>
and 3D information and transmits the channel-based side information to the<br>
multi-channel decoder 205. In addition, the transcoder 203 generates 3D information<br>
for the multi-channel decoder 205 and transmits the 3D information to the multichannel<br>
decoder 205. If an input downmix signal needs to be preprocessed before<br>
being input to the multi-channel decoder 205, the transcoder 203 may transmit information<br>
regarding the input downmix signal.<br>
[117]	The transcoder 203 may receive object-based side information indicating how a<br>
plurality of object signals are included in an input downmix signal. The object-based<br>
side information may indicate how a plurality of object signals are included in an input<br>
downmix signal by using an OTT box and a TTT box and using CLD, ICC and CPC<br>
information. The object-based side information may provide descriptions of various<br>
methods that can be performed by an object encoder for indicating information<br>
regarding each of a plurality of object signals, and may thus be able to indicate how the<br>
object signals are included in side information.<br>
[118]	In the case of a TTT box of a multi-channel codec, L, C and R signals may be<br>
downmixed or upmixed into L and R signals. In this case, the C signal may share a<br>
little bit of both the L and R signals. However, this rarely happens in the case of<br><br>
downmixing or upmixing object signals. Therefore, an OTT box is widely used to<br>
perform upmixing or downmixing for object coding. Even if a C signal includes an in-<br>
dependent signal component, rather than parts of L and R signals, a TTT box may be<br>
used to perform upmixing or downmixing for object coding.<br>
[119]	For example, if there are six object signals, the six object signals may be converted<br>
into a downmix signal by an OTT box, and information regarding each of the object<br>
signals may be obtained by using an OTT box, as illustrated in FIG. 11.<br>
[120]	Referring to FIG. 11, six object signals may be represented by one downmix signal<br>
and information (such as CLD and ICC information) provided by a total of five OTT<br>
boxes 211, 213, 215, 217 and 219. The structure illustrated in FIG. 11 may be altered<br>
in various manners. That is, referring to FIG. 11, the first OTT box 211 may receive<br>
two of the six object signals. In addition, the way in which the OTT boxes 211, 213,<br>
215, 217 and 219 are hierarchically connected may be freely varied. Therefore, side information<br>
may include hierarchical structure information indicating how the OTT<br>
boxes 211, 213, 215, 217 and 219 are hierarchically connected and input position information<br>
indicating to which OTT box each object signal is input. If the OTT boxes<br>
211, 213, 215, 217 and 219 form an arbitrary tree structure, a method used in a multichannel<br>
codec for representing an arbitrary tree structure may be used to indicate such<br>
hierarchical structure information. In addition, such input position information may be<br>
indicated in various manners.<br>
[121]	Side information may also include information regarding a mute period of each<br>
object signal during. In this case, the tree structure of the OTT boxes 211, 213, 215,<br>
217 and 219 may adaptively vary over time. For example, referring to FIG. 11, when<br>
the first object signal OBJECT 1 is mute, information regarding the first OTT box 211<br>
is unnecessary, and only the second object signal OBJECT2 may be input to the fourth<br>
OTT box 217. Then, the tree structure of the OTT boxes 211, 213, 215, 217 and 219<br>
may vary accordingly. Thus, information regarding a variation, if any, in the tree<br>
structure of the OTT boxes 211, 213, 215, 217 and 219 may be included in side information.<br>
[ 122]	If a predetermined object signal is mute, information indicating that an OTT box<br>
corresponding to the predetermined object signal is not in use and information<br>
indicating that no cues from the OTT box are available may be provided. In this<br>
manner, it is possible to reduce the size of side information by not including information<br>
regarding OTT boxes or TTT boxes that are not in use in side information.<br>
Even if a tree structure of a plurality of OTT or TTT boxes is modified, it is possible to<br>
easily determine which of the OTT or TTT boxes are turned on or off based on information<br>
indicating what object signals are mute. Therefore, there is no need to<br>
frequently transmit information regarding modifications, if any, to the tree structure of<br><br>
the OTT or TTT boxes. Instead, information indicating what object signal is mute may<br>
be transmitted. Then, a decoder may easily determine what part of the tree structure of<br>
the OTT or TTT boxes needs to be modified. Therefore, it is possible to minimize the<br>
size of information that needs to be transmitted to a decoder. In addition, it is possible<br>
to easily transmit cues regarding object signals to a decoder.<br>
[123]	FIG. 12 illustrates a diagram for explaining how a plurality of object signals are<br>
included in a downmix signal. In the embodiment of FIG. 11, an OTT box structure of<br>
multi-channel coding is adopted as it is. However, in the embodiment of FIG. 12, a<br>
variation of the OTT box structure of multi-channel coding is used. That is, referring to<br>
FIG. 12, a plurality of object signals are input to each box, and only one downmix<br>
signal is generated in the end. Referring to FIG. 12, information regarding each of a<br>
plurality of object signals may be represented by the ratio of the energy level of each of<br>
the object signals to the total energy level of the object signals. However, as the<br>
number of object signals increases, the ratio of the energy level of each of the object<br>
signals to the total energy level of the object signals decreases. In order to address this,<br>
one of a plurality of object signal (hereinafter referred to as a highest-energy object<br>
signal) having a highest energy level in a predetermined parameter band is searched<br>
for, and the ratios of the energy levels of the other object signals (hereinafter referred<br>
to as non-highest-energy object signals) to the energy level of the highest-energy<br>
object signal may be provided as information regarding each of the object signals. In<br>
this case, once information indicating a highest-energy object signal and the absolute<br>
value of the energy level of the highest-energy object signal is given, the energy levels<br>
of other non-highest-energy object signals may be easily determined.<br>
[ 124]	The energy level of a highest-energy object signal is necessary for incorporating a<br>
plurality of bitstreams into a single bitstream as performed in a multipoint control unit<br>
(MCU). However, in most cases, the energy level of a highest-energy object signal is<br>
not necessary because the absolute value of the energy level of a highest-energy object<br>
signal can be easily obtained from the ratios of the energy levels of other non-<br>
highest-energy object signals to the energy level of the highest-energy object signal.<br>
[125]	For example, assume that there are four object signals A, B, C and D belonging to<br>
a predetermined parameter band, and that the object signal A is a highest-energy object<br>
signal. Then, the energy E of the predetermined parameter band and the absolute<br>
value E of the energy level of the object signal A satisfy Equation (2):<br>
[126]<br>
[127]	[Equation 2]<br>
[128]<br><br><br>
[129]<br>
[130]	Where a, b, and c respectively indicate the ratios of the energy level of the object<br>
signals B, C and D to the energy level of the object signal. Referring to Equation (2), it<br>
is possible to calculate the absolute value E of the energy level of the object signal A<br>
based on the ratios a, b, and c and the energy E of the predetermined parameter band.<br>
Therefore, unless there is the need to incorporate a plurality of bitstreams into a single<br>
bitstream with the use of an MCU, the absolute value EA of the energy level of the<br>
object signal A may not need to be included in a bitstream. Information indicating<br>
whether the absolute value E of the energy level of the object signal A is included in a<br>
bitstream may be included in a header of the bitstream, thereby reducing the size of the<br>
bitstream.<br>
[131]	On the other hand, if there is the need to incorporate a plurality of bitstreams into a<br>
signal bitstream with the use of an MCU, the energy level of a highest-energy object<br>
signal is necessary. In this case, the sum of energy levels calculated based on the ratios<br>
of the energy levels of non-highest-energy object signals to the energy level of a<br>
highest-energy object signal may not be the same as the energy level of a downmix<br>
signal obtained by downmixing all the object signals. For example, when the energy<br>
level of the downmix signal is 100, the sum of the calculated energy levels may be 98<br>
or 103 due to, for example, errors caused during quantization and dequantization<br>
operations. In order to address this, the difference between the energy level of the<br>
downmix signal and the sum of the calculated energy levels may be appropriately<br>
compensated for by multiplying each of the calculated energy levels by a predetermined<br>
coefficient. If the energy level of the downmix signal is X and the sum of<br>
the calculated energy levels is Y, each of the calculated energy levels may be<br>
multiplied by X/Y. If the difference between the energy level of the downmix signal<br>
and the sum of the calculated energy levels is not compensated for, such quantization<br>
errors may be included in parameter bands and frames, thereby causing signal<br>
distortions.<br>
[132]	Therefore, information indicating which of a plurality of object signals has a<br>
greatest absolute value of energy in a predetermined parameter band is necessary. Such<br>
information may be represented by a number of bits. The number of bits necessary for i<br>
ndicating which of a plurality of object signals has a greatest absolute value of energy<br>
in a predetermined parameter band vary according to the number of object signals. As<br><br>
the number of object signals increases, the number of bits necessary for indicating<br>
which of a plurality of object signals has a greatest absolute value of energy in a predetermined<br>
parameter band increases. On the other hand, as the number of object<br>
signals decreases, the number of bits necessary for indicating which of a plurality of<br>
object signals has a greatest absolute value of energy in a predetermined parameter<br>
band decreases. A predetermined number of bits may be allocated in advance for<br>
indicating which of a plurality of object signals has a greatest absolute value of energy<br>
in a predetermined parameter band increases. Alternatively, the number of bits for<br>
indicating which of a plurality of object signals has a greatest absolute value of energy<br>
in a predetermined parameter band may be determined based on certain information.<br>
[133]	The size of information indicating which of a plurality of object signals has a<br>
greatest absolute value of energy in each parameter band can be reduced by using the<br>
same method used to reduce the size of CLD, ICC, and CPC information for use in<br>
OTT and/or TTT boxes of a multi-channel codec, for example, by using a time differential<br>
method, a frequency differential method, or a pilot coding method.<br>
[134]	In order to indicate which of a plurality of object signals has a greatest absolute<br>
value of energy in each parameter band, an optimized Huffman table may be used. In<br>
this case, information indicating in what order the energy levels of the object signals<br>
are compared with the energy level of whichever of the object signals has the greatest<br>
absolute energy may be required. For example, if there are five object signals (i.e., first<br>
through fifth object signals) and the third object signal is a highest-energy object<br>
signal, information regarding the third object signal may be provided. Then, the ratios<br>
of the energy levels of the first, second, fourth and fifth object signals to the energy<br>
level of the third object signal may be provided in various manners, and this will<br>
hereinafter be described in further detail.<br>
[135]	The ratios of the energy levels of the first, second, fourth and fifth object signals to<br>
the energy level of the third object signal may be sequentially provided. Alternatively,<br>
the ratios of the energy levels of the fourth, fifth, first and second object signals to the<br>
energy level of the third object signal may be sequentially provided in a circular<br>
manner. Then, information indicating the order in which the ratios of the energy levels<br>
of the first, second, fourth and fifth object signals to the energy level of the third object<br>
signal are provided may be included in a file header or may be transmitted at intervals<br>
of a number of frames. A multi-channel codec may determine CLD and ICC information<br>
based on the serial numbers of OTT boxes. Likewise, information indicating<br>
how each object signal is mapped to a bitstream is necessary.<br>
[136]	In the case of a multi-channel codec, information regarding signals corresponding<br>
to each channel may be identified by the serial numbers of OTT or TTT boxes.<br>
According to an object-based audio encoding method, if there are N object signals, the<br><br>
N object signals may need to be appropriately numbered. However, it is necessary<br>
sometimes for an end user to control the N object signals using an object decoder. In<br>
this case, the end user may have need of not only the serial numbers of the N object<br>
signals but also descriptions of the N object signals such as descriptions indicating that<br>
the first object signal corresponds to the voice of a woman and that the second object<br>
signal corresponds to the sound of a piano. The descriptions of the N object signals<br>
may be included in a header of a bitstream as metadata and then transmitted along with<br>
the bitstream. More specifically, the descriptions of the N object signals may be<br>
provided as text or may be provided by using a code table or codewords.<br>
[137]	Correlation information regarding the correlations between object signals is<br>
necessary sometimes. For this, the correlations between a highest-energy object signal<br>
and other non-highest-energy object signals may be calculated. In this case, a single<br>
correlation value may be designated for all the object signals, which is comparable to<br>
the use of a single ICC value in all OTT boxes.<br>
[138]	If object signals are stereo signals, the left channel energy-to-right channel energy<br>
ratios of the object signals and ICC information is necessary. The left channel energy-<br>
to-right channel energy ratios of the object signals may be calculated using the same<br>
method used to calculate the energy levels of a plurality of object signals based on the<br>
absolute value of the energy level of whichever of the object signals is a highest-<br>
energy object signal and the ratios of the energy levels of the other non-highest-energy<br>
object signals to the energy level of the highest-energy object signal. For example, if<br>
the absolute values of the energy levels of left and right channels of a highest-energy<br>
object signal are A and B, respectively, and the ratio of the energy level of the left<br>
channel of a non-highest-energy object signal to A and the ratio of the energy level of<br>
the right channel of the non-highest-energy object signal to B are x and y, respectively,<br>
the energy levels of the left and right channels of the non-highest-energy object signal<br>
may be calculated as A*x and B*y. In this manner, the left channel energy-to-right<br>
channel energy ratio of a stereo object signal can be calculated.<br>
[139]	The absolute value of the energy level of a highest-energy object signal and the<br>
ratios of the energy levels of other non-highest-energy object signals to the energy<br>
level of the highest-energy object signal may also be used when the object signals are<br>
mono signals, a downmix signal obtained by the mono object signals is a stereo signal,<br>
and the mono object signals are included in both channels of the stereo downmix<br>
signal. In this case, the ratio of the energy of part of each mono object signal included<br>
in the left channel of a stereo downmix signal and the energy of part of a corresponding<br>
mono object signal included in the right channel of the stereo downmix<br>
signal and correlation information is necessary, and this directly applies to stereo<br>
object signals. If a mono object signal is included in both L and R channels of a stereo<br><br>
downmix signal, L- and R-channel components of the mono object signal may only<br>
have a level difference, and the mono object signal may have a correlation value of 1<br>
throughout whole parameter bands. In this case, in order to reduce the amount of data,<br>
information indicating that the mono object signal has a correlation value of 1<br>
throughout the whole parameter bands may be additionally provided. Then, there is no<br>
need to indicate the correlation value of 1 for each of the parameter bands. Instead, the<br>
correlation value of 1 may be indicated for the whole parameter bands.<br>
[140]	During the generation of a downmix signal through the summation of a plurality of<br>
object signals, clipping may occur. In order to address this, a downmix signal may be<br>
multiplied by a predefined gain so that the maximum level of the downmix signal can<br>
exceed a clipping threshold. The predefined gain may vary over time. Therefore, information<br>
regarding the predefined gain is necessary. If the downmix signal is a stereo<br>
signal, different gain values may be provided for L- and R-channels of the downmix<br>
signal in order to prevent clipping. In order to reduce the amount of data transmission,<br>
the different gain values may not be transmitted separately. Instead, the sum of the<br>
different gain values and the ratio of the different gain values may be transmitted.<br>
Then, it is possible to reduce a dynamic range and reduce the amount of data<br>
transmission, compared to the case of transmitting the different gain values separately.<br>
[141]	In order to further reduce the amount of data transmission, a bit indicating whether<br>
clipping has occurred during the generation of a downmix signal through the<br>
summation of a plurality of object signals may be provided. Then, only if it is<br>
determined that clipping has occurred, gain values may be transmitted. Such clipping<br>
information may be necessary for preventing clipping during the summation of a<br>
plurality of downmix signals in order to incorporate a plurality of bitstreams. In order<br>
to prevent clipping, the sum of a plurality of downmix signals may be multiplied by the<br>
inverse number of a predefined gain value for preventing clipping.<br>
[142]	FIGS. 13 through 16 illustrate diagrams for explaining various methods of<br>
configuring object-based side information. The embodiments of FIGS. 13 through 16<br>
can be applied not only mono or stereo object signals but also to multi-channel object<br>
signals.<br>
[ 143]	Referring to FIG. 13, a multi-channel object signal (OBJECT A(CH1) through<br>
OBJECT A(CHn)) is input to an object encoder 221. Then, the object encoder 221<br>
generates a downmix signal and side information based on the multi-channel object<br>
signal (OBJECT A(CHl) through OBJECT A(CHn)). An object encoder 223 receives<br>
a plurality of object signals OBJECT 1 through OBJECTn and the downmix signal<br>
generated by the object encoder 221 and generates another downmix signal and<br>
another side information based on the object signals OBJ. 1 through OB J.N and the<br>
received downmix signal. A multiplexer 225 incorporates the side information<br><br>
generated by the object encoder 221 and the side information generated by the object<br>
encoder 223.<br>
[144]	Referring to FIG. 14, an object encoder 233 generates a first bitstream based on a<br>
multi-channel object signal (OBJECT A(CH1) through OBJECT A(CHn)). Then, an<br>
object encoder 231 generates a second bitstream based on a plurality of non-<br>
multi-channel object signals OBJECT 1 through OBJECTn. Then, an object encoder<br>
235 combines the first and second bitstreams into a single bitstream by using almost<br>
the same method used to incorporate a plurality of bitstreams into a single bitstream<br>
with the aid of an MCU.<br>
[145]	Referring to FIG. 15, a multi-channel encoder 241 generates a downmix signal and<br>
channel-based side information based on a multi-channel object signal (OBJECT<br>
A(CHl) through OBJECT A(CHn)). An object encoder 243 receives the downmix<br>
signal generated by the multi-channel encoder 241 and a plurality of non-multi-channel<br>
object signals OBJECT 1 through OBJECTn and generates an object bitstream and side<br>
information based on the received downmix signal and the object signals OBJECT 1<br>
through OBJECTn. A multiplexer 245 combines the channel-based side information<br>
generated by the multi-channel encoder 241 and the side information generated by the<br>
object encoder 243 and outputs the result of the combination.<br>
[146]	Referring to FIG. 16, a multi-channel encoder 253 generates a downmix signal and<br>
channel-based side information based on a multi-channel object signal (OBJECT<br>
A(CHl) through OBJECT A(CHn)). An object encoder 251 generates a downmix si<br>
gnal and side information based on a plurality of non-multi-channel object signals<br>
OBJECT 1 through OBJECTn. An object encoder 255 receives the downmix signal<br>
generated by the multi-channel encoder 253 and the downmix signal generated by the<br>
object encoder 251 and combines the received downmix signals. A multiplexer 257<br>
combines the side information generated by the object encoder 251 and the channel-<br>
based side information generated by the multi-channel encoder 253 and outputs the<br>
result of the combination.<br>
[ 147]	In the case of using object-based audio encoding in teleconferencing, it is<br>
necessary sometimes to incorporate a plurality of object bitstreams into a single<br>
bitstream. The incorporation of a plurality of object bitstreams into a single object<br>
bitstream will hereinafter be described in detail.<br>
[148]	FIG. 17 illustrates a diagram for explaining the incorporation of two object<br>
bitstreams. Referring to FIG. 17, when two object bitstreams are incorporated into a<br>
single object bitstream, side information such as CLD and ICC information present in<br>
the two object bitstreams, respectively, needs to be modified. The two object<br>
bitstreams may be incorporated into a single object bitstream simply by using an<br>
additional OTT box, i.e., an eleventh OTT box, and using side information such as<br><br>
CLD and ICC information provided by the eleventh OTT box.<br>
[149]	Tree configuration information of each of the two object bitstreams must be incorporated<br>
into integrated tree configuration information in order to incorporate the<br>
two object bitstreams into a single object bitstream. For this, additional configuration<br>
information, if any, generated by the incorporation of the two object bitstreams may be<br>
modified, the indexes of a number of OTT boxes used to generate the two object<br>
bitstreams may be modified, and only a few additional processes such as a computation<br>
process performed by the eleventh OTT box and the downmixing of two downmix<br>
signals of the two object bitstreams may be performed. In this manner, the two object<br>
bitstreams can be easily incorporated into a single object bitstream without the need to<br>
modify information regarding each of a plurality of object signals from which the two<br>
object signals originate.<br>
[150]	Referring to FIG. 17, the eleventh OTT box may be optional. In this case, the two<br>
downmix signals of the two object bitstreams may be used as they are as a two-channel<br>
downmix signal. Thus, the two object bitstreams can be incorporated into a single<br>
object bitstream without a requirement of additional computation.<br>
[151]	FIG. 18 illustrates a diagram for explaining the incorporation of two or more independent<br>
object bitstreams into a single object bitstream having a stereo downmix<br>
signal. Referring to FIG. 18, if two or more independent object bitstreams have<br>
different numbers of parameter bands, parameter band mapping may be performed on<br>
the object bitstreams so that the number of parameter bands of one of the object<br>
bitstreams having fewer parameter bands can be increased to be the same as the<br>
number of parameter bands of the other object bitstream.<br>
[152]	More specifically, parameter band mapping may be performed using a predetermined<br>
mapping table. In this case, parameter band mapping may be performed<br>
using a simple linear formula.<br>
[153]	If there are overlapping parameter bands, parameter values may be appropriately<br>
mixed in consideration of the amount by which the overlapping parameter bands<br>
overlap each other. In the situations when low complexity is prioritized, parameter<br>
band mapping may be performed on two object bitstreams so that the number of<br>
parameter bands of one of the two object bitstreams having more parameter bands can<br>
be reduced to be the same as the number of parameter bands of the other object<br>
bitstream.<br>
[154]	In the embodiments of FIGS. 17 and 18, two or more independent object bitstreams<br>
can be incorporated into an integrated object bitstream without a requirement of the<br>
computation of existing parameters of the independent object bitstreams. However, in<br>
the case of incorporating a plurality of downmix signals, parameters regarding the<br>
downmix signals may need to be calculated again through QMF/hybrid analysis.<br><br>
However, this computation requires a large amount of computation, thereby compromising<br>
the benefits of the embodiments of FIGS. 17 and 18. Therefore, it is<br>
necessary to come up with methods of extracting parameters without a requirement of<br>
QMF/hybrid analysis or synthesis even when downmix signals are downmixed. For<br>
this, energy information regarding the energy of each parameter band of each downmix<br>
signal may be included in an object bitstream. Then, when downmix signals are<br>
downmixed, information such as CLD information may be easily calculated based on<br>
such energy information without a requirement of QMF/hybrid analysis or synthesis.<br>
Such energy information may represent a highest energy level for each parameter band<br>
or the absolute value of the energy level of a highest-energy object signal for each<br>
parameter band. The amount of computation may be further reduced by using ICC<br>
values obtained from a time domain for an entire parameter band.<br>
[155]	During the downmix of a plurality of downmix signals, clipping may occur. In<br>
order to address this, the levels of downmix signals may be reduced. If the levels of<br>
downmix signals are reduced, level information regarding the reduced levels of the<br>
downmix signals may need to be included in an object bitstream. The level information<br>
for preventing clipping may be applied to each frame of an object bitstream or may be<br>
applied only to some frames in which clipping occurs. The levels of the original<br>
downmix signals may be calculated by inversely applying the level information for<br>
preventing clipping during a decoding operation. The level information for preventing<br>
clipping may be calculated in a time domain and thus does not need to be subjected to<br>
QMF/hybrid synthesis or analysis. The incorporation of a plurality of object signals<br>
into a single object bitstream may be performed using the structure illustrated in FIG.<br>
12, and this will hereinafter be described in detail with reference to FIG. 19.<br>
[156]	FIG. 19 illustrates a diagram for explaining the incorporation of two independent<br>
object bitstreams into a single object bitstream. Referring to FIG. 19, a first box 261<br>
generates a first object bitstream, and a second box 263 generates a second object<br>
bitstream. Then, a third box 265 generates a third object bitstream by combining the<br>
first and second bitstreams. In this case, if the first and second object bitstreams<br>
include information the absolute value of the energy level of a highest-energy object<br>
signal for each parameter band and the ratios of the energy levels of other non-<br>
highest-energy object signals to the energy level of the highest-energy object signal<br>
and gain information regarding gain values, which are multiplied by downmix signals<br>
by the first and second boxes 261 and 263, the third box 265 may generate the third<br>
object bitstream simply by incorporating the first and second bitstreams without a re-<br>
quirement of additional parameter computation or extraction.<br>
[157]	The third box 265 receives a plurality of downmix signals DOWNMEX_A and<br>
DOWNMIX_B. The third box 265 converts the downmix signals DOWNMIX_A and<br><br>
DOWNMIXJB into PCM signals and adds up the PCM signals, thereby generating a<br>
single downmix signal. During this process, however, clipping may occur. In order to<br>
address this, the downmix signals DOWNMIX_A and DOWNMIX_B may be<br>
multiplied by a predefined gain value. Information regarding the predefined gain value<br>
may be included in the third object bitstream and transmitted along with the third<br>
object bitstream.<br>
[158]	The incorporation of a plurality of object bitstreams into a single object bitstream<br>
will hereinafter be described in further detail. Referring to FIG. 19, paramA may<br>
include information regarding whichever of a plurality of object signals OBJECT 1<br>
through OBJECTn is a highest-energy object signal and the ratios of the energy levels<br>
of the other non-highest-energy object signals to the energy level of the highest-energy<br>
object signal. Likewise, SIDE INFO may include SIDE INFO A may include information<br>
regarding whichever of a plurality of object signals OBJECT 1 through<br>
OBJECTn is a highest-energy object signal and the ratios of the energy levels of the<br>
other non-highest-energy object signals to the energy level of the highest-energy object<br>
signal.<br>
[159]	SIDE_INFO_A and SIDE._INFO_B may be included in parallel in one bitstream,<br>
as illustrated in FIG. 20. In this case, a bit indicating whether more than one bitstream<br>
exists in parallel may be additionally provided.<br>
[160]	Referring to FIG. 20, in order to indicate whether a predetermined bitstream is an<br>
integrated bitstream including more than one bitstream therein or not, information<br>
indicating whether the predetermined bitstream is an integrated bitstream, information<br>
regarding the number of bitstreams, if any, included in the predetermined bitstream,<br>
and information regarding the original positions of bitstreams, if any, included in the<br>
predetermined bitstream may be provided at the head of the predetermined bitstream<br>
and followed by more than one bitstream, if any, in the predetermined bitstream. In<br>
this case, a decoder may determine whether the predetermined bitstream is an<br>
integrated bitstream including more than one bitstream by analyzing the information at<br>
the head of the predetermined bitstream. This type of bitstream incorporation method<br>
does not require additional processes, other than the addition of a few identifiers to a<br>
bitstream. However, such identifiers need to be provided at intervals of a number of<br>
frames. In addition, this type of bitstream incorporation method requires a decoder to<br>
determine whether every bitstream that the decoder receives is an integrated bitstream<br>
or not.<br>
[161]	As an alternative to the above-mentioned bitstream incorporation method, a<br>
plurality of bitstreams may be incorporated into a single bitstream in such a manner<br>
that a decoder cannot recognize that the single bitstream is an integrated bitstream or<br>
not. This will hereinafter be described in detail with reference to FIG. 21.<br><br>
[162]	Referring to FIG. 21, the energy level of a highest-energy object signal represented<br>
by SIDE_INFO_A and the energy level of a highest-energy object signal represented<br>
by SIDE_INFO_B are compared. Then, whichever of the two object signals has a<br>
higher energy level is determined to be a highest-energy object signal of an integrated<br>
bitstream. For example, if the energy level of the highest-energy object signal<br>
represented by SIDE_INFO_ A is higher than the energy level of the highest-energy<br>
object signal represented by SIDE_INFO_B, the highest-energy object signal<br>
represented by SIDE_INFO_A may become a highest-energy object signal of an<br>
integrated bitstream. Then, energy ratio information of SIDE_INFO_A may be used in<br>
the integrated bitstream as it is, whereas energy ratio information of SIDE_INFO_B<br>
may be multiplied by the ratio of the energy levels of the<br>
[163]	Then, energy ratio information of whichever of SIDE__INFO_A and<br>
SIDE_INFO_B includes information regarding the highest-energy object signal of the<br>
integrated bitstream may be used in the integrated bitstream, and energy ratio information<br>
of the highest-energy object signal represented by Param A and the highest-<br>
energy object signal represented by SIDE_INFO_B. This method involves the recalculation<br>
of energy ratio information of SIDE_INFO_B. However, the recalculation<br>
of energy ratio information of SIDE_INFO_B is relatively not complicated. In this<br>
method, a decoder may not be able to determine whether a bitstream that it receives is<br>
an integrated bitstream including more than one bitstream or not, and thus, a typical<br>
decoding method may be used.<br>
[164]	Two object bitstreams including stereo downmix signals may be easily incorporated<br>
into a single object bitstream without a requirement of the recalculation of<br>
information regarding object signals by using almost the same method used to incorporate<br>
bitstreams including mono downmix signals. In an object bitstream, information<br>
regarding a tree structure that downmixes object signals is followed by<br>
object signal information obtained from each branch (i.e., each box) of the tree<br>
structure.<br>
[165]	Object bitstreams have been described above, assuming that certain object are only<br>
distributed to a left channel or a right channel of a stereo downmix signal. However,<br>
object signals are generally distributed between both channels of a stereo downmix<br>
signal. Therefore, it will hereinafter be described in detail how to generate an object<br>
bitstream based on object bitstreams that are distributed between two channels of a<br>
stereo downmix signal.<br>
[166]	FIG. 22 illustrates a diagram for explaining a method of generating a stereo<br>
downmix signal by mixing a plurality of object signals, and more particularly, a<br>
method of downmixing four object signals OBJECT 1 through OBJECT4 into L and R<br>
stereo signals. Referring to FIG. 22, some of the four object signals OBJECT 1 through<br><br>
OBJECT4 belong to both L and R channels of a downmix signal. For example, the first<br>
object signal OBJECT 1 is distributed between the L and R channels at a ratio of a:b, as<br>
indicated by Equation (3):<br>
[167]<br>
[168]	[Equation 3]<br>
[169]<br><br>
[170]<br>
[171]	If an object signal is distributed between the L and R channels of a stereo downmix<br>
signal, channel distribution ratio information regarding the ratio (a:b) at which the<br>
object signal is distributed between the L and R channels may be additionally required.<br>
Then, information regarding the object signal such as CLD and ICC information may<br>
be calculated by performing downmixing using OTT boxes for the L and R channels of<br>
a stereo downmix signal, and this will hereinafter be described in further detail with<br>
reference to FIG. 23.<br>
[172]	Referring to FIG. 23, once CLD and ICC information obtained from a plurality of<br>
OTT boxes during a downmixing operation and channel distribution ratio information<br>
of each of a plurality of object signals is provided, it is possible to calculate a multichannel<br>
bitstream that varies adaptively to any modification made to object position information<br>
and playback configuration information by an end user. In addition, if a<br>
stereo downmix signal needs to be processed through downmix preprocessing, it is<br>
possible to obtain information regarding how the stereo downmix signal is processed<br>
through downmix preprocessing and to transmit the obtained information to a preprocessor.<br>
That is, if there is no channel distribution ratio information of each of a<br>
plurality of object signals provided, there is no way to calculate a multi-channel<br>
bitstream and obtain information necessary for the operation of a preprocessor.<br>
Channel distribution ratio information of an object signal may be represented as a ratio<br>
of two integers or a scalar (unit: dB).<br>
[173]	As described above, if an object signal is distributed between two channels of a<br>
stereo downmix signal, channel distribution ratio information of the object signal may<br>
be required. Channel distribution ratio information may have a fixed value indicating<br>
the ratio at which an object signal is distributed between two channels of a stereo<br>
downmix signal. Alternatively, channel distribution ratio information of an object<br><br>
signal may vary from one frequency band to another frequency band of the object<br>
signal especially when the channel distribution ratio information is used as ICC information.<br>
If a stereo downmix signal is obtained by a complicated downmix<br>
operation, i.e., if an object signal belongs to two channels of a stereo downmix signal<br>
and is downmixed by varying ICC information from one frequency band to another<br>
frequency band of the object signal, a detailed description of the downmixing of the<br>
object signal may be additionally required in order to decode a finally-rendered object<br>
signal. This embodiment may be applied to all possible object structures that have<br>
already been described.<br>
[174]	Thereafter, preprocessing will hereinafter be described in detail with reference to<br>
FIGS. 24 through 27. If a downmix signal input to an object decoder is a stereo signal,<br>
the input downmix signal may need to be preprocessed before being input to a multichannel<br>
decoder of the object decoder because the multi-channel decoder cannot map a<br>
signal belonging to a left channel of the input downmix signal to a right channel.<br>
Therefore, in order for an end user to shift the position of an object signal belonging to<br>
the left channel of the input downmix signal to a right channel, the input downmix<br>
signal may need to be preprocessed, and the preprocessed downmix signal may be<br>
input to the multi-channel decoder.<br>
[175]	The preprocessing of a stereo downmix signal may be performed by obtaining preprocessing<br>
information from an object bitstream and from a rendering matrix and appropriately<br>
processing the stereo downmix signal according to the preprocessing information,<br>
and this will hereinafter be described in detail.<br>
[176]	FIG. 24 illustrates a diagram for explaining how to configure a stereo downmix<br>
signal based on four object signals OBJECT 1 through OBJECT4. Referring to FIG. 24,<br>
the first object signal OBJECT 1 is distributed between L and R channels at a ratio of<br>
a:b, the second object signal OB JECT2 is distributed between the L and R channels at<br>
a ratio of c:d, the third object signal OBJECT3 is distributed only to the L channel, and<br>
the fourth object signal OBJECT4 is distributed only to the R channel. Information<br>
such as CLD and ICC may be generated by passing each of the first through fourth<br>
object signals OBJECT 1 through OBJECT4 through a number of OTT, and a downmix<br>
signal may be generated based on the generated information.<br>
[177]	Assume that an end user obtains a rendering matrix by appropriately setting the<br>
positions and the levels of the first through fourth object signals OBJECT 1 through<br>
OBJECT4, and that there are five channels. The rendering matrix may be represented<br>
by Equation (4):<br>
[178]<br>
[179]	[Equation 4]<br>
[180]<br><br><br>
[181]	Referring to Equation (4), when the sum of five coefficients in each of the four<br>
rows is equal to a predefined reference value, i.e., 100, it is determined that the level of<br>
a corresponding object signal has not been varied. The amount by which the sum of the<br>
five coefficients in each of the four rows is discrepant from the predefined reference<br>
value may be the amount (unit: dB) by which the level of a corresponding object signal<br>
has been varied. The first, second, third, fourth and fifth columns of the rendering<br>
matrix of Equation (4) represent FL, FR, C, RL, and RR channels, respectively.<br>
[182]	The first row of the rendering matrix of Equation (4) corresponds to the first object<br>
signal OBJECTl and has a total of five coefficients, i.e., 30, 10, 20, 30, and 10. Since<br>
the sum of the five coefficients of the first row is 100, it is determined that the level of<br>
the first object signal OBJECT1 has not been varied, and that only the spatial position<br>
of the first object signal OBJECT1 has changed. Even though the five coefficients of<br>
the first row represent different channel directions, they may be largely classified into<br>
two channels: L and R channels. Then, the ratio at which the first object signal<br>
OBJECT1 is distributed between the L and R channels may be calculated as 70%<br>
(=(30+30+20)*0.5):30%(=(10+10+20)*0.5). Therefore, the rendering matrix of<br>
Equation (4) indicates that the level of the first object signal OBJECT1 has not been<br>
varied, and that the first object signal OBJECT1 is distributed between the L and R<br>
channels at a ratio of 70%: 30%. If the sum of five coefficients of any one of the rows<br>
of the rendering matrix of Equation (4) is less than or greater than 100, it may be<br>
determined that the level of a corresponding object signal has changed, and then, the<br>
corresponding object signal may be processed through preprocessing or may be<br>
converted into and transmitted as ADG.<br>
[183]	In order to preprocess downmix signals, the ratio at which the downmix signals are<br>
distributed between parameter bands, from which parameters are extracted from<br>
signals obtained by performing QMF/hybrid conversion on the downmix signals, may<br>
be calculated, and the downmix signals may be redistributed between the parameter<br>
bands according to the setting of a rendering matrix. Various methods of redistributing<br>
downmix signals between parameter bands will hereinafter be described in detail.<br>
[184]	In a first redistribution method, L- and R-channel downmix signals are decoded<br>
separately using their respective side information (such as CLD and ICC information)<br>
and using almost the same method used by a multi-channel codec. Then, object signals<br><br>
distributed between the L- and R-channel downmix signals are restored. In order to<br>
reduce the amount of computation, the L- and R-channel downmix signals may be<br>
decoded only using CLD information. The ratio at which each of the restored object<br>
signals is distributed between the L- and R-channel downmix signals may be<br>
determined based on side information.<br>
[185]	Each of the restored object signals may be redistributed between the L- and R-<br>
channel downmix signals according to a rendering matrix. Then, the redistributed<br>
object signals are downmixed on a channel-by-channel basis by OTT boxes, thereby<br>
completing preprocessing. In short, the first redistribution method adopts the same<br>
method used by a multi-channel codec. However, the first redistribution method requi<br>
res as many decoding processes as there are object signals for each channel, and<br>
requires a redistribution process and a channel-based downmix process.<br>
[186]	In a second redistribution method, unlike in the first redistribution method, object<br>
signals are not restored from L- and R-downmix signals. Instead, each of the L- and R-<br>
downmix signals is divided into two portions: one portion L_L or R_R that should be<br>
left in a corresponding channel and the other portion L_R or R_.L that should be redistributed,<br>
as illustrated in FIG. 25. Referring to FIG. 25, L_L indicates a portion of<br>
the L-channel downmix signal that should be left in an L channel, and L_R indicates a<br>
portion of the L-channel downmix signal that should be added to an R channel.<br>
Likewise, R_R indicates a portion of the R-channel downmix signal that should be left<br>
in the R channel, and R_L indicates a portion of the R-channel downmix signal that<br>
should be added to the L channel. Each of the L- and R-channel downmix signals may<br>
be divided into two portions (L_L and L_R or R_R and R_L) according to the ratio at<br>
which each object signal is distributed between the L- and R-downmix signals, as<br>
defined by Equation (2), and the ratio at which each object signal should be distributed<br>
between preprocessed L and R channels L and R , as defined by Equation (3).<br>
Therefore, it may be determined how the L- and R-channel downmix signals should be<br>
redistributed between the preprocessed L and R channels L and R by comparing the<br>
ratio at which each object signal is distributed between the L- and R-downmix signals<br>
and the ratio at which each object signal should be distributed between preprocessed L<br>
and R channels L and R .<br>
[187]	The division of an L-channel signal into signals L_L and L_R according to a<br>
predefined energy ratio has been described above. Once the L-channel signal is divided<br>
into signals L_L and L_R, an ICC between the signals L_L and L_R may need to be<br>
determined. The ICC between the signals L_L and L_R may be easily determined<br>
based on ICC information regarding object signals. That is, the ICC between the<br>
signals L_L and L_R may be determined based on the ratio at which each object signal<br>
is distributed between the signals L_L and L_R.<br><br>
[ 188]	The second downmix redistribution method will hereinafter be described in further<br>
detail. Assume that L- and R-channel downmix signals L and R are obtained by the<br>
method illustrated in FIG. 24, and that first, second, third and fourth object signals<br>
OBJECTl, OBJECT2, OBJECT3, and OBJECT4 are distributed between the L- and<br>
R-channel downmix signals L and R at ratios of 1:2, 2:3, 1:0, and 0:1, respectively. A<br>
plurality of object signals may be downmixed by a number of OTT boxes, and in-<br>
formation such as CLD and ICC information may be obtained from the downmixing of<br>
the object signals.<br>
[ 189]	An example of a rendering matrix established for the first through fourth object<br>
signals OBJECTl through OBJECT4 is as represented by Equation (4). The rendering<br>
matrix includes position information of the first through fourth object signals<br>
OBJECTl through OBJECT4. Thus, preprocessed L- and R-channel downmix signals<br>
L and R may be obtained by performing preprocessing using the rendering matrix.<br>
How to establish and interpret the rendering matrix has already been described above<br>
with reference to Equation (3).<br>
[190]	The ratio at which each of the first through fourth object signals OBJECTl through<br>
OBJECT4 is distributed between the preprocessed L- and R-channel downmix signals<br>
L and R may be calculated as indicated by Equation (5):<br>
[191]<br>
[192]	[Equation 5]<br>
[193]<br>
[194]<br><br>
[195]<br>
[ 196]	The ratio at which each of the first through fourth object signals OBJECT 1 through<br>
OBJECT4 is distributed between the L- and R-channel downmix signals L and R may<br>
be calculated as indicated by Equation (6):<br>
[197]<br>
[198]	[Equation 6]<br><br><br>
[200]<br>
[201]	Referring to Equation (5), the sum of part of the third object signal OBJECT3<br>
distributed to the preprocessed L-channel downmix signal L and part of the third object<br>
signal OBJECT3 distributed to the R-channel downmix signal R is 110, and thus, it is<br>
determined that the level of the third object signal OBJECT3 has been increased by 10<br>
. On the other hand, the sum of part of the fourth object signal 0BJECT4 distributed to<br>
the preprocessed L-channel downmix signal L and part of the fourth object signal<br>
OBJECT4 distributed to the R-channel downmix signal R is 95, and thus, it is<br>
determined that the level of the fourth object signal OBJECT4 has been reduced by 5.<br>
If the rendering matrix for the first through fourth object signals OBJECT 1 through<br>
OBJECT4 has a reference value of 100 and the amount by which the sum of the co-<br>
efficients in each of the rows of the rendering matrix is discrepant from the reference<br>
value of 100 represents the amount (unit: dB) by which the level of a corresponding<br>
object signal has been varied, it may be determined that the level of the third object<br>
signal OBJECT3 has been increased by 10 dB, and that the level of the fourth object<br>
signal OBJECT4 has been reduced by 5 dB.<br>
[202]	Equations (5) and (6) may be rearranged into Equation (7):<br>
[203]<br><br>
[206]<br>
[207]	Equation (7) compares the ratio at which each of the first through fourth object<br>
signals OBJECT 1 through OBJECT4 is distributed between L- and R-channel<br>
downmix signals before being preprocessed and the ratio at which each of the first<br>
through fourth object signals OBJECT 1 through OBJECT4 is distributed between the<br><br>
L- and R-channel downmix signals after being preprocessed. Therefore, by using<br>
Equation (7), it is possible to easily determine how much of each of the first through<br>
fourth object signals OBJECT 1 through OBJECT4 should be redistributed through pre-<br>
processing. For example, referring to Equation (7), the ratio at which the second object<br>
signal OB JECT2 is distributed between the L- and R-channel downmix signals<br>
changes from 40:60 to 30:70, and thus, it may be determined that one fourth (25%) of<br>
part of the second object signal OBJECT2 previously distributed to the L-channel<br>
downmix signal needs to be shifted to the R-channel downmix signal. This may<br>
become more apparent by referencing Equation (8):<br>
[208]<br>
[209]	[Equation 8]<br>
[210]	OBJECTl: 55% of part of OBJECTl previously distributed to R needs to be shifted<br>
to L<br>
[211]	OBJECT2: 25% of part of OBJECTl previously distributed to L needs to be shifted<br>
to R<br>
[212]	OBJECT3: 50% of part of OBJECTl previously distributed to L needs to be shifted<br>
to R<br>
[213]	OBJECT4: 50% of part of OBJECTl previously distributed to R needs to be shifted<br>
to L.<br>
[214]<br>
[215]	By using Equation (8), signals L_L, L_R, R_L and R..R of FIG. 25 may be<br>
represented, as indicated by Equation (9):<br>
[216]<br>
[217]	[Equation 9]<br>
[218]<br><br>
[219]	The value of each object signal in Equation (9) may be represented as the ratio at<br>
which a corresponding object signal is distributed between L and R channels by using<br>
dequantized CLD information provided by an OTT box, as indicated by Equation (10):<br>
[220]<br>
[221]	[Equation 10]<br><br><br>
[223]<br>
[224]	CLD information used in each parsing block of FIG. 25 may be determined, as<br>
indicated by Equation (11):<br>
[225]<br>
[226]	[Equation 11]<br>
[227]<br><br><br>
[228]<br>
[229]	In this manner, CLD and ICC information used in a parsing block for generating<br>
the signals L_L and L_R based on an L-channel downmix signal may be determined,<br>
and CLD and ICC information used in a parsing block for generating the signals R_L<br>
and R_R signals based on an R-channel downmix signal may also be determined. Once<br>
the signals L_L, L_R, R_L, and R_R are obtained, as illustrated in FIG. 25, the signals<br>
L_R and R_R may be added, thereby obtaining a preprocessed stereo downmix signal.<br>
If a final channel is a stereo channel, L- and R-channel downmix signals obtained by<br>
preprocessing may be output. In this case, a variation, if any, in the level of each object<br>
signal is yet to be adjusted. For this, a predetermined module which performs the<br>
functions of an ADG module may be additionally provided. Information for adjusting<br>
the level of each object signal may be calculated using the same method used to<br>
calculate ADG information, and this will be described later in further detail. Alternatively,<br>
the level of each object signal may be adjusted during a preprocessing<br><br>
operation. In this case, the adjustment of the level of each object signal may be<br>
performed using the same method used to process ADG. Alternatively to the<br>
embodiment of FIG. 25, a decorrelation operation may be performed by a decorrelator<br>
and a mixer, rather than by parsing modules PARSING 1 and PARSING 2, as illustrated<br>
in FIG. 26, in order to adjust the correlation between signals L and R obtained<br>
by mixing. Referring to FIG. 26, Pre_L and Pre_R indicate L- and R-channel signals<br>
obtained by level adjustment. One of the signals Pre_L and Pre_R may be input to the<br>
decorrelator, and then subjected to a mixing operation performed by the mixer, thereby<br>
obtaining a correlation-adjusted signal.<br>
[230]	A preprocessed stereo downmix signal may be input to a multi-channel decoder. In<br>
order to provide multi-channel output compatible with object position information and<br>
playback configuration information set by an end user, not only a preprocessed<br>
downmix signal but also channel-based side information for performing multi-channel<br>
decoding is necessary. It will hereinafter be described in detail how to obtain channel-<br>
based side information by taking the above-mentioned example again. Preprocessed<br>
downmix signals L and R , which are input to a multi-channel decoder, may be defined<br>
based on Equation (5), as indicated by Equation (12):<br>
[231]<br>
[232]	[Equation 12]<br>
[233]<br><br>
[234]<br>
[235]	The ratio at which each of first through fourth object signals OBJECT 1 through<br>
OBJECT4 is distributed among FL, RL, C, FR and RR channels may be determined as<br>
indicated by Equation (13):<br>
[236]<br>
[237]	[Equation 13]<br>
[238]<br><br><br>
[239]<br>
[240]	The preprocessed downmix signals L and R may be expanded to 5.1 channels<br>
through MPS, as illustrated in FIG. 27. Referring to FIG. 27, parameters of a TTT box<br>
TTTO and OTT boxes OTTA, OTTB and OTTC may need to be calculated in units of<br>
parameter bands even though the parameter bands are not illustrated for convenience.<br>
[241]	The TTT box TTTO may be used in two different modes: an energy-based mode<br>
and a prediction mode. When used in the energy-based mode, the TTT box TTTO<br>
needs two pieces of CLD information. When used in the prediction mode, the TTT box<br>
TTTO needs two pieces of CPC information and a piece of ICC information.<br>
[242]	In order to calculate CLD information in the energy-based mode, the energy ratio<br>
of signals L", R" and C of FIG. 27 may be calculated using Equations (6), (10), and<br>
(13). The energy level of the signal L" may be calculated as indicated by Equation<br>
(14):<br>
[243]<br>
[244]	[Equation 14]<br>
[245]<br><br><br>
[246]	Equation (14) may also be used to calculate the energy level of R" or C. Thereafter,<br>
CLD information used in the TTT box TTTO may be calculated based on the energy<br>
levels of signals L", R" and C, as indicated by Equation (15):<br>
[247]<br>
[248]	[Equation 15]<br>
[249]<br>
[250]<br><br>
[251]<br>
[252]	Equation (14) may be established based on Equation (10). Even though Equation<br><br>
(10) only defines how to calculate energy values for an L channel, energy values for an<br>
R channel can be calculated using Equation (10). In this manner, CLD and ICC values<br>
of third and fourth OTT boxes can be calculated based on CLD and ICC values of first<br>
and second OTT boxes. This, however, may not necessarily apply to all tree structures<br>
but only to certain tree structures for decoding object signals. Information included in<br>
an object bitstream may be transmitted to each OTT box. Alternatively, Information<br>
included in an object bitstream may be transmitted only to some OTT boxes, and information<br>
indicating OTT boxes that have not received the information may be<br>
obtained through computation.<br>
[253]	Parameters such as CLD and ICC information may be calculated for the OTT<br>
boxes OTT A, OTTB and OTTC by using the above-mentioned method. Such multichannel<br>
parameters may be input to a multi-channel decoder and then subjected to<br>
multi-channel decoding, thereby obtaining a multi-channel signal that is appropriately<br>
rendered according to object position information and playback configuration information<br>
desired by an end user.<br>
[254]	The multi-channel parameters may include ADG parameter if the level of object<br>
signals have not yet been adjusted by preprocessing. The calculation of an ADG<br>
parameter will hereinafter be described in detail by taking the above-mentioned<br>
example again.<br>
[255]	When a rendering matrix is established so that the level of a third object signal can<br>
be increased by 10 dB, that the level of a fourth object signal can be reduced by 5 dB,<br>
that the level of a third object signal component in L can be increased by 10 dB, and<br>
that the level of a fourth object signal component in L can be reduced by 5 dB, a ratio<br>
RatioADG,L of energy levels before and after the adjustment of the levels of the third<br>
and fourth object signals may be calculated using Equation (16):<br>
[256]<br>
[257]	[Equation 16]<br>
[258]<br><br><br>
[259]	The ratio RatioADG.L may be determined by substituting Equation (10) into Equation<br>
(16). A ratio RatioADG.R for an R channel may also be calculated using Equation (16).<br>
Each of the ratios RatioADG.L and RatioADG.R represents a variation in the energy of a<br>
corresponding parameter band due to the adjustment of the levels of object signals.<br>
Thus, ADG values ADG(L ) and ADG(R ) can be calculated using the ratios RatioADG.L<br>
and RatioADG.R, as indicated by Equation (17):<br>
[260]<br>
[261]	[Equation 17]<br>
[262]<br><br><br>
[264]	Once the ADG parameters ADG(L ) and ADG(R ) are determined, the ADG<br>
parameters ADG(L ) and ADG(R ) are quantized by using an ADG quantization table,<br>
and the quantized ADG values are transmitted. If there is the need to further precisely<br>
adjust the ADG values ADG(L) and ADG(R ), the adjustment of the ADG values<br>
ADG(L ) and ADG(R) may be performed by a preprocessor, rather than by an MPS<br>
decoder.<br>
[265]	The number and interval of parameter bands for representing object signals in an<br>
object bitstream may be different from the number and interval of parameter bands<br>
used in a multi-channel decoder. In this case, the parameter bands of the object<br>
bitstream may be linearly mapped to the parameter bands of the multi-channel decoder.<br>
More specifically, if a certain parameter band of an object bitstream ranges over two<br>
parameter bands of a multi-channel decoder, linear mapping may be performed so that<br>
the certain parameter band of the object bitstream can be divided according to the ratio<br>
at which the corresponding parameter band is distributed between the two parameter<br>
bands of the multi-channel decoder. On the other hand, if more than one parameter<br>
band of an object bitstream is included in a certain parameter band of a multi-channel<br>
decoder, the values of parameters of the object bitstream may be averaged. Al-<br>
ternatively, parameter band mapping may be performed using an existing parameter<br>
band mapping table of the multi-channel standard.<br>
[266]	When object coding is used for teleconferencing, the voices of various people<br>
correspond to object signals. An object decoder outputs the voices respectively corresponding<br>
to the object signals to certain speakers. However, when more than one<br>
person talks at the same time, it is difficult for an object decoder to appropriately<br>
distribute the voices of the people to different speakers through decoding, and the<br>
rendering of the voices of the people may cause sound distortions and deteriorate the<br>
quality of sound. In order to address this, information indicating whether more than<br>
one person talks at the same time may be included in a bitstream. Then, if it is<br>
determined based on the information that more than one person talks at the same time,<br>
a channel-based bitstream may be modified so that barely-decoded signals almost like<br>
downmix signals can be output to each speaker.<br>
[267]	For example, assume that there are three people a, b and c and the voices of the<br>
three people a, b and c need to be decoded and thus to be output to speakers A, B and<br>
C, respectively. When the three people a, b and c talk at the same time, the voices of<br>
the three people a, b and c may all be included in a downmix signal, which is obtained<br>
by downmixing object signals respectively representing the voices of the three people<br>
a, b and c. In this case, information regarding parts of the downmix signal respectively<br>
corresponding to the voices of the three people a, b and c may be configured as a<br>
multi-channel bitstream. Then, the downmix signal may be decoded using a typical<br><br>
object decoding method so that the voices of the three people a, b and c can be output<br>
to the speakers A, B and C, respectively. The output of each of the speakers A, B and<br>
C, however, may be distorted and may thus have lower recognition rates than the<br>
original downmix signal. In addition, the voices of the three people a, b and c may not<br>
be properly isolated from one another. In order to address this, information indicating<br>
that the simultaneous utterances of the three people a, b and c talk may be included in a<br>
bitstream. Then, a transcoder may generate a multi-channel bitstream so that the<br>
downmix signal obtained by downmixing the object signals respectively corresponding<br>
to the voices of the three people a, b and c can be output to each of the speakers A, B<br>
and C as it is. In this manner, it is possible to prevent signal distortions.<br>
[268]	In reality, when more than one person talks at the same time, it is hard to isolate the<br>
voice of each person. Therefore, the quality of sound may be higher when a downmix<br>
signal is output as it is than when the downmix signal is rendered so that the voices of<br>
different people can be isolated from one another and output to different speakers. For<br>
this, a transcoder may generate a multi-channel bitstream so that a downmix signal<br>
obtained from the simultaneous utterances of more than one person can be output to all<br>
speakers, or that the downmix signal can be amplified and then output to the speakers.<br>
[269]	In order to indicate whether a downmix signal of an object bitstream originates<br>
from the simultaneous utterances of one or more persons, an object encoder may appropriately<br>
modify the object bitstream, instead of providing additional information, as<br>
described above. In this case, an object decoder may perform a typical decoding<br>
operation on the object bitstream so that the downmix signal can be output to speakers<br>
as it is, or that the downmix signal can be amplified, but not to the extent that signal<br>
distortions occur, and then output to the speakers.<br>
[270]	3D information such as an HTRF, which is provided to a multi-channel decoder,<br>
will hereinafter be described in detail.<br>
[271]	When an object decoder operates in a binaural mode, a multi-channel decoder in<br>
the object decoder also operates in the binaural mode. An end user may transmit 3D information<br>
such as an HRTF that is optimized based on the spatial positions of object<br>
signals to the multi-channel decoder.<br>
[272]	More specifically, when there are two object signals, i.e., OBJECT 1 and<br>
OBJECT2, and the two object signals OBJECT 1 and OBJECT2 are disposed at<br>
positions 1 and 2, respectively, a rendering matrix generator or transcoder may have<br>
3D information indicating the positions of the object signals OBJECT 1 and<br>
OBJECT2. If the rendering matrix generator has the 3D information indicating the<br>
positions of the object signals OBJECT 1 and OBJECT2, the rendering matrix<br>
generator may transmit the 3D information indicating the positions of the object<br>
signals OBJECT 1 and OBJECT2 to the transcoder. On the other hand, if the<br><br>
transcoder has the 3D information indicating the positions of the object signals<br>
OBJECT 1 and OBJECT2, the rendering matrix generator may only transmit index in-<br>
formation corresponding to 3D information to the transcoder.<br>
[273]	In this case, a binaural signal may be generated based on the 3D information<br>
specifying positions 1 and 2, as indicated by Equation (18):<br>
[274]<br>
[275]	[Equation 18]<br>
[276]<br><br>
[277]	A multi-channel binaural decoder obtains binaural sound by performing decoding<br>
on the assumption that a 5.1-channel speaker system will be used to reproduce sound,<br>
and the binaural sound may be represented by Equation (19):<br>
[278]<br>
[279]	[Equation 19]<br>
[280] <br>
[281]<br>
[282]	An L-channel component of the object signal OBJECT 1 may be represented by Eq<br>
uation (20):<br>
[283]<br>
[284]	[Equation 20]<br>
[285] <br>
[286]<br>
[287]	An R-channel component of the object signal OBJECT 1 and L- and R-channel<br><br>
components of the object signal OBJECT2 may all be defined by using Equation (20).<br>
[288]	For example, if the ratios of the energy levels of the object signals OBJECT 1 and<br>
OBJECT2 to a total energy level are a and b, respectively, the ratio of part of the object<br>
signal OBJECT 1 distributed to an FL channel to the entire object signal OBJECT 1 is c<br>
and the ratio of part of the object signal OBJECT2 distributed to the FL channel to the<br>
entire object signal OBJECT2 is d, the ratio at which the object signals OBJECT 1 and<br>
OBJECT2 are distributed to the FL channel is ac:bd. In this case, an HRTF of the FL<br>
channel may be determined, as indicated by Equation (21):<br>
[289]<br>
[290]	[Equation 21]<br>
[291]<br><br>
[292]<br>
[293]	In this manner, 3D information for use in a multi-channel binaural decoder can be<br>
obtained. Since 3D information for use in a multi-channel binaural decoder better<br>
represents the actual positions of object signals, it is possible to more vividly reproduce<br>
binaural signals through binaural decoding using 3D information for use in a multichannel<br>
binaural decoder than when performing multi-channel decoding using 3D information<br>
corresponding to five speaker positions.<br>
[294]	As described above, 3D information for use in a multi-channel binaural decoder<br>
may be calculated based on 3D information representing the spatial positions of object<br>
signals and energy ratio information. Alternatively, 3D information for use in a multichannel<br>
binaural decoder may be generated by appropriately performing decorrelation<br>
when adding up 3D information representing the spatial positions of object signals<br>
based on ICC information of the object signals.<br>
[295]	Effect processing may be performed as part of preprocessing. Alternatively, the<br>
result of effect processing may simply be added to the output of a multi-channel<br>
decoder. In the former case, in order to perform effect processing on an object signal,<br>
the extraction of the object signal may need to be performed in addition to the division<br>
of an L-channel signal into L_L and L_R and the division of an R-channel signal into<br>
R_R and R_L.<br>
[296]	More specifically, an object signal may be extracted from L- and R-channel signals<br>
first. Then, the L-channel signal may be divided into L_L and L_R, and the R-channel<br><br>
signal may be divided into R_ R and R_L. Effect processing may be performed on the<br>
object signal. Then, the effect-processed object signal may be divided into L- and R-<br>
channel components according to a rendering matrix. Thereafter, the L-channel<br>
component of the effect-processed object signal may be added to L_L and RJL, and<br>
the R-channel component of the effect-processed object signal may be added to R_R<br>
and L_R.<br>
[297]	Alternatively, preprocessed L- and R-channel signals L and R may be generated<br>
first. Thereafter, an object signal may be extracted from the preprocessed L- and R-<br>
channel signals L and R . Thereafter, effect processing may be performed on the object<br>
signal, and the result of effect processing may be added back to the preprocessed L-<br>
and R-channel signals.<br>
[298]	The spectrum of an object signal may be modified through effect processing. For<br>
example, the level of a high-pitch portion or a low-pitch portion of an object signal<br>
may be selectively increased. For this, only a spectrum portion corresponding to the<br>
high-pitch portion or the low-pitch portion of the object signal may be modified. In this<br>
case, object-related information included in an object bitstream may need to be<br>
modified accordingly. For example, if the level of a low-pitch portion of a certain<br>
object signal is increased, the energy of the low-pitch portion of the certain object<br>
signal may also be increased. Thus, energy information included in an object bitstream<br>
does not properly represent the energy of the certain object signal any longer. In order<br>
to address this, the energy information included in the object bitstream may be directly<br>
modified according to a variation in the energy of the certain object signal. Al-<br>
ternatively, spectrum variation information provided by a transcoder may be applied to<br>
the formation of a multi-channel bitstream so that the variation in the energy of the<br>
certain object signal can be reflected into the multi-channel bitstream.<br>
[299]	FIGS. 28 through 33 illustrate diagrams for explaining the incorporation of a<br>
plurality of pieces of object-based side information and a plurality of downmix signal<br>
into a piece of side information and a downmix signal. In the case of teleconferencing,<br>
it is necessary sometimes to combine a plurality of pieces of object-based side information<br>
and a plurality of downmix signal into side information and a downmix<br>
signal. In this case, a number of factors need to be considered.<br>
[300]	FIG. 28 illustrates a diagram of an object-encoded bitstream. Referring to FIG. 28,<br>
the object-encoded bitstream includes a downmix signal and side information. The<br>
downmix signal is synchronized with the side information. Therefore, the object-<br>
encoded bitstream may be readily decoded without consideration of additional factors.<br>
However, in the case of incorporating a plurality of bitstreams into a single bitstream,<br>
it is necessary to make sure that a downmix signal of the single bitstream is synchronized<br>
with side information of the single bitstream.<br><br>
[301]	FIG. 29 illustrates a diagram for explaining the incorporation of a plurality of<br>
object-encoded bitstreams BSl and BS2. Referring to FIG. 29, reference numerals 1, 2,<br>
and 3 indicate frame numbers. In order to incorporate a plurality of downmix signals<br>
into a single downmix signal, the downmix signals may be converted into pulse code<br>
modulation (PCM) signals, the PCM signals may be downmixed on a time domain,<br>
and the downmixed PCM signal may be converted to a compression codec format.<br>
During these processes, a delay d may be generated, as illustrated in FIG. 29(b).<br>
Therefore, when a bitstream to be decoded is obtained by incorporating a plurality of<br>
bitstreams, it is necessary to make sure that a downmix signal of a bitstream to be<br>
decoded is properly synchronized with side information of the bitstream to be decoded.<br>
[302]	If a delay between a downmix signal and side information of a bitstream is given,<br>
the bitstream may be compensated for by a predetermined amount corresponding to the<br>
delay. A delay between a downmix signal and side information of a bitstream may<br>
vary according to the type of compression codec used for generating the downmix<br>
signal. Therefore, a bit indicating a delay, if any, between a downmix signal and side<br>
information of a bitstream may be included in the side information.<br>
[303]	FIG. 30 illustrates the incorporation of two bitstreams BS1 and BS2 into a single<br>
bitstream when the downmix signals of the bitstreams BSl and BS2 are generated by<br>
different types of codecs or the configuration of side information of the bitstream BS 1<br>
is different from the configuration of side information of the bitstream BS2. Referring<br>
to FIG. 30, when the downmix signals of the bitstreams BSl and BS2 are generated by<br>
different types of codecs or the configuration of side information of the bitstream BS1<br>
is different from the configuration of side information of the bitstream BS2, it may be<br>
determined that the bitstreams BS1 and BS2 have different signal delays d1 and d2<br>
resulting from the conversion of downmix signals into time-domain signals and the<br>
conversion of the time-domain signals with the use of a single compression codec. In<br>
this case, if the bitstreams BS1 and BS2 are simply added up without consideration of<br>
the different signal delays, the downmix signal of the bitstream BS1 may be<br>
misaligned with the downmix signal of the bitstream BS2 and the side information of<br>
the bitstream BS1 may be misaligned with the side information of the bitstream BS2.<br>
In order to address this, the downmix signal of the bitstream BS1, which is delayed by<br>
dl, may be further delayed so as to be synchronized with the downmix signal of the<br>
bitstream BS2, which is delayed by d2. Then, the bitstreams BS1 and BS2 may be<br>
combined using the same method of the embodiment of FIG. 30. If there is more than<br>
one bitstream to be incorporated, whichever of the bitstreams has a greatest delay may<br>
be used as a reference bitstream, and then, the other bitstreams may be further delayed<br>
so to be synchronized with the reference bitstream. A bit indicating a delay between a<br>
downmix signal and side information may be included in an object bitstream.<br><br>
[304]	Bit indicating whether there is a signal delay in a bitstream may be provided. Only<br>
if the bit information indicates that there is a signal delay in a bitstream, information<br>
specifying the signal delay may be additionally provided. In this manner, it is possible<br>
to minimize the amount of information required for indicating a signal delay, if any, in<br>
a bitstream.<br>
[305]	FIG. 32 illustrates a diagram for explaining how to compensate for one of two<br>
bitstreams BS1 and BS2 having different signal delays by the difference between the<br>
different signal delays, and particularly, how to compensate for the bitstream BS2,<br>
which has a longer signal delay than the bitstream BS1. Referring to FIG. 32, first<br>
through third frames of side information of the bitstream BS1 may all be used as they<br>
are. On the other hand, first through third frames of side information of the bitstream<br>
BS2 may not be used as they are because the first through third frames of the side information<br>
of the bitstream BS2 are not respectively synchronized with the first through<br>
third frames of the side information of the bitstream BS1. For example, the second<br>
frame of the side information of the bitstream BS1 corresponds not only to part of the<br>
first frame of the side information of the bitstream BS2 but also to part of the second<br>
frame of the side information of the bitstream BS2. The proportion of part of the<br>
second frame of the side information of the bitstream BS2 corresponding to the second<br>
frame of the side information of the bitstream BS 1 to the whole second frame of the<br>
side information of the bitstream BS2 and the proportion of part of the first frame of<br>
the side information of the bitstream BS2 corresponding to the second frame of the<br>
side information of the bitstream BS1 to the whole first frame of the side information<br>
of the bitstream BS2 may be calculated, and the first and second frames of the side information<br>
of the bitstream BS2 may be averaged or interpolated based on the results of<br>
the calculation. In this manner, the first through third frames of the side information of<br>
the bitstream BS2 can be respectively synchronized with the first through third frames<br>
of the side information of the bitstream BS1, as illustrated in FIG. 32(b). Then, the side<br>
information of the bitstream BS1 and the side information of the bitstream BS2 may be<br>
incorporated using the method of the embodiment of FIG. 29. Downmix signals of the<br>
bitstreams BS1 and BS2 may be incorporated into a single downmix signal without a<br>
requirement of delay compensation. In this case, delay information corresponding to<br>
the signal delay d1 may be stored in an incorporated bitstream obtained by incorporating<br>
the bitstreams BS1 and BS2.<br>
[306]	FIG. 33 illustrates a diagram for explaining how to compensate for whichever of<br>
two bitstreams having different signal delays has a shorter signal delay. Referring to<br>
FIG. 33, first through third frames of side information of the bitstream BS2 may all be<br>
used as they are. On the other hand, first through third frames of side information of<br>
the bitstream BS1 may not be used as they are because the first through third frames of<br><br>
the side information of the bitstream BS1 are not respectively synchronized with the<br>
first through third frames of the side information of the bitstream BS2. For example,<br>
the first frame of the side information of the bitstream BS2 corresponds not only to<br>
part of the first frame of the side information of the bitstream BS1 but also to part of<br>
the second frame of the side information of the bitstream BS1. The proportion of part<br>
of the first frame of the side information of the bitstream BS1 corresponding to the first<br>
frame of the side information of the bitstream BS2 to the whole first frame of the side<br>
information of the bitstream BS1 and the proportion of part of the second frame of the<br>
side information of the bitstream BS1 corresponding to the first frame of the side information<br>
of the bitstream BS2 to the whole second frame of the side information of<br>
the bitstream BS1 may be calculated, and the first and second frames of the side information<br>
of the bitstream BS1 may be averaged or interpolated based on the results of<br>
the calculation. In this manner, the first through third frames of the side information of<br>
the bitstream BS1 can be respectively synchronized with the first through third frames<br>
of the side information of the bitstream BS2, as illustrated in FIG. 33(b). Then, the side<br>
information of the bitstream BS1 and the side information of the bitstream BS2 may be<br>
incorporated using the method of the embodiment of FIG. 29. Downmix signals of the<br>
bitstreams BS1 and BS2 may be incorporated into a single downmix signal without a<br>
requirement of delay compensation, even if the downmix signals have different signal<br>
delays. In this case, delay information corresponding to the signal delay d2 may be<br>
stored in an incorporated bitstream obtained by incorporating the bitstreams BS1 and<br>
BS2.<br>
[307]	If a plurality of object-encoded bitstreams are incorporated into a single bitstream,<br>
the downmix signals of the object-encoded bitstreams may need to be incorporated into<br>
a single downmix signal. In order to incorporate a plurality of downmix signals corresponding<br>
to different compression codecs into a single downmix signals, the<br>
downmix signals may be converted into PCM signals or frequency-domain signals, and<br>
the PCM signals or the frequency-domain signals may be added up in a corresponding<br>
domain. Thereafter, the result of the addition may be converted using a predetermined<br>
compression codec. Various signal delays may occur according to whether to the<br>
downmix signals are added up during a PCM operation or added up in a frequency<br>
domain and according to the type of compression codec. Since a decoder cannot<br>
readily recognize the various signal delays from a bitstream to be decoded, delay information<br>
specifying the various signal delays may need to be included in the<br>
bitstream. Such delay information may represent the number of delay samples in a<br>
PCM signal or the number of delay samples in a frequency domain.<br>
[308]	The present invention can be realized as computer-readable code written on a<br>
computer-readable recording medium. The computer-readable recording medium may<br><br>
be any type of recording device in which data is stored in a computer-readable manner.<br>
Examples of the computer-readable recording medium include a ROM, a RAM, a CD-<br>
ROM, a magnetic tape, a floppy disc, an optical data storage, and a carrier wave (e.g.,<br>
data transmission through the Internet). The computer-readable recording medium can<br>
be distributed over a plurality of computer systems connected to a network so that<br>
computer-readable code is written thereto and executed therefrom in a decentralized<br>
manner. Functional programs, code, and code segments needed for realizing the<br>
present invention can be easily construed by one of ordinary skill in the art.<br>
[309]	As described above, according to the present invention, sound images are localized<br>
for each object signal by benefiting from the advantages of object-based audio<br>
encoding and decoding methods. Thus, it is possible to offer more realistic sounds<br>
during the playback object signals. In addition, the present invention may be applied to<br>
interactive games, and may thus provide a user with a more realistic virtual reality<br>
experience.<br>
[310]	While the present invention has been particularly shown and described with<br>
reference to exemplary embodiments thereof, it will be understood by those of<br>
ordinary skill in the art that various changes in form and details may be made therein<br>
without departing from the spirit and scope of the present invention as defined by the<br>
following claims.<br><br>
Claims<br>
[ 1 ]	A method of decoding an audio signal, comprising:<br>
receiving object information and a downmix signal including at least two<br>
channels;<br>
receiving object information including a downmix parameter; and<br>
generating weighted information using for downmixing and conversion of an<br>
object signal based on the downmix parameter.<br>
[2]	An audio decoding method comprising:<br>
receiving a downmix signal and object-based side information, the downmix<br>
signal comprising at least two downmix channel signals;<br>
extracting gain information from the object-based side information and<br>
generating modification information for modifying the downmix channel signals<br>
on a channel-by-channel basis based on the gain information; and<br>
modifying the downmix channel signals by applying the modification information<br>
to the downmix channel signals.<br>
[3]	The audio decoding method of claim 2, further comprising:<br>
generating channel-based side information based on the object-based side information<br>
and rendering control information.<br>
[4]	The audio decoding method of claim 3, further comprising:<br>
generating a multi-channel audio signal based on the channel-based side information<br>
and the modified downmix channel signals.<br>
[5]	The audio decoding method of claim 2, wherein the object-based side information<br>
comprises flag information indicating whether the downmix gain information<br>
is included in the object-based side information.<br>
[6]	An audio encoding method comprising:<br>
generating a downmix signal by downmixing an object signal, the downmix<br>
signal comprising at least two downmix channel signals;<br>
extracting object-related information regarding the object signal and generating<br>
object-based side information based on the object-related information; and<br>
inserting gain information for modifying the downmix channel signals on a<br>
channel-by-channel basis into the object-based side information..<br>
[7]	The audio encoding method of claim 6, further comprising:<br>
generating a bitstream by combining the downmix signal and the object-based<br>
side information into which the gain information is inserted.<br>
[8]	The audio encoding method of claim 7, wherein the bitstream comprises flag information<br>
whether to transmit the gain information.<br>
[9]	An audio decoding apparatus comprising:<br><br>
a demultiplexer configured to extract a downmix signal and object-based side information a<br>
from an input audio signal, the downmix signal comprising at least<br>
two downmix channel signals; and<br>
a transcoder configured to generate modification information for modifying the<br>
downmix channel signals on a channel-by-channel basis based on gain information<br>
extracted from the object-based side information and modifies the<br>
downmix channel signals by applying the modification information to the<br>
downmix channel signals.<br>
[10]	The audio decoding apparatus of claim 9, wherein the transcoder generates<br>
channel-based side information based on the object-based side information and<br>
rendering control information.<br>
[11]	The audio decoding apparatus of claim 10, further comprising a multi-channel<br>
decoder which generates a multi-channel audio signal based on the channel-<br>
based side information and the modified downmix channel signals.<br>
[12]	A processor readable recording medium on which a program for executing the<br>
method cited in claim 1 in the processor is recorded.<br>
[13]	A computer-readable recording medium having recorded thereon a computer<br>
program for executing an audio decoding method, the audio decoding method<br>
comprising:<br>
receiving a downmix signal and object-based side information, the downmix<br>
signal comprising at least two downmix channel signals;<br>
extracting gain information from the object-based side information and<br>
generating modification information for modifying the downmix channel signals<br>
on a channel-by-channel basis based on the gain information; and<br>
modifying the downmix channel signals by applying the modification information<br>
to the downmix channel signals.<br>
[14]	The computer-readable recording medium of claim 13, wherein the audio<br>
decoding method further comprises:<br>
generating channel-based side information based on the object-based side information<br>
and rendering control information; and<br>
generating a multi-channel audio signal based on the channel-based side information<br>
and the modified downmix channel signals.<br>
[15]	A computer-readable recording medium having recorded thereon a computer<br>
program for executing an audio encoding method, the audio encoding method<br>
comprising:<br>
generating a downmix signal by downmixing an object signal, the downmix<br>
signal comprising at least two downmix channel signals;<br>
extracting object-related information regarding the object signal and generating<br><br>
object-based side information based on the object-related information; and<br>
inserting gain information for modifying the downmix channel signals on a<br>
channel-by-channel basis into the object-based side information.<br>
[16]	The computer-readable recording medium of claim 15, wherein the audio<br>
encoding method further comprises:<br>
generating a bitstream by combining the downmix signal and the object-based<br>
side information into which the gain information is inserted.<br><br>
An audio decoding method and apparatus and an audio encoding method and apparatus which can efficiently process object-based audio signals are provided. The audio decoding method includes receiving a downmix signal and object-based side<br>
information, the downmix signal comprising at least two downmix channel signals; extracting gain information from the object-based side information and generating modification information for modifying the downmix channel signals on a channel-by-channel basis based on the gain information; and modifying the downmix channel signals by applying the modification information to the downmix channel signals.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=zd85HHaqGzXnnJ3xOG9YeQ==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==" target="_blank" style="word-wrap:break-word;">http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=zd85HHaqGzXnnJ3xOG9YeQ==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==</a></p>
		<br>
		<div class="pull-left">
			<a href="269111-capsule-formulation-of-pirfenidone-and-pharmaceutically-acceptable-excipients.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="269113-process-for-the-production-of-alcohols.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>269112</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>3788/KOLNP/2008</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>41/2015</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>09-Oct-2015</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>30-Sep-2015</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>17-Sep-2008</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>LG ELECTRONICS INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>20, YEOUIDO-DONG, YEONGDEUNGPO-GU, SEOUL</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>PANG, HEE SUK</td>
											<td>#16 WOOMYUN-DONG, SEOCHO-KU, SEOUL 137-724</td>
										</tr>
										<tr>
											<td>2</td>
											<td>LIM, JAE HYUN</td>
											<td>#16 WOOMYUN-DONG, SEOCHO-KU, SEOUL 137-724</td>
										</tr>
										<tr>
											<td>3</td>
											<td>YOON, SUNG YONG</td>
											<td>#16 WOOMYUN-DONG, SEOCHO-KU, SEOUL 137-724</td>
										</tr>
										<tr>
											<td>4</td>
											<td>LEE, HYUN KOOK</td>
											<td>#16 WOOMYUN-DONG, SEOCHO-KU, SEOUL 137-724</td>
										</tr>
										<tr>
											<td>5</td>
											<td>KIM, DONG SOO</td>
											<td>#16 WOOMYUN-DONG, SEOCHO-KU, SEOUL 137-724</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 19/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/KR2008/000885</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2008-02-14</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/901,089</td>
									<td>2007-02-14</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>60/948,373</td>
									<td>2007-07-06</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>3</td>
									<td>60/947,620</td>
									<td>2007-07-02</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>4</td>
									<td>60/901,642</td>
									<td>2007-02-16</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>5</td>
									<td>60/924,027</td>
									<td>2007-04-27</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/269112-methods-and-apparatuses-for-encoding-and-decoding-object-based-audio-signals by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 08:17:51 GMT -->
</html>
