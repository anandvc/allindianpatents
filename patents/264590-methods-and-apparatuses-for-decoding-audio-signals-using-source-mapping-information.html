<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/264590-methods-and-apparatuses-for-decoding-audio-signals-using-source-mapping-information by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 02:03:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 264590:&quot;METHODS AND APPARATUSES FOR DECODING AUDIO SIGNALS USING SOURCE MAPPING INFORMATION&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;METHODS AND APPARATUSES FOR DECODING AUDIO SIGNALS USING SOURCE MAPPING INFORMATION&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>An apparatus for processing a media signal and method thereof are disclosed, by which the media signal can be converted to a surround signal by using spatial information of the media signal. The present invention provides a method of processing a signal, the method comprising of generating source mapping information corresponding to each source of multi-sources by using spatial information indicating features between the multi-sources; generating sub-rendering information by applying filter information giving a surround effect to the source mapping information per the source; generating rendering information for generating a surround signal by integrating the at least one of the sub-rendering information; and generating the surround signal by applying the rendering information to a downmix signal generated by downmixing the multi- sources.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Description<br>
METHOD AND APPARATUS FOR PROCESSING A MEDIA<br>
SIGNAL<br>
Technical Field<br>
[ 1 ]	The present invention relates to an apparatus for processing a media signal and<br>
method thereof, and more particularly to an apparatus for generating a surround signal<br>
by using spatial information of the media signal and method thereof.<br>
Background Art<br>
[2]	Generally, various kinds of apparatuses and methods have been widely used to<br>
generate a multi-channel media signal by using spatial information for the multichannel<br>
media signal and a downmix signal, in which the downmix signal is generated<br>
by downmixing the multi-channel media signal into mono or stereo signal.<br>
[3]	However, the above methods and apparatuses are not usable in environments<br>
unsuitable for generating a multi-channel signal. For instance, they are not usable for a<br>
device capable of generating only a stereo signal. In other words, there exists no<br>
method or apparatus for generating a surround signal, in which the surround signal has<br>
multi-channel features in the environment incapable of generating a multi-channel<br>
signal by using spatial information of the multi-channel signal.<br>
[4]	So, since there exists no method or apparatus for generating a surround signal in a<br>
device capable of generating only a mono or stereo signal, it is difficult to process the<br>
media signal efficiently.<br>
Disclosure of Invention<br>
Technical Problem<br>
[5]	Accordingly, the present invention is directed to an apparatus for processing a<br>
media signal and method thereof that substantially obviate one or more of the problems<br>
due to limitations and disadvantages of the related art.<br>
[6]	An object of the present invention is to provide an apparatus for processing a media<br>
signal and method thereof, by which the media signal can be converted to a surround<br>
signal by using spatial information for the media signal.<br>
[7]	Additional features and advantages of the invention will be set forth in a description<br>
which follows, and in part will be apparent from the description, or may be learned by<br>
practice of the invention. The objectives and other advantages of the invention will be<br>
realized and attained by the structure particularly pointed out in the written description<br>
and claims thereof as well as the appended drawings.<br>
Technical Solution<br>
[8]	To achieve these and other advantages and in accordance with the purpose of the<br><br>
present invention, a method of processing a signal according to the present invention<br>
includes of: generating source mapping information corresponding to each source of<br>
multi-sources by using spatial information indicating features between the multi-<br>
sources; generating sub-rendering information by applying filter information giving a<br>
surround effect to the source mapping information per the source; generating rendering<br>
information for generating a surround signal by integrating at least one of the sub-<br>
rendering information; and generating the surround signal by applying the rendering<br>
information to a downmix signal generated by downmixing the multi-sources.<br>
[9]	To further achieve these and other advantages and in accordance with the purpose .<br>
of the present invention, an apparatus for processing a signal includes a source map<br>
ping unit generating source mapping information corresponding to each source of<br>
multi-sources by using spatial information indicating features between the multi-<br>
sources; a sub-rendering information generating unit generating sub-rendering in-<br>
formation by applying filter information having a surround effect to the source<br>
mapping information per the source; an integrating unit generating rendering in-<br>
formation for generating a surround signal by integrating the at least one of the sub-<br>
rendering information; and a rendering unit generating the surround signal by applying<br>
the rendering information to a downmix signal generated by downmixing the multi-<br>
sources.<br>
[10]	It is to be understood that both the foregoing general description and the following<br>
detailed description are exemplary and explanatory and are intended to provide further<br>
explanation of the invention as claimed.<br>
Advantageous Effects<br>
[11]	A signal processing apparatus and method according to the present invention enable<br>
a decoder, which receives a bitstream including a downmix signal generated by<br>
downmixing a multi-channel signal and spatial information of the multi-channel signal,<br>
to generate a signal having a surround effect in environments in incapable of<br>
recovering the multi-channel signal.<br>
Brief Description of the Drawings<br>
[12]	The accompanying drawings, which are included to provide a further understanding<br>
of the invention and are incorporated in and constitute a part of this specification,<br>
illustrate embodiments of the invention and together with the description serve to<br>
explain the principles of the invention.<br>
[13]	In the drawings:<br>
[14]	FIG. 1 is a block diagram of an audio signal encoding apparatus and an audio signal<br>
decoding apparatus according to one embodiment of the present invention;<br>
[15]	FIG. 2 is a structural diagram of a bitstream of an audio signal according to one<br><br>
embodiment of the present invention;<br>
[16]	FIG. 3 is a detailed block diagram of a spatial information converting unit<br>
according to one embodiment of the present invention;<br>
[17]	FIG. 4 and FIG. 5 are block diagrams of channel configurations used for source<br>
mapping process according to one embodiment of the present invention;<br>
[18]	FIG. 6 and FIG. 7 are detailed block diagrams of a rendering unit for a stereo<br>
downmix signal according to one embodiment of the present invention;<br>
[19]	FIG. 8 and FIG. 9 are detailed block diagrams of a rendering unit for a mono<br>
downmix signal according to one embodiment of the present invention;<br>
[20]	FIG. 10 and FIG. 11 are block diagrams of a smoothing unit and an expanding unit<br>
according to one embodiment of the present invention;<br>
[21]	FIG. 12 is a graph to explain a first smoothing method according to one<br>
embodiment of the present invention;<br>
[22]	FIG. 13 is a graph to explain a second smoothing method according to one<br>
embodiment of the present invention;<br>
[23]	FIG. 14 is a graph to explain a third smoothing method according to one<br>
embodiment of the present invention;<br>
[24]	FIG. 15 is a graph to explain a fourth smoothing method according to one<br>
embodiment of the present invention;<br>
[25]	FIG. 16 is a graph to explain a fifth smoothing method according to one<br>
embodiment of the present invention;<br>
[26]	FIG. 17 is a diagram to explain prototype filter information corresponding to each<br>
channel;<br>
[27]	FIG. 18 is a block diagram for a first method of generating rendering filter in-<br>
formation in a spatial information converting unit according to one embodiment of the<br>
present invention;<br>
[28]	FIG. 19 is a block diagram for a second method of generating rendering filter in-<br>
formation in a spatial information converting unit according to one embodiment of the<br>
present invention;<br>
[29]	FIG. 20 is a block diagram for a third method of generating rendering filter in-<br>
formation in a spatial information converting unit according to one embodiment of the<br>
present invention;<br>
[30]	FIG. 21 is a diagram to explain a method of generating a surround signal in a<br>
rendering unit according to one embodiment of the present invention;<br>
[31]	FIG. 22 is a diagram for a first interpolating method according to one embodiment<br>
of the present invention;<br>
[32]	FIG. 23 is a diagram for a second interpolating method according to one<br>
embodiment of the present invention;<br><br>
[33]	FIG. 24 is a diagram for a block switching method according to one embodiment of<br>
the present invention;<br>
[34]	FIG. 25 is a block diagram for a position to which a window length decided by a<br>
window, length deciding unit is applied according to one embodiment of the present<br>
invention;<br>
[35]	FIG. 26 is a diagram for filters having various lengths used in processing an audio<br>
signal according to one embodiment of the present invention;<br>
[36]	FIG. 27 is a diagram for a method of processing an audio signal dividedly by using<br>
a plurality of subfilters according to one embodiment of the present invention;<br>
[37]	FIG. 28 is a block diagram for a method of rendering partition rendering in-<br>
formation generated by a plurality of subfilters to a mono downmix signal according to<br>
one embodiment of the present invention;<br>
[38]	FIG. 29 is a block diagram for a method of rendering partition rendering in-<br>
formation generated by a plurality of subfilters to a stereo downmix signal according to<br>
one embodiment of the present invention;<br>
[39]	FIG. 30 is a block diagram for a first domain converting method of a downmix<br>
signal according to one embodiment of the present invention; and<br>
[40]	FIG. 31 is a block diagram for a second domain converting method of a downmix<br>
signal according to one embodiment of the present invention.<br>
Best Mode for Carrying Out the Invention<br>
[41]	Reference will now be made in detail to the preferred embodiments of the present<br>
invention, examples of which are illustrated in the accompanying drawings..<br>
[42]	FIG. 1 is a block diagram of an audio signal encoding apparatus and an audio signal<br>
decoding apparatus according to one embodiment of the present invention.<br>
[43]	Referring to FIG. 1, an encoding apparatus 10 includes a downmixing unit 100, a<br>
spatial information generating unit 200, a downmix signal encoding unit 300, a spatial<br>
information encoding unit 400, and a multiplexing unit 500.<br>
[44]	If multi-source(Xl, X2,..., Xn) audio signal is inputted to the downmixing unit 100,<br>
the downmixing unit 100 downmixes the inputted signal into a downmix signal. In this<br>
case, the downmix signal includes mono, stereo and multi-source audio signal.<br>
[45]	The source includes a channel and ,in convenience, is represented as a channel in<br>
the following description. In the present specification, the mono or stereo downmix<br>
signal is referred to as a reference. Yet, the present invention is not limited to the mono<br>
or stereo downmix signal.<br>
[46]	The encoding apparatus 10 is able to optionally use an arbitrary downmix signal<br>
directly provided from an external environment.<br>
[47]	The spatial information generating unit 200 generates spatial information from a<br><br>
multi-channel audio signal. The spatial information can be generated in the course of a<br>
downmixing process. The generated downmix signal and spatial information are<br>
encoded by the downmix signal encoding unit 300 and the spatial information<br>
encoding unit 400, respectively and are then transferred to the multiplexing unit 500.<br>
[48]	In the present invention, 'spatial information' means information necessary to<br>
generate a multi-channel signal from upmixing a downmix signal by a decoding<br>
apparatus, in which the downmix signal is generated by downmixing the multi-channel<br>
signal by an encoding apparatus and transferred to the decoding apparatus. The spatial<br>
information includes spatial parameters. The spatial parameters include CLD(channel<br>
level difference) indicating an energy difference between channels, ICC(inter-channel<br>
coherences) indicating a correlation between channels, CPC(channel prediction co-<br>
efficients) used in generating three channels from two channels, etc.<br>
[49]	In the present invention, 'downmix signal encoding unit' or 'downmix signal<br>
decoding unit' means a codec that encodes or decodes an audio signal instead of spatial<br>
information. In the present specification, a downmix audio signal is taken as an<br>
example of the audio signal instead of the spatial information. And, the downmix<br>
signal encoding or decoding unit may include MP3, AC-3, DTS, or AAC. Moreover,<br>
the downmix signal encoding or decoding unit may include a codec of the future as<br>
well as the previously developed codec.<br>
[50]	The multiplexing unit 500 generates a bitstream by multiplexing the downmix<br>
signal and the spatial information and then transfers the generated bitstream to the<br>
decoding apparatus 20. Besides, the structure of the bitstream will be explained in FIG.<br>
2 later.<br>
[51]	A decoding apparatus 20 includes a demultiplexing unit 600, a downmix signal<br>
decoding unit 700, a spatial information decoding unit 800, a rendering unit 900, and a<br>
spatial information converting unit 1000.<br>
[52]	The demultiplexing unit 600 receives a bitstream and then separates an encoded<br>
downmix signal and an encoded spatial information from the bitstream. Subsequently,<br>
the downmix signal decoding unit 700 decodes the encoded downmix signal and the<br>
spatial information decoding unit 800 decodes the encoded spatial information.<br>
[53]	The spatial information converting unit 1000 generates rendering information<br>
applicable to a downmix signal using the decoded spatial information and filter in-<br>
formation. In this case, the rendering information is applied to the downmix signal to<br>
generate a surround signal.<br>
[54]	For instance, the surround signal is generated in the following manner. First of all, a<br>
process for generating a downmix signal from a multi-channel audio signal by the<br>
encoding apparatus 10 can include several steps using an OTT (one-to-two) or TTT<br>
(three-to-three) box. In this case, spatial information can be generated from each of the<br><br>
steps. The spatial information is transferred to the decoding apparatus 20. The<br>
decoding apparatus 20 then generates a surround signal by converting the spatial in-<br>
formation and then rendering the converted spatial information with a downmix signal.<br>
Instead of generating a multi-channel signal by upmixing a downmix signal, the<br>
present invention relates to a rendering method including the steps of extracting spatial<br>
information for each upmixing step and performing a rendering by using the extracted<br>
spatial information. For example, HRTF (head-related transfer functions) filtering is<br>
usable in the rendering method.<br>
[55]	In this case, the spatial information is a value applicable to a hybrid domain as well.<br>
So, the rendering can be classified into the following types according to a domain.<br>
[56]	The first type is that the rendering is executed on a hybrid domain by having a<br>
downmix signal pass through a hybrid filterbank. In this case, a conversion of domain<br>
for spatial information is unnecessary.<br>
[57]	The second type is that the rendering is executed on a time domain. In this case, the<br>
second type uses a fact that a HRTF filter is modeled as a FIR (finite inverse response)<br>
filter or an IIR (infinite inverse response) filter on a time domain. So, a process for<br>
converting spatial information to a filter coefficient of time domain is needed.<br>
[58]	The third type is that the rendering is executed on a different frequency domain. For<br>
instance, the rendering is executed on a DFT (discrete Fourier transform) domain. In<br>
this case, a process for transforming spatial information into a corresponding domain is<br>
necessary. In particular, the third type enables a fast operation by replacing a filtering<br>
on a time domain into an operation on a frequency domain.<br>
[59]	In the present invention, filter information is the information for a filter necessary<br>
for processing an audio signal and includes a filter coefficient provided to a specific<br>
filter. Examples of the filter information are explained as follows. First of all,<br>
prototype filter information is original filter information of a specific filter and can be<br>
represented as GL_L or the like. Converted filter information indicates a filter co-<br>
efficient after the prototype filter information has been converted and can be<br>
represented as GL_L or the like. Sub-rendering information means the filter in-<br>
formation resulting from spatializing the prototype filter information to generate a<br>
surround signal and can be represented as FL_L1 or the like. Rendering information<br>
means the filter information necessary for executing rendering and can be represented<br>
as HLJL or the like. Interpolated/smoothed rendering information means the filter in-<br>
formation resulting from interpolation/smoothing the rendering information and can be<br>
represented as HL_L or the like. In the present specification, the above filter in-<br>
formations are referred to. Yet, the present invention is not restricted by the names of<br>
the filter informations. In particular, HRTF is taken as an example of the filter in-<br>
formation. Yet, the present invention is not limited to the HRTF.<br><br>
[60]	The rendering unit 900 receives the decoded downmix signal and the rendering in-<br>
formation and then generates a surround signal using the decoded downmix signal and<br>
the rendering information. The surround signal may be the signal for providing a<br>
surround effect to an audio system capable of generating only a stereo signal. Besides,<br>
the present invention can be applied to various systems as well as the audio system<br>
capable of generating only the stereo signal.<br>
[61]	FIG. 2 is a structural diagram for a bitstream of an audio signal according to one<br>
embodiment of the present invention, in which the bitstream includes an encoded<br>
downmix signal and encoded spatial information.<br>
[62]	Referring to FIG. 2, a 1-frame audio payload includes a downmix signal field and<br>
an ancillary data field. Encoded spatial information can be stored in the ancillary data<br>
field. For instance, if an audio payload is 48~128kbps, spatial information can have a<br>
range of 5~32kbps. Yet, no limitations are put on the ranges of the audio payload and<br>
spatial information.<br>
[63]	FIG. 3 is a detailed block diagram of a spatial information converting unit<br>
according to one embodiment of the present invention.<br>
[64]	Referring to FIG. 3, a spatial information converting unit 1000 includes a source<br>
mapping unit 1010, a sub-rendering information generating unit 1020, an integrating<br>
unit 1030, a processing unit 1040, and a domain converting unit 1050.<br>
[65]	The source mapping unit 101 generates source mapping information corresponding<br>
to each source of an audio signal by executing source mapping using spatial in-<br>
formation. In this case, the source mapping information means per-source information<br>
generated to correspond to each source of an audio signal by using spatial information<br>
and the like. The source includes a channel and ,in this case, the source mapping in-<br>
formation corresponding to each channel is generated. The source mapping in-<br>
formation can be represented as a coefficient. And, the source mapping process will be<br>
explained in detail later with reference to FIG. 4 and FIG. 5.<br>
[66]	The sub-rendering information generating unit 1020 generates sub-rendering in-<br>
formation corresponding to each source by using the source mapping information and<br>
the filter information. For instance, if the rendering unit 900 is the HRTF filter, the<br>
sub-rendering information generating unit 1020 is able to generate sub-rendering in-<br>
formation by using HRTF filter information.<br>
[67]	The integrating unit 1030 generates rendering information by integrating the sub-<br>
rendering information to correspond to each source of a downmix signal. The<br>
rendering information, which is generated by using the spatial information and the<br>
filter information, means the information to generate a surround signal by being<br>
applied to the downmix signal. And, the rendering information includes a filter co-<br>
efficient type. The integration can be omitted to reduce an operation quantity of the<br><br>
rendering process. Subsequently, the rendering information is transferred to the<br>
processing unit 1042.<br>
[68]	The processing unit 1042 includes an interpolating unit 1041 and/or a smoothing<br>
unit 1042. The rendering information is interpolated by the interpolating unit 1041<br>
and/or smoothed by the smoothing unit 1042.<br>
[69]	The domain converting unit 1050 converts a domain of the rendering information to<br>
a domain of the downmix signal used by the rendering unit 900. And, the domain<br>
converting unit 1050 can be provided to one of various positions including the position<br>
shown in FIG. 3. So, if the rendering information is generated on the same domain of<br>
the rendering unit 900, it is able to omit the domain converting unit 1050. The domain-<br>
converted rendering information is then transferred to the rendering unit 900.<br>
[70]	The spatial information converting unit 1000 can include a filter information<br>
converting unit 1060. In FIG. 3, the filter information converting unit 1060 is provided<br>
within the spatial information converting unit 100. Alternatively, the filter information<br>
converting unit 1060 can be provided outside the spatial information converting unit<br>
100. The filter information converting unit 1060 is converted to be suitable for<br>
generating sub-rendering information or rendering information from random filter in-<br>
formation, e.g., HRTF. The converting process of the filter information can include the<br>
following steps.<br>
[71]	First of all, a step of matching a domain to be applicable is included. If a domain of<br>
filter information does not match a domain for executing rendering, the domain<br>
matching step is required. For instance, a step of converting time domain HRTF to<br>
DFT, QMF or hybrid domain for generating rendering information is necessary.<br>
[72]	Secondly, a coefficient reducing step can be included. In this case, it is easy to save<br>
the domain-converted HRTF and apply the domain-converted HRTF to spatial in-<br>
formation. For instance, if a prototype filter coefficient has a response of a long tap<br>
number (length), a corresponding coefficient has to be stored in a memory cor-<br>
responding to a response amounting to a corresponding length of total 10 in case of 5.1<br>
channels. This increases a load of the memory and an operational quantity. To prevent<br>
this problem, a method of reducing a filter coefficient to be stored while maintaining<br>
filter characteristics in the domain converting process can be used. For instance, the<br>
HRTF response can be converted to a few parameter value. In this case, a parameter<br>
generating process and a parameter value can differ according to an applied domain.<br>
[73]	The downmix signal passes through a domain converting unit 1110 and/or a<br>
decorrelating unit 1200 before being rendered with the rendering information. In case<br>
that a domain of the rendering information is different from that of the downmix<br>
signal, the domain converting unit 1110 converts the domain of the downmix signal in<br>
order to match the two domains together.<br><br>
[74]	The decorrelating unit 1200 is applied to the domain-converted downmix signal.<br>
This may have an operational quantity relatively higher than that of a method of<br>
applying a decorrelator to the rendering information. Yet, it is able to prevent<br>
distortions from occurring in the process of generating rendering information. The<br>
decorrelating unit 1200 can include a plurality of decorrelators differing from each<br>
other in characteristics if an operational quantity is allowable. If the downmix signal is<br>
a stereo signal, the decorrelating unit 1200 may not be used. In FIG. 3, in case that a<br>
domain-converted mono downmix signal, i.e., a mono downmix signal on a frequency,<br>
hybrid, QMF or DFT domain is used in the rendering process, a decorrelator is used on<br>
the corresponding domain. And, the present invention includes a decorrelator used on a<br>
time domain as well. In this case, a mono downmix signal before the domain<br>
converting unit 1100 is directly inputted to the decorrelating unit 1200. A first order or<br>
higher IIR filter (or FIR filter) is usable as the decorrelator.<br>
[75]	Subsequently, the rendering unit 900 generates a surround signal using the<br>
downmix signal, the decorrelated downmix signal, and the rendering information. If<br>
the downmix signal is a stereo signal, the decorrelated downmix signal may not be<br>
used. Details of the rendering process will be described later with reference to FIGs. 6<br>
to 9.<br>
[76]	The surround signal is converted to a time domain by an inverse domain converting<br>
unit 1300 and then outputted. If so, a user is able to listen to a sound having a multi-<br>
channel effect though stereophonic earphones or the like.<br>
[77]	FIG. 4 and FIG. 5 are block diagrams of channel configurations used for source<br>
mapping process according to one embodiment of the present invention. A source<br>
mapping process is a process for generating source mapping information cor-<br>
responding to each source of an audio signal by using spatial information. As<br>
mentioned in the foregoing description, the source includes a channel and source<br>
mapping information can be generated to correspond to the channels shown in FIG. 4<br>
and FIG. 5. The source mapping information is generated in a type suitable for a<br>
rendering process.<br>
[78]	For instance, if a downmix signal is a mono signal, it is able to generate source<br>
mapping information using spatial information such as CLD1~CLD5, ICC1~ICC5, and<br>
the like.<br>
[79]	The source mapping information can be represented as such a value as D_L (=DL ),<br>
D_R (=DR ), D_C (=DC ), D_LFE (=DLFE), D_Ls (=DLS), D_Rs (=DRS), and the like. In<br>
R	C	LFE	Ls	Rs<br>
this case, the process for generating the source mapping information is variable<br>
according to a tree structure corresponding to spatial information, a range of spatial in-<br>
formation to be used, and the like. In the present specification, the downmix signal is a<br>
mono signal for example, which does not put limitation of the present invention.<br><br>
[80]	Right and left channel outputs outputted from the rendering unit 900 can be<br>
expressed as Math Figure 1.<br>
[81]	MathFigure 1<br><br>
[82]	In this case, the operator '*' indicates a product on a DFT domain and can be<br>
replaced by a convolution on a QMF or time domain.<br>
[83]	The present invention includes a method of generating the L, C, R, Ls and Rs by<br>
source mapping information using spatial information or by source mapping in-<br>
formation using spatial information and filter information. For instance, source<br>
mapping information can be generated using CLD of spatial information only or CLD<br>
and ICC of spatial information. The method of generating source mapping information<br>
using the CLD only is explained as follows.<br>
[84]	In case that the tree structure has a structure shown in FIG. 4, a first method of<br>
obtaining source mapping information using CLD only can be expressed as Math<br>
Figure 2.<br>
[85]	MathFigure 2<br><br><br><br>
, and 'm' indicates a mono downmix signal.<br>
[87]	In case that the tree structure has a structure shown in FIG. 5, a second method of<br>
obtaining source mapping information using CLD only can be expressed as Math<br>
Figure 3.<br>
[88]	MathFigure 3<br><br>
[89]	If source mapping information is generated using CLD only, a 3-dimensional effect<br>
may be reduced. So, it is able to generate source mapping information using ICC and/<br>
or decorrelator. And, a multi-channel signal generated by using a decorrelator output<br>
signal dx(m) can be expresses as Math Figure 4.<br>
[90]	MathFigure 4<br><br>
[91]	In this case, 'A', 'B' and 'C are values that can be represented by using CLD and<br>
ICC. 'd0' to 'd3' indicate decorrelators. And, 'm' indicates a mono downmix signal. Yet,<br>
this method is unable to generate source mapping information such as D_L, D_R, and<br>
the like.<br>
[92]	Hence, the first method of generating the source mapping information using the<br><br>
CLD, ICC and/or decorrelators for the downmix signal regards dx(m) (x=0, 1, 2) as an<br>
independent input. In this case, the 'dx' is usable for a process for generating sub-<br>
rendering filter information according to Math Figure 5.<br>
[93]	MathFigure 5<br>
FL_L_M = d_L_M*GL_L'   (Mono input  Left output)<br>
FL_R_M = d_L_M*GL_R'   (Mono input  Right output)<br>
FL_L_Dx = d_L_Dx*GL_L'    (Dx output  Left output)<br>
FL_R_Dx = d_L_Dx*GL_R'   (Dx output  Right output)<br>
[94]	And, rendering information can be generated according to Math Figure 6 using a<br>
result of Math Figure 5.<br>
[95]	MathFigure 6<br>
HM_L = FL_L_M + FR_L_M + FC_L_M + FLS_L_M + FRS_L_M + FLFE_L_M<br>
HM_R = FL_R_M + FR_R_M + FC_R_M + FLS_R_M + FRS_R_M + FLFE_R_M<br>
HDx_L = FL_L_Dx + FR_L_Dx + FC_L_Dx + FLS_L_Dx + FRS_L_Dx + FLFE_L_Dx<br>
HDx_R = FL_R_Dx + FR_R_Dx + FC_R_Dx + FLS_R_Dx + FRS_R_Dx + FLFE_R_Dx<br>
[96]	Details of the rendering information generating process are explained later. The first<br>
method of generating the source mapping information using the CLD, ICC and/or<br>
decorrelators handles a dx output value, i.e., 'dx(m)' as an independent input, which<br>
may increase an operational quantity.<br>
[97]	A second method of generating source mapping information using CLD, ICC and/<br>
or decorrelators employs decorrelators applied on a frequency domain. In this case, the<br>
source mapping information can be expresses as Math Figure 7.<br>
[98]	MathFigure 7<br><br><br>
[99]	In this case, by applying decorrelators on a frequency domain, the same source<br>
mapping information such as D_L, D_R, and the like before the application of the<br>
decorrelators can be generated. So, it can be implemented in a simple manner.<br>
[100]	A third method of generating source mapping information using CLD, ICC and/or<br>
decorrelators employs decorrelators having the all-pass characteristic as the<br>
decorrelators of the second method. In this case, the all-pass characteristic means that a<br>
size is fixed with a phase variation only. And, the present invention can use<br>
decorrelators having the all-pass characteristic as the decorrelators of the first method.<br>
[101]	A fourth method of generating source mapping information using CLD, ICC and/or<br>
decorrelators carries out decorrelation by using decorrelators for the respective<br>
channels (e.g., L, R, C, Ls, Rs, etc.) instead of using 'd0' to 'd3' of the second method. In<br>
this case, the source mapping information can be expressed as Math Figure 8.<br>
[102]	MathFigure8<br><br>
[103]	In this case, 'k' is an energy value of a decorrelated signal determined from CLD<br>
and ICC values. And, 'd_L', 'd_R', 'd_C, 'd_Ls' and 'd_Rs' indicate decorrelators<br>
applied to channels, respectively.<br>
[104]	A fifth method of generating source mapping information using CLD, ICC and/or<br>
decorrelators maximizes a decorrelation effect by configuring 'd_L' and 'd_R'<br><br>
symmetric to each other in the fourth method and configuring 'd_Ls' and 'd_Rs'<br>
symmetric to each other in the fourth method. In particular, assuming d_R=f(d_L) and<br>
d_Rs=f(d_Ls), it is necessary to design 'd_L', 'd_C and 'd_Ls' only.<br>
. [105]	A sixth method of generating source mapping information using CLD, ICC and/or<br>
decorrelators is to configure the 'd_L' and 'd_Ls' to have a correlation in the fifth<br>
method. And, the 'd_L' and 'd_C can be configured to have a correlation as well.<br>
[106]	A seventh method of generating source mapping information using CLD, ICC and/<br>
or decorrelators is to use the decorrelators in the third method as a serial or nested<br>
structure of the all-pas filters. The seventh method utilizes a fact that the all-pass char-<br>
acteristic is maintained even if the all-pass filter is used as the serial or nested<br>
structure. In case of using the all-pass filter as the serial or nested structure, it is able to<br>
obtain more various kinds of phase responses. Hence, the decorrelation effect can be<br>
maximized.<br>
[107]	An eighth method of generating source mapping information using CLD, ICC and/<br>
or decorrelators is to use the related art decorrelator and the frequency-domain<br>
decorrelator of the second method together. In this case, a multi-channel signal can be<br>
expressed as Math Figure 9.<br>
[108]	MathFigure 9<br><br>
[109]	In this case, a filter coefficient generating process uses the same process explained<br>
in the first method except that 'A' is changed into 'A+Kd'.<br>
[110]	A ninth method of generating source mapping information using CLD, ICC and/or<br>
decorrelators is to generate an additionally decorrelated value by applying a frequency<br>
domain decorrelator to an output of the related art decorrelator in case of using the<br>
related art decorrelator. Hence, it is able to generate source mapping information with<br>
a small operational quantity by overcoming the limitation of the frequency domain<br>
decorrelator.<br>
[Ill]	A tenth method of generating source mapping information using CLD, ICC and/or<br>
decorrelators is expressed as Math Figure 10.<br>
[112]	MathFigure 10<br><br><br>
[113]	In this case, 'di_(m)'(i=L, R, C, Ls, Rs) is a decorrelator output value applied to a<br>
channel-i. And, the output value can be processed on a time domain, a frequency<br>
domain, a QMF domain, a hybrid domain, or the like. If the output value is processed<br>
on a domain different from a currently processed domain, it can be converted by<br>
domain conversion. It is able to use the same 'd for d_L, d_R, d_C, d_Ls, and d_Rs. In<br>
this case, Math Figure 10 can be expressed in a very simple manner.<br>
[114]	If Math Figure 10 is applied to Math Figure 1, Math Figure 1 can be expressed as<br>
Math Figure 11.<br>
[115]	MathFigure ll<br>
Lo = HM_L*m + HMD_L*d(m)<br>
Ro = HM_R*R + HMD_R*d(m)<br>
[116]	In this case, rendering information HMJL is a value resulting from combining<br>
spatial information and filter information to generate a surround signal Lo with an<br>
input m. And, rendering information HM_R is a value resulting from combining spatial<br>
information and filter information to generate a surround signal Ro with an input m.<br>
Moreover, 'd(m)' is a decorrelator output value generated by transferring a decorrelator<br>
output value on an arbitrary domain to a value on a current domain or a decorrelator<br>
output value generated by being processed on a current domain. Rendering information<br>
HMD_L is a value indicating an extent of the decorrelator output value d(m) that is<br>
added to 'Lo' in rendering the d(m), and also a value resulting from combining spatial<br>
information and filter information together. Rendering information HMD_R is a value<br>
indicating an extent of the decorrelator output value d(m) that is added to 'Ro' in<br>
rendering the d(m).<br>
[117]	Thus, in order to perform a rendering process on a mono downmix signal, the<br>
present invention proposes a method of generating a surround signal by rendering the<br>
rendering information generated by combining spatial information and filter in-<br>
formation (e.g., HRTF filter coefficient) to a downmix signal and a decorrelated<br>
downmix signal. The rendering process can be executed regardless of domains. If<br>
'd(m)' is expressed as 'd*m'(product operator) being executed on a frequency domain,<br><br>
Math Figure 11 can be expressed as Math Figure 12.<br>
[118]	MathFigure l2<br>
Lo = HH_L*m + HMD_L*d*m = HMoveralI_L*m<br>
Ro = HM_R*m + HMD_R*d*m = HMoverall_R*m<br>
[119]	Thus, in case of performing a rendering process on a downmix signal on a<br>
frequency domain, it is ale to minimize an operational quantity in a manner of rep-<br>
resenting a value resulting from combining spatial information, filter information and<br>
decorrelators appropriately as a product form.<br>
[120]	FIG. 6 and FIG. 7 are detailed block diagrams of a rendering unit for a stereo<br>
downmix signal according to one embodiment of the present invention.<br>
[121]	Referring to FIG. 6, the rendering unit 900 includes a rendering unit-A 910 and a<br>
rendering unit-B 920.<br>
[122]	If a downmix signal is a stereo signal, the spatial information converting unit 1000<br>
generates rendering information for left and right channels of the downmix signal. The<br>
rendering unit-A 910 generates a surround signal by rendering the rendering in-<br>
formation for the left channel of the downmix signal to the left channel of the<br>
downmix signal. And, the rendering unit-B 920 generates a surround signal by<br>
rendering the rendering information for the right channel of the downmix signal to the<br>
right channel of the downmix signal. The names of the channels are just exemplary,<br>
which does not put limitation on the present invention.<br>
[123]	The rendering information can include rendering information delivered to a same<br>
channel and rendering information delivered to another channel.<br>
[124]	For instance, the spatial information converting unit 1000 is able to generate<br>
rendering information HL_L and HL_R inputted to the rendering unit for the left<br>
channel of the downmix signal, in which rendering information HL_L is delivered to a<br>
left output corresponding to the same channel and the rendering information HL_R is<br>
delivered to a right output corresponding to the another channel. And, the spatial in-<br>
formation converting unit 1000 is able to generate rendering information HR_R and<br>
HR_L inputted to the rendering unit for the right channel of the downmix signal, in<br>
which the rendering information HR_R is delivered to a right output corresponding to<br>
the same channel and the rendering information HRJL is delivered to a left output cor-<br>
responding to the another channel.<br>
[125]	Referring to FIG. 7, the rendering unit 900 includes a rendering unit-1A 911, a<br>
rendering unit-2A 912, a rendering unit-1B 921, and a rendering unit-2B 922.<br>
[126]	The rendering unit 900 receives a stereo downmix signal and rendering information<br>
from the spatial information converting unit 1000. Subsequently, the rendering unit<br>
900 generates a surround signal by rendering the rendering information to the stereo<br><br>
downmix signal.<br>
[127]	In particular, the rendering unit-1A 911 performs rendering by using rendering in-<br>
formation HL_L delivered to a same channel among rendering information for a left<br>
channel of a downmix signal. The rendering unit-2A 912 performs rendering by using<br>
rendering information HL_R delivered to a another channel among rendering in-<br>
formation for a left channel of a downmix signal. The rendering unit-1B 921 performs<br>
rendering by using rendering information HR_R delivered to a same channel among<br>
rendering information for a right channel of a downmix signal. And, the rendering<br>
unit-2B 922 performs rendering by using rendering information HR_L delivered to<br>
another channel among rendering information for a right channel of a downmix signal.<br>
[128]	In the following description, the rendering information delivered to another channel<br>
is named 'cross-rendering information' The cross-rendering information HL_R or<br>
HR_L is applied to a same channel and then added to another channel by an adder. In<br>
this case, the cross-rendering information HL_R and/or HR_L can be zero. If the cross-<br>
rendering information HL_R and/or HR_L is zero, it means that no contribution is<br>
made to the corresponding path.<br>
[129]	An example of the surround signal generating method shown in FIG. 6 or FIG. 7 is<br>
explained as follows.<br>
[130]	First of all, if a downmix signal is a stereo signal, the downmix signal defined as 'x',<br>
source mapping information generated by using spatial information defined as 'D',<br>
prototype filter information defined as 'G', a multi-channel signal defined as 'p' and a<br>
surround signal defined as 'y1 can be represented by matrixes shown in Math Figure 13.<br>
[131]	MathFigure 13<br><br>
[132]	In this case, if the above values are on a frequency domain, they can be developed<br>
as follows.<br>
[133]	First of all, the multi-channel signal p, as shown in Math Figure 14, can be<br>
expressed as a product between the source mapping information D generated by using<br>
the spatial information and the downmix signal x.<br>
[134]	MathFigure 14<br><br><br>
[135]	The surround signal y, as shown in Math Figure 15, can be generated by rendering<br>
the prototype filter information G to the multi-channel signal p.<br>
[136]	MathFigure 15<br>
y=G.p<br>
[137]	In this case, if Math Figure 14 is inserted in the p, it can be generated as Math<br>
Figure 16.<br>
[138]	MathFigure 16<br>
y = GDx<br>
[139]	In this case, if rendering information H is defined as H=GD, the surround signal y<br>
and the downmix signal x can have a relation of Math Figure 17.<br>
[140]	MathFigure 17<br><br>
[141]	Hence, after the rendering information H has been generated by processing the<br>
product between the filter information and the source mapping information, the<br>
downmix signal x is multiplied by the rendering information H to generate the<br>
surround signal y.<br>
[142]	According to the definition of the rendering information H, the rendering in-<br>
formation H can be expressed as Math Figure 18.<br>
[143]	MathFigure 18<br>
H = GD<br><br>
[144]	FIG. 8 and FIG. 9 are detailed block diagrams of a rendering unit for a mono<br>
downmix signal according to one embodiment of the present invention.<br>
[145]	Referring to FIG. 8, the rendering unit 900 includes a rendering unit-A 930 and a<br>
rendering unit-B 940.<br>
[146]	If a downmix signal is a mono signal, the spatial information converting unit 1000<br><br>
generates rendering information HM_L and HM_R, in which the rendering in-<br>
formation HM_L is used in rendering the mono signal to a left channel and the<br>
rendering information HM_R is used in rendering the mono signal to a right channel.<br>
[147]	The rendering unit-A 930 applies the rendering information HM_L to the mono<br>
downmix signal to generate a surround signal of the left channel. The rendering unit-B<br>
940 applies the rendering information HM_R to the mono downmix signal to generate<br>
a surround signal of the right channel.<br>
[148]	The rendering unit 900 in the drawing does not use a decorrelator. Yet, if the<br>
rendering unit-A 930 and the rendering unit-B 940 performs rendering by using the<br>
rendering information Hmoverall_R and Hmoverall_L defined in Math Figure 12, re-<br>
spectively, it is able to obtain the outputs to which the decorrelator is applied, re-<br>
spectively.<br>
[ 149]	Meanwhile, in case of attempting to obtain an output in a stereo signal instead of a<br>
surround signal after completion of the rendering performed on a mono downmix<br>
signal, the following two methods are possible.<br>
[ 150]	The first method is that instead of using rendering information for a surround effect,<br>
a value used for a stereo output is used. In this case, it is able to obtain a stereo signal<br>
by modifying only the rendering information in the structure shown in FIG. 3.<br>
[151]	The second method is that in a decoding process for generating a multi-channel<br>
signal by using a downmix signal and spatial information, it is able to obtain a stereo<br>
signal by performing the decoding process to only a corresponding step to obtain a<br>
specific channel number.<br>
[152]	Referring to FIG. 9, the rendering unit 900 corresponds to a case in which a<br>
decorrelated signal is represented as one, i.e., Math Figure 11. The rendering unit 900<br>
includes a rendering unit-1A 931, a rendering unit-2A 932, a rendering unit-IB 941,<br>
and a rendering unit-2B 942. The rendering unit 900 is similar to the rendering unit for<br>
the stereo downmix signal except that the rendering unit 900 includes the rendering<br>
units 941 and 942 for a decorrelated signal.<br>
[153]	In case of the stereo downmix signal, it can be interpreted that one of two channels<br>
is a decorrelated signal. So, without employing additional decorrelators, it is able to<br>
perform a rendering process by using the formerly defined four kinds of rendering in-<br>
formation HL_L, HL_R and the like. In particular, the rendering unit-1A 931 generates<br>
a signal to be delivered to a same channel by applying the rendering information<br>
HM_L to a mono downmix signal. The rendering unit-2A 932 generates a signal to be<br>
delivered to another channel by applying the rendering information HM_R to the mono<br>
downmix signal. The rendering unit-IB 941 generates a signal to be delivered to a<br>
same channel by applying the rendering information HMD_R to a decorrelated signal.<br>
And, the rendering unit-2B 942 generates a signal to be delivered to another channel<br><br>
by applying the rendering information HMD_L to the decorrelated signal.<br>
[ 154]	If a downmix signal is a mono signal, a downmix signal defined as x, source<br>
channel information defined as D, prototype filter information defined as G, a multi-<br>
channel signal defined as p, and a surround signal defined as y can be represented by<br>
matrixes shown in Math Figure 19.<br>
[155]	MathFigure 19<br><br>
[156]	In this case, the relation between the matrixes is similar to that of the case that the<br>
downmix signal is the stereo signal. So its details are omitted.<br>
[157]	Meanwhile, the source mapping information described with reference to FIG. 4 and<br>
FIG. 5 and the rendering information generated by using the source mapping in-<br>
formation have values differing per frequency band, parameter band, and/or<br>
transmitted timeslot. In this case, if a value of the source mapping information and/or<br>
the rendering information has a considerably big difference between neighbor bands or<br>
between boundary timeslots, distortion may take place in the rendering process. To<br>
prevent the distortion, a smoothing process on a frequency and/or time domain is<br>
needed. Another smoothing method suitable for the rendering is usable as well as the<br>
frequency domain smoothing and/or the time domain smoothing. And, it is able to use<br>
a value resulting from multiplying the source mapping information or the rendering in-<br>
formation by a specific gain.<br>
[158]	FIG. 10 and FIG. 11 are block diagrams of a smoothing unit and an expanding unit<br>
according to one embodiment of the present invention.<br>
[159]	A smoothing method according to the present invention, as shown in FIG. 10 and<br>
FIG. 11, is applicable to rendering information and/or source mapping information.<br>
Yet, the smoothing method is applicable to other type information. In the following de-<br>
scription, smoothing on a frequency domain is described. Yet, the present invention<br>
includes time domain smoothing as well as the frequency domain smoothing.<br>
[ 160]	Referring to FIG. 10 and FIG. 11, the smoothing unit 1042 is capable of performing<br>
smoothing on rendering information and/or source mapping information. A detailed<br>
example of a position of the smoothing occurrence will be described with reference to<br><br>
FIGs. 18 to 20 later.<br>
[161]	The smoothing unit 1042 can be configured with an expanding unit 1043, in which<br>
the rendering information and/or source mapping information can be expanded into a<br>
wider range, for example filter band, than that of a parameter band. In particular, the<br>
source mapping information can be expanded to a frequency resolution (e.g., filter<br>
band) corresponding to filter information to be multiplied by the filter information<br>
(e.g., HRTF filter coefficient). The smoothing according to the present invention is<br>
executed prior to or together with the expansion. The smoothing used together with the<br>
expansion can employ one of the methods shown in FIGs. 12 to 16.<br>
[162]	FIG. 12 is a graph to explain a first smoothing method according to one<br>
embodiment of the present invention.<br>
[163]	Referring to FIG. 12, a first smoothing method uses a value having the same size as<br>
spatial information in each parameter band. In this case, it is able to achieve a<br>
smoothing effect by using a suitable smoothing function.<br>
[164]	FIG. 13 is a graph to explain a second smoothing method according to one<br>
embodiment of the present invention.<br>
[165]	Referring to FIG. 13, a second smoothing method is to obtain a smoothing effect by<br>
connecting representative positions of parameter band. The representative position is a<br>
right center of each of the parameter bands, a central position proportional to a log<br>
scale, a bark scale, or the like, a lowest frequency value, or a position previously<br>
determined by a different method.<br>
[166]	FIG. 14 is a graph to explain a third smoothing method according to one<br>
embodiment of the present invention.<br>
[167]	Referring to FIG. 14, a third smoothing method is to perform smoothing in a form<br>
of a curve or straight line smoothly connecting boundaries of parameters. In this case,<br>
the third smoothing method uses a preset boundary smoothing curve or low pass<br>
filtering by the first order or higher 1TR filter or FIR filter.<br>
[168]	FIG. 15 is a graph to explain a fourth smoothing method according to one<br>
embodiment of the present invention.<br>
[169]	Referring to FIG. 15, a fourth smoothing method is to achieve a smoothing effect<br>
by adding a signal such as a random noise to a spatial information contour. In this case,<br>
a value differing in channel or band is usable as the random noise. In case of adding a<br>
random noise on a frequency domain, it is able to add only a size value while leaving a<br>
phase value intact. The fourth smoothing method is able to achieve an inter-channel<br>
decorrelation effect as well as a smoothing effect on a frequency domain.<br>
[170]	FIG. 16 is a graph to explain a fifth smoothing method according to one<br>
embodiment of the present invention.<br>
[171]	Referring to FIG. 16, a fifth smoothing method is to use a combination of the<br><br>
second to fourth smoothing methods. For instance, after the representative positions of<br>
the respective parameter bands have been connected, the random noise is added and<br>
low path filtering is then applied. In doing so, the sequence can be modified. The fifth<br>
smoothing method minimizes discontinuous points on a frequency domain and an<br>
inter-channel decorrelation effect can be enhanced.<br>
[172]	In the first to fifth smoothing methods, a total of powers for spatial information<br>
values (e.g., CLD values) on the respective frequency domains per channel should be<br>
uniform as a constant. For this, after the smoothing method is performed per channel,<br>
power normalization should be performed. For instance, if a downmix signal is a mono<br>
signal, level values of the respective channels should meet the relation of Math Figure<br>
20.<br>
[173]	MathFigure 20<br>
D_L(pb) + D_R(pb) + D_C(pb) + D_Ls(pb) + D_Rs(pb) + D_LfeCpb) = C<br>
[174]	In this case, 'pb = 0~ total parameter band number 1' and 'C' is an arbitrary constant.<br>
[175]	FIG. 17 is a diagram to explain prototype filter information per channel.<br>
[ 176]	Referring to FIG. 17, for rendering, a signal having passed through GL_L filter for<br>
a left channel source is sent to a left output, whereas a signal having passed through<br>
GL_R filter is sent to a right output.<br>
[177]	Subsequently, a left final output (e.g., Lo) and a right final output (e.g., Ro) are<br>
generated by adding all signals received from the respective channels. In particular, the<br>
rendered left/right channel outputs can be expressed as Math Figure 21.<br>
[178]	MathFigure 21<br>
Lo = L * GL_L + C * GC_L + R * GR_L + Ls * GLs_L + Rs * GRs_L<br>
Ro = L * GL_R + C * GC_R + R * GR_R + Ls * GLs + Rs * GRs_R<br>
[179]	In the present invention, the rendered left/right channel outputs can be generated by<br>
using the L, R, C, Ls, and Rs generated by decoding the downmix signal into the multi-<br>
channel signal using the spatial information. And, the present invention is able to<br>
generate the rendered left/right channel outputs using the rendering information<br>
without generating the L, R, C, Ls, and Rs, in which the rendering information is<br>
generated by using the spatial information and the filter information.<br>
[180]	A process for generating rendering information using spatial information is<br>
explained with reference to FIGs. 18 to 20 as follows.<br>
[181]	FIG. 18 is a block diagram for a first method of generating rendering information in<br>
a spatial information converting unit 900 according to one embodiment of the present<br>
invention.<br>
[182]	Referring to FIG. 18, as mentioned in the foregoing description, the spatial in-<br><br>
formation converting unit 900 includes the source mapping unit 1010, the sub-<br>
rendering information generating unit 1020, the integrating unit 1030, the processing<br>
unit 1040, and the domain converting unit 1050. The spatial information converting<br>
unit 900 has the same configuration shown in FIG. 3.<br>
[183]	The sub-rendering information generating unit 1020 includes at least one or more<br>
sub-rendering information generating units (1st sub-rendering information generating<br>
unit to Nth  sub-rendering information generating unit).<br>
[184]	The sub-rendering information generating unit 1020 generates sub-rendering in-<br>
formation by using filter information and source mapping information.<br>
[185]	For instance, if a downmix signal is a mono signal, the first sub-rendering in-<br>
formation generating unit is able to generate sub-rendering information corresponding<br>
to a left channel on a multi-channel. And, the sub-rendering information can be<br>
represented as Math Figure 22 using the source mapping information D_L and the<br>
converted filter information GL_L' and GL_R'<br>
[186]	MathFigure 22<br>
FL__L = D_L * GL_L'<br>
(mono input  filter coefficient to left output channel)<br>
FL_R = D_L * GL_R'<br>
(mono input  filter coefficient to right output channel)<br>
[187]	In this case, the D_L is a value generated by using the spatial information in the<br>
source mapping unit 1010. Yet, a process for generating the D_L can follow the tree<br>
structure.<br>
[188]	The second sub-rendering information generating unit is able to generate sub-<br>
rendering information FR_L and FR_R corresponding to a right channel on the multi-<br>
channel. And, the Nth sub-rendering information generating unit is able to generate sub-<br>
rendering information FRs_L and FRs_R corresponding to a right surround channel on<br>
the multi-channel.<br>
[189]	If a downmix signal is a stereo signal, the first sub-rendering information generating<br>
unit is able to generate sub-rendering information corresponding to the left channel on<br>
the multi-channel. And, the sub-rendering information can be represented as Math<br>
Figure 23 by using the source mapping information D_L1 and D_L2.<br>
[190]	MathFigure 23<br><br>
FL_L1 = D_L1 * 6L_L'<br>
(left input filter coefficient to left output channel)<br>
FL_L2 = D_L2 * GL_L'<br>
(right input filter coefficient to left output channel)<br>
FL_R1 = D_L1 * GL_R'<br>
(left input filter coefficient to right output channel)<br>
FL_R2 = D_L2 * GL_R'<br>
(right input filter coefficient to right output channel)<br>
[191]	In Math Figure 23, the FL_R1 is explained for example as follows.<br>
[ 192]	First of all, in the FL_R1, 'L' indicates a position of the multi-channel, 'R' indicates<br>
an output channel of a surround signal, and T indicates a channel of the downmix<br>
signal. Namely, the FL_R1 indicates the sub-rendering information used in generating<br>
the right output channel of the surround signal from the left channel of the downmix<br>
signal.<br>
[193]	Secondly, the D_L1 and the D_L2 are values generated by using the spatial in-<br>
formation in the source mapping unit 1010.<br>
[194]	If a downmix signal is a stereo signal, it is able to generate a plurality of sub-<br>
rendering informations from at least one sub-rendering information generating unit in<br>
the same manner of the case that the downmix signal is the mono signal. The types of<br>
the sub-rendering informations generated by a plurality of the sub-rendering in-<br>
formation generating units are exemplary, which does not put limitation on the present<br>
invention.<br>
[195]	The sub-rendering information generated by the sub-rendering information<br>
generating unit 1020 is transferred to the rendering unit 900 via the integrating unit<br>
1030, the processing unit 1040, and the domain converting unit 1050.<br>
[196]	The integrating unit 1030 integrates the sub-rendering informations generated per<br>
channel into rendering information (e.g., HL_L, HL_R, HR_L, HR_R) for a rendering<br>
process. An integrating process in the integrating unit 1030 is explained for a case of a<br>
mono signal and a case of a stereo signal as follows.<br>
[197]	First of all, if a downmix signal is a mono signal, rendering information can be<br>
expressed as Math Figure 24.<br>
[198]	MathFigure 24<br>
HM_L = FL_L + FR_L + FC_L + FLs_L + FRs_L + FLFE_L<br>
HO = FL_R + FR_R + FC_R + FLs_R + FRs.R + FLFE_R<br><br>
[199]	Secondly, if a downmix signal is a stereo signal, rendering information can be<br>
expressed as Math Figure 25.<br>
[200]	MathFigure 25<br>
HL_L - FL_L1 + FR_L1 4 FC_L1 + FLs_Ll + FRs_Ll + FLFE_L1<br>
HR_L = FL_L2 + FR_L2 + FC_L2 + FLs_L2 + FRs_L2 4 FLFE_L2<br>
HL_R = FL_R1 + FR_R1 + FC_R1 + FLs_Rl + FRs_Rl + FLFE_R1<br>
HR_R - FL_R2 + FR_R2 + FC_R2 + FLs_R2 + FRs_R2 + FLFE_R2<br>
[201]	Subsequently, the processing unit 1040 includes an interpolating unit 1041 and/or a<br>
smoothing unit 1042 and performs interpolation and/or smoothing for the rendering in-<br>
formation. The interpolation and/or smoothing can be executed on a time domain, a<br>
frequency domain, or a QMF domain. In the specification, the time domain is taken as<br>
an example, which does not put limitation on the present invention.<br>
[202]	The interpolation is performed to obtain rendering information non-existing<br>
between the rendering informations if the transmitted rendering information has a wide<br>
interval on the time domain. For instance, assuming that rendering informations exist<br>
in an nth timeslot and an (n+k)th timeslot (k&gt;l), respectively, it is able to perform linear<br>
interpolation on a not-transmitted timeslot by using the generated rendering in-<br>
formations (e.g., HL_L, HR_L, HL_R, HR_R).<br>
[203]	The rendering information generated from the interpolation is explained with<br>
reference to a case that a downmix signal is a mono signal and a case that the downmix<br>
signal is a stereo signal.<br>
[204]	If the downmix signal is the mono signal, the interpolated rendering information<br>
can be expressed as Math Figure 26.<br>
[205]	MathFigure 26<br>
HM_L(n+j) = HM_L(n) *(l-a) + HM_L(n4k) * a<br>
HM_R(n+j) = HM_R(n) *(l-a) + HM_R(n+k) * a<br>
[206]	If the downmix signal is the stereo signal, the interpolated rendering information<br>
can be expressed as Math Figure 27.<br>
[207]	MathFigure 27<br>
HL_L(n+j)	= HL_L(n) *(l-a) + HL_L(n+k)	* a<br>
HR_L(n+j)	= HR_L(n) *(l-a) + HR_L(n+k)	* a<br>
HL_R(n+j)	= HL_R(n) *(l-a) + HL_R(n+k)	* a<br>
HR_R(n+j)	= HR_R(n) *(l-a) + HR_R(n+k)	* a<br>
[208]	In this case, it is 0<j and are integers. is a real number cor-></j>
responding to '0<a to be expressed as math figure></a>
 <br>
[209]	MathFigure 28<br>
a = j/k<br>
[210]	If so, it is able to obtain a value corresponding to the not-transmitted timeslot on a<br>
straight line connecting the values in the two timeslots according to Math Figure 27<br>
and Math Figure 28. Details of the interpolation will be explained with reference to<br>
FIG. 22 and FIG. 23 later.<br>
[211]	In case that a filter coefficient value abruptly varies between two neighboring<br>
timeslots on a time domain, the smoothing unit 1042 executes smoothing to prevent a<br>
problem of distortion due to an occurrence of a discontinuous point. The smoothing on<br>
the time domain can be carried out using the smoothing method described with<br>
reference to FIGs. 12 to 16. The smoothing can be performed together with expansion.<br>
And, the smoothing may differ according to its applied position. If a downmix signal is<br>
a mono signal, the time domain smoothing can be represented as Math Figure 29.<br>
[212]	MathFigure 29<br>
HM_L(n)' = HM_L(n)*b + HM_L(n-l)'*(1-b)<br>
HM_R(n)'  = HM_R(n)*b + HM_R(n-l) '*(1-b)<br>
[213]	Namely, the smoothing can be executed by the 1-pol IIR filter type performed in a<br>
manner of multiplying the rendering information HM_L(n-l) or HM_R(n-l) smoothed<br>
in a previous timeslot n-1 by (1-b), multiplying the rendering information HM_L(n) or<br>
HM)R(n) generated in a current timeslot n by b, and adding the two multiplications<br>
together. In this case, 'b' is a constant for 0<b if gets smaller a smoothing effect></b>
becomes greater. If 'b' gets bigger, a smoothing effect becomes smaller. And, the rest<br>
of the filters can be applied in the same manner.<br>
[214]	The interpolation and the smoothing can be represented as one expression shown in<br>
Math Figure 30 by using Math Figure 29 for the time domain smoothing.<br>
[215]	MathFigure 30<br>
HM_L(n+j)'=(HM_L(n)*(1 -a)+HM_L(n+k)*a)*b+HM_L(n+j-1 )'*(1 -b)<br>
HM_R(n+j)'=(HM_R(n)*(1-a)+HM_R(n+k)*a)*b+HM_R(n+j-1)'*(1-b)<br>
[216]	If the interpolation is performed by the interpolating unit 1041 and/or if the<br>
smoothing is performed by the smoothing unit 1042, rendering information having an<br>
energy value different from that of prototype rendering information may be obtained.<br>
To prevent this problem, energy normalization may be executed in addition.<br>
[217]	Finally, the domain converting unit 1050 performs domain conversion on the<br>
rendering information for a domain for executing the rendering. If the domain for<br>
executing the rendering is identical to the domain of rendering information, the domain<br><br>
conversion may not be executed. Thereafter, the domain-converted rendering in-<br>
formation is transferred to the rendering unit 900.<br>
[218]         FIG. 19 is a block diagram for a second method of generating rendering information<br>
in a spatial information converting unit according to one embodiment of the present<br>
invention.<br>
[219]	The second method is similar to the first method in that a spatial information<br>
converting unit 1000 includes a source mapping unit 1010, a sub-rendering information<br>
generating unit 1020, an integrating unit 1030, a processing unit 1040, and a domain<br>
converting unit 1050 and in that the sub-rendering information generating unit 1020<br>
includes at least one sub-rendering information generating unit.<br>
[220]	Referring to FIG. 19, the second method of generating the rendering information<br>
differs from the first method in a position of the processing unit 1040. So, interpolation<br>
and/or smoothing can be performed per channel on sub-rendering informations (e.g.,<br>
FLJL and FL_R in case of mono signal or FL_L1, FL_L2 ,FL_R1, FL_R2 in case of<br>
stereo signal) generated per channel in the sub-rendering information generating unit<br>
1020.<br>
[221]	Subsequently, the integrating unit 1030 integrates the interpolated and/or smoothed<br>
sub-rendering informations into rendering information.<br>
[222]	The generated rendering information is transferred to the rendering unit 900 via the<br>
domain converting unit 1050.<br>
[223]	FIG. 20 is a block diagram for a third method of generating rendering filter in-<br>
formation in a spatial information converting unit according to one embodiment of the<br>
present invention.<br>
[224]	The third method is similar to the first or second method in that a spatial in-<br>
formation converting unit 1000 includes a source mapping unit 1010, a sub-rendering<br>
information generating unit 1020, an integrating unit 1030, a processing unit 1040, and<br>
a domain converting unit 1050 and in that the sub-rendering information generating<br>
unit 1020 includes at least one sub-rendering information generating unit.<br>
[225]	Referring to FIG. 20, the third method of generating the rendering information<br>
differs from the first or second method in that the processing unit 1040 is located next<br>
to the source mapping unit 1010. So, interpolation and/or smoothing can be performed<br>
per channel on source mapping information generated by using spatial information in<br>
the source mapping unit 1010.<br>
[226]	Subsequently, the sub-rendering information generating unit 1020 generates sub-<br>
rendering information by using the interpolated and/or smoothed source mapping in-<br>
formation and filter information.<br>
[227]	The sub-rendering information is integrated into rendering information in the in-<br>
tegrating unit 1030. And, the generated rendering information is transferred to the<br><br>
rendering unit 900 via the domain converting unit 1050.<br>
[228]	FIG. 21 is a diagram to explain a method of generating a surround signal in a<br>
rendering unit according to one embodiment of the present invention. FIG. 21 shows a<br>
rendering process executed on a DFT domain. Yet, the rendering process can be im-<br>
plemented on a different domain in a similar manner as well. FIG. 21 shows a case that<br>
an input signal is a mono downmix signal. Yet, FIG. 21 is applicable to other input<br>
channels including a stereo downmix signal and the like in the same manner.<br>
[229]	Referring to FIG. 21, a mono downmix signal on a time domain preferentially<br>
executes windowing having an overlap interval OL in the domain converting unit. Fig.<br>
21 shows a case that 50% overlap is used. Yet, the present invention includes cases of<br>
using other overlaps.<br>
[230]	A window function for executing the windowing can employ a function having a<br>
good frequency selectivity on a DFT domain by being seamlessly connected without<br>
discontinuity on a time domain. For instance, a sine square window function can be<br>
used as the window function.<br>
[231]	Subsequently, zero padding ZL of a tab length [precisely, (tab length) -1 ] of a<br>
rendering filter using rendering information converted in the domain converting unit is<br>
performed on a mono downmix signal having a length OL*2 obtained from the<br>
windowing. A domain conversion is then performed into a DFT domain. FIG. 20<br>
shows that a block-k downmix signal is domain-converted into a DFT domain.<br>
[232]	The domain-converted downmix signal is rendered by a rendering filter that uses<br>
rendering information. The rendering process can be represented as a product of a<br>
downmix signal and rendering information. The rendered downmix signal undergoes<br>
IDFT (Inverse Discrete Fourier Transform) in the inverse domain converting unit and<br>
is then overlapped with the downmix signal (block k-1 in FIG. 20) previously executed<br>
with a delay of a length OL to generate a surround signal.<br>
[233]	Interpolation can be performed on each block undergoing the rendering process.<br>
The interpolating method is explained as follows.<br>
[234]	FIG. 22 is a diagram for a first interpolating method according to one embodiment<br>
of the present invention. Interpolation according to the present invention can be<br>
executed on various positions. For instance, the interpolation can be executed on<br>
various positions in the spatial information converting unit shown in FIGs. 18 to 20 or<br>
can be executed in the rendering unit. Spatial information, source mapping in-<br>
formation, filter information and the like can be used as the values to be interpolated.<br>
In the specification, the spatial information is exemplarily used for description. Yet,<br>
the present invention is not limited to the spatial information. The interpolation is<br>
executed after or together with expansion to a wider band.<br>
[235]	Referring to FIG. 22, spatial information transferred from an encoding apparatus c<br><br>
an be transferred from a random position instead of being transmitted each timeslot.<br>
One spatial frame is able to carry a plurality of spatial information sets (e.g., parameter<br>
sets n and n+1 in FIG. 22). In case of a low bit rate, one spatial frame is able to carry a<br>
single new spatial information set. So, interpolation is carried out for a not-transmitted<br>
timeslot using values of a neighboring transmitted spatial information set. An interval<br>
between windows for executing rendering does not always match a timeslot. So, an in-<br>
terpolated value at a center of the rendering windows (K-l, K, K+l, K+2, etc.), as<br>
shown in FIG. 22, is found to use. Although FIG. 22 shows that linear interpolation is<br>
carried out between timeslots where a spatial information set exists, the present<br>
invention is not limited to the interpolating method. For instance, interpolation is not<br>
carried out on a timeslot where a spatial information set does not exist. Instead, a<br>
previous or preset value can be used.<br>
[236]	FIG. 23 is a diagram for a second interpolating method according to one<br>
embodiment of the present invention.<br>
[237]	Referring to FIG. 23, a second interpolating method according to one embodiment<br>
of the present invention has a structure that an interval using a previous value, an<br>
interval using a preset default value and the like are combined. For instance, in-<br>
terpolation can be performed by using at least one of a method of maintaining a<br>
previous value, a method of using a preset default value, and a method of executing<br>
linear interpolation in an interval of one spatial frame. In case that at least two new<br>
spatial information sets exist in one window, distortion may take place. In the<br>
following description, block switching for preventing the distortion is explained.<br>
[238]	FIG. 24 is a diagram for a block switching method according to one embodiment of<br>
the present invention.<br>
[239]	Referring to (a) shown in FIG. 24, since a window length is greater than a timeslot<br>
length, at least two spatial information sets (e.g., parameter sets n and n+1 in FIG. 24)<br>
can exist in one window interval. In this case, each of the spatial information sets<br>
should be applied to a different timeslot. Yet, if one value resulting from interpolating<br>
the at least two spatial information sets is applied, distortion may take place. Namely,<br>
distortion attributed to time resolution shortage according to a window length can take<br>
place.<br>
[240]	To solve this problem, a switching method of varying a window size to fit<br>
resolution of a timeslot can be used. For instance, a window size, as shown in (b) of<br>
FIG. 24, can be switched to a shorter-sized window for an interval requesting a high<br>
resolution. In this case, at a beginning and an ending portion of switched windows,<br>
connecting windows is used to prevent seams from occurring on a time domain of the<br>
switched windows.<br>
[241]	The window length can be decided by using spatial information in a decoding<br><br>
apparatus instead of being transferred as separate additional information. For instance,<br>
a window length can be determined by using an interval of a timeslot for updating<br>
spatial information. Namely, if the interval for updating the spatial information is<br>
narrow, a window, function of short length is used. If the interval for updating the<br>
spatial information is wide, a window function of long length is used. In this case, by<br>
using a variable length window in rendering, it is advantageous not to use bits for<br>
sending window length information separately. Two types of window length are shown<br>
in (b) of FIG. 24. Yet, windows having various lengths can be used according to<br>
transmission frequency and relations of spatial information. The decided window<br>
length information is applicable to various steps for generating a surround signal,<br>
which is explained in the following description.<br>
[242]	FIG. 25 is a block diagram for a position to which a window length decided by a<br>
window length deciding unit is applied according to one embodiment of the present<br>
invention.<br>
[243]	Referring to FIG. 25, a window length deciding unit 1400 is able to decide a<br>
window length by using spatial information. Information for the decided window<br>
length is applicable to a source mapping unit 1010, an integrating unit 1030, a<br>
processing unit 1040, domain converting units 1050 and 1100, and a inverse domain<br>
converting unit 1300. Fig. 25 shows a case that a stereo downmix signal is used. Yet,<br>
the present invention is not limited to the stereo downmix signal only. As mentioned in<br>
the foregoing description, even if a window length is shortened, a length of zero<br>
padding decided according to a filter tab number is not adjustable. So, a solution for<br>
the problem is explained in the following description.<br>
[244]	FIG. 26 is a diagram for filters having various lengths used in processing an audio<br>
signal according to one embodiment of the present invention. As mentioned in the<br>
foregoing description, if a length of zero padding decided according to a filter tab<br>
number is not adjusted, an overlapping amounting to a corresponding length sub-<br>
stantially occurs to bring about time resolution shortage. A solution for the problem is<br>
to reduce the length of the zero padding by restricting a length of a filter tab. A method<br>
of reducing the length of the zero padding can be achieved by truncating a rear portion<br>
of a response (e.g., a diffusing interval corresponding to reverberation). In this case, a<br>
rendering process may be less accurate than a case of not truncating the rear portion of<br>
the filter response. Yet, filter coefficient values on a time domain are very small to<br>
mainly affect reverberation. So, a sound quality is not considerably affected by the<br>
truncating.<br>
[245]	Referring to FIG. 26, four kinds of filters are usable. The four kinds of the filters are<br>
usable on a DFT domain, which does not put limitation on the present invention.<br>
[246]	A filter-N indicates a filter having a long filter length FL and a length 2*OL of a<br><br>
long zero padding of which filter tab number is not restricted. A filter-N2 indicates a<br>
filter having a zero padding length 2*OL shorter than that of the filter-Nl by restricting<br>
a tab number of filter with the same filter length FL. A filter-N3 indicates a filter<br>
having a long zero padding length 2*OL by not restricting a tab number of filter with a<br>
filter length FL shorter than that of the filter-Nl. And, a filter-N4 indicates a filter<br>
having a window length FL shorter than that of the filter-Nl with a short zero padding<br>
length 2*OL by restricting a tab number of filter.<br>
[247]	As mentioned in the foregoing description, it is able to solve the problem of time<br>
resolution using the above exemplary four kinds of the filters. And, for the rear portion<br>
of the filter response, a different filter coefficient is usable for each domain.<br>
[248]	FIG. 27 is a diagram for a method of processing an audio signal dividedly by using<br>
a plurality of subfilters according to one embodiment of the present invention, one<br>
filter may be divided into subfilters having filter coefficients differing from each other.<br>
After processing the audio signal by using the subfilters, a method of adding results of<br>
the processing can be used. In case applying spatial information to a rear portion of a<br>
filter response having small energy, i.e., in case of performing rendering by using a<br>
filter with a long filter tab, the method provides function for processing dividedly the<br>
audio signal by a predetermined length unit. For instance, since the rear portion of the<br>
filter response is not considerably varied per HRTF corresponding to each channel, it is<br>
able to perform the rendering by extracting a coefficient common to a plurality of<br>
windows. In the present specification, a case of execution on a DFT domain is<br>
described. Yet, the present invention is not limited to the DFT domain.<br>
[249]	Referring to FIG. 27, after one filter FL has been divided into a plurality of sub-<br>
areas, a plurality of the sub-areas can be processed by a plurality of subfilters (filter-A<br>
and filter-B) having filter coefficients differing from each other.<br>
[250]	Subsequently, an output processed by the filter-A and an output processed by the<br>
filter-B are combined together. For instance, IDFT (Inverse Discrete Fourier<br>
Transform) is performed on each of the output processed by the filter-A and the output<br>
processed by the filter-B to generate a time domain signal. And, the generated signals<br>
are added together. In this case, a position, to which the output processed by the filter-<br>
B is added, is time-delayed by FL more than a position of the output processed by the<br>
filter-A. In this way, the signal processed by a plurality of the subfilters brings the<br>
same effect of the case that the signal is processed by a single filter.<br>
[251 ]	And, the present invention includes a method of rendering the output processed by<br>
the filter-B to a downmix signal directly. In this case, it is able to render the output to<br>
the downmix signal by using coefficients extracting from spatial information, the<br>
spatial information in part or without using the spatial information.<br>
[252]	The method is characterized in that a filter having a long tab number can be applied<br><br>
dividedly and that a rear portion of the filter having small energy is applicable without<br>
conversion using spatial information. In this case, if conversion using spatial in-<br>
formation is not applied, a different filter is not applied to each processed window. So,<br>
it is unnecessary to apply the same scheme as the block switching. FIG. 26 shows that<br>
the filter is divided into two areas. Yet, the present invention is able to divide the filter<br>
into a plurality of areas.<br>
[253]	FIG. 28 is a block diagram for a method of rendering partition rendering in-<br>
formation generated by a plurality of subfilters to a mono downmix signal according to<br>
one embodiment of the present invention. FIG. 28 relates to one rendering coefficient.<br>
The method can be executed per rendering coefficient.<br>
[254]         Referring to FIG. 28, the filter-A information of FIG. 27 corresponds to first<br>
partition rendering information HM_L_A and the filter-B information of FIG. 27<br>
corresponds to second partition rendering information HM_L_B. FIG. 28 shows an<br>
embodiment of partition into two subfilters. Yet, the present invention is not limited to<br>
the two subfilters. The two subfilters can be obtained via a splitting unit 1500 using the<br>
rendering information HM_L generated in the spatial information generating unit<br>
1000. Alternatively, the two subfilters can be obtained using prototype HRTF in-<br>
formation or information decided according to a user's selection. The information<br>
decided according to a user's selection may include spatial information selected<br>
according to a user's taste for example. In this case, HM_L_A is the rendering in-<br>
formation based on the received spatial information, and, HM_L_B may be the<br>
rendering information for providing a 3-dimensional effect commonly applied to<br>
signals.<br>
[255]	As mentioned in the foregoing description, the processing with a plurality of the<br>
subfilters is applicable to a time domain and a QMF domain as well as the DFT<br>
domain. In particular, the coefficient values split by the filter-A and the filter-B are<br>
applied to the downmix signal by time or QMF domain rendering and are then added<br>
to generate a final signal.<br>
[256]	The rendering unit 900 includes a first partition rendering unit 950 and a second<br>
partition rendering unit 960. The first partition rendering unit 950 performs a rendering<br>
process using HM_L_A, whereas the second partition rendering unit 960 performs a<br>
rendering process using HM_L_B.<br>
[257]	If the filter-A and the filter-B, as shown in FIG. 27, are splits of a same filter<br>
according to time, it is able to consider a proper delay to correspond to the time<br>
interval. FIG. 28 shows an example of a mono downmix signal. In case of using mono<br>
downmix signal and decorrelator, a portion corresponding to the filter-B is applied not<br>
to the decorrelator but to the mono downmix signal directly.<br>
[258]	FIG. 29 is a block diagram for a method of rendering partition rendering in-<br><br>
formation generated using a plurality of subfilters to a stereo downmix signal<br>
according to one embodiment of the present invention.<br>
[259]	A partition rendering process shown in FIG. 29 is similar to that of FIG. 28 in that<br>
two subfilters are obtained in a splitter 1500 by using rendering information generated<br>
by the spatial information converting unit 1000, prototype HRTF filter information or<br>
user decision information. The difference from FIG. 28 lies in that a partition rendering<br>
process corresponding to the filter-B is commonly applied to L/R signals.<br>
[260]	In particular, the splitter 1500 generates first partition rendering information cor-<br>
responding to filter-A information, second partition rendering information, and third<br>
partition rendering information corresponding to filter-B information. In this case, the<br>
third partition rendering information can be generated by using filter information or<br>
spatial information commonly applicable to the L/R signals.<br>
[261 ]	Referring to FIG. 29, a rendering unit 900 includes a first partition rendering unit<br>
970, a second partition rendering unit 980, and a third partition rendering unit 990.<br>
[262]	The third partition rendering information generates is applied to a sum signal of the<br>
L/R signals in the third partition rendering unit 990 to generate one output signal. The<br>
output signal is added to the L/R output signals, which are independently rendered by a<br>
filter-A1 and a filter-A2 in the first and second partition rendering units 970 and 980,<br>
respectively, to generate surround signals. In this case, the output signal of the third<br>
partition rendering unit 990 can be added after an appropriate delay. In FIG. 29, an<br>
expression of cross rendering information applied to another channel from L/R inputs<br>
is omitted for convenience of explanation.<br>
[263]	FIG. 30 is a block diagram for a first domain converting method of a downmix<br>
signal according to one embodiment of the present invention. The rendering process<br>
executed on the DFT domain has been described so far. As mentioned in the foregoing<br>
description, the rendering process is executable on other domains as well as the DFT<br>
domain. Yet, FIG. 30 shows the rendering process executed on the DFT domain. A<br>
domain converting unit 1100 includes a QMF filter and a DFT filter. An inverse<br>
domain converting unit 1300 includes an IDFT filter and an IQMF filter. FIG. 30<br>
relates to a mono downmix signal, which does not put limitation on the present<br>
invention.<br>
[264]	Referring to Fig. 30, a time domain downmix signal of p samples passes through a<br>
QMF filter to generate P sub-band samples. W samples are recollected per band. After<br>
windowing is performed on the recollected samples, zero padding is performed. M-<br>
point DFT (FFT) is then executed. In this case, the DFT enables a processing by the<br>
aforesaid type windowing. A value connecting the M/2 frequency domain values per<br>
band obtained by the M-point DFT to P bands can be regarded as an approximate value<br>
of a frequency spectrum obtained by M/2*P-point DFT. So, a filter coefficient<br><br>
represented on a M/2*P-point DFT domain is multiplied by the frequency spectrum to<br>
bring the same effect of the rendering process on the DFT domain.<br>
[265]	In this case, the signal having passed through the QMF filter has leakage, e.g.,<br>
aliasing between neighboring bands. In particular, a value corresponding to a neighbor<br>
band smears in a current band and a portion of a value existing in the current band is<br>
shifted to the neighbor band. In this case, if QMF integration is executed, an original<br>
signal can be recovered due to QMF characteristics. Yet, if a filtering process is<br>
performed on the signal of the corresponding band as the case in the present invention,<br>
the signal is distorted by the leakage. To minimize this problem, a process for<br>
recovering an original signal can be added in a manner of having a signal pass through<br>
a leakage minimizing butterfly B prior to performing DFT per band after QMF in the<br>
domain converting unit 100 and performing a reversing process V after IDFT in the<br>
inverse domain converting unit 1300.<br>
[266]	Meanwhile, to match the generating process of the rendering information generated<br>
in the spatial information converting unit 1000 with the generating process of the<br>
downmix signal, DFT can be performed on a QMF pass signal for prototype filter in-<br>
formation instead of executing M/2*P-point DFT in the beginning. In this case, delay<br>
and data spreading due to QMF filter may exist.<br>
[267]	FIG. 31 is a block diagram for a second domain converting method of a downmix<br>
signal according to one embodiment of the present invention. FIG. 31 shows a<br>
rendering process performed on a QMF domain.<br>
[268]	Referring to FIG. 31, a domain converting unit 1100 includes a QMF domain<br>
converting unit and an inverse domain converting unit 1300 includes an IQMF domain<br>
converting unit. A configuration shown in FIG. 31 is equal to that of the case of using<br>
DFT only except that the domain converting unit is a QMF filter. In the following de-<br>
scription, the QMF is referred to as including a QMF and a hybrid QMF having the<br>
same bandwidth. The difference from the case of using DFT only lies in that the<br>
generation of the rendering information is performed on the QMF domain and that the<br>
rendering process is represented as a convolution instead of the product on the DFT<br>
domain, since the rendering process performed by a renderer-M 3012 is executed on<br>
the QMF domain.<br>
[269]	Assuming that the QMF filter is provided with B bands, a filter coefficient can be<br>
represented as a set of filter coefficients having different features (coefficients) for the<br>
B bands. Occasionally, if a filter tab number becomes a first order (i.e., multiplied by a<br>
constant), a rendering process on a DFT domain having B frequency spectrums and an<br>
operational process are matched. Math Figure 31 represents a rendering process<br>
executed in one QMF band (b) for one path for performing the rendering process using<br>
rendering information HM_L.<br><br>
[270]	MathFigure 31<br><br>
[271]	In this case, k indicates a time order in QMF band, i.e., a timeslot unit. The<br>
rendering process executed on the QMF domain is advantageous in that, if spatial in-<br>
formation transmitted is a value applicable to the QMF domain, application of cor-<br>
responding data is most facilitated and that distortion in the course of application can<br>
be minimized. Yet, in case of QMF domain conversion in the prototype filter in-<br>
formation (e.g., prototype filter coefficient) converting process, a considerable op-<br>
erational quantity is required for a process of applying the converted value. In this<br>
case, the operational quantity can be minimized by the method of parameterizing the<br>
HRTF coefficient in the filter information converting process.<br>
Industrial Applicability<br>
[272]	Accordingly, the signal processing method and apparatus of the present invention<br>
uses spatial information provided by an encoder to generate surround signals by using<br>
HRTF filter information or filter information according to a user in a decoding<br>
apparatus in capable of generating multi-channels. And, the present invention is<br>
usefully applicable to various kinds of decoders capable of reproducing stereo signals<br>
only.<br>
[273]	While the present invention has been described and illustrated herein with reference<br>
to the preferred embodiments thereof, it will be apparent to those skilled in the art that<br>
various modifications and variations can be made therein without departing from the<br>
spirit and scope of the invention. Thus, it is intended that the present invention covers<br>
the modifications and variations of this invention that come within the scope of the<br>
appended claims and their equivalents.<br><br>
Claims<br>
[I]	A method of processing a signal, the method comprising:<br>
generating source mapping information corresponding to each source of multi-<br>
sources by using spatial information indicating features between the multi-<br>
sources;<br>
generating sub-rendering information by applying filter information having a<br>
surround effect to the source mapping information per the source;<br>
generating rendering information for generating a surround signal by integrating<br>
at least one of the sub-rendering information; and<br>
generating the surround signal by applying the rendering information to a<br>
downmix signal generated by downmixing the multi-sources.<br>
[2]	The method of claim 1, wherein the spatial information includes at least one of a<br>
source level difference and an inter-source correlation.<br>
[3]	The method of claim 2, wherein the source mapping information includes<br>
decorrelating information.<br>
[4]	The method of claim 3, wherein the decorrelating information corresponds to<br>
each source of the multi-sources.<br>
[5]	The method of claim 4, wherein the decorrelating information is configured in a<br>
manner that the decorrelating information is symmetric for left source and right<br>
source of the multi-sources, and that the decorrelating information is symmetric<br>
for left surround source and right surround source of the multi-sources.<br>
[6]	The method of claim 3, wherein the decorrelating information has an all-pass<br>
feature.<br>
[7]	The method of claim 6, wherein the decorrelating information has a serial or<br>
overlapped structural feature.<br>
[8]	The method of claim 2, wherein the source mapping information is generated by<br>
using features of a decorrelator applied to a frequency domain or a domain<br>
different from the frequency domain.<br>
[9]	The method of claim 1, wherein the sub-rendering information includes in-<br>
formation generated by applying the filter information to at least two of the<br>
source mapping information corresponding to each source of the multi-sources.<br>
[ 10]	The method of claim 1, wherein the filter information includes HRTF filter in-<br>
formation or a value decided according to a user's selection.<br>
[II]	The method of claim 10, wherein the filter information is domain-converted into<br>
a domain for generating the surround signal.<br>
[12]	The method of claim 11, wherein the filter information is generated by<br>
converting the HRTF filter information into a parameter.<br><br>
[13]	An apparatus for processing a signal, the apparatus comprising:<br>
a source mapping unit generating source mapping information corresponding to<br>
each source of multi-sources by using spatial information indicating features<br>
between the multi-sources;<br>
a sub-rendering information generating unit generating sub-rendering in-<br>
formation by applying filter information having a surround effect to the source<br>
mapping information per the source;<br>
an integrating unit generating rendering information for generating a surround<br>
signal by integrating the at least one of the sub-rendering information; and<br>
a rendering unit generating the surround signal by applying the rendering in-<br>
formation to a downmix signal generated by downmixing the multi-sources.<br>
[14]	The apparatus of claim 13, wherein the spatial information includes at least one<br>
of a source level difference and an inter-source correlation.<br>
[15]	The apparatus of claim 13, wherein the sub-rendering information generating<br>
unit generates the sub-rendering information by applying the filter information to<br>
at least two of the source mapping information corresponding to each source of<br>
the multi-sources.<br>
[16]	The apparatus of claim 13, further comprising a filter information converting unit<br>
converting a domain of the filter information to a domain for generating the<br>
surround signal, in which the filter information includes HRTF filter information<br>
or a value decided according to a user's selection.<br><br>
An apparatus for processing a media signal and method thereof are disclosed, by which the media signal can be converted to a surround signal by using spatial information of the media signal. The present invention provides a method of processing a signal, the method comprising of generating source mapping information corresponding to each source of<br>
multi-sources by using spatial information<br>
indicating features between the multi-sources;<br>
generating sub-rendering information by<br>
applying filter information giving a surround<br>
effect to the source mapping information per<br>
the source; generating rendering information for<br>
generating a surround signal by integrating the<br>
at least one of the sub-rendering information;<br>
and generating the surround signal by applying<br>
the rendering information to a downmix signal<br>
generated by downmixing the multi- sources.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1hYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1jbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1jb3JyZXNwb25kZW5jZSBvdGhlcnMucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-correspondence others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1kZXNjcmlwdGlvbiBjb21wbGV0ZS5wZGY=" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-description complete.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1kcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1mb3JtIDEucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-form 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1mb3JtIDMucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-form 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1mb3JtIDUucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-form 5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1ncGEucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-gpa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1pbnRlcm5hdGlvbmFsIHB1YmxpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-international publication.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1pbnRlcm5hdGlvbmFsIHNlYXJjaCByZXBvcnQucGRm" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-international search report.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDIzMzMta29sbnAtMjAwOC1wY3QgcHJpb3JpdHkgZG9jdW1lbnQgbm90aWZpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">02333-kolnp-2008-pct priority document notification.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1BQlNUUkFDVC5wZGY=" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1BTk5FWFVSRSBUTyBGT1JNIDMucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-ANNEXURE TO FORM 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1DTEFJTVMucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1GT1JNLTIucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-FORM-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1GT1JNLTMucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-FORM-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1GT1JNLTUucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-FORM-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1PVEhFUlMucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1QQS5wZGY=" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-PA.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1QRVRJVElPTiBVTkRFUiBSVUxFIDEzNy0xLnBkZg==" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-PETITION UNDER RULE 137-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LSgxOC0wOS0yMDE0KS1QRVRJVElPTiBVTkRFUiBSVUxFIDEzNy5wZGY=" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-(18-09-2014)-PETITION UNDER RULE 137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LUFTU0lHTk1FTlQucGRm" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-ASSIGNMENT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1LT0xOUC0yMDA4LUNPUlJFU1BPTkRFTkNFIDEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">2333-KOLNP-2008-CORRESPONDENCE 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjMzMy1rb2xucC0yMDA4LWZvcm0gMTgucGRm" target="_blank" style="word-wrap:break-word;">2333-kolnp-2008-form 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=YWJzdHJhY3QtMjMzMy1rb2xucC0yMDA4LmpwZw==" target="_blank" style="word-wrap:break-word;">abstract-2333-kolnp-2008.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="264589-window-pane-with-a-safety-element-for-attenauting-the-effect-of-shock-or-pressure-wave.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="264591-band-stop-filter.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>264590</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>2333/KOLNP/2008</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>02/2015</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>09-Jan-2015</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>07-Jan-2015</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>11-Jun-2008</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>LG ELECTRONICS INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>20 YOIDO-DONG, YOUNGDUNGPO-GU, SEOUL</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>LIM JAE HYUN</td>
											<td>609, PARKVILLE OFFICETEL, #1062-20, NAMHYEON-DONG, GWANAK-GU, SEOUL 150-801</td>
										</tr>
										<tr>
											<td>2</td>
											<td>JUNG YANG WON</td>
											<td>2-803, YEOKSAM HANSHIN APT., DOGOK-DONG, GANGNAM-GU, SEOUL 135-270</td>
										</tr>
										<tr>
											<td>3</td>
											<td>OH HYEN O</td>
											<td>306-403, HANSIN APT., GANGSEON-MAEUL 3-DANJI, JUYEOP 1-DONG, ILSAN-GU, GOYANG-SI, GYEONGGI-DO 411-744</td>
										</tr>
										<tr>
											<td>4</td>
											<td>KIM DONG SOO</td>
											<td>502, WOOLIM VILLA, #602-265, NAMHYEON-DONG, GWANAK-GU, SEOUL 151-801</td>
										</tr>
										<tr>
											<td>5</td>
											<td>PANG HEE SUCK</td>
											<td>101, 4/7, #14-10, YANGJAE-DONG, SEOCHO-GU, SEOUL 137-130</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 19/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/KR2007/000349</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2007-01-19</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/779441</td>
									<td>2006-03-07</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>60/787172</td>
									<td>2006-03-30</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>3</td>
									<td>60/776724</td>
									<td>2006-02-27</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>4</td>
									<td>60/779417</td>
									<td>2006-03-07</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>5</td>
									<td>60/759980</td>
									<td>2006-01-19</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/264590-methods-and-apparatuses-for-decoding-audio-signals-using-source-mapping-information by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 02:03:13 GMT -->
</html>
