<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/268753-apparatus-and-method-for-generating-a-coded-video-sequence-by-using-an-intermediate-layer-motion-data-prediction by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 08:10:57 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 268753:APPARATUS AND METHOD FOR GENERATING A CODED VIDEO SEQUENCE BY USING AN INTERMEDIATE LAYER MOTION DATA PREDICTION</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">APPARATUS AND METHOD FOR GENERATING A CODED VIDEO SEQUENCE BY USING AN INTERMEDIATE LAYER MOTION DATA PREDICTION</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>In the scalable video coding in connection with motion compensation (1006, 1014) both in a base layer (1002) and in an enhancement layer, a prediction (1014, 1016) of the motion data of the enhancement layer (1004) is performed by using the motion data of the base layer (1004) to obtain a scalability concept, which provides, on the one hand, a maximum flexibility for the calculation of the motion data of the different layers and, on the other hand, allows a lower bit rate.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Apparatus and Method for Generating a Coded Video Sequence<br>
by using an Intermediate Layer Motion Data Prediction<br>
Description<br>
5<br>
The present invention relates to video coding systems and<br>
particularly to scalable video coding systems, which can be<br>
used in connection with the video coding standard H.264/AVC<br>
or with new MPEG video coding systems.<br>
10<br>
The standard H.264/AVC is the result of a video<br>
standardization project of the ITU-T video coding expert<br>
group VCEG and the ISO/IEC motion picture expert group<br>
(MPEG). The main goals of this standardization project are<br>
15 to provide a clear video coding concept with very good<br>
compression behavior and at the same time to generate a<br>
network-friendly video representation, which comprise both<br>
application with "conversation character", such as video<br>
telephony,  as  well  as  applications without  conversion<br>
20  character (storage, broadcast, stream transmission).<br>
Apart from the above-mentioned standard ISO/IEC 14496-10,<br>
there is also a plurality of publications relating to the<br>
standard. Merely exemplarily, reference is made to "The<br>
25 Emerging H.264-AVC standard", Ralf Schafer, Thomas Wiegand<br>
and Heiko Schwarz, EBU Technical Review, January 2003.<br>
Additionally, the expert publication "Overview of the<br>
H.264/AVC Video Coding Standard", Thomas Wiegand, Gary J.<br>
Sullivan,  Gesle  Bjontegaard  and  Ajay  Lothra,   IEEE<br>
30 Transactions on Circuits and Systems for Video Technology,<br>
July 2003 as well as the expert publication ,,Context-based<br>
adaptive Binary Arithmethic Coding in the H.264/AVC Video<br>
Compression Standard", Detlev Marpe, Heiko Schwarz and<br>
Thomas Wiegand, IEEE Transactions on Circuits and Systems<br>
35 for Video Technology, September 2003, comprise a detailed<br>
overview over different aspects of the video coding<br>
standard.<br><br>
- 2 -<br>
However, for a better understanding, an overview over the<br>
video coding/decoding algorithm will be given with<br>
reference to Figs. 9 to 11.<br>
5 Fig. 9 shows a full structure of a video coder, which<br>
generally consists of two different stages. Generally, the<br>
first stage, which generally operates video-related,<br>
generates output data, which are then subject to an entropy<br>
coding by a second stage, which is designated by 80 in Fig.<br>
10 9. The data are data 81a, quantized transformation<br>
coefficients 81b as well as motion data 81c, wherein these<br>
data 81a, 81b, 81c are supplied to the entropy coder 80 to<br>
generate a coded video signal at the output of the entropy<br>
coder 80.<br>
15<br>
Specifically, the input video signal is partitioned and<br>
splitted, respectively, into macroblocks, wherein every<br>
macroblock has 16 x 16 pixels. Then, the association of the<br>
macroblocks to slice groups and slices is chosen, according<br>
20 to which every macroblock of every slice is processed by<br>
the net of operation blocks as illustrated in Fig. 8. It<br>
should be noted that an efficient parallel-processing of<br>
macroblocks is possible when different slices exist in a<br>
video picture.  The association of macroblocks to slice<br>
25 groups and slices is performed via a block coder control 82<br>
in Fig. 8. There are different slices, which are defined as<br>
follows:<br>
I slice: The I slice is a slice wherein all macroblocks of<br>
30  the slice are coded by using an intra prediction.<br>
P slice: Additionally to the coding types of the I slices,<br>
certain macroblocks of the P slice can also be coded by<br>
using  an  inter prediction with  at  least  one motion-<br>
35  compensated prediction signal per prediction block.<br>
B slice: Additionally to the coder types available in the P<br>
slice, certain macroblocks of the B slice can also be coded<br><br>
- 3 -<br>
by using an inter prediction with two motion-compensated<br>
prediction signals per prediction block.<br>
The above three coder types are very similar to the ones in<br>
5 earlier standards, but with the exception of using<br>
reference pictures, as will be described below. The<br>
following two coder types for slices are new in the<br>
standard H.264/AVC:<br>
10 SP slice: It is also referred to as switch P slice, which<br>
is coded such that efficient switching between different<br>
precoded pictures is made possible.<br>
SI slice: The SI slice is also referred to as switch I<br>
15  slice, which allows an exact adaptation of the macroblocks<br>
in a SP slice for a direct random access and for error<br>
recovery purposes.<br>
All in all, slices are a sequence of macroblocks, which are<br>
20 processed in the order of a raster scan, if not a property<br>
of the flexible macroblock ordering FMO is used, which is<br>
also defined in the standard. A picture can be partitioned<br>
into one or several slices, as illustrated in Fig. 11.<br>
Thus, a picture is a collection of one or several slices.<br>
25 In that sense, slices are independent of one another, since<br>
their syntax elements can be analyzed (parsed) from the bit<br>
stream, wherein the values of the samples can be decoded<br>
correctly in the range of the picture represented by the<br>
slice, without requiring data from other slices, provided<br>
30 that used reference pictures are identical both in the<br>
coder and in the decoder. However, certain information from<br>
other slices can be required to apply the deblocking filter<br>
across slice borders.<br>
35 The FMO characteristic modifies the way how pictures are<br>
partitioned into slices and macroblocks, by using the<br>
concept of slice groups. Every slice group is a set of<br>
macroblocks defined by a macroblock to slice group mapping,<br><br>
- 4 -<br>
which is specified by the content of a picture parameter<br>
set and by certain information from slice headers. This<br>
macroblock to slice group mapping consists of a slice group<br>
identification number for every macroblock in the picture,<br>
5 wherein it is specified to which slice group the associated<br>
macroblock belongs. Every slice group can be partitioned<br>
into one or several slices, so that a slice is a sequence<br>
of macroblocks within the same slice group, which is<br>
processed in the order of a raster sampling within the set<br>
10  of macroblocks of specific slice group.<br>
Every macroblock can be transmitted in one or several coder<br>
types, depending on the slice coder type. In all slice<br>
coder types, the following types of intra coding are<br>
15 supported, which are referred to as intra-4*4 or intra-16*16<br>
wherein additionally a chroma prediction mode and an I-PCM<br>
prediction mode are supported.<br>
The intra-4x4 mode is based on the prediction of every 4x4<br>
20 chroma block separately and is very well suited for coding<br>
parts of a picture with outstanding details. The intra_16x16<br>
mode, on the other hand, performs a prediction of the whole<br>
16x16 chroma block and is more suited for coding "soft"<br>
regions of a picture.<br>
25<br>
Additionally to these two chroma prediction types, a<br>
separate chroma prediction is performed. As an alternative<br>
for intra.4x4 and intra-16x16 the I-4X4 coder type allows that<br>
the coder simply skips the prediction as well as the<br>
30 transformation coding and instead transmits the values of<br>
the coded samples directly. The I-PCM mode has the following<br>
purposes: It allows the coder to represent the values of<br>
the samples precisely. It provides a way to represent the<br>
values of very abnormal picture content exactly without<br>
35 data enlargement. Further, it allows to determine a hard<br>
boundary for the number of bits, which a coder needs to<br>
have for macroblock handling without loss of coding<br>
efficiency.<br><br>
- 5 -<br>
In contrary to earlier video coding standards (namely H.263<br>
plus and MPEG-4 visual), where the intra prediction has<br>
been performed in the transformation domain, the intra<br>
5 prediction in H.264/AVC is always performed in the spatial<br>
domain, by referring to adjacent samples of previously<br>
coded blocks, which are on the left of and above,<br>
respectively, the block to be predicted (Fig. 10) . In<br>
certain environments, where transmission errors occur, this<br>
10 can cause an error propagation, wherein this error<br>
propagation takes place due to the motion compensation in<br>
intra coded macroblocks. Thus, a limited intra coding mode<br>
can be signaled, which enables a prediction of only intra<br>
coded adjacent macroblocks.<br>
15<br>
When the intra-4x4 mode is used, every 4x4 block of<br>
spatially adjacent samples is predicted. The 16 samples of<br>
the 4x4 block are predicted by using previously decoded<br>
samples in adjacent blocks. One of 9 prediction modes can<br>
20 be used for every 4x4 block. Additionally to the "DC<br>
prediction" (where a value is used to predict the whole 4x4<br>
block) , 8 direction prediction modes are specified. These<br>
modes are suitable to predict direction structures in a<br>
picture, such as edges in different angles.<br>
25<br>
Additionally to the intra macroblock coder types, different<br>
predictive or motion-compensated coder types are specified<br>
as P macroblock types. Every P macroblock type corresponds<br>
to a specific partition of the macroblock into the block<br>
30 forms, which are used for a motion-compensated prediction.<br>
Partitions with luma block sizes of 16x16, 16x8, 8x8 or<br>
8x16 samples are supported by the syntax. In the case of<br>
partitions of 8x8 samples, an additional syntax element is<br>
transmitted for every 8x8 partition. This syntax element<br>
35 specifies whether the respective 8x8 partition is further<br>
partitioned into partitions of 8x4, 4x8 or 4x4 luma samples<br>
and corresponding chroma samples.<br><br>
- 6 -<br>
The prediction signal for every prediction-coded MxM luma<br>
block is obtained by shifting a region of the respective<br>
reference picture specified by a translation motion vector<br>
and a picture reference index. Thus, if the macroblock is<br>
5 coded by using four 8x8 partitions, and when every 8x8<br>
partition is further partitioned into four 4x4 partitions,<br>
a maximum amount of 16 motion vectors for a single P<br>
macroblock can be transmitted within the so-called motion<br>
field.<br>
10<br>
The quantization parameter slice QP is used to determine<br>
the quantization of the transformation coefficients in<br>
H.264/AVC. The parameter can assume 52 values. These values<br>
are disposed such that an increase of 1 with regard to the<br>
15 quantization parameter means an increase of the<br>
quantization step width by about 12 %. This means that an<br>
increase of the quantization parameter by 6 causes an<br>
increase of the quantizer step width by exactly a factor of<br>
2. It should be noted that a change of the step size by<br>
20 about 12 % also means a reduction of the bit rate by about<br>
12 %.<br>
The quantized transformation coefficients of a block are<br>
generally sampled in zigzag path and processed by using<br>
25 entropy coding methods. The 2x2 DC coefficients of the<br>
chroma component are sampled in raster scan sequence and<br>
all inverse transformation operations within H.264/AVC can<br>
be implemented by using only additions and shift operations<br>
of 16 bit integer values.<br>
30<br>
With reference to Fig. 9, the input signal is first<br>
partitioned picture by picture in a video sequence, for<br>
every picture, into the macroblocks with 16x16 pixels.<br>
Then, every picture is supplied to a subtractor 84, which<br>
35 subtracts the original picture, which is supplied by a<br>
decoder 85, which is contained in the coder. The<br>
subtraction result, which means the residual signals in the<br>
spatial domain, are now transformed, scaled and quantized<br><br>
- 7 -<br>
(block 86) to obtain the quantized transformation<br>
coefficients on line 81b. For generating the subtraction<br>
signal, which is fed into the subtractor 874, the quantized<br>
transformation coefficients are first again scaled and<br>
5 inverse transformed (block 87), to be supplied to an adder<br>
88, the output of which feeds the deblocking filter 89,<br>
wherein the output video signal, as, for example, will be<br>
decoded by a decoder, can be monitored at the output of the<br>
deblocking filter, for example for control purposes (output<br>
10  90) .<br>
By using the decoded output signal at output 90, a motion<br>
estimation is performed in block 91. For motion estimation<br>
in block 90, a picture of the original video signal is<br>
15 supplied, as seen from Fig. 9. The standard allows two<br>
different motion estimations, namely a forward motion<br>
estimation and a backward motion estimation. In the forward<br>
motion estimation, the motion of the current picture is<br>
estimated with regard to the previous picture.  In the<br>
20 backward motion estimation, however, the motion of the<br>
current picture is estimated by using the future picture.<br>
The results of the motion estimation (block 91) are<br>
supplied to a motion compensation block 92, which performs<br>
a motion-compensated inter prediction, particularly when a<br>
25 switch 93 is switched to the inter prediction mode, as it<br>
is the case in Fig. 9. If, however, the switch 93 is<br>
switched to intra frame prediction, an intra frame<br>
prediction is performed by using a block 490. Therefore,<br>
the  motion  data  are  not  required,  since  no  motion<br>
30  compensation is performed for an intra frame prediction.<br>
The motion estimation block 91 generates motion data and<br>
motion fields, respectively, wherein motion data and motion<br>
fields, respectively, which consist of motion vectors, are<br>
35 transmitted to the decoder so that a corresponding inverse<br>
prediction, which means reconstruction by using the<br>
transformation coefficients and the motion data, can be<br>
performed. It should be noted that in the case of a forward<br><br>
- 8 -<br>
prediction, the motion vector can be calculated from the<br>
immediately previous picture and from several previous<br>
pictures, respectively. Above that, it should be noted that<br>
in the case of a backward prediction, a current picture can<br>
5 be calculated by using the immediately adjacent future<br>
picture and of course also by using further future<br>
pictures.<br>
It  is  a  disadvantage  of  the  video  coding  concept<br>
10 illustrated in Fig. 9 that it provides no simple<br>
scalability possibility. As known in the art, the term<br>
"scalability" means a coder/decoder concept where the coder<br>
provides a scaled data stream. The scaled data stream<br>
comprises a base scaling layer as well as one or several<br>
15 enhancement scaling layers. The base scaling layer<br>
comprises a representation of the signal to be coded,<br>
generally with lower quality, but also with lower data<br>
rate. The enhancement scaling layer contains a further<br>
representation of  the video  signal,  which provides  a<br>
20 representation with improved quality with regard to the<br>
base scaling layer, typically together with the<br>
representation of the video signal in the base scaling<br>
layer. On the other hand, the enhancement scaling layer<br>
has, of course, individual bit requirements, so that the<br>
25 number of bits for representing the signal to be coded<br>
increases with every enhancement layer.<br>
Depending on design and possibilities, a decoder will<br>
decode,  either only the base scaling layer to provide<br>
30 comparatively qualitatively bad representation of the<br>
picture signal represented by the coded signal. With every<br>
"addition" of a further scaling layer, however, the decoder<br>
can improve the quality of the signal step by step (at the<br>
expense of the bit rate).<br>
35<br>
Depending on the implementation and the transmission<br>
channel from a coder to a decoder, at least the base<br>
scaling layer is transmitted, since the bit rate of the<br><br>
- 9 -<br>
base scaling layer is typically so low that also a so far<br>
limited transmission channel will be sufficient. If the<br>
transmission channel allows no more bandwidth for the<br>
application, only the base scaling layer but no enhancement<br>
5 scaling layer will be transmitted. As a consequence, the<br>
decoder can generate merely a low quality representation of<br>
the picture signal. Compared to the unsealed case, where<br>
the data rate would have been so high that a transmission<br>
system would not have been possible,  the low quality<br>
10 representation is advantageous. If the transmission channel<br>
allows the transmission of one or several enhancement<br>
layers, the coder will transmit one or several enhancement<br>
layers to the decoder, so that it can increase the quality<br>
of the output video signal step by step, depending on the<br>
15  request.<br>
With regard to the coding of video sequences, two different<br>
scalings can be distinguished. One scaling is a temporal<br>
scaling, in so far that not all video frames of a video<br>
20 sequence are transmitted, but that for reducing the data<br>
rate, for example, only every second frame, every third<br>
frame, every fourth frame, etc. is transmitted.<br>
The other scaling is the SNR scalability (SNR = signal to<br>
25  noise ratio), wherein every scaling layer, e.g. both the<br>
base  scaling  layer  and  the  first,  second,  third,<br>
enhancement scaling layer comprise all time information,<br>
but with varying quality. Thus, the base scaling layer<br>
would have a low data rate, but a low signal noise ratio,<br>
30  wherein this signal noise ratio can then be improved step<br>
by step by adding one enhancement scaling layer each.<br>
The coder concept illustrated in Fig. 9 is problematic in<br>
that it is based on the fact that merely residual values<br>
35 are generated by the subtracter 84, and are then processed.<br>
These residual values are calculated based on prediction<br>
algorithms, in the arrangement shown in Fig. 9, which forms<br>
a closed loop by using the blocks 86, 87, 88, 89, 93, 94<br><br>
- 10 -<br>
and 84, wherein a quantization parameter enters the closed<br>
loop, which means in blocks 86, 87. If now a simple SNR<br>
scalability would be implemented in that for example every<br>
predicted residual signal is quantized first with a coarse<br>
5 quantizer step width, and then quantized step by step with<br>
finer quantizer step widths, by using enhancement layers,<br>
this would have the following consequences. Due to the<br>
inverse quantization and the prediction, particularly with<br>
regard to the motion estimation (block 91) and the motion<br>
10 compensation (block 92), which take place by using the<br>
original picture on the one hand and the quantized picture<br>
on the other hand, a "diverging" of the quantizer step<br>
widths results both in the coder and the decoder. This<br>
leads to the fact that the generation of the enhancement<br>
15 scaling layers on the coder side becomes very problematic.<br>
Further, processing the enhancement scaling layers on the<br>
decoder side becomes impossible, at least with regard to<br>
the elements defined in the standard H.264/AVC. The reason<br>
therefore is the closed loop in the video coder illustrated<br>
20 with regard to Fig. 9, wherein the quantization is<br>
contained.<br>
In the standardization document JVT-I 032 tl titled "SNR-<br>
Scalable Extension of H.264/AVC",  Heiko Schwarz,  Detlev<br>
25 Marpe and Thomas Wiegand, presented in the ninth JVT<br>
meeting from 2nd to 5th December 2003 in San Diego, a<br>
scalable extension to H.264/AVC is presented, which<br>
comprises a scalability both with regard to time and signal<br>
noise ratio (with equal or different temporal accuracy).<br>
30 Therefore, a lifting representation of time subband<br>
partitions is introduced, which allows the usage of known<br>
methods for motion-compensated prediction.<br>
Wavelet based video coder algorithms, wherein lifting<br>
35 implementations are used for the wavelet analysis and for<br>
wavelet synthesis, are described in J.-R. Ohm, ,,Complexity<br>
and delay analysis of MCTF interframe wavelet structures",<br>
ISO/IECJTC1/WG11  Doc.M8520,  July  2002.  Comments  on<br><br>
- 11 -<br>
scalability can also be found in D. Taubman, Successive<br>
refinement of video: fundamental issues, past efforts and<br>
new directions", Proc. of SPIE (VCIP'03), vol. 5150, pp.<br>
649-663, 2003, wherein, however, significant coder<br>
5 structure alterations are required. According to the<br>
invention, a coder/decoder concept is achieved, which has,<br>
on the one hand, the scalability possibility and can, on<br>
the other hand, be based on elements in conformity with the<br>
standard, particularly, e.g., for the motion compensation.<br>
10<br>
Before reference will be made in more detail to a<br>
coder/decoder structure with regard to Fig. 3, first, a<br>
basic lifting scheme on the side of the coder and an<br>
inverse  lifting  scheme  on  the  side  of  the  decoder,<br>
15 respectively, will be illustrated with regard to Fig. 4.<br>
Detailed explanations about the background of the<br>
combination of lifting schemes and wavelet transformations<br>
can be found in W. Sweldens, ,,A custom design construction<br>
of biorthogonal wavelets", J. Appl. Comp. Harm. Anal., vol.<br>
20 3 (no. 2), pp. 186-200, 1996 and I. Daubechies and W.<br>
Sweldens, ,,Factoring wavelet transforms into lifting<br>
Steps", J. Fourier Anal. Appl., vol. 4 (no.3), pp. 247-269,<br>
1998. Generally, the lifting scheme consists of three<br>
steps,  the polyphase decomposition step, the prediction<br>
25  step and the update step.<br>
The decomposition step comprises partitioning the input<br>
side data stream into an identical first copy for a lower<br>
branch 40a as well as an identical copy for an upper branch<br>
30 40b. Further, the identical copy of the upper branch 40b is<br>
delayed by a time stage (z-1) , so that a sample S2k+1 with an<br>
odd index k passes through a respective decimator and<br>
downsampler 42a, 42b, respectively, at the same as a sample<br>
with  an  even  index  S2k-  The  decimator  42a  and  42b,<br>
35 respectively, reduces the number of samples in the upper<br>
and the lower branch 40b, 40a, respectively, by eliminating<br>
every second sample.<br><br>
- 12 -<br>
The second region II, which relates to the prediction step,<br>
comprises a prediction operator 43 as well as a subtracter<br>
44. The third region, which means the update step,<br>
comprises an update operator 45 as well as an adder 46. On<br>
5 the output side, two normalizers 47, 48 exist, for<br>
normalizing the high-pass signal hk (normalizer 47) and for<br>
normalizing the low-pass signal lk through the normalizer<br>
48.<br>
10 Particularly, the polyphase decomposition leads to the<br>
partitioning of even and odd samples of a given signal<br>
s[k]. Since the correlation structure typically shows a<br>
local characteristic, the even and odd polyphase components<br>
are highly correlated. Thus, in a final step, a prediction<br>
15 (P) of the odd samples is performed by using the integer<br>
samples. The corresponding prediction operator (P) for<br>
every odd sample sodd[k]=s[2k+1] is a linear combination of<br>
the adjacent even samples<br><br>
As a result of the prediction step, the odd samples are<br>
replaced by their respective prediction residual values<br><br>
It should be noted that the prediction step is equivalent<br>
to performing a high-pass filter of a two channel filter<br>
bank, as it is illustrated in I. Daubechies and W.<br>
30 Sweldens, ,,Factoring wavelet transforms into lifting<br>
steps", J. Fourier Anal. Appl. vol 4 (no.3), pp. 247-269,<br>
1998.<br>
In the third step of the lifting scheme, low-pass filtering<br>
35  is performed, by replacing the even samples seven[k] by a<br>
linear combination of prediction residual values h[k]. The<br>
respective update operator U is given by<br><br><br>
- 13 -<br><br>
By replacing the even samples with<br><br>
the given signal s[k] can finally be represented by l(k)<br>
and h(k), wherein every signal has half the sample rate.<br>
Since both the update step and the prediction step are<br>
10 fully invertible, the corresponding transformation can be<br>
interpreted as critically sampled perfect reconstruction<br>
filter bank. Indeed, it can be shown that any biorthogonal<br>
family of wavelet filters can be realized by a sequence of<br>
one or several prediction steps and one or several update<br>
15 steps. For a normalization of low-pass and high-pass<br>
components, the normalizers 47 and 48 are supplied with<br>
suitably chosen scaling factors F1 and Fh, as has been<br>
explained.<br>
20 The inverse lifting scheme, which corresponds to the<br>
synthesis filter bank, is shown in Fig. 4 on the right hand<br>
side. It consists simply of the application of the<br>
prediction and update operator in inverse order and with<br>
inverse signs, followed by the reconstruction by using the<br>
25 even and odd polyphase components. Specifically, the right<br>
decoder shown in Fig. 4 comprises again a first decoder<br>
region I, a second decoder region II as well as a third<br>
decoder region III. The first decoder region cancels the<br>
effect of the update operator 45. This is effected by<br>
30 supplying the high-pass signal, which has been re-<br>
normalized by a further normalizer 50, to the update<br>
operator 45. Then, the output signal of the decoder side<br>
update operator 45 is supplied to a subtracter 52, in<br>
contrary to the adder 4 6 in Fig. 4. Correspondingly, the<br>
35 output signal of the predictor 43 is processed, the output<br>
signal of which is now supplied to an adder 53 and not to a<br>
subtracter as on the coder side. Now, an upsampling of the<br>
signal by the factor 2 takes place in every branch (blocks<br><br>
- 14 -<br>
54a, 54b). Then, the upper branch is shifted by one sample<br>
into the future, which is equivalent to delaying the lower<br>
branch, to perform then an addition of the data streams on<br>
the upper branch and the lower branch in an adder 55, to<br>
5 obtain the reconstructed signal Sk at the output of the<br>
synthesis filter bank.<br>
Several wavelets can be implemented by the predictor 43 and<br>
the update-operator 45, respectively. If the so-called hair<br>
10  wavelet is to be implemented, the prediction operator and<br>
the update operator are given by the following equation:<br><br>
15  such that<br><br>
20  correspond to the non-normalized high-pass and low-pass<br>
(analysis) output signal, respectively, of the hair filter.<br>
In the case of the 5/3 biorthogonal spline wavelet, the<br>
low-pass and high-pass analysis filter of this wavelet have<br>
25  5  and  3  filter  taps,   respectively,   wherein  the<br>
corresponding scaling function is a second order B spline.<br>
In coder applications for still pictures, such as JPEG<br>
2000, this wavelet is used for a time subband coder scheme.<br>
In a lifting environment, the corresponding prediction and<br>
30  update operators of the 5/3 transformation are given as<br>
follows:<br><br>
35 Fig. 3 shows a block diagram of a coder/decoder structure<br>
with exemplary four filter levels both on the side of the<br>
coder and on the side of the decoder. From Fig. 3, it can<br>
be seen that the first filter level, the second filter<br><br>
- 15 -<br>
level, the third filter level and the fourth filter level<br>
are identical with regard to the coder. The filter levels<br>
with regard to the decoder are also identical. On the coder<br>
side, every filter level comprises a backward predictor Mi0<br>
5 as well as a forward predictor Mil 61 as central elements.<br>
The backward predictor 60 corresponds in principle to the<br>
predictor 43 of Fig. 4, while the forward predictor 61<br>
corresponds to the update operator of Fig. 4.<br>
10 In contrary to Fig. 4, it should be noted that Fig. 4<br>
relates to a stream of samples, where a sample has an odd<br>
index 2k+l, while another sample has an even index 2k.<br>
However, as has already been explained with regard to Fig.<br>
1, the notation in Fig. 3 relates to a group of pictures<br>
15 instead of to a group of samples. If a picture has for<br>
example a number of samples and pictures, respectively,<br>
this picture is fed in fully. Then, the next picture is fed<br>
in, etc. Thus, there are no longer odd and even samples,<br>
but odd and even pictures. According to the invention, the<br>
20 lifting scheme described for odd and even samples is<br>
applied to odd and even pictures, respectively, each of<br>
which has a plurality of samples. Now, the sample by sample<br>
predictor 43 of Fig. 4 becomes the backward motion<br>
compensation prediction 60, while the sample by sample<br>
25 update operator 45 becomes the picture by picture forward<br>
motion compensation prediction 61.<br>
It should be noted that the motion filters, which consist<br>
of motion vectors and represent coefficients for the block<br>
30 60 and 61, are calculated for two subsequent related<br>
pictures and are transmitted as side information from coder<br>
to decoder. However, it is a main advantage of the<br>
inventive concept that the elements 91, 92, as they are<br>
described with reference to Fig. 9 and standardized in<br>
35 standard H.264/AVC, can easily be used to calculate both<br>
the motion fields Mi0 and the motion fields Mil Thus, no<br>
new predictor/update operator has to be used for the<br>
inventive concept,  but the already existing algorithm<br><br>
- 16 -<br>
mentioned in the video standard, which is examined and<br>
checked for functionality and efficiency, can be used for<br>
the motion compensation in forward direction or backward<br>
direction.<br>
5<br>
Particularly, the general structure of the used filter bank<br>
illustrated in Fig. 3 shows a temporal decomposition of the<br>
video signal with a group of 16 pictures, which are fed in<br>
at an input 64. The decomposition is a dyadic temporal<br>
10 decomposition of the video signal, wherein in the<br>
embodiment shown in Fig. 3 with four levels 24=16 pictures,<br>
which means a group size of 16 pictures, is required to<br>
achieve the representation with the smallest temporal<br>
resolution, which means the signals at the output 28a and<br>
15 at the output 28b. Thus, if 16 pictures are grouped, this<br>
leads to a delay of 16 pictures, which makes the concept<br>
shown in Fig. 3 with four levels rather problematic for<br>
interactive applications. Thus, if interactive applications<br>
are aimed at, it is preferred to form smaller groups of<br>
20 pictures, such as to group four or eight pictures. Then,<br>
the delay is correspondingly reduced, so that the usage for<br>
interactive applications becomes possible. In cases where<br>
interactivity is not required, such as for storage<br>
purposes, etc., the number of pictures in a group, which<br>
25 means the group size, can be correspondingly increased,<br>
such as to 32, 64, etc. pictures.<br>
In that way, an interactive application of the hair-based<br>
motion-compensated lifting scheme is used, which consists<br>
30 of the backward motion compensation prediction (Mio) , as in<br>
H.264/AVC, and that further comprises an update step, which<br>
comprises a forward motion compensation (Mil) . Both the<br>
prediction step and the update step use the motion<br>
compensation process, as it is illustrated in H.264/AVC.<br>
35 Further, not only the motion compensation is used, but also<br>
the deblocking filter 89 designated with the reference<br>
number 8 9 in Fig. 9.<br><br>
- 17 -<br>
The second filter level comprises again downsampler 66a,<br>
66b, a subtracter 69, a backward predictor 67, a forward<br>
predictor 68 as well as an adder 70 and a further<br>
processing means to output the first and second high-pass<br>
5 picture of the second level at an output of the further<br>
processing means, while the first and second low-pass<br>
picture of the second level are output at the output of the<br>
adder 70.<br>
10 Additionally, the coder in Fig. 3 comprises a third level<br>
as well as a fourth level, wherein a group of 16 pictures<br>
is fed into the fourth-level input 64. At a fourth-level<br>
high-pass output 72, which is also referred to as HP4,<br>
eight high-pass pictures quantized with a quantization<br>
15 parameter Q and correspondingly processed are output.<br>
Correspondingly, eight low-pass pictures are output at a<br>
low-pass output 73 of the fourth filter level, which is fed<br>
into an input 74 of the third filter level. This level,<br>
again, is effective to generate four high-pass pictures at<br>
20 a high-pass output 75, which is also referred to as HP3,<br>
and to generate four low-pass pictures at a low-pass output<br>
76, which are fed into the input 10 of the second filter<br>
level and decomposed.<br>
25 It should particularly be noted that the group of pictures<br>
processed by a filter level does not necessarily have to be<br>
video pictures originating from an original video sequence,<br>
but can also be low-pass pictures, which are output by a<br>
next higher filter level at a low-pass output of the filter<br>
30  level.<br>
Further, it should be noted that the coder concept shown in<br>
Fig. 3 for 16 pictures can easily be reduced to eight<br>
pictures, when simply the fourth filter level is omitted<br>
35 and the group of pictures is fed into the input 74. In the<br>
same way, the concept shown in Fig. 3 can also be extended<br>
to a group of 32 pictures, by adding a fifth filter level<br>
and by outputting then 16 high-pass pictures at a high-pass<br><br>
- 18 -<br>
output of the fifth filter level and feeding the sixteen<br>
low-pass pictures at the output of the fifth filter level<br>
into the input 64 of the fourth filter level.<br>
5 The tree-like concept of the coder side is also applied to<br>
the decoder side, but now no longer, like on the coder<br>
side, from the high level to the lower level but, on the<br>
decoder side, from the lower level to the higher level.<br>
Therefore, the data stream is received from a transmission<br>
10 medium, which is schematically referred to as network<br>
abstraction layer 100, and the received bit stream is first<br>
subject to an inverse further processing by using the<br>
inverse further processing means, to obtain a reconstructed<br>
version of the first high-pass picture of the first level<br>
15 at the output of means 30a and a reconstructed version of<br>
the first-level low-pass picture at the output of block 30b<br>
of Fig. 3. Then, analogous to the right half of Fig. 4,<br>
first the forward motion compensation prediction is<br>
reversed via the predictor 61, to subtract then the output<br>
20 signal of the predictor 61 from the reconstructed version<br>
of the low-pass signal (subtracter 101).<br>
The output signal of the subtracter 101 is fed into a<br>
backward compensation predictor 60 to generate a prediction<br>
25 result, which is added to the reconstructed version of the<br>
high-pass picture in an adder 102. Then, both signals,<br>
which means the signals in the lower branch 103a, 103b, are<br>
brought to the double sample rate, by using the upsampler<br>
104a, 104b, wherein then the signal on the upper branch is<br>
30 either delayed or "accelerated", depending on the<br>
implementation. It should be noted that the upsampling is<br>
performed by the bridge 104a, 104b simply by inserting a<br>
number of zeros which corresponds to the number of samples<br>
for a picture. The shift by the delay of a picture by the<br>
35 element shown with z-1 in the upper branch 103b against the<br>
lower branch 103a effects that the addition by an adder 106<br>
causes that the two second-level low-pass pictures occur<br><br>
- 19 -<br>
subsequently on the output side with regard to the adder<br>
106.<br>
The reconstructed versions of the first and second second-<br>
5 level low-pass picture are then fed into the decoder-side<br>
inverse filter of the second level and there they are<br>
combined again with the transmitted second-level high-pass<br>
pictures by the identical implementation of the inverse<br>
filter bank to obtain a sequence of four third-level low-<br>
10 pass pictures at an output 101 of the second level. The<br>
four third-level low-pass pictures are then combined in an<br>
inverse filter level of the third level with the<br>
transmitted third-level high-pass pictures to obtain eight<br>
fourth-level low-pass pictures in subsequent format at an<br>
15 output 110 of the inverse third-level filter. These eight<br>
third-level low-pass pictures will then be combined again<br>
with the eight fourth-level high-pass pictures received<br>
from the transmission medium 100 via the input HP4, in an<br>
inverse fourth-level filter, as discussed with regard to<br>
20 the first level, to obtain a reconstructed group of 16<br>
pictures at an output 112 of the inverse fourth-level<br>
filter.<br>
Thus, in every stage of the analysis filter bank, two<br>
25 pictures, either original pictures or pictures representing<br>
low-pass signals and generated in a next higher level, are<br>
decomposed into a low-pass signal and a high-pass signal.<br>
The low-pass signal can be considered as representation of<br>
the common characteristics of the input pictures, while the<br>
30 high-pass signal can be considered as representation of the<br>
differences between the input pictures. In the<br>
corresponding stage of the synthesis filter bank, the two<br>
input pictures are again reconstructed by using the low-<br>
pass signal and the high-pass signal. Since the inverse<br>
35 operations of the analysis step are performed in the<br>
synthesis step, the analysis/synthesis filter bank (without<br>
quantization, of course) guarantees a perfect<br>
reconstruction.<br><br>
- 20 -<br>
The only occurring losses occur due to the quantization in<br>
the further processing means, such as 26a, 26b, 18. If<br>
quantization is performed very finely, a good signal noise<br>
5 ratio is achieved. If, however, quantization is performed<br>
very coarsely, a relatively bad signal noise ratio is<br>
achieved, but with a low bit rate, which means low demand.<br>
Without SNR scalability, a time scaling control could be<br>
10 implemented already with the concept shown in Fig. 3.<br>
Therefore, a time scaling control 120 is used, which is<br>
formed to obtain the high-pass and low-pass output,<br>
respectively, and the outputs of the further processing<br>
means (26a, 26b, 18 ...) , respectively, at the input side to<br>
15 generate a scaled data stream from these partial data<br>
streams TP1, HP1, HP2, HP3, HP4, which has the processed<br>
version of the first low-pass picture and the first high-<br>
pass picture in a base scaling layer. Then, the processed<br>
version  of  the  second  high-pass  picture  could  be<br>
20 accommodated in a first enhancement scaling layer. The<br>
processed versions of the third-level high-pass pictures<br>
could be accommodated in a second enhancement scaling<br>
layer, while the processed versions of the fourth-level<br>
high-pass pictures are introduced in a third enhancement<br>
25 scaling layer. Thereby, merely based on the base scaling<br>
layer, a decoder could already generate a sequence of<br>
lower-level low-pass pictures with a lower time quality,<br>
which means two first-level low-pass pictures per group of<br>
pictures. With the addition of every enhancement scaling<br>
30 layer, the number of reconstructed pictures per group can<br>
always be doubled. The functionality of the decoder is<br>
typically controlled by a scaling control, which is formed<br>
to detect how many scaling layers are contained in the data<br>
stream and how many scaling layers have to be considered by<br>
35  the decoder during decoding, respectively.<br>
The JVT document JVT-J 035 with the title "SNR-Scalable<br>
Extension of H.264/AVC" Heiko Schwarz, Detlev Marpe and<br><br>
- 21 -<br>
Thomas Wiegand, presented during the tenth JVT meeting in<br>
Waikoloa Hawaii, 8th to 12th December 2003, shows a SNR<br>
scalable extension of the temporal decomposition scheme<br>
illustrated in Figs. 3 and 4. Particularly, a time scaling<br>
5 layer is partitioned into individual "SNR scaling<br>
sublayers", wherein a SNR base layer is obtained in such<br>
that a certain time scaling layer is quantized with a first<br>
coarser quantizer step width to obtain the SNR base layer.<br>
Then,  among other  things,  an  inverse quantization  is<br>
10 performed, and the result signal from the inverse<br>
quantization is subtracted from the original signal to<br>
obtain a difference signal, which is then quantized with a<br>
finer quantizer step width to obtain the second scaling<br>
layer. However,  the second scaling layer is requantized<br>
15 with the finer quantizer step width to subtract the signal<br>
obtained after the requantization from the original signal<br>
to obtain a further difference signal, which, again after<br>
quantization, but now with a finer quantizer step width,<br>
represents  a  second  SNR  scaling  layer  and  an  SNR<br>
20  enhancement layer, respectively.<br>
Thus, it has been found out that the above described<br>
scalability schemes, which are based on the motion-<br>
compensated temporal filtering (MCTF), already provide a<br>
25 high flexibility with regard to the temporal scalability<br>
and also the SNR scalability. But there is still a problem<br>
in that the bit rate of several scaling layers together is<br>
still significantly above the bit rate, which can be<br>
achieved when pictures of the highest quality would be<br>
30 coded without scalability. Due to the side information for<br>
the different scaling layers, scalable coders might never<br>
obtain the bit rate of the unsealed case. However, the bit<br>
rate of a data stream with several scaling layers should<br>
approach the bit rate of the unsealed case as closely as<br>
35  possible.<br>
Further, the scalability concept should provide high<br>
flexibility for all scalability types, which means a high<br><br>
- 22 -<br>
flexibility both with regard to time and space and also<br>
with regard to SNR.<br>
The  high  flexibility  is  particularly  important  where<br>
5  already pictures with low resolution would be sufficient<br>
but a higher temporal resolution is desirable.  Such a<br>
situation results, for example, when fast changes exist in<br>
pictures, such as, for example, in videos of team sports,<br>
where additionally to the ball, many persons move at the<br>
10  same time.<br>
A further disadvantage of existing scalability concepts is<br>
that they either use the identical motion data for all<br>
scaling layers, which either limits the flexibility of the<br>
15 scalability or results in a non-optimum motion prediction<br>
and an increasing residual signal of the motion prediction,<br>
respectively.<br>
On the other hand,  a completely different motion data<br>
20 transmission of two different scaling layers leads to a<br>
significant overhead, since particularly when relatively<br>
low SNR scaling layers are considered, where quantization<br>
is performed relatively coarse, the portion of motion data<br>
in the overall bit stream becomes noticeable. A flexible<br>
25 scalability concept, wherein different motion data and<br>
different scaling layers become possible at all, is thus<br>
paid for by an additional bit rate, which is particularly<br>
disadvantageous with regard to the fact that all efforts<br>
are to reduce the bit rate. Further, the additional bits<br>
30 for the transmission of motion data stand out particularly<br>
in the lower scaling layers, compared to the bits for the<br>
motion prediction residual values. However, exactly there,<br>
this is particularly unpleasant, since in the lower scaling<br>
layers  the  effort  is made  to obtain  a  sufficiently<br>
35 acceptable quality which means to use at least a<br>
sufficiently reasonable quantization parameter and at the<br>
same time to obtain a lower bit rate.<br><br>
- 23 -<br>
It is the object of the present invention to provide a<br>
scalable video coder system concept, which provides a lower<br>
data rate and still shows flexibility.<br>
5 This object is achieved by an apparatus for generating a<br>
coded video sequence in accordance with claim 1, a method<br>
for generating a coded video sequence in accordance with<br>
claim 15, an apparatus for decoding a coded video sequence<br>
in accordance with claim 16, a method for decoding a coded<br>
10 video sequence in accordance with claim 21, a computer<br>
program in accordance with claim 22, or a computer-readable<br>
medium in accordance with claim 23.<br>
The present invention is based on the knowledge that<br>
15 further data rate savings with simultaneous flexibility<br>
with regard to different SNR or spatial scaling layers is<br>
obtained by using the base motion data in the calculation<br>
of enhancement motion data within an enhancement motion<br>
compensation for the enhancement scaling layer. Thus,<br>
20 according to the invention, in the calculation of the<br>
enhancement motion data, it is not pretended that there<br>
were no motion data of the base layer, but the motion data<br>
of the base layer are integrated into the calculation.<br>
25 Here, according to preferred embodiments of the present<br>
invention, an adaptive concept is used, i.e. that for<br>
different blocks of a picture different ways of considering<br>
the base motion data can be performed, and that obviously<br>
for one block an enhancement motion data prediction with<br>
30 the base motion data as predictor can be fully omitted when<br>
it is proved that the prediction provides no success in the<br>
data reduction. Whether an enhancement motion data<br>
prediction has been performed at all by using the base<br>
motion data and of what type it was, is transmitted in the<br>
35 bit stream with signalization information associated to a<br>
block and indicated to the decoder. Thereby, the decoder is<br>
able to resort to the base motion data already<br>
reconstructed in the decoder for the reconstruction of the<br><br>
- 24 -<br>
motion data for a block to the, wherein the fact that is<br>
has to resort at all and in what way it has to resort is<br>
signalized by signalization information in the bit stream<br>
transmitted block by block.<br>
5<br>
Depending on the implementation, the base motion data can<br>
be considered in the actual calculation of the enhancement<br>
motion data, as they will be used subsequently by the<br>
enhancement motion compensator. However, according to the<br>
10 invention, it is also preferred to calculate the<br>
enhancement motion data independently of the base motion<br>
data and to use the base motion data merely when<br>
postprocessing the enhancement motion data to obtain the<br>
enhancement motion data which are actually transmitted to<br>
15 the enhancement picture coder. Thus, according to the<br>
invention, in the sense of a high flexibility, an<br>
independent calculation of enhancement motion data is<br>
performed, wherein these are used independent of the<br>
enhancement motion data calculated from the base motion<br>
20 data for coder side motion prediction, while the base<br>
motion data are merely used for the purpose of calculating<br>
a residual signal of any type to reduce the required bits<br>
for transmitting the enhancement motion vectors.<br>
25 In a preferred embodiment of the present invention, the<br>
motion data intermediate layer prediction is supplemented<br>
by an intermediate layer residual value prediction, to<br>
utilize redundancies between the different scaling layers<br>
as best as possible also in residual values of the motion-<br>
30 compensated prediction and to consider them for data rate<br>
reduction purposes.<br>
In a preferred embodiment of the present invention, a bit<br>
rate reduction is not only obtained by a motion-compensated<br>
35 prediction performed within a scaling layer, but also with<br>
an intermediate scaling layer prediction of the residual<br>
pictures after the motion-compensated prediction of a lower<br><br>
- 25 -<br>
layer, for example the base layer, to a higher layer, such<br>
as the enhancement layer.<br>
It has been found out that within the same temporal scaling<br>
5 layer, the residual values of the individual considered<br>
other scaling layers, which are scaled preferably with<br>
regard to the resolution or with regard to the signal noise<br>
ratio (SNR), also have correlations between the residual<br>
values after the motion-compensated prediction. According<br>
10 to the invention, these correlations are advantageously<br>
utilized in that an intermediate layer predictor is<br>
provided on the coder side for the enhancement scaling<br>
layer, which corresponds to an intermediate layer combiner<br>
on the decoder side. Preferably, this intermediate layer<br>
15 predictor is designed adaptively, in order to decide, e.g.,<br>
for every macroblock, whether an intermediate layer<br>
prediction is worth the effort, or whether the prediction<br>
would rather lead to a bit rate increase. The latter is the<br>
case when the prediction residual signal becomes larger<br>
20 than the original motion compensation residual signal of<br>
the enhancement layer with regard to a subsequent entropy<br>
coder. However, the situation will not occur in many cases,<br>
so that the intermediate layer predictor is activated and<br>
leads to a significant bit rate reduction.<br>
25<br>
Preferred embodiments of the present invention will be<br>
explained in the following with reference to the<br>
accompanying drawings, in which:<br>
30  Fig. la is a preferred embodiment of an inventive coder;<br>
Fig. lb is a detailed representation of a base picture<br>
coder of Fig. la;<br>
35  Fig. lc is  a  discussion  of  the  functionality  of  an<br>
intermediate layer prediction flag;<br>
Fig. Id is a description of a motion data flag;<br><br>
(<br>
- 26 -<br>
Fig. le is a preferred implementation of the enhancement<br>
motion compensator 1014 of Fig. la;<br>
5  Fig. If is a preferred implementation of the enhancement<br>
motion data determination means 1078 of Fig. 2;<br>
Fig. lg is an overview representation of three preferred<br>
embodiments for calculating the enhancement motion<br>
10                         data and for enhancement motion data processing<br>
for the purpose of signalization and residual data<br>
transmission, if necessary;<br>
Fig. 2  is a preferred embodiment of an inventive decoder;<br>
15<br>
Fig. 3  is a block diagram of a decoder with four levels;<br>
Fig. 4   is a block diagram for illustrating the lifting<br>
decomposition of a time subband filter bank;<br>
20<br>
Fig. 5a is a representation of the functionality of the<br>
lifting scheme shown in Fig. 4;<br>
Fig. 5b is  a  representation  of  two preferred  lifting<br>
25                         specifications  with  unidirectional  prediction<br>
(hair wavelet) and bidirectional prediction (5/3<br>
transformation);<br>
Fig. 5c is a preferred embodiment of the prediction and<br>
30                         update  operators  with motion compensation  and<br>
reference indices for an arbitrary choice of the<br>
two pictures to be processed by the lifting<br>
scheme;<br>
35 Fig. 5d is a representation of the intra mode where<br>
original picture information can be inserted<br>
macroblock by macroblock into high-pass pictures;<br><br>
- 27 -<br>
Fig. 6a is a schematic representation for signalizing a<br>
macroblock mode;<br>
Fig. 6b is a schematic representation for upsampling of<br>
5                         motion data in a spatial scalability according to<br>
a preferred embodiment of the present invention;<br>
Fig. 6c is a schematic representation of the data stream<br>
syntax for motion vector differences;<br>
10<br>
Fig. 6d is a schematic representation of a residual value<br>
syntax enhancement according to a preferred<br>
embodiment of the present invention;<br>
15 Fig. 7 is an overview diagram for illustrating the time<br>
shift of a group of, for example, 8 pictures;<br>
Fig. 8  is a preferred time placement of low-pass pictures<br>
for a group of 16 pictures;<br>
20<br>
Fig. 9 is an overview block diagram for illustrating the<br>
basic coder structure for a coder according to the<br>
standard H.264/AVC for a macroblock;<br>
25 Fig. 10 is a context arrangement consisting of two<br>
adjacent pixel elements A and B on the left and<br>
above a current syntax element C, respectively,<br>
and<br>
30 Fig. 11 is a representation of the partition of a picture<br>
into slices.<br>
Fig. la shows a preferred embodiment of an apparatus for<br>
generating a coded video sequence, which has a base scaling<br>
35 layer and an enhancement scaling layer. An original video<br>
sequence with a group of 8, 16 or any number of pictures is<br>
fed in via an input 1000. On the output side, the coded<br>
video sequence contains the base scaling layer 1002 and the<br><br>
- 28 -<br>
enhancement scaling layer 1004. The enhancement scaling<br>
layer 1004 and the base scaling layer 1002 can be supplied<br>
to a bit stream multiplexer, which generates a single<br>
scalable bit stream on the output side. Depending on the<br>
5 implementation, however, a separate transmission of the two<br>
scaling layers is also possible and useful in some cases.<br>
Fig. la shows a coder for generating two scaling layers,<br>
which means the base scaling layer and an enhancement<br>
scaling layer.  In order to obtain a coder,  which,  if<br>
10 necessary, generates one or several further enhancement<br>
layers, the functionality of the enhancement scaling layer<br>
is to be repeated, wherein a higher enhancement scaling<br>
layer is always supplied with data by the next lower<br>
enhancement scaling layer, as the enhancement scaling layer<br>
15 1004 shown in Fig. 1 is supplied with data by the base<br>
scaling layer 1002.<br>
Before reference will be made to different scaling types in<br>
detail, such as a SNR scalability or a spatial scalability<br>
20 or a combined scalability of spatial and SNR scalability,<br>
first, the basic principle of the present invention will be<br>
illustrated. First, the coder comprises a base motion<br>
compensator or base motion estimator 1006 for calculating<br>
base motion data, which indicates how a macroblock has<br>
25 moved in a current picture in relation to another picture<br>
in a group of pictures, which the base motioned compensator<br>
1006 obtains on the input side. Techniques for calculating<br>
motion data, particularly for calculating a motion vector<br>
for a macroblock, which is basically a region of pixels in<br>
30 a digital video picture, are known. Preferably, the motion<br>
compensation calculation is used, as it is standardized in<br>
the video coding standard H.264/AVC. Thereby, a macroblock<br>
of a later picture is considered and it is determined, how<br>
the macroblock "moved" in comparison to an earlier picture.<br>
35 This motion (in xy direction) is indicated by a two-<br>
dimensional motion vector, which is calculated by block<br>
1006 for every macroblock and supplied to a base picture<br>
coder 1010 via a motion data line 1008.  Then,  it is<br><br>
- 29 -<br>
calculated for the next picture, how a macroblock has moved<br>
from the previous picture to the next picture.<br>
In one implementation, this new motion vector, which, in a<br>
5 way, indicates the motion from second to a third picture,<br>
can be transmitted again as two-dimensional vector. For<br>
efficiency reasons, however, it is preferred to transmit<br>
only a motion vector difference, which means the difference<br>
of the motion vector of a macroblock from the second to the<br>
10 third picture and the motion vector of the macroblock from<br>
the first to the second picture. Alternative referencings<br>
and motion vector differences, respectively, to not<br>
immediately previous pictures, but to further preceding<br>
pictures can also be used.<br>
15<br>
The motion data, which have been calculated by block 1006,<br>
will then be supplied to a base motion predictor 1012,<br>
which is designed to calculate a base sequence of residual<br>
error pictures for using the motion data and the group of<br>
20 pictures. Thus, the base motion predictor performs the<br>
motion compensation, which has, in a way, been prepared by<br>
the motion compensator and motion estimator, respectively.<br>
This base sequence of residual error pictures will then be<br>
supplied to the base picture coder. The base picture coder<br>
25 is formed to provide the base scaling layer 1002 at its<br>
output.<br>
Further, the inventive coder comprises an enhancement<br>
motion compensator or enhancement motion estimator 1014 for<br>
30 detecting enhancement motion data. These enhancement motion<br>
data are supplied to an enhancement motion predictor 1016,<br>
which generates an enhancement sequence of residual error<br>
pictures on the output side and supplies them to a<br>
downstream intermediate layer predictor 1018. Thus, the<br>
35 enhancement motion predictor performs the motion<br>
compensation, which, in a way, has been prepared by the<br>
motion compensator and motion estimator, respectively.<br><br>
- 30 -<br>
The intermediate layer predictor is formed to calculate<br>
enhancement prediction residual error pictures on the<br>
output side. Depending on the implementation, the<br>
intermediate layer predictor uses additionally to the data,<br>
5 which it obtains from block 1016, which means additionally<br>
to the enhancement sequence of residual error pictures, the<br>
base sequence of residual error pictures, as it is provided<br>
by block 1012 via a dotted bypass line 1020. Alternatively,<br>
the block 1018 can also use an interpolated sequence of<br>
10 residual error pictures, which is provided at the output of<br>
block 1012 and interpolated by an interpolator 1022. Again<br>
alternatively, the intermediate layer predictor can also<br>
provide a reconstructed base sequence of residual error<br>
pictures, as it is provided to an output 1024 of the base<br>
15 picture coder 1010. As can be seen from Fig., la, this<br>
reconstructed base sequence of residual error pictures can<br>
be interpolated 1022 or not interpolated 1020. Thus,<br>
generally, the intermediate layer predictor operates by<br>
using the base sequence of residual error pictures, wherein<br>
20 the information at the intermediate layer predictor input<br>
1026 is derived, e.g. by a reconstruction or interpolation<br>
of the base sequence of residual error pictures at the<br>
output of block 1012.<br>
25 Downstream to the intermediate layer predictor 1018, there<br>
is an enhancement picture coder 1028, which is formed to<br>
code the enhancement prediction residual error pictures to<br>
obtain the coded enhancement scaling layer 1004.<br>
30 In a preferred embodiment of the present invention, the<br>
intermediate layer predictor is formed to subtract the<br>
signal at its output 1026 macroblock by macroblock and<br>
picture by picture from the respective signal, which the<br>
intermediate  layer  predictor  1018  obtains  from  the<br>
35 enhancement motion predictor 1016. The result signal<br>
obtained in this subtraction represents then a macroblock<br>
of a picture of the enhancement prediction residual error<br>
pictures.<br><br>
- 31 -<br>
In a preferred embodiment of the present invention, the<br>
intermediate layer predictor is formed adaptively. For<br>
every macroblock, an intermediate layer prediction flag<br>
5 1030 is provided, which indicates the intermediate layer<br>
predictor that it has to perform a prediction, or which<br>
indicates in its other state that no prediction is to be<br>
performed, but that the corresponding macroblock at the<br>
output of the enhancement motion predictor 1016 is to be<br>
10 supplied to the enhancement picture coder 1028 without<br>
further prediction. This adaptive implementation has the<br>
advantage that an intermediate layer prediction is only<br>
performed where it is useful, where the prediction residual<br>
signal leads to a lower output picture rate compared to the<br>
15 case where no intermediate layer prediction has been<br>
performed, but where the output data of the enhancement<br>
motion predictor 1016 have been coded directly.<br>
In the case of a spatial scalability, a decimator 1032 is<br>
20 provided between the enhancement scaling layer and the base<br>
scaling layer, which is formed to convert the video<br>
sequence at its input, which has a certain spatial<br>
resolution, to a video sequence at its output, which has a<br>
lower resolution. If a pure SNR scalability is intended,<br>
25 which means if the base picture coder 1010 and 1028 for the<br>
two scaling layers operate with different quantization<br>
parameters 1034 and 1036, respectively, the decimator 1032<br>
is not provided. This is illustrated schematically in Fig.<br>
la by the bypass line 1038.<br>
30<br>
Further, in the case of spatial scalability, the<br>
interpolator 1022 has to be provided. In the case of a pure<br>
SNR scalability, the interpolator 1022 is not provided.<br>
Instead, the bypass line 1020 is taken, as illustrated in<br>
35  Fig. la.<br>
In one implementation, the enhancement motion compensator<br>
1014 is formed to fully calculate an individual motion<br><br>
- 32 -<br>
field, or to use the motion field calculated by the base<br>
motion compensator 1006 directly (bypass line 1040) or<br>
after upsampling by an upsampler 1042. In the case of a<br>
spatial scalability, the upsampler 1042 has to be provided<br>
5 to upsample a motion vector of the base motion data to the<br>
higher resolution, which means, for example, to scale. If,<br>
for example, the enhancement resolution is twice as high<br>
and wide as the base resolution, a macroblock (16x16<br>
luminance samples) in the enhancement layer covers a region<br>
10 of a picture, which corresponds to a sub-macroblock (8x8<br>
luminance samples) in the base layer.<br>
Thus, in order to be able to use the base motion vector for<br>
the macroblock of the enhancement scaling layer, the base<br>
15 motion vector is doubled in its x component and its y<br>
component, which means scaled by the factor 2. This will be<br>
discussed in more detail with reference to Fig. 6b.<br>
If, however, there is merely an SNR scalability, the motion<br>
20 field is the same for all scaling layers. Thus, it has to<br>
be calculated only once and can be directly used by every<br>
higher scaling layer in the way it has been calculated by<br>
the lower scaling layer.<br>
25 For intermediate layer prediction, the signal at the output<br>
of the base motion predictor 1012 can also be used.<br>
Alternatively, the reconstructed signal on line 1024 can be<br>
used. The selection, which of these two signals is used for<br>
prediction, is made by a switch 1044. The signal on line<br>
30 1024 differs from the signal at the output of block 1012 by<br>
the fact that it has already experienced a quantization.<br>
This means that the signal on line 1024 has a quantization<br>
error in comparison to the signal at the output of block<br>
1012. The alternative of using the signal on line 1024 for<br>
35 intermediate layer prediction is particularly advantageous<br>
when an SNR scalability is either used alone or in<br>
connection with a spatial scalability, since then the<br>
quantization error made by the base picture coder 1010 is<br><br>
- 33 -<br>
then "taken along" to the higher scaling layer, since the<br>
output signal at block 1018 will then contain the<br>
quantization error made by the first scaling layer, which<br>
will then be quantized at the input 1036 by the enhancement<br>
5 picture coder with a typically finer quantizer step width<br>
and a changed quantization parameter 2, respectively, and<br>
will be written into the enhancement scaling layer 1004.<br>
Analogous to the intermediate layer prediction flag 1030, a<br>
10  motion data flag 1048 is fed into the picture coder, so<br>
that a corresponding information about that is contained in<br>
the enhancement scaling layer 1004, which will then be used<br>
by the decoder, which will be discussed with reference to<br>
Fig. 2.<br>
15<br>
If a pure spatial scalability is used, the output signal of<br>
the base motion predictor 1012, which means the base<br>
sequence of residual error pictures, can be used instead of<br>
the signal on line 1024, which means instead of the<br>
20  reconstructed sequence of base residual error pictures.<br>
Depending on the implementation, the control of this switch<br>
can take place manually or based on a prediction benefit<br>
function.<br>
25<br>
Here, it should be noted that preferably all predictions,<br>
which means the motion prediction, the enhancement motion<br>
data prediction and the intermediate layer residual value<br>
prediction are designed adaptively. This means that motion<br>
30 data prediction residual values do not necessarily have to<br>
be present for every macroblock or sub-macroblock in a<br>
picture of the base sequence of residual error pictures,<br>
for example. Thus, a picture of the base sequence of<br>
residual error pictures can also contain non-predicted<br>
35 macroblocks and sub-macroblocks, respectively, despite the<br>
fact that it is referred to as "residual error picture".<br>
This situation will occur when it has been found out that,<br>
e.g., a new object occurs in a picture. Here, a motion-<br><br>
- 34 -<br>
compensated prediction would be useless, since the<br>
prediction residual signal would become larger than the<br>
original signal in the picture. In the enhancement motion<br>
prediction in block 1016, in such a case, both the<br>
5 prediction operator and eventually the update operator for<br>
this block (e.g. macroblock or sub-macroblock) would be<br>
deactivated.<br>
Still,  for  clarity  reasons,  e.g.  a  base  sequence  of<br>
10  residual error pictures is mentioned, despite maybe only a<br>
single residual error picture of the base sequence of<br>
residual error pictures has a single block, which actually<br>
includes motion prediction residual signals.  In typical<br>
application cases, however, every residual error picture<br>
15  will actually have a high number of blocks with motion<br>
prediction residual data.<br>
In the sense of the present invention, this applies also<br>
for the enhancement sequence of residual error pictures. In<br>
20 that way, the situation in the enhancement layer will be<br>
similar to the situation in the base layer. Thus, in the<br>
sense of the present invention, an enhancement sequence of<br>
residual error pictures is already a sequence of pictures,<br>
wherein in the extreme case only a single block of a single<br>
25 "residual error picture" will have motion prediction<br>
residual values, while in all other blocks of this picture<br>
and even in all other "residual error pictures" actually no<br>
residual errors exist, since the motion-compensated<br>
prediction and, if necessary, the motion-compensated update<br>
30  have been deactivated for all these pictures/blocks.<br>
According to the present invention, this applies also for<br>
the intermediate layer predictor, which calculates<br>
enhancement prediction residual error pictures. Typically,<br>
35 the enhancement prediction residual error pictures will be<br>
present in a sequence. However, the intermediate layer<br>
predictor is also preferably formed adaptively. If, for<br>
example,  it has been found out that a residual data<br><br>
t<br>
- 35 -<br>
prediction of a base layer from the base layer to the<br>
enhancement layer has been useful only for a single block<br>
of a single "residual error picture", while for all other<br>
blocks of this picture and, if necessary, even for all<br>
5 other pictures of the sequence of enhancement prediction<br>
residual error pictures, the intermediate layer residual<br>
data prediction has been deactivated, in the present<br>
context, for clarity reasons, the sequence will still be<br>
referred  to  as  enhancement  prediction  residual  error<br>
10 picture. In this connection, it should be noted that the<br>
intermediate layer predictor can only predict residual<br>
data, when in a corresponding block of a residual error<br>
picture in the base layer motion compensation residual<br>
values have already been calculated, and when for a block<br>
15 corresponding to this block (e.g. at the same x, y<br>
position) a motion-compensated prediction has also been<br>
performed in a residual error picture of the enhancement<br>
sequence, so that in this block, residual error values<br>
exist in the enhancement layer due to a motion-compensated<br>
20 prediction. Only when actual motion-compensated prediction<br>
residual values exist in both blocks to be considered, the<br>
intermediate layer predictor will preferably become active<br>
to use a block of residual error values in a picture of the<br>
base layer as predictor for a block of residual error<br>
25 values in a picture of the enhancement layer and then to<br>
transmit only the residual values of this prediction, which<br>
means enhancement prediction residual error data in this<br>
block of the considered picture to the enhancement picture<br>
coder.<br>
30<br>
In the following, a detailed illustration of the base<br>
picture coder 1010 or the enhancement picture coder 1028<br>
and any picture coder, respectively, will be discussed with<br>
reference to Fig. lb. On the input side, the picture coder<br>
35 receives the group of residual error pictures and supplies<br>
them macroblock by macroblock to a transformation 1050. The<br>
transformed macroblocks will then be scaled in a block 1052<br>
and quantized by using a quantization parameter 1034, 1036,<br><br>
- 36 -<br>
 At the output of block 1052, the used quantization<br>
parameter, which means the used quantizer step width for a<br>
macroblock as well as quantization indices for the spectral<br>
values of the macroblock, will be output. This information<br>
5 will then be supplied to an entropy coder stage not shown<br>
in Fig. lb, which comprises a Huffman coder or preferably<br>
an arithmetic coder, which operates with the known CABAC<br>
concept according to H.264/AVC. The output signal of means<br>
1052 will also be supplied to block 1054, which performs an<br>
10 inverse scaling and requantization to convert the<br>
quantization indices together with the quantization<br>
parameter again into numerical values, which will then be<br>
supplied to an inverse transformation in block 1056 to<br>
obtain a reconstructed group of residual error pictures,<br>
15 which will now have a quantization error at the input of<br>
the transformation block 1050 compared to the original<br>
group of residual error pictures, which depends on the<br>
quantization parameters and the quantizer step width,<br>
respectively. Depending on the control of the switch 1044,<br>
20 either the one signal or the other signal is supplied to<br>
the interpolator 1022 or already to the intermediate layer<br>
predictor 1018 in order to perform the inventive residual<br>
value prediction.<br>
25 A simple implementation of the intermediate layer predictor<br>
flag 1030 is illustrated in Fig. lc. If the intermediate<br>
layer prediction flag is set, the intermediate layer<br>
predictor 1018 is activated. However, if the flag is not<br>
set, the intermediate layer predictor is deactivated, so<br>
30 that a simulcast operation is performed for this macroblock<br>
or a sub-macroblock subordinate to this macroblock. The<br>
reason therefore could be that the coder gain by the<br>
prediction is actually a coder loss, which means that a<br>
transmission of the corresponding macroblock at the output<br>
35 of block 1016 provides a better coder gain in the<br>
subsequent entropy coding than when prediction residual<br>
values would be used.<br><br>
- 37 -<br>
A simple implementation of the motion data flag 1048 is<br>
shown in Fig. Id. If the flag is set, motion data of the<br>
enhancement layer are derived from upsampled motion data of<br>
the base layer. In the case of an SNR scalability, the<br>
5 upsampler 1042 is not required. Here, when the flag 1048 is<br>
set, the motion data of the enhancement layer can be<br>
derived directly from the base motion data. It should be<br>
noted that this motion data "derivation" can be the direct<br>
takeover of the motion data or a real prediction wherein<br>
10 block 1014 subtracts the motion vectors obtained from the<br>
base layer from corresponding motion vectors for the<br>
enhancement scaling layer calculated by block 1014, to<br>
obtain motion data prediction values. The motion data of<br>
the enhancement layer (if no prediction of any type has<br>
15 been performed) or the residual values of the prediction<br>
(if a real prediction has been performed) will be supplied<br>
to the enhancement picture coder 1028 via an output shown<br>
in Fig. la, so that they will be contained in the<br>
enhancement scaling layer bit stream 1004 in the end. If,<br>
20 however, a full take over of the motion data from the base<br>
scaling layer with or without scaling is performed, no<br>
enhancement motion data have to be written into the<br>
enhancement scaling layer bit stream 1004. It is merely<br>
sufficient to signalize this fact by the motion data flag<br>
25  1048 in the enhancement scaling layer bit stream.<br>
Fig. 2 shows an apparatus for decoding a coded video<br>
sequence, which comprises the base scaling layer 1002 and<br>
the enhancement scaling layer 1004. The enhancement scaling<br>
30 layer 1004 and the base scaling layer 1002 can originate<br>
from a bit stream demultiplexer, which demultiplexes a<br>
scalable bit stream with both scaling layers<br>
correspondingly, to extract both the base scaling layer<br>
1002 and the enhancement scaling layer 1004 from the common<br>
35 bit stream. The base scaling layer 1002 is supplied to a<br>
base picture decoder 1060, which is formed to decode the<br>
base scaling layer to obtain a decoded base sequence of<br>
residual error pictures and base motion data, which are<br><br>
- 38 -<br>
applied to an output line 1062. The output signals at line<br>
1062 will then be supplied to a base motion combiner 1064,<br>
which cancels the base motion predictor introduced in the<br>
coder in block 1012, to output decoded pictures of the<br>
5 first scaling layer on the output side. Further, the<br>
inventive decoder comprises an enhancement picture decoder<br>
1066 for decoding the enhancement scaling layer 1004 to<br>
obtain enhancement prediction residual error pictures at an<br>
output line 1068. Further, the output line 1068 comprises<br>
10 motion data information, such as the motion data flag 1070<br>
or, if actually enhancement motion data or enhancement<br>
motion data residual values existed in the enhancement<br>
scaling layer 1004, these enhancement motion data. Now, the<br>
decoded base sequence on the line 1062 will either be<br>
15 interpolated by an interpolator 1070 or supplied unchanged<br>
(line 1072) to an intermediate layer combiner 1074 in order<br>
to cancel the intermediate layer prediction performed by<br>
the intermediate layer predictor 1018 of Fig. la. Thus, the<br>
intermediate  layer  combiner  is  formed to  combine  the<br>
20 enhancement prediction residual error pictures with<br>
information about the decoded base sequence on line 1062,<br>
either interpolated (1070) or not (1072), to obtain an<br>
enhancement sequence of residual error pictures, which will<br>
finally be provided to an enhancement motion combiner 1076,<br>
25 which, like the base motion combiner 1064, cancels the<br>
motion compensation performed in the enhancement layer. The<br>
enhancement motion combiner 107 6 is coupled to a motion<br>
data determination means 1078, to provide the motion data<br>
for the motion combination in block 1076. The motion data<br>
30 can actually be full enhancement motion data for the<br>
enhancement layer provided by the enhancement picture<br>
decoder at output 1068. Alternatively, the enhancement<br>
motion data can also be motion data residual values. In<br>
both cases, the corresponding data will be supplied to the<br>
35 motion data determination means 1078 via an enhancement<br>
motion data line 1080. If, however, the motion data flag<br>
1070 signals that no individual enhancement motion data<br>
have been transmitted for the enhancement layer, necessary<br><br>
- 39 -<br>
motion data will be taken from the base layer via a line<br>
1082, depending on the used scalability either directly<br>
(line 1084) or after upsampling by an upsampler 1086.<br>
5 Further, in the case of an intermediate layer prediction of<br>
intrablocks, which means no motion data residual values, a<br>
corresponding connection between the enhancement motion<br>
combiner 1076 and the base motion combiner 1064 is provided<br>
on the decoder side,  which has,  depending on spatial<br>
10 scalability, an interpolator 1090 or a bypass line when<br>
only an SNR scalability has been used. In the case of an<br>
optional intrablock prediction between two layers, merely a<br>
prediction residual signal will be transmitted to the<br>
enhancement layer for this intramacroblock, which will be<br>
15 indicated by corresponding signalization information in bit<br>
stream. In this case, the enhancement motion combiner will<br>
also perform a summation for this one macroblock,<br>
additionally to the below explained functionality, which<br>
means to perform a combination between the macroblock<br>
20 residual values and the macroblock values from the lower<br>
scaling layer and to supply the obtained macroblock to the<br>
actual inverse motion compensation processing.<br>
In the following,  with reference to Figs.  3 to 5d,  a<br>
25  preferred embodiment of the base motion predictor 1012 or<br>
the enhancement motion predictor 1016 and the inverse<br>
element, respectively, which means the enhancement motion<br>
combiner 1076 or the base motion compensator 1064 will be<br>
explained.<br>
30<br>
Basically, any motion-compensated prediction algorithm can<br>
be used, which means also the motion compensation algorithm<br>
illustrated at 92 in Fig. 9. Thus, the conventional motion<br>
compensation algorithm also follows the systematic shown in<br>
35 Fig. 1, wherein, however, the update operator U illustrated<br>
in Fig. 4 with reference number 45, is deactivated. This<br>
leads to the fact that a group of pictures is converted<br>
into  an  original  picture  and  residual  pictures  and<br><br>
- 40 -<br>
prediction residual signals, respectively, or residual<br>
error pictures depending thereon. If, however, an<br>
enhancement is implemented in the known motion compensation<br>
scheme in that the update operator, as illustrated in Fig.<br>
5 4, is active and is calculated, for example as it is<br>
illustrated with regard to Figs. 5a to 5d, the normal<br>
motion-compensated prediction calculation becomes the so-<br>
called MCTF processing, which is also referred to as<br>
motion-compensated time filtering. Here, the normal picture<br>
10 and intra picture of the conventional motion compensation,<br>
respectively, becomes a low-pass picture through the update<br>
operation, since the original picture combined with the<br>
prediction residual signal weighted by the update operator.<br>
15 As has already been described with regard to Figs, la and<br>
2, in a preferred embodiment of the present invention, such<br>
an MCTF processing is performed for every scaling layer,<br>
wherein the MCTF processing is preferably performed as it<br>
is described with reference to Figs. 3 to 5d and 7 to 8.<br>
20<br>
In the following, the preferred embodiment of the motion-<br>
compensated prediction filter will be described with<br>
reference to Fig. 4 and the subsequent Figs. 5a - 5d. As<br>
has already been explained, the motion-compensated temporal<br>
25 filter (MCTF) consists of a general lifting scheme with<br>
three steps, namely the polyphase decomposition, the<br>
prediction and the update. The corresponding<br>
analysis/synthesis filter bank structure is shown in Fig.<br>
4. On the analysis side, the odd samples of a given signal<br>
30 are filtered by a linear combination of the even samples by<br>
using the prediction operator P and the high-pass signal H<br>
to the prediction residual values. A corresponding low-pass<br>
signal 1 is formed by adding a linear combination of the<br>
prediction residual values h with the even samples of the<br>
35 input signal s by using the update operator. The equation<br>
connection of the variables h and 1 shown in Fig. 4 as well<br>
as the basic embodiments of the operators P and U is shown<br>
in Fig. 5a.<br><br>
- 41 -<br>
Since both the prediction step and the update step can be<br>
fully inverted, the corresponding transformation can be<br>
considered as critically sampled perfect reconstruction<br>
5 filter bank. The synthesis filter bank comprises the<br>
application of the prediction operator and the update<br>
operator in inverse sequence with the inverted signs in the<br>
summation process, wherein the even and odd polyphase<br>
components are used. For a normalization of the high-<br>
10 pass/low-pass components, corresponding scaling factors Fl<br>
and Fh are used. These scaling factors do not necessarily<br>
have to be used, but they can be used when quantizer step<br>
sizes are chosen during coding.<br>
15 f[x,k] shows a video signal with the space coordinates x =<br>
(x,y)T, wherein k is the time coordinate. The prediction<br>
operator P and the update operator U for the temporal<br>
decomposition by using the lifting representation of the<br>
hair wavelet is given as shown on the left hand side in<br>
20 Fig. 5b. For the 5/3 transformation, corresponding<br>
operators result as shown on the right hand side in Fig.<br>
5b. The enhancement to the motion-compensated temporal<br>
filtering is obtained by modification of the prediction<br>
operator and the update operator, as shown in Fig. 5c.<br>
25 Particularly, reference will be made to the reference<br>
indices r &gt; 0, which allow a general picture adaptive<br>
motion-compensated filtering. Through these reference<br>
indices, it can be ensured that in the scenario illustrated<br>
in Fig.  4  not only merely two temporally immediately<br>
30 subsequent pictures are decomposited into a high-pass<br>
picture and a low-pass picture, but that, for example, a<br>
first picture can be filtered in a motion compensated way<br>
with a third picture of a sequence. Alternatively, the<br>
appropriate choice of reference indices allows that, e.g.,<br>
35 one and the same picture of a sequence of sequences can be<br>
used to serve as base for the motion vector. This means<br>
that the reference indices allow for example in a sequence<br>
of eight pictures that all motion vectors are related, e.g.<br><br>
- 42 -<br>
to the fourth picture of the sequence, so that a single<br>
low-pass picture results at the end by processing these<br>
eight pictures through the filter scheme in Fig. 4, and<br>
that seven high-pass pictures (enhancement pictures) result<br>
5 and that all motion vectors relate to one and the same<br>
picture of the original sequence where one enhancement<br>
picture is associated to every motion vector.<br>
If thus one and the same picture of a sequence is used as<br>
10 reference for filtering several further pictures, this<br>
leads to a temporal resolution scaling not obeying to the<br>
factor of 2, which can be advantageous for certain<br>
applications. Always the same picture, namely, for example,<br>
the fourth picture of the sequence of eight pictures, is<br>
15 fed into the lower branch of the analysis filter bank in<br>
Fig. 4. The low-pass picture is the same in every<br>
filtering, namely the finally desired single low-pass<br>
picture of the sequence of pictures. When the update<br>
parameter is zero,  the base picture is simply "passed<br>
20 through" through the lower branch. In comparison, the high-<br>
pass picture is always dependent on the corresponding other<br>
picture of the original sequence and the prediction<br>
operator, wherein the motion vector associated to this<br>
input picture is used in the prediction. Thus, in this case<br>
25 it can be said that the finally obtained low-pass picture<br>
is associated to a certain picture of the original sequence<br>
of pictures, and that also every high-pass picture is<br>
associated to a picture of the original sequence, wherein<br>
exactly the deviation of the original picture correspond to<br>
30 the sequence (a motion compensation) from the chosen base<br>
picture of the sequence (which is fed into the lower branch<br>
of the analysis filter bank of Fig. 4) . When every update<br>
parameter MO1, M11, M21 and M31 is equal to zero, this leads<br>
to the fact that the picture fed into the lower branch 73<br>
35 of the fourth level is simply "passed through" towards the<br>
bottom. In a way, the low-pass picture TP1 is fed<br>
"repeatedly" into the filter bank, while the other pictures<br><br>
- 43 -<br>
- controlled by the reference indices - are introduced one<br>
after the other into the input 64 of Fig. 3.<br>
As can be seen from the previous equations, the prediction<br>
5  and update operators for the motion-compensated filtering,<br>
respectively, provide different predictions for the two<br>
different wavelets.  When the hair wavelet is used,  a<br>
unidirectional motion-compensated prediction is achieved.<br>
If,  however,  the 5/3 spline wavelet is used,  the two<br>
10  operators  specify  a  bidirectional  motion-compensated<br>
prediction.<br>
Since the bidirectional compensated prediction generally<br>
reduces the energy of the prediction residual value, but<br>
15 increases the motion vector rate compared to an<br>
unidirectional prediction, it is desirable to switch<br>
dynamically between the unidirectional and the<br>
bidirectional prediction, which means that one can switch<br>
between a lifting representation of the hair wavelet and<br>
20 the 5/3 spline wavelet dependent on a picture dependent<br>
control signal. The inventive concept, which uses no closed<br>
feedback loop for temporal filtering, easily allows this<br>
macroblock by macroblock switching between two wavelets,<br>
which again supports flexibility and particularly data rate<br>
25 saving, which can be performed optimally in a signal-<br>
adapted way.<br>
In order to represent the motion fields or generally the<br>
prediction data fields Mp and Mu ideally, the existing<br>
30  syntax of the B slices in H.264/AVC can be used.<br>
By cascading the pair-wise picture decomposition stages, a<br>
dyadic tree structure is obtained, which decomposits a<br>
group of 2n pictures into 2"'1 residual pictures and a single<br>
35 low-pass (or intra) picture, as it is illustrated with<br>
regard to Fig. 7 for a group of eight pictures.<br>
Particularly, Fig. 7 shows the first-level high-pass<br>
picture HP1 at the output 22 of the filter of the first<br><br>
- 44 -<br>
level as well as the first-level low-pass picture at the<br>
output 24 of the first-level filter. The two low-pass<br>
pictures TP2 at the output 16 of the second-level filter as<br>
well as the high-pass pictures obtained from the second<br>
5 level are shown in Fig. 7 as second level pictures. The<br>
third level low-pass pictures are applied to the output 76<br>
of the third level filter, while the third level high-pass<br>
pictures are applied to the output 75 in processed form.<br>
The group of eight pictures could originally comprise eight<br>
10 video pictures, wherein then the decoder of Fig. 3 would be<br>
used without fourth filter level. If, however, the group of<br>
eight pictures is a group of eight low-pass pictures, as<br>
they are used at the output 73 of the fourth level filter,<br>
the inventive MCTF decomposition can be used as base motion<br>
15 predictor, enhancement motion predictor and as base motion<br>
combiner or enhancement motion combiner, respectively.<br>
Thus,  generally,  in this decomposition a group of  2n<br>
pictures, (2n+1-2) motion field descriptions, (2n-1) residual<br>
20  pictures as well as a single low-pass (or intra) picture<br>
are transmitted.<br>
Both the base motion compensator and the enhancement motion<br>
compensator are preferably controlled by a base control<br>
25 parameter and an enhancement control parameter,<br>
respectively, to calculate an optimum combination of a<br>
quantization parameter (1034 or 1036) and motion<br>
information, which is fixed in dependence on a certain<br>
rate. This is performed according to the following method<br>
30 to obtain an optimum ratio with regard to a certain maximum<br>
bit rate. Thus, it has been found out that for lower bit<br>
rates, which means relatively coarse quantization<br>
parameters, the motion vectors count more than for higher<br>
scaling  layers,  where  relatively  fine  quantization<br>
35 parameters are taken. Thus, for cases of coarse quantizing<br>
and thus lower bit rate, less motion data are calculated<br>
than for higher scaling layers. Thus, it is preferred in<br>
higher scaling layers to move to sub-macroblock modes to<br><br>
- 45 -<br>
calculate rather a lot of motion data for a good quality<br>
and for an optimum situation in the high bit rate, than in<br>
the case of a lower bit rate, where the motion data<br>
proportionally count more with regard to the residual data<br>
5 than in the case of a higher scaling layer. This will be<br>
discussed below.<br>
Pictures A and B are given, which are either original<br>
pictures or pictures representing low-pass signals, which<br>
10 are generated in a previous analysis stage. Further, the<br>
corresponding arrays of luma samples a[] and b[] are<br>
provided. The motion description Mio is estimated in a<br>
macroblock by macroblock way as follows:<br>
15 For all possible macroblock and sub-macroblock partitions<br>
of a macroblock i within a picture B, the associated motion<br>
vectors<br><br>
20<br>
are determined by minimizing the Lagrange function<br><br>
25  wherein the deterioration term is given as follows:<br><br>
Here, S specifies the motion vector search region within<br>
30  the reference picture A. P is the region covered by the<br>
considered macroblock partition or sub-macroblock<br>
partition. R(i,m) specifies the number of bits, which are<br>
required to transmit all components of the motion vector m,<br>
wherein X. is a fixed Lagrange multiplier.<br><br>
- 46 -<br>
First, the motion search proceeds across all integer sample<br>
exact motion vectors in the given search region S. Then, by<br>
using the best integer motion vector, the eight surrounding<br>
5  half sample exact motion vectors are tested. Finally, by<br>
using the best half sample exact motion vector, the eight<br>
surrounding quarter sample exact motion vectors are tested.<br>
For the half and quarter half exact motion vector<br>
improvement, the term<br>
10<br><br>
is interpreted as interpolation operator.<br>
Generally, the mode decision for the macroblock mode and<br>
15 the sub-macroblock mode follows the same approach. The mode<br>
Pi, which minimizes the following Lagrange function, is<br>
chosen from a given set of possible macroblock or sub-<br>
macroblock modes Smode<br><br>
The deterioration term is given as follows:<br><br>
wherein P specifies the macroblock or sub-macroblock<br>
region, and wherein m[p,x,y] is the motion vector which is<br>
associated to the macroblock or sub-macroblock mode p and<br>
the partition or sub-macroblock partition, which comprises<br>
30  the luma position (x,y).<br>
The rate term R(i,p) represents the number of bits, which<br>
are associated to the choice of the coder mode p. For the<br>
motion compensated coder modes, the same comprises the bits<br>
35 for the macroblock mode (if applicable), the sub-macroblock<br>
mode and modes  (if applicable),  respectively,  and the<br><br>
- 47 -<br>
motion vector and vectors, respectively. For the intra<br>
mode, the same comprises the bits for the macroblock mode<br>
and the arrays of quantized luma and chroma transformation<br>
coefficient levels.<br>
5<br>
The set of possible sub-macroblock modes is given by<br>
{P_8x8, P_8x4, P_4x8, P_4x4}.<br>
10  The set of possible macroblock modes is given by<br>
{P_16xl6, P_16x8, P_8xl6, P_8x8, INTRA},<br>
wherein the INTRA mode is only used when a motion field<br>
15  description Mi0 used for the prediction step is estimated.<br>
The Lagrange multiplier A is set according to the following<br>
equation in dependence on the base layer quantization<br>
parameter for the high-pass picture or pictures QPHl of the<br>
20 decomposition stage, for which the motion field is<br>
estimated:<br><br>
25 According to the invention, the decomposition scheme shown<br>
in Fig. 8 is used, which is assumed to enable a sensible<br>
compromise between temporal scalability and coder<br>
efficiency. The sequence of the original pictures is<br>
treated as sequence of input pictures A, B, A, B, A, B, ...,<br>
30 A, B. Thus, this scheme provides a stage with optimum<br>
temporal scalability (equal distance between the low-pass<br>
pictures) . The sequence of low-pass pictures, which are<br>
used as input signal to all following decomposition stages,<br>
are treated as sequences of input pictures B, A, A, B, B, A<br>
35 ... A, B, whereby the spaces between the low-pass pictures<br>
which are decomposited, are kept small in the following two<br>
channel analysis scheme, as can be seen in Fig. 8.<br><br>
- 48 -<br>
In the following, reference will be made to preferred<br>
implementations of both the motion data intermediate layer<br>
prediction and the residual data intermediate layer<br>
5 prediction with regard to Figs. 6a to 6d. To obtain a<br>
spatial and an SNR scalability, respectively, basically,<br>
motion data and texture data of a lower scaling layer are<br>
used for prediction purposes for a higher scaling layer.<br>
Here,  particularly  in  the  spatial  scalability,  an<br>
10 upsampling of the motion data will be required, before they<br>
can be used as prediction for the decoding of spatial<br>
enhancement layers. The motion prediction data of a base<br>
layer representation are transmitted by using a subset of<br>
the  existing B  slice  syntax of AVC.  Preferably,  two<br>
15 additional macroblock modes are introduced for coding the<br>
motion field of an enhancement layer.<br>
The first macroblock mode is "base_layer_mode" and the<br>
second mode is the "qpel_refinement_mode". For signalizing<br>
20 these two additional macroblock modes, two flags, namely<br>
the BLFlag and the QrefFlag are added to the macroblock<br>
layer syntax, prior to the syntax element mb_mode, as shown<br>
in Fig. 1. Thus, the first flag BLFlag 1098 signalizes the<br>
base layer mode, while the other flag 1100 symbolizes the<br>
25 qpel refinement mode. If such a flag is set, it has the<br>
value 1, and the data stream is as shown in Fig. 6a. Thus,<br>
if the flag 1098 has the value 1, the flag 1100 and the<br>
syntax element macroblock mode 1102 have no further<br>
importance. If, however, the flag 1098 has the value zero,<br>
30 it is not set, and the flag 1100 will be used, which, when<br>
it is set, again bridges the element 1102. If, however,<br>
both flags 1098 and 1100 have a value zero, which means<br>
they are both not set, the macroblock mode will be<br>
evaluated in the syntax element 1102.<br>
35<br>
When BLFlag = 1, the base layer mode is used, and no<br>
further information is used for the corresponding<br>
macroblock. This macroblock mode indicates that the motion<br><br>
- 49 -<br>
prediction information including the macroblock partition<br>
of the corresponding macroblock of the base layer is<br>
directly used in that way for the enhancement layer. It<br>
should be noted that here and in the whole specification,<br>
5 the term "base layer" is to represent a next lower layer<br>
with regard to the currently considered layer, which means<br>
the enhancement layer. When the base layer represents a<br>
layer with half the spatial resolution, the motion vector<br>
field, which means the field of motion vectors including<br>
10 the macroblock partition is scaled correspondingly, as it<br>
is illustrated in Fig. 6b. In this case, the current<br>
macroblock comprises the same region as an 8x8 sub-<br>
macroblock of the base layer motion field. Thus, if the<br>
corresponding base layer macroblock is coded in a direct,<br>
15 16x16, 16x8 or 8x16 mode, or when the corresponding base<br>
layer sub-macroblock is coded in the 8x8 mode or in the<br>
direct 8x8 mode, the 16x16 mode is used for the current<br>
macroblock. If, on the other hand, the base layer sub-<br>
macroblock is coded in the 8x4,  4x8 or 4x4 mode,  the<br>
20 macroblock mode for the current macroblock = 16x8, 8x16 or<br>
8x8 (with all sub-macroblock modes = 8x8) . When the base<br>
layer macroblock represents an INTRA macroblock, the<br>
current macroblock is set to INTRA_BASE, which means that<br>
it is a macroblock with a prediction from the base layer.<br>
25 For the macroblock partitions of the current macroblock,<br>
the same reference indices are used as for the<br>
corresponding macroblock/sub-macroblock partitions of the<br>
base layer block. The associated motion vectors are<br>
multiplied by a factor of 2. This factor applies for the<br>
30 situation shown in Fig. 6b, where a base layer 1102<br>
comprises half the region and number of pixels,<br>
respectively, than the enhancement layer 1104. If the ratio<br>
of the spatial resolution of the base layer to the spatial<br>
resolution of the enhancement layer is unequal to 1/2,<br>
35 corresponding scaling factors are used for the motion<br>
vector.<br><br>
- 50 -<br>
If, however, the flag 1098 equals zero and flag 1100 equals<br>
1, macroblock mode qpel_refinement_mode is signalized. The<br>
flag 1100 is preferably only present when the base layer<br>
represents a layer with half the spatial resolution of the<br>
5 current layer. Otherwise, the macroblock mode<br>
(qpel_ref inement_mode) is not contained in the set of<br>
possible macroblock modes. This macroblock mode is similar<br>
to the base layer mode. The macroblock partition as well as<br>
the reference indices and the motion vectors are derived as<br>
10 in the base layer mode. However, for every motion vector,<br>
there is an additional quarter sample motion vector<br>
refinement -1.0 or +1 for every motion vector component,<br>
which is transmitted additionally and added to the derived<br>
motion vector.<br>
15<br>
When the flag 1098 = zero and the flag 1100 = zero, or when<br>
the flag 1100 is not present, the macroblock mode as well<br>
as the corresponding reference indices and motion vector<br>
differences are specified as usual. This means that the<br>
20 complete set of motion data is transmitted for the<br>
enhancement layer the same way as for the base layer.<br>
However, according to the invention, the possibility is<br>
provided to use the base layer motion vector as predictor<br>
for the current enhancement layer motion vector (instead of<br>
25 the spatial motion vector predictor). Thus, the list X<br>
(wherein X lies between 0 and 1) is to specify the<br>
reference index list of the considered motion vector. If<br>
all subsequent conditions are true, a flag MvPrdFlag is<br>
transmitted, as shown in Fig. 6c, for every motion vector<br>
30  difference:<br>
the  base  layer macroblock  comprising  the  current<br>
macroblock/sub-macroblock partitions is not coded in an<br>
INTRA macroblock mode;<br>
35<br>
the  base  layer  macroblock/sub-macroblock  partition<br>
covering  the  upper  left  sample  of  the  current<br><br>
- 51 -<br>
macroblock/sub-macroblock partition uses the list X or<br>
a biprediction;<br>
the  list  X  reference  index  of  the  base  layer<br>
5     macroblock/sub-macroblock partition,  which  comprises<br>
the upper  left sample of the current macroblock/<br>
sub-macroblock  partition  is  equal  to  the  list  X<br>
reference  index  of  the  current  macroblock/sub-<br>
macroblock partition.<br>
10<br>
If the flag 1106 of Fig. 6c is not present, or if this flag<br>
1106 = zero, the spatial motion vector predictor is<br>
specified as it is the case in the standard AVC. Otherwise,<br>
when the flag 1106 is present and = 1, the corresponding<br>
15 base layer vector is used as motion vector predictor. In<br>
this case, the list X motion vector (wherein X = 0 or 1) of<br>
the current macroblock/sub-macroblock partition is obtained<br>
by adding the transmitted list X motion vector difference<br>
to the possibly scaled list X motion vector of the base<br>
20  layer macroblock/sub-macroblock partition.<br>
Thus, the flags 1098, 1100 and 1106 represent together a<br>
possibility to implement the motion data flag 1048<br>
generally indicated in Fig. la and generally a motion data<br>
25 control signal 1048, respectively. There are, of course,<br>
different other possibilities of signalizing, wherein<br>
naturally a fixed agreement between transmitter and<br>
receiver can be used, which allows a reduction of<br>
signalizing information.<br>
30<br>
In summary, a detailed implementation of the enhancement<br>
motion compensator 1014 of Fig. la and the enhancement<br>
motion data determination means 1078 of Fig. 2,<br>
respectively, is illustrated in more detail with regard to<br>
35  Figs, le, If and lg.<br>
With reference to Fig. le, it can be seen that the<br>
enhancement motion compensator 1014 basically has to do two<br><br>
- 52 -<br>
things. Thus, it first has to calculate the enhancement<br>
motion data, typically the whole motion vectors and supply<br>
them to the enhancement motion predictor 1016, so that the<br>
same can use these vectors in uncoded form to obtain the<br>
5 enhancement sequence of residual error pictures which are,<br>
in the prior art, typically performed adaptively and block<br>
by block. Another matter, however, is the enhancement<br>
motion data processing, which means how the motion data<br>
used  for  a motion-compensated prediction will  now be<br>
10 compressed as much as possible and written into a bit<br>
stream. In order for something to be written into the bit<br>
stream, respective data have to be brought to the<br>
enhancement picture coder 1028, as it is illustrated with<br>
regard to Fig.  le.  Thus,  the enhancement motion data<br>
15 processing means 1014b has the function to reduce the<br>
redundancy contained in the enhancement motion data, which<br>
the enhancement motion data calculation means 1014a has<br>
determined, with regard to the base layer as much as<br>
possible.<br>
20<br>
According to the invention, the base motion data or the<br>
upsampled base motion data can be used both by the<br>
enhancement motion data calculation means 1014a for<br>
calculating the actually to be used enhancement motion data<br>
25 or can also be used only for enhancement motion data<br>
processing, which means for enhancement motion data<br>
compression, while they are of no importance for the<br>
calculation of the enhancement motion data. While the two<br>
possibilities 1.) and 2.) of Fig. lg show embodiments where<br>
30 the base motion data and the upsampled base motion data are<br>
already used in the enhancement motion data calculation,<br>
the embodiment 3.) of Fig. lb shows a case where<br>
information about the base motion data are not used for<br>
calculating the enhancement motion data but merely for<br>
35  coding and capture of residual data, respectively.<br>
Fig. If shows the decoder side implementation of the<br>
enhancement motion data determination means 1078, which has<br><br>
- 53 -<br>
a control module 1078a for block by block control, which<br>
contains the signalizing information from the bit stream<br>
and from the enhancement picture decoder 1066,<br>
respectively. Further, the enhancement motion data<br>
5 determination means 1078 comprises an enhancement motion<br>
data reconstruction means 1078b, which actually determines<br>
the motion vectors of the enhancement motion data field,<br>
either only by using the decoded base motion data or<br>
decoded  upsampling  base  motion  data  or  by  combining<br>
10 information about the decoded base motion data and from the<br>
residual data extracted from the enhancement motion decoder<br>
1066 from the enhancement scaling layer 1004, which can<br>
then be used by the enhancement motion combiner 1076, which<br>
can be formed as common combiner to reverse the coder side<br>
15  motion-compensated prediction.<br>
In the following, reference will be made to the different<br>
embodiments as they are illustrated in Fig. lg in overview.<br>
As has already been illustrated with regard to Fig. 6a, the<br>
20 BLFlag 1098 signalizes a complete takeover of the upscaled<br>
base motion data for the enhancement motion prediction. In<br>
that case, means 1014a is formed to completely take over<br>
the base motion data and in the case of different<br>
resolutions of the different layers,  to take over the<br>
25 motion data in upscaled form and transmit them to means<br>
1016, respectively. However, no information about motion<br>
fields or motion vectors is transmitted to the enhancement<br>
picture coder. Instead, merely an individual flag 1098 is<br>
transmitted for every block, either macroblock or a sub-<br>
30  macroblock.<br>
On the decoder side, this means that means 1078a of Fig. If<br>
decodes the flag 1098 for one block and, if it was active,<br>
uses the decoded base motion data present from the base<br>
35 layer or the decoded upsampled base motion data to<br>
calculate the enhancement motion data, which are then<br>
supplied to block 1076. In this case, the means 1078<br>
requires no motion vector residual data.<br><br>
- 54 -<br>
In the second embodiment of the present invention, which is<br>
signalized by the flag QrefFlag 1100,  the base motion<br>
vector is integrated into the enhancement motion data<br>
5  calculation, which is performed by means 1014a. As it is<br>
illustrated in Fig. lg in portion 2.) and described above,<br>
the motion data calculation and the calculation of the<br>
motion vector m, respectively, is performed by searching<br>
the minimum of the term<br>
10<br>
(D +  R).<br>
The difference between a block of a current picture B and a<br>
block of a previous and/or later picture shifted by a<br>
15 certain potential motion vector is introduced into the<br>
distortion term D. The quantization parameter of the<br>
enhancement picture coder indicated in Fig. la by 1036 is<br>
introduced into the factor ,. The term R provides<br>
information about the number of bits used for coding a<br>
20  potential motion vector.<br>
Normally, a search is performed among different potential<br>
motion vectors, wherein the distortion term D is calculated<br>
for every new motion vector,  and the rate term R is<br>
25 calculated, and wherein the enhancement quantization<br>
parameter 1036, which is preferably fixed, but could also<br>
vary, is considered. The described sum term is evaluated<br>
for different potential motion vectors, whereupon the<br>
motion vector is used, which provides the minimum result of<br>
30  the sum.<br>
Now, according to the invention, the base motion vector of<br>
the corresponding block from the base layer is also<br>
integrated into this iterative search. If it fulfills the<br>
35 search criteria, again merely the flag 1100 has to be<br>
transmitted, but no residual values or anything else for<br>
this block has to be transmitted. Thus, when the base<br><br>
- 55 -<br>
motion vector fulfills the criterion (minimum of the<br>
previous term) for a block, means 1014a uses the base<br>
motion vector in order to transmit it to means 1016.<br>
However, merely the flag 1100 is transmitted to the<br>
5  enhancement picture coder.<br>
On the decoder side, this means that the means 1078a<br>
controls the means 1078b when it decodes the flag 1100 to<br>
determine the motion vector for this block from the base<br>
10 motion data, since the enhancement picture decoder has<br>
transmitted no residual data.<br>
In a variation of the second embodiment, not only the base<br>
motion vector but also a plurality of base motion vectors<br>
15 derived from the base motion vector and (slightly) altered<br>
are integrated into the search. Depending on the<br>
implementation, any component of the motion vector can be<br>
independently increased or decreased by one increment, or<br>
be left the same. This increment can represent a certain<br>
20 granularity of a motion vector, e.g. a resolution step, a<br>
half resolution step or a quarter resolution step. If such<br>
an altered base motion vector fulfills the search criteria,<br>
the alteration, which means the increment, which means +1,<br>
0 or -1 is transmitted as "residual data", additionally to<br>
25  the flag 1100.<br>
Activated by flag 1100, a decoder will then search for the<br>
increment in the data stream and further recover the base<br>
motion vector or the upsampled base motion vector and<br>
30 combine the increment with the corresponding base motion<br>
vector in block 1078b, to obtain the motion vector for the<br>
corresponding block in the enhancement layer.<br>
In the third embodiment, which is signalized by the flag<br>
35 1106, the determination of the motion vectors can basically<br>
be performed arbitrarily. With regard to the full<br>
flexibility, the means 1014a can determine the enhancement<br>
motion data e.g.  according to the minimization object<br><br>
- 56 -<br>
mentioned in connection with the second embodiment. Then,<br>
the determined motion vector is used for coder side motion-<br>
compensated prediction, without considering information<br>
from the base layer. However, in that case, the enhancement<br>
5 motion data processing 1014a is formed to incorporate the<br>
base motion vectors into the motion vector processing for<br>
redundancy reduction, which means prior to the actual<br>
arithmetic coding.<br>
10 Thus, according to the standard H.264/AVC, a transmission<br>
of motion vector differences is performed, wherein<br>
differences between adjacent blocks are determined within a<br>
picture. In the implementation, the difference can be<br>
formed between different adjacent blocks, to select then<br>
15 the smallest difference. Now, according to the invention,<br>
the base motion vector for the corresponding block in a<br>
picture is incorporated into this search for the most<br>
favorable predictor for the motion vector difference. If it<br>
fulfills  the  criterion  that  it provides  the  smallest<br>
20 residual error value as predictor, this is signalized by<br>
the flag 1106 and merely the residual error value is<br>
transmitted to block 1028. If the base motion vector does<br>
not fulfill this criterion, the flag 1106 is not set, and a<br>
spatial motion vector difference calculation is performed.<br>
25<br>
For simpler coder implementations, however, instead of the<br>
iterative search, simply always and for adaptively<br>
determined blocks the base motion vector, respectively, and<br>
an upsampled version of the same, respectively, can serve<br>
30  as predictor.<br>
According to the invention, an intermediate layer<br>
prediction of residual data will also be performed. This<br>
will be discussed below. When the motion information is<br>
35 changed from one layer to the next, it can be favorable or<br>
unfavorable to predict residual information and, in the<br>
case of a MCTF decomposition, high-pass information of the<br>
enhancement layer, respectively, from the base layer. When<br><br>
- 57 -<br>
the motion vectors for a block of the current layer are<br>
similar to the motion vectors of the corresponding base<br>
layer and macroblock by macroblock to corresponding motion<br>
vectors of the corresponding base layer, it is likely that<br>
5 the coder efficiency can be increased when the coded base<br>
layer residual signal (high-pass signal) is used as<br>
prediction for the enhancement residual signal (enhancement<br>
high-pass signal), whereby only the difference between the<br>
enhancement   residual   signal   and   the   base   layer<br>
10 reconstruction (line 1024 of Fig. la) is coded. However,<br>
when the motion vectors are not similar, it is very<br>
unlikely that a prediction of the residual signal will<br>
improve the coder efficiency. Consequently, an adaptive<br>
approach is used for the prediction of the residual signal<br>
15 and high-pass signal, respectively. This adaptive approach,<br>
which means whether the intermediate layer predictor is<br>
active or not, can be performed by an actual calculation of<br>
the benefit based on the difference signal or can be<br>
performed based on an estimation, how different the motion<br>
20 vector of a base scaling layer for a macroblock is to a<br>
corresponding macroblock in the enhancement scaling layer.<br>
If the difference is smaller than a certain threshold, the<br>
intermediate layer predictor is activated via the control<br>
line 130. However,  if the difference is higher than a<br>
25 certain threshold, the intermediate layer predictor for<br>
this macroblock is deactivated.<br>
A flag ResPrdFlag 1108 is transmitted. When the flag 1108 =<br>
1, the reconstructed residual signal of the base layer is<br>
30 used as prediction for the residual signal of the current<br>
macroblock of the enhancement layer, wherein only an<br>
approximation of the difference between the current<br>
residual signal of the enhancement layer and its base layer<br>
reconstruction will be coded. Otherwise, the flag 1108 does<br>
35 not exist or equals zero. Here, the residual signal of the<br>
current macroblock in the enhancement layer will then be<br>
coded without prediction from the base layer.<br><br>
- 58 -<br>
When the base layer represents a layer with half the<br>
spatial resolution of the enhancement layer, the residual<br>
signal is upsampled by using an interpolation filter,<br>
before the upsampled residual signal of the base layer is<br>
5 used as prediction signal. This filter is an interpolation<br>
filter with six taps, such that for interpolating a value<br>
of the higher spatial resolution of the enhancement layer,<br>
which was not present in the base layer due to the lower<br>
resolution, values from the surroundings are used to obtain<br>
10  an interpolation result, which is as good as possible.<br>
If, however, values at the edge of a transformation block<br>
are interpolated, and the interpolation filter would use<br>
only values of another transformation block for<br>
15 interpolation, it is preferred not to do this, but to<br>
synthesize the values of the interpolation filter outside<br>
the considered block so that an interpolation with as<br>
little artifacts as possible takes place.<br>
20 Based on a so-called core experiment, it was found out that<br>
the intermediate layer prediction of motion and residual<br>
values significantly improves the coder efficiency of the<br>
AVC based MCTF approach. For certain test points, PSNR<br>
gains of more than 1 dB were obtained. Particularly with<br>
25 very low bit rates for every spatial resolution (with the<br>
exception of the base layer), the improvement of the<br>
reconstruction quality was clearly visible.<br>
Depending on the circumstances, the inventive method can be<br>
30 implemented in hardware or in software. The implementation<br>
can be performed on a digital storage medium, particularly<br>
a disc or CD with electronically readable control signals,<br>
which can cooperate with a programmable computer system<br>
such that the method is performed. Thus, generally, the<br>
35 invention consist also in a computer program product with a<br>
program code for performing the inventive method stored on<br>
a machine readable carrier, when the computer program<br>
product runs on a computer. In other words, the invention<br><br>
- 59 -<br>
can also be realized as computer program with a program<br>
code for performing the method when the computer program<br>
runs on a computer.<br>
5 Further, the present invention concerns a computer readable<br>
medium, whereon a scalable data stream with a first scaling<br>
layer and a second scaling layer together with the<br>
associated control characters are stored for the different<br>
decoder-side means. Thus, the computer readable medium can<br>
10 be a data carrier or the internet whereon a data stream is<br>
transmitted from a provider to a receiver.<br><br>
- 60 -<br>
Claims<br>
1.   Apparatus for generating a coded video sequence having<br>
5      a base scaling layer (1002) and enhancement scaling<br>
layer (1004), comprising:<br>
a base motion compensator (1006) for calculating base<br>
motion data, which indicate how a block in a current<br>
10      picture has moved in relation to another picture in a<br>
group of pictures;<br>
a base motion predictor (1012) for calculating a base<br>
sequence of residual error pictures by using the base<br>
15       motion data;<br>
a base picture coder (1010), which is formed to<br>
generate a coded first scaling layer from the base<br>
sequence of residual error pictures;<br>
20<br>
an enhancement motion compensator (1014) for<br>
determining enhancement motion data; wherein the<br>
enhancement motion compensator is formed to determine<br>
enhancement motion data adaptively and block by block<br>
25      by  using  the  base  motion  data  and  to  provide<br>
signalization information block by block;<br>
an enhancement motion predictor (1016) for calculating<br>
an enhancement sequence of residual error pictures by<br>
30      using the enhancement motion data; and<br>
an enhancement picture coder (1028) for coding<br>
information about the enhancement sequence of residual<br>
error pictures and for coding the signalization<br>
35 information block by block to obtain a coded<br>
enhancement scaling layer.<br><br>
- 61 -<br>
2.   Apparatus according to claim 1,  wherein the base<br>
motion compensator is formed to calculate the base<br>
motion  data  for pictures  having  a  lower  spatial<br>
resolution  than  the pictures  based  on which  the<br>
5      enhancement   motion   compensator   determines   the<br>
enhancement motion data,<br>
wherein further an upsampler (1042)  is provided to<br>
scale the base motion data according to a difference<br>
10      of the spatial resolution of the group of pictures,<br>
and<br>
wherein the enhancement motion compensator (1014) is<br>
formed to calculate the enhancement motion data based<br>
15      on the scaled base motion data.<br>
3.   Apparatus  according  to  claim  2,   wherein  the<br>
enhancement motion compensator  (1014)  is formed to<br>
take over the scaled base motion data for a block as<br>
20 enhancement motion data, and to supply a takeover<br>
signal (1098) to the enhancement picture coder (1028)<br>
for this block.<br>
4.   Apparatus  according  to  claim  2,   wherein  the<br>
25      enhancement motion compensator (1014) is formed to use<br>
the scaled base motion data as predictor for a block<br>
of enhancement motion data to calculate an enhancement<br>
motion data residual signal and to supply the<br>
enhancement motion data residual signal together with<br>
30 a prediction signalization to the enhancement picture<br>
coder (1028).<br>
5.   Apparatus according to claim 1 or 2, wherein the base<br>
picture coder (1010) is formed to quantize with a base<br>
35 quantization parameter (1034), wherein the base motion<br>
compensator is formed to calculate the base motion<br>
data depending on a base control parameter (1034)<br>
which can depend on the base quantization parameter,<br><br>
- 62 -<br>
wherein the enhancement picture coder (1028) is formed<br>
to quantize with an enhancement quantization parameter<br>
(1036), and<br>
5<br>
wherein the enhancement motion compensator (1014) is<br>
formed to calculate the enhancement motion data in<br>
dependence on an enhancement control parameter (1036),<br>
which can dependent on the enhancement quantization<br>
10 parameter and differs from the base control parameter<br>
for the base picture coder.<br>
6. Apparatus according to claim 5, wherein the<br>
enhancement motion compensator is formed to use the<br>
15 base motion data as predictor for the enhancement<br>
motion data and to supply an enhancement motion data<br>
residual signal with a block by block signalization to<br>
the enhancement picture coder (1028).<br>
20 7. Apparatus according to claim 5, wherein the<br>
enhancement motion compensator (1014) is formed to<br>
perform a search among a number of potential motion<br>
vectors in the determination of a motion vector for a<br>
macroblock according to a search criterion, wherein<br>
25 the enhancement motion compensator (1014) is formed to<br>
use a motion vector already determined for the<br>
corresponding block of the base layer in the search,<br>
and when the search criterion is fulfilled by the<br>
motion vector of the base layer, to take over then the<br>
30 motion vector of the base layer and to supply<br>
information (1100) regarding this to the enhancement<br>
picture coder (1028).<br>
8.  Apparatus according to one of claims 5 to 7, wherein<br>
35      the enhancement motion compensator (1014) is further<br>
formed to also consider an incrementally altered<br>
motion  vector  of  the  base  layer  and  when  the<br>
incrementally altered motion vector of the base layer<br><br>
- 63 -<br>
fulfills  the  search  criterion,  to  supply  the<br>
incremental  change  of  the  motion  vector  to  the<br>
enhancement picture coder (1028) for a block together<br>
with a signalization (1100) for the block.<br>
5<br>
9. Apparatus according to one of the preceding claims,<br>
wherein the enhancement motion compensator (1014) is<br>
formed to determine motion vectors for blocks of a<br>
picture and to further postprocess the motion vectors<br>
10 to determine motion vector differences between two<br>
motion vectors and supply them to the enhancement<br>
picture coder (1028), and<br>
wherein the enhancement motion compensator (1014) is<br>
15 further formed to use, in dependence on a cost<br>
function instead of a difference between motion<br>
vectors for two blocks of the same picture, a<br>
difference between a motion vector of the block of one<br>
picture from the enhancement layer and a modified or<br>
20 unmodified motion vector of a corresponding block of a<br>
picture of the base layer and to supply this<br>
difference to the enhancement picture coder (1028)<br>
together with a signalization (1106) for the block.<br>
25 10. Apparatus according to claim 9, wherein the<br>
enhancement motion compensator (1014) is formed to use<br>
an amount of a difference as cost function.<br>
11.  Apparatus according to one of the preceding claims,<br>
30      further having an intermediate layer predictor (1018)<br>
formed to calculate enhancement prediction residual<br>
error pictures by using the enhancement sequence of<br>
residual error pictures and information about the base<br>
sequence of residual error pictures.<br>
35<br>
12.  Apparatus according to claim 11,<br><br>
- 64 -<br>
wherein the base picture coder (1010) is formed to<br>
perform quantization with a base quantization<br>
parameter (1034),<br>
5 wherein the enhancement picture coder (1028) is formed<br>
to perform quantization with an enhancement<br>
quantization parameter (1036), wherein the enhancement<br>
quantization parameter (1036) can result in a finer<br>
quantization than the base quantization parameter<br>
10       (1034),<br>
wherein the base picture coder (1010) is formed to<br>
reconstruct  the  base  sequence  of  residual  error<br>
pictures  quantized  with  the  first  quantization<br>
15      parameter to obtain a reconstructed base sequence, and<br>
wherein the intermediate layer predictor (1026) is<br>
formed to calculate the enhancement prediction<br>
residual error pictures by using the enhancement<br>
20 sequence of residual error pictures and the<br>
reconstructed base sequence of residual error pictures<br>
as information about the base sequence of residual<br>
error pictures.<br>
25 13. Apparatus according to claim 11 or 12, further<br>
comprising:<br>
a decimator (1032) for decimating a resolution of a<br>
group of pictures, wherein the decimator (1032) is<br>
30 formed to provide a group of pictures with a base<br>
resolution to the base motion compensator (1006),<br>
which is smaller than an enhancement resolution of a<br>
group of pictures, which is provided to the<br>
enhancement motion compensator (1014); and<br>
35<br>
an interpolator (1022) for spatially interpolating the<br>
base sequence of residual error pictures or a<br>
reconstructed base sequence of residual error pictures<br><br>
- 65 -<br>
to obtain an interpolated base sequence of residual<br>
error pictures, which can be supplied to the<br>
intermediate layer predictor (1018) as information<br>
(1026) about the base sequence of residual error<br>
5      pictures.<br>
14.  Method for generating a coded video sequence having a<br>
base scaling layer (1002) and an enhancement scaling<br>
layer (1004), comprising the steps of:<br>
10<br>
calculating (1006) base motion data, which indicate<br>
how a block in a current picture has moved in relation<br>
to another picture in a group of pictures;<br>
15       calculating (1012) a base sequence of residual error<br>
pictures by using the base motion data;<br>
performing a base picture coding (1010) to generate a<br>
coded first scaling layer from the base sequence of<br>
20       residual error pictures;<br>
determining (1014) enhancement motion data wherein<br>
enhancement motion data are determined adaptively and<br>
block by block by using the base motion data, and<br>
25 wherein signalization information are provided<br>
adaptively and block by block;<br>
calculating (1016) an enhancement sequence of residual<br>
error pictures by using the enhancement motion data;<br>
30      and<br>
performing an enhancement picture coding (1028) by<br>
coding information about the enhancement sequence of<br>
residual error pictures and by coding the block by<br>
35 block signalization information to obtain a coded<br>
enhancement scaling layer.<br><br>
- 66 -<br>
15.  Apparatus for decoding a coded video sequence with a<br>
base scaling layer (1002) and an enhancement scaling<br>
layer (1004), comprising:<br>
5 a base picture decoder (1060) for decoding the base<br>
scaling layer to obtain a decoded base sequence of<br>
residual error pictures and base motion data;<br>
a base motion combiner (1064), which is formed to<br>
10      obtain a sequence of pictures of the base scaling<br>
layer by using the base motion data and the decoded<br>
sequence of residual error pictures;<br>
an enhancement picture decoder (1066) for decoding the<br>
15      enhancement scaling layer to obtain information about<br>
an enhancement sequence of residual error pictures and<br>
information about enhancement motion data;<br>
an  enhancement  motion  data  calculator  (1078)  for<br>
20      calculating the enhancement motion data by evaluating<br>
the information about the enhancement motion data and<br>
by using information about base motion data due to the<br>
evaluated information about the enhancement motion<br>
data; and<br>
25<br>
an enhancement motion combiner (1076), which is formed<br>
to obtain a sequence of pictures of the enhancement<br>
scaling layer by using the enhancement sequence of<br>
residual error pictures and the enhancement motion<br>
30      data.<br>
16.  Apparatus according to claim 15,<br>
wherein the enhancement picture decoder  (1066)  is<br>
35      formed to provide a motion data takeover signal from<br>
the enhancement scaling layer,<br><br>
- 67 -<br>
wherein further an upsampler (1086) is provided to<br>
convert the base motion data from a base scaling layer<br>
resolution to an enhancement scaling layer resolution,<br>
and<br>
5<br>
wherein the enhancement motion data calculator (1078)<br>
is formed to provide the converted base motion data as<br>
enhancement motion data in dependence on the motion<br>
data takeover signal (1098).<br>
10<br>
17.  Apparatus  according  to  claim  15,  wherein  the<br>
enhancement  picture  decoder  (1066)  is  formed  to<br>
provide a prediction signalization (1100, 1106) and an<br>
enhancement motion data  residual  signal  from the<br>
15      enhancement scaling layer,<br>
wherein the enhancement motion data calculator (1078)<br>
is formed to combine the enhancement motion data<br>
residual signal in dependence on the prediction<br>
20 signalization (1100, 1106) with the base motion data<br>
or base motion data converted in their resolution to<br>
obtain the enhancement motion data.<br>
18.  Apparatus  according  to  claim  15,  wherein  the<br>
25      enhancement  picture  decoder  (1066)  is  formed  to<br>
provide a difference prediction signalization (1106)<br>
and an enhancement motion data residual signal in the<br>
form of motion vector differences for blocks from the<br>
enhancement scaling layer, and<br>
30<br>
wherein the enhancement motion data calculator (1078)<br>
is formed to combine the motion vector difference with<br>
a base motion vector for a corresponding block for<br>
calculating a motion vector for a block in dependence<br>
35      on the difference prediction signalization (1106).<br>
19.  Apparatus according to one of claims 15 to 18, further<br>
having  an  intermediate  layer  combiner  (1074)  to<br><br>
- 68 -<br>
combine enhancement prediction residual error data<br>
contained in the enhancement layer with the decoded<br>
base sequence of residual error pictures or an<br>
interpolated base sequence of residual error pictures<br>
5 to obtain the enhancement sequence of residual error<br>
pictures.<br>
20.  Method for decoding a coded video sequence with a base<br>
scaling layer (1002) and an enhancement scaling layer<br>
10       (1004), comprising the steps of:<br>
decoding (1060) the base scaling layer to obtain a<br>
decoded base sequence of residual error pictures and<br>
base motion data;<br>
15<br>
performing a base motion combination (1064) by using<br>
the base motion data and the decoded sequence of<br>
residual error pictures, so that a sequence of<br>
pictures of the base scaling layer is obtained;<br>
20<br>
decoding (1066) the enhancement scaling layer to<br>
obtain information about an enhancement sequence of<br>
residual error pictures and information about<br>
enhancement motion data;<br>
25<br>
calculating (1078) the enhancement motion data by<br>
evaluating the information about the enhancement<br>
motion data and by using information about base motion<br>
data  due  to the  evaluated  information about  the<br>
30      enhancement motion data; and<br>
performing an enhancement motion combination (1076) to<br>
obtain a sequence of pictures of the enhancement<br>
scaling layer by using the enhancement sequence of<br>
35 residual error pictures and the enhancement motion<br>
data.<br><br>
- 69 -<br>
21.Computer program for performing a method according<br>
to claim 15 or 20, when the method runs on a computer.<br>
22.  Computer readable medium with a coded video sequence<br>
5      having a base scaling layer (1002) and an enhancement<br>
scaling layer (1004), wherein the coded video sequence<br>
is formed such that it results in a decoded first<br>
scaling layer and a decoded second scaling layer when<br>
it is decoded in an apparatus for decoding according<br>
10      to claim 15.<br><br>
In the scalable video coding in connection with motion<br>
compensation (1006, 1014) both in a base layer (1002) and<br>
in an enhancement layer, a prediction (1014, 1016) of the<br>
motion data of the enhancement layer (1004) is performed by<br>
using the motion data of the base layer (1004) to obtain a<br>
scalability concept, which provides, on the one hand, a<br>
maximum flexibility for the calculation of the motion data<br>
of the different layers and, on the other hand, allows a<br>
lower bit rate.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=ws1VsrX1Od9QbSIeZ29R4g==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==" target="_blank" style="word-wrap:break-word;">http://ipindiaonline.gov.in/patentsearch/GrantedSearch/viewdoc.aspx?id=ws1VsrX1Od9QbSIeZ29R4g==&amp;amp;loc=wDBSZCsAt7zoiVrqcFJsRw==</a></p>
		<br>
		<div class="pull-left">
			<a href="268752-method-and-apparatus-for-monitoring-an-intake-air-filter.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="268754-catalytic-converter-and-associated-method-of-assembly.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>268753</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>1266/KOLNP/2007</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>38/2015</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>18-Sep-2015</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>15-Sep-2015</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>11-Apr-2007</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>FRAUNHOFER-GESELLSCHAFT ZUR FOERDERUNG DER ANGEWANDTEN FORSCHUNG E.V.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>HANSASTRASSE 27C 80686 MUNICH</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>DETLEV MARPE</td>
											<td>SUEDWESTKORSO 70 12161 BERLIN</td>
										</tr>
										<tr>
											<td>2</td>
											<td>HEIKO SCHWARZ</td>
											<td>KLAUSTHALER STR. 3 13187 BERLIN</td>
										</tr>
										<tr>
											<td>3</td>
											<td>THOMAS WIEGAND</td>
											<td>NUERNBERGER STR. 18 10789 BERLIN</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H04N 7/26</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/EP2005/010223</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2005-09-21</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/619,457</td>
									<td>2004-10-15</td>
								    <td>U.S.A.</td>
								</tr>
								<tr>
									<td>2</td>
									<td>10 2004 059 993.9</td>
									<td>2004-12-13</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/268753-apparatus-and-method-for-generating-a-coded-video-sequence-by-using-an-intermediate-layer-motion-data-prediction by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 08:10:58 GMT -->
</html>
