<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/251743-system-and-method-enabling-acoustic-barge-in by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 13:45:08 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 251743:SYSTEM AND METHOD ENABLING ACOUSTIC BARGE-IN</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">SYSTEM AND METHOD ENABLING ACOUSTIC BARGE-IN</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>A system and method enabling acoustic barge-in during a voice prompt in a communication system. An acoustic prompt model is trained to represent the system prompt using the specific speech signal of the prompt. The acoustic prompt model is utilized in a speech recognizer in parallel with the recognizer&#x27;s active vocabulary words to suppress the echo of the prompt within the recognizer. The speech recognizer may also use a silence model and traditional garbage models such as noise models and out-of-vocabulary word models to reduce the likelihood that noises and out-of-vocabulary words in the user utterance will be mapped erroneously onto active vocabulary words.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>Technical Field<br>
The present invention relates to communication systems. More particularly, and not by<br>
way of limitation, the present invention is directed to a system and method that enables<br>
the user of a communication device to interrupt, or barge-in, during a system voice<br>
announcement, or prompt, to initiate a desired function prior to the conclusion of the<br>
prompt.<br>
Background Art<br>
In communication networks, system operators often find it convenient to implement<br>
automated system voice announcements, or "prompts", to inform subscribers, for<br>
example, of certain available features or certain actions that the subscriber must take to<br>
activate particular features. For many subscribers, this information is useful the first few<br>
times they hear it, but after hearing it several times, subscribers may wish to interrupt, or<br>
"barge-in", during the system voice prompt because they already know what the prompt<br>
says and what actions they need to take. In existing communication networks, a barge-in<br>
capability is normally realized by running a standard speech recognizer during the voice<br>
prompt. In order to avoid erroneous speech recognizer output due to the input of both the<br>
user's voice and the sound (echo) of the prompt originating from the device's<br>
loudspeaker, an acoustic echo cancellation technique is normally utilized to suppress<br>
feedback of the prompt echo to the recognizer.<br>
As next generation communication devices are developed, it will be increasingly<br>
important to have user-friendly man-machine interfaces (MMIs) that enable the devices to<br>
be operated in a hands-free mode. Multi-modal MMIs and intelligent speech-driven<br>
dialog interfaces are needed that are well accepted by users and that provide flexible<br>
interaction with the system. An improved capability to barge-in will be required in order<br>
to enable a user to interrupt a system prompt by simply speaking while the prompt is<br>
being played.<br><br><br>
There are three major shortcomings of the existing methods of providing a barge-in<br>
capability. First, conventional echo cancellation algorithms may provide a very weak<br>
attenuation of the prompt echo. For example, echo attenuation of 10 dB or less may<br>
occur. This may cause serious problems with misrecognitions by the speech recognizer<br>
because the speech recognizer is triggered by the prompt echo. Second, standard adaptive<br>
echo cancellation methods require a fixed timing correlation between the voice input<br>
channel and the voice output channel. In distributed systems, however, it is often too<br>
expensive, or not possible, to obtain this temporal correlation, especially when the<br>
recognition server and the server that plays the prompt are separated by some distance.<br>
Third, standard adaptive echo cancellation methods require a considerable amount of<br>
processing power. This is a significant challenge for embedded systems with restrictive<br>
hardware constraints or for multi-channel applications where as many channels as possible<br>
have to be processed in parallel.<br>
Due to the above-described shortcomings of conventional echo cancellation<br>
methodologies, an alternative methodology for enabling the user of a hands-free<br>
communication device to barge-in during a system voice prompt is needed.<br>
Summary of the Invention<br>
In one aspect, the present invention is directed to a method of suppressing speech<br>
recognition errors in a speech recognition system in which an input signal includes an<br>
echo from a system voice prompt combined with user input speech. The method includes<br>
the steps of generating an acoustic model of the system voice prompt that mathematically<br>
represents the system voice prompt; supplying the input signal to a speech recognizer<br>
having an acoustic model of a target vocabulary that mathematically represents at least<br>
one command word; and comparing the input signal to the acoustic prompt model and to<br>
the acoustic target vocabulary model. The method then determines whether the acoustic<br>
prompt model or the acoustic target vocabulary model provides a best match for the input<br>
signal during the comparing step. If the acoustic target vocabulary model provides the<br>
best match, the best match is accepted. If the acoustic prompt model provides the best<br>
match, the best match is ignored. The speech recognizer may also use a silence model and<br>
traditional garbage models such as noise models and out-of-vocabulary word models to<br><br>
reduce the likelihood that noises and out-of-vocabulary words in the user utterance will be<br>
mapped erroneously onto active vocabulary words.<br>
In another aspect, the present invention is directed to a method of suppressing speech<br>
recognition errors and improving word accuracy in a speech recognition system that<br>
enables a user of a communication device to interrupt a system voice prompt with<br>
command words that halt the voice prompt and initiate desired actions. The method<br>
includes the steps of generating an acoustic model of the system voice prompt that<br>
mathematically represents the system voice prompt; storing the acoustic prompt model in<br>
a speech recognizer; and storing a target vocabulary model in the speech recognizer that<br>
includes models of a plurality of command words. The method also includes supplying<br>
the input signal to a comparer in the speech recognizer; comparing the input signal to the<br>
acoustic target vocabulary model and the acoustic prompt model to identify which model<br>
provides a best match for the input signal; ignoring the best match if the acoustic prompt<br>
model provides the best match; and accepting the best match if the acoustic target<br>
vocabulary model provides the best match. The method also includes supplying to an<br>
action table, any command word corresponding to the best match provided by the acoustic<br>
target vocabulary model; identifying from the action table, an action corresponding to the<br>
supplied command word; halting the system voice prompt; and initiating the identified<br>
action.<br>
In yet another aspect, the present invention is directed to a speech recognizer for<br>
recognizing input command words while suppressing speech recognition errors, wherein a<br>
signal input to the speech recognizer includes an echo from a system voice prompt<br>
combined with user input speech. The speech recognizer includes an acoustic vocabulary<br>
model that mathematically represents at least one command word; an acoustic prompt<br>
model that mathematically represents the system voice prompt; and a comparer that<br>
receives the input signal and compares the input signal to the acoustic vocabulary model<br>
and to the acoustic prompt model to determine which model provides a best match for the<br>
input signal. The comparer accepts the best match if the acoustic target vocabulary model<br>
provides the best match, and ignores the best match if the acoustic prompt model provides<br>
the best match. The speech recognizer may also use a silence model and traditional<br><br><br>
garbage models such as noise models and out-of-vocabulary word models to reduce the<br>
likelihood that noises and out-of-vocabulary words in the user utterance will be mapped<br>
erroneously onto active vocabulary words.<br>
In still yet another aspect, the present invention is directed to a speech recognition system<br>
for suppressing speech recognition errors and improving word accuracy. The system<br>
enables a user of a communication device to interrupt a system voice prompt with<br>
command words that halt the voice prompt and initiate desired actions. The system<br>
includes means for generating an acoustic model of the system voice prompt that<br>
mathematically represents the system voice prompt; an acoustic vocabulary model<br>
comprising mathematical models of a plurality of command words; and a comparer that<br>
receives an input signal comprising both user speech and an echo of the system voice<br>
prompt, and compares the input signal to the acoustic vocabulary model and to the<br>
acoustic prompt model to determine which model provides a best match for the input<br>
signal. The comparer accepts the best match if the acoustic target vocabulary model<br>
provides the best match, and ignores the best match if the acoustic prompt model provides<br>
the best match. The system also includes an action table that receives a command word<br>
from the comparer upon a determination by the comparer that the acoustic target<br>
vocabulary model provides the best match. The action table associates the received<br>
command word with a corresponding action, and notifies an associated network to initiate<br>
the corresponding action, and to halt the system voice prompt.<br>
Brief Description of the Drawings<br>
FIG. 1 is a simplified block diagram of an Automated Speech Recognition (ASR) system<br>
suitable for use with the present invention;<br>
FIG. 2 is a simplified block diagram of a speech recognizer in a preferred embodiment of<br>
the present invention;<br>
FIG. 3 is a flow chart illustrating the steps of the preferred embodiment of the method of<br>
the present invention;<br><br><br>
FIG. 4 is a graph of false acceptance rate of the speech recognizer as a function of the<br>
level of attenuation of an acoustic car echo, illustrating the effect of attenuating the<br>
prompt for prompt model training, for an exemplary scenario utilizing a car echo transfer<br>
function and car noise at 15 dB;<br>
FIG. 5 is a graph of false acceptance rate of the speech recognizer as a function of the<br>
level of attenuation of an acoustic car echo, comparing the false acceptance rates with and<br>
without the use of a prompt model, when only the acoustic prompt echo is present, for an<br>
exemplary scenario utilizing a car echo transfer function and car noise at 15 dB;<br>
FIG. 6 is a graph of word correct rate of the speech recognizer as a function of the level of<br>
attenuation of an acoustic car echo, comparing word correct rates with and without the use<br>
of a prompt model, when user input is present in addition to the acoustic prompt echo; and<br>
FIG. 7 is a graph of word accuracy rate of the speech recognizer as a function of the level<br>
of attenuation of an acoustic car echo, comparing word accuracy rates with and without<br>
the use of a prompt model, when user input is present in addition to the acoustic prompt<br>
echo.<br>
Detailed Description of Embodiments<br>
Contrary to traditional approaches utilizing an echo cancellation algorithm, the present<br>
invention generates an acoustic prompt model and utilizes the acoustic prompt model as a<br>
"garbage" model in parallel with the active vocabulary words of the speech recognizer to<br>
compensate within the recognizer for the acoustic echo of the prompt. A specific acoustic<br>
prompt model is generated from the announcement to compensate for the false alarms of<br>
the recognizer. Since the system prompt is typically announced using a single speaker's<br>
voice, a speaker-dependent acoustic model of the system prompt is preferably generated.<br>
In this process, the acoustic prompt model is trained using the speech signal of the specific<br>
system prompt, and thus represents the system prompt, including speech and any other<br>
sounds (e.g., beeps and the like) that are present in the prompt. Alternatively, a speaker-<br>
independent acoustic model of the system prompt may be generated. In this process, the<br><br><br>
orthographic text of the prompt is utilized to build the prompt model by concatenating the<br>
appropriate phonetic units.<br>
The prompt model may be created through offline training in advance, or by online<br>
generation. In the first alternative, the various system prompts are known, and the<br>
acoustic prompt models are trained in advance. This method is suitable for both speaker-<br>
independent phonetic training and speaker-dependent training using the speech signal of<br>
the prompt. The prompt server is provided with a list of system prompts, and the speech<br>
recognizer is provided with a corresponding list of acoustic prompt models. When a<br>
prompt is to be played, a system controller instructs the prompt server to play, for example<br>
prompt "n", and instructs the speech recognizer to use the corresponding acoustic prompt<br>
model "n". When utilizing speaker-independent phonetic training, phonetic units are<br>
determined from the various system prompts, and may be stored in a database accessible<br>
by the recognizer. When the controller informs the recognizer of the prompt model to be<br>
utilized, the controller sends the orthographic text of the prompt to the recognizer. The<br>
recognizer then builds the prompt model by concatenating the appropriate phonetic units,<br>
which are selected utilizing the orthographic text of the prompt. When using online<br>
generation, the acoustic prompt model is generated immediately before the speech<br>
recognizer is started. The system controller sends the speech signal of the prompt to the<br>
recognizer, and the recognizer builds up the prompt model from the speech signal and<br>
starts recognition.<br>
During recognition, the Viterbi path is allowed to leave the acoustic prompt model in each<br>
state to enable the beginning of a user input. Thus, as soon as the user utters a valid<br>
command word, the Viterbi path leaves the acoustic prompt model and enters an active<br>
vocabulary word model. Thus, the prompt model needs only to be partially recognized<br>
until the user interrupts the system by his user input.<br>
FIG. 1 is a simplified block diagram of an Automated Speech Recognition (ASR) system<br>
10 suitable for use with the present invention. A prompt server 11 plays a system voice<br>
prompt. At the same time, a speech recognizer 12 analyzes speech input from the user.<br>
The user's speech input 13 and background noise 14 are input through a microphone 15.<br><br><br>
Function H(z) 16 represents the microphone and voice input channel characteristics. In<br>
the experimental verification of the present invention, no special microphone or voice<br>
input channel characteristics, H(z), were considered because the user's speech input was<br>
considered to implicitly contain the microphone and channel characteristics.<br>
In addition to the noisy user input, two types of echos also arrive at the speech recognizer<br>
12. Function L(z) 17 models the path of the line echo, which is caused by electrical<br>
feedback in the telephone device and telephone network. In the development of the<br>
present invention, the line echo, L(z), was assumed to be negligible with respect to the<br>
acoustic echo, especially in hands-free environments. Function P(z) 18 represents an echo<br>
transfer function for the acoustic echo of the prompt. The acoustic transfer function is a<br>
mathematical description of how the sound waves of the prompt are modified when they<br>
travel from the loudspeaker to the microphone on the user's device. It is particularly<br>
important to consider this type of echo, and the echo transfer function, when operating in a<br>
hands-free environment where the user's loudspeaker is set at a high volume. Of course,<br>
the characteristics of the echo transfer function depend on the actual physical environment<br>
of the user.<br>
As noted above, a system controller 19 instructs the prompt server 11 to play a selected<br>
system voice prompt, and instructs the speech recognizer 12 to use an acoustic prompt<br>
model corresponding to the selected voice prompt. Thus, a system prompt database 11a<br>
may be associated with the prompt server, and may be accessed by the prompt server<br>
when directed by the system controller. The system prompt database may be internal or<br>
external to the prompt server. Likewise, a phonetic units database 12a may be associated<br>
with the speech recognizer. The phonetic units database may be internal or external to the<br>
speech recognizer. When utilizing offline, speaker-independent phonetic training of the<br>
acoustic prompt model, the system controller 19 provides the speech recognizer with the<br>
orthographic text of the selected prompt. The speech recognizer 12 then retrieves the<br>
appropriate phonetic units from the phonetic units database 12a and concatenates them to<br>
build the acoustic prompt model corresponding to the selected system voice prompt.<br><br><br>
For the experiments discussed below, sound measurements were taken under three<br>
different sets of real-world conditions, and three different echo transfer functions were<br>
defined and utilized. The three conditions were a cafeteria, a conference room, and a car.<br>
Additionally, two types of noise (babble noise and car noise) were added to the input<br>
signal. The noise inputs were added to the system with varying attenuation of the noise<br>
signal so that experiments could be carried out at varying signal-to-noise ratios (SNRs).<br>
FIG. 2 is a simplified block diagram of the speech recognizer 12 in a preferred<br>
embodiment of the present invention. The speech recognizer works by matching a set of<br>
acoustic models 21-26 against an input user utterance 27. A comparer 28 receives the<br>
input user utterance and compares the input to the various models. The comparer may<br>
include a simple connected word recognition grammar, an arbitrary grammar, or a so-<br>
called "language model" to make the comparison. Target vocabulary acoustic models 21<br>
represent the words that the speech recognizer should be able to recognize. These models<br>
form the active vocabulary (command words) of the speech recognizer. In addition to<br>
these target vocabulary acoustic models, other acoustic models are used within the<br>
recognizer to represent the rest of the speech signal. First, there is a "silence" model 22<br>
that is an acoustic representation of silence (i.e., no user input and no noise).<br>
Additionally, there are "garbage" models 23 that include models of common noises 24<br>
such as babble or car noise, and/or models of Out-Of-Vocabulary (OOV) words 25 that<br>
are not among the active vocabulary. The use of these garbage models reduces the<br>
likelihood that noises and OOV words in the user utterance will be mapped erroneously<br>
onto vocabulary words. The present invention adds an acoustic prompt model 26 to the<br>
speech recognizer as an additional garbage model. This model reduces the likelihood that<br>
words in a system prompt echo contained in the user utterance will be mapped<br>
erroneously onto vocabulary words. If any of the garbage models gives the best match<br>
during the recognition process, the matching noises, OOV words, and system prompt<br>
words are ignored, and no recognition errors occur.<br>
When the best match during the recognition process is an active vocabulary command<br>
word, the recognizer enters an action table 29 where the recognized word is associated<br>
with a corresponding action 30. The speech recognizer 12 may then send the<br><br><br>
corresponding action to the network to be executed. In a barge-in situation, then network<br>
then sends a command to the prompt server 11 to halt the system voice prompt.<br>
FIG. 3 is a flow chart illustrating the steps of the preferred embodiment of the method of<br>
the present invention. At step 31, the speech recognizer 12 is prepared for the recognition<br>
task by storing in it, the target vocabulary acoustic models 21, the silence model 22, the<br>
noise models 24, and the OOV words models 25. At step 32, the acoustic prompt model<br>
26 is generated or "trained". As noted above, the prompt model may be created through<br>
offline training in advance, or by online generation. At step 33, the acoustic prompt<br>
model 26 is stored in, or otherwise made available to, the speech recognizer 12 as an<br>
additional garbage model 23. At step 34, a user input signal is supplied to the speech<br>
recognizer. The input signal includes the user's speech input along with background<br>
noise, and microphone and voice input channel characteristics. In a barge-in situation in<br>
which a system voice prompt is playing, the input signal also includes line echo caused by<br>
electrical feedback in the telephone device and telephone network, and the system voice<br>
prompt echo.<br>
At step 35, the speech recognizer compares the input signal against the various acoustic<br>
models to determine a best match. At step 36, it is determined which model gives the best<br>
match during the recognition process. If the silence, noise, OOV words, or acoustic<br>
prompt model gives the best match, the method moves to step 37, and then ignores the<br>
match at step 38, and no recognition errors occur. However, if the target vocabulary<br>
acoustic model gives the best match, the method moves to step 39 and then at step 40,<br>
sends the recognized word to the action table 29. At step 41, the action table associates<br>
the recognized word with a corresponding action and sends the corresponding action to<br>
the network for execution. At step 42, the method concludes by sending a halt command<br>
from the network to the prompt server 11, and executing the corresponding action within<br>
the network.<br>
Experimental Results<br>
A large number of experiments, with a wide range of conditions (i.e., type of noise, type<br>
of echo, SNR) were performed. For each scenario, a baseline experiment without acoustic<br><br><br>
prompt models was compared to an experiment with acoustic prompt models. Several<br>
different recordings of 32 male and 32 female users were utilized as user input for the<br>
tests.<br>
The recognition task analyzed in the experiments was typical for voice-controlled<br>
telephone applications. The active vocabulary consisted of five command words. In<br>
addition, a set of garbage models was used to cope with unexpected user input,<br>
background noise (i.e., babble noise and car noise), and the echo of the announcement. In<br>
the experiments, all command words, garbage models, and the prompt model (when<br>
applicable) were inserted in parallel into a simple connected word recognition grammar.<br>
The prompts for the experiments were generated using a state-of-the-art text-to-speech<br>
system producing naturally sounding voice announcements. For this purpose, fourteen<br>
different prompts with durations between one and seven seconds were chosen and<br>
synthesized by one male and one female speaker. As is typical for telephone applications,<br>
the prompts themselves contained vocabulary command words that the recognizer is<br>
trying to hear from the user's speech input. For example, "help" is a vocabulary<br>
command word, and "say help for help" is one of the prompts. Thus, a goal of the<br>
invention is to enable the recognizer to reject the word "help" when it is contained in a<br>
prompt, but to accept the word "help" as a command word when it is spoken by the user.<br>
Basically, two types of experiments were conducted. First, only the prompt echo, with no<br>
user input, was supplied to the speech recognizer. This type of experiment measures the<br>
ability of the recognizer to ignore the prompt echo to avoid false acceptances. Second,<br>
both the prompt echo and the user input were supplied to the speech recognizer. The user<br>
input was randomly varied to start at different times within the boundaries of the prompt,<br>
to realistically represent a barge-in case. This type of experiment measures the ability of<br>
the recognizer to reliably accept a user-input command word while minimizing false<br>
acceptances generated by command words within the prompt echo.<br>
The prompt echo supplied to the speech recognizer is generally attenuated, and is<br>
convolved with a specific room impulse response. Therefore, the invention utilizes an<br><br><br>
attenuated version of the original prompt signal to train a speaker-dependent prompt<br>
model. This procedure helps reduce the mismatch between training and test for prompt<br>
models, and therefore improves recognition performance.<br>
The graph in FIG. 4 illustrates the effect of attenuating the prompt for prompt model<br>
training for an exemplary scenario utilizing the car echo transfer function and car noise at<br>
15 dB. The graph shows the false acceptance rate of the speech recognizer as a function<br>
of the level of echo attenuation when only the prompt is present (i.e., there is no user<br>
input). Each of the curves represents the false acceptance rate of the speech recognizer for<br>
prompt model training at different attenuation levels of the original signal. The graph<br>
illustrates that the best results were obtained when the prompt model was trained at a 20<br>
dB attenuation level. Other experiments, utilizing the cafeteria and the conference room<br>
echo transfer functions, also showed that, on average, training at the 20 dB attenuation<br>
level yielded the best results for the complete range of echo attenuation. Thus, this<br>
procedure should be applied in all cases in which the actual attenuation is unknown.<br>
Training of the prompt model at 20 dB attenuation was also used for all of the following<br>
experiments.<br>
The graph in FIG. 5 illustrates the results of an exemplary experiment comparing the false<br>
acceptance rates without the use of a prompt model and with the use of a prompt model,<br>
when only the acoustic prompt echo was present (i.e., there was no user input). The<br>
experiment utilized the car echo transfer function and car noise at 15 dB. Each of the<br>
curves represents the false acceptance rate of the speech recognizer as a function of the<br>
level of attenuation of the acoustic car echo. It can be seen that the use of a prompt model<br>
drastically improved the false acceptance rate, virtually eliminating false acceptances at<br>
all levels of attenuation.<br>
The graph in FIG. 6 illustrates the results of an exemplary experiment comparing the word<br>
correct rates without the use of a prompt model and with the use of a prompt model, when<br>
user input is present in addition to the acoustic prompt echo. Only substitutions and<br>
deletions were considered as errors. FIG. 6 again illustrates a scenario utilizing the car<br>
echo transfer function and car noise at 15 dB. Each of the curves represents the word<br><br><br>
correct rate of the speech recognizer as a function of the level of attenuation of the<br>
acoustic car echo. It is to be expected that as a tradeoff for eliminating false acceptances,<br>
the word correct rate would be degraded. The experiment showed that for a low level of<br>
echo attenuation of 10 dB, the word correct rate decreased from 93.5% to 88.9% when<br>
using the prompt model. However, for higher levels of attenuation (for example, 20 dB<br>
and above), use of the prompt model had a negligible effect on the word correct rate.<br>
The graph in FIG. 7 illustrates the results of an exemplary experiment comparing word<br>
accuracy rates without the use of a prompt model and with the use of a prompt model,<br>
when user input is present in addition to the acoustic prompt echo. Substitutions,<br>
deletions, and insertions were considered as errors. FIG. 7 again illustrates a scenario<br>
utilizing the car echo transfer function and car noise at 15 dB. Each of the curves<br>
represents the word accuracy rate of the speech recognizer as a function of the level of<br>
attenuation of the acoustic car echo. When analyzing the effect of the prompt model on<br>
word accuracy, the benefits of the present invention become even more apparent. The<br>
small negative effect of the prompt model on the word correct rate (see FIG. 6) is more<br>
than overcome by the enormous reduction of false acceptances (i.e., a reduction of<br>
insertions) (see FIG. 5). The overall result is a very significant improvement of<br>
recognition accuracy when the prompt model is utilized. For example, the recognition<br>
accuracy rises from 83.1% to 90.6% when the echo attenuation is 20 dB. For lower levels<br>
of attenuation, the improvement is even greater.<br>
As will be recognized by those skilled in the art, the innovative concepts described in the<br>
present application can be modified and varied over a wide range of applications.<br>
Accordingly, the scope of patented subject matter should not be limited to any of the<br>
specific exemplary teachings discussed above, but is instead defined by the following<br>
claims.<br><br><br>
WE CLAIM:<br>
1.	A method of suppressing speech recognition errors in a speech recognition system<br>
in which an input signal includes an echo from a system voice prompt combined with user input<br>
speech, said method comprising the steps of:<br>
generating an acoustic model (26) of the system voice prompt, said acoustic prompt<br>
model (26) mathematically representing the system voice prompt;<br>
supplying the input signal to a speech recognizer (12) having an acoustic model of a<br>
target vocabulary, said acoustic target vocabulary model (21) mathematically representing at<br>
least one command word;<br>
comparing the input signal to the acoustic prompt model (26) and to the acoustic target<br>
vocabulary model (21) by means of comparer (28);<br>
determining which of the acoustic prompt model (26) and the acoustic target vocabulary<br>
model (21) provides a best match for the input signal during the comparing step by means of said<br>
speech recognizer (12);<br>
accepting the best match if the acoustic target vocabulary model (21) provides the best<br>
match; and<br>
ignoring the best match if the acoustic prompt model (26) provides the best match.<br>
2.	The method as claimed in claim 1, wherein the step of generating an acoustic<br>
model of the system voice prompt is performed in advance of the comparing step and includes<br>
the steps of:<br>
determining phonetic units utilized in the system prompt;<br>
storing the phonetic units in a phonetic unit database accessible by the speech recognizer<br>
(12);<br>
providing the speech recognizer (12) with an orthographic text of the prompt prior to<br>
playing the prompt; and<br>
building the prompt model by the speech recognizer (12) , said speech recognizer<br>
selecting and concatenating appropriate phonetic units based on the orthographic text of the<br>
prompt.<br><br>
3.	The method as claimed in claim 2, wherein a plurality of system voice prompts<br>
are stored in a system prompt database accessible by a prompt server that plays selected prompts,<br>
and phonetic units associated with the plurality of system voice prompts are stored in the<br>
phonetic unit database, and wherein the method further comprises, prior to supplying the input<br>
signal to the speech recognizer (12), the steps of:<br>
instructing the prompt server (11) to select and play a selected system prompt;<br>
informing the speech recognizer (12) which system prompt is going to be played; and<br>
retrieving by the speech recognizer (12), phonetic units from the phonetic unit database<br>
that are appropriate for an acoustic prompt model corresponding to the selected system prompt.<br>
4.	The method as claimed in claim 1, wherein the step of generating an acoustic<br>
model of the system voice prompt includes the steps of:<br>
sending the speech signal of the system prompt to the speech recognizer (12); and<br>
generating the acoustic prompt model (26) from the speech signal immediately before the<br>
comparing step.<br>
5.	The method as claimed in claim 1, wherein the step of generating an acoustic<br>
model of the system voice prompt includes generating the acoustic prompt model (26) at an<br>
attenuation level of approximately 20 dB relative to the system voice prompt.<br>
6.	The method as claimed in claim 1, further comprising the steps of:<br>
comparing the input signal to a silence model , at least one out-of-vocabulary word<br>
model, and at least one noise model;<br>
determining whether one of the silence, out-of-vocabulary, or noise models provides the<br>
best match during the comparing step; and<br>
ignoring the best match if one of the silence, out-of-vocabulary, or noise models provides<br>
the best match.<br>
7.	The method as claimed in claim 6, wherein the step of comparing the input signal<br>
to a silence model, at least one out-of-vocabulary word model, and at least one noise model<br>
includes comparing the input signal to a noise model that represents background babble.<br><br>
8.	The method as claimed in claim 6, wherein the step of comparing the input signal<br>
to a silence model, at least one out-of-vocabulary word model, and at least one noise model<br>
includes comparing the input signal to a noise model that represents background car noise.<br>
9.	The method as claimed in claim 1, wherein the step of supplying the input signal<br>
to the speech recognizer (12) includes supplying to a simple connected word recognition<br>
grammar, the input signal in parallel with the acoustic target vocabulary model and the acoustic<br>
prompt model.<br>
10.	A method of suppressing speech recognition errors and improving word accuracy<br>
in a speech recognition system that enables a user of a communication device to interrupt a<br>
system voice prompt with command words that halt the voice prompt and initiate desired actions,<br>
said method comprising the steps of:<br>
generating an acoustic model (26) of the system voice prompt, said acoustic prompt<br>
model mathematically representing the system voice prompt;<br>
storing the acoustic prompt model in a speech recognizer (12);<br>
storing an acoustic target vocabulary model (21) in the speech recognizer (12), said<br>
acoustic target vocabulary model (21) including models of a plurality of command words;<br>
supplying the input signal to a comparer in the speech recognizer (12);<br>
comparing the input signal to the acoustic target vocabulary model (21) and the acoustic<br>
prompt model (26) to identify which model provides a best match for the input signal;<br>
ignoring the best match if the acoustic prompt model provides the best match;<br>
accepting the best match if the acoustic target vocabulary model provides the best match;<br>
supplying to an action table, any command word corresponding to the best match<br>
provided by the acoustic target vocabulary model;<br>
identifying from the action table, an action corresponding to the supplied command word;<br>
halting the system voice prompt; and<br>
initiating the identified action.<br><br>
11.	A speech recognizer (12) for recognizing input command words while<br>
suppressing speech recognition errors, wherein a signal input to the speech recognizer includes<br>
an echo from a system voice prompt combined with user input speech, said speech recognizer<br>
comprising:<br>
an acoustic vocabulary model that mathematically represents at least one command word;<br>
an acoustic prompt model that mathematically represents the system voice prompt; and<br>
a comparer (28) that receives the input signal and compares the input signal to the<br>
acoustic vocabulary model and to the acoustic prompt model to determine which model provides<br>
a best match for the input signal, said comparer accepting the best match if the acoustic target<br>
vocabulary model provides the best match, and ignoring the best match if the acoustic prompt<br>
model provides the best match.<br>
12.	The speech recognizer (12) as claimed in claim 11, further comprising means for<br>
generating the acoustic prompt model from a known text.<br>
13.	The speech recognizer (12) as claimed in claim 11, further comprising means for<br>
generating the acoustic prompt model from the speech signal of the system voice prompt prior to<br>
playing the prompt.<br>
14.	The speech recognizer (12) as claimed in claim 11, further comprising means for<br>
generating the acoustic prompt model at an attenuation level of approximately 20 dB relative to<br>
the system voice prompt.<br>
15.	The speech recognizer (12) as claimed in claim 11, further comprising a silence<br>
model, at least one out-of-vocabulary word model, and at least one noise model connected to the<br>
comparer in parallel with the acoustic vocabulary model and the acoustic prompt model, wherein<br>
the comparer also determines whether the best match is provided by the silence model, the at<br>
least one out-of-vocabulary word model, or the at least one noise model, and if so, ignores the<br>
best match.<br><br>
16.	The speech recognizer (12) as claimed in claim 15, wherein the at least one noise<br>
model includes a noise model that represents background babble.<br>
17.	The speech recognizer (12) as claimed in claim 15, wherein the at least one noise<br>
model includes a noise model that represents background car noise.<br>
18.	The speech recognizer (12) as claimed in claim 11, wherein the comparer<br>
includes a comparison function selected from a group consisting of:<br>
an arbitrary grammar;<br>
a simple connected word recognition grammar; and<br>
a language model.<br>
19.	A speech recognition system (10) for suppressing speech recognition errors and<br>
improving word accuracy, said system enabling a user of a communication device to interrupt a<br>
system voice prompt with command words that halt the voice prompt and initiate desired actions,<br>
said system comprising:<br>
means for generating an acoustic model of the system voice prompt, said acoustic prompt<br>
model (26) mathematically representing the system voice prompt;<br>
an acoustic vocabulary model comprising mathematical models of a plurality of<br>
command words;<br>
a comparer (28) that receives the input signal and compares the input signal to the<br>
acoustic vocabulary model and to the acoustic prompt model to determine which model provides<br>
a best match for the input signal, said comparer accepting the best match if the acoustic target<br>
vocabulary model provides the best match, and ignoring the best match if the acoustic prompt<br>
model provides the best match; and<br>
an action table that receives a command word from the comparer (28) upon a<br>
determination by the comparer that the acoustic target vocabulary model provides the best match,<br>
said action table associating the received command word with a corresponding action, and<br>
notifying an associated network to initiate the corresponding action, and to halt the system voice<br>
prompt.<br><br>
20.	The speech recognition system (10) as claimed in claim 19, wherein the means for<br>
generating the acoustic prompt model includes means for generating the acoustic prompt model<br>
from a known text.<br>
21.	The speech recognition system (10) as claimed in claim 19, wherein the means<br>
for generating the acoustic prompt model includes means for generating the acoustic prompt<br>
model from the speech signal of the system voice prompt prior to playing the prompt.<br>
22.	The speech recognition system (10) as claimed in claim 19, wherein the means<br>
for generating the acoustic prompt model includes means for generating the acoustic prompt<br>
model at an attenuation level of approximately 20 dB relative to the system voice prompt.<br><br><br>
A system and method enabling acoustic barge-in during a voice prompt in a<br>
communication system. An acoustic prompt model is trained to represent the system prompt<br>
using the specific speech signal of the prompt. The acoustic prompt model is utilized in a<br>
speech recognizer in parallel with the recognizer's active vocabulary words to suppress the<br>
echo of the prompt within the recognizer. The speech recognizer may also use a silence<br>
model and traditional garbage models such as noise models and out-of-vocabulary word<br>
models to reduce the likelihood that noises and out-of-vocabulary words in the user utterance<br>
will be mapped erroneously onto active vocabulary words.<br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1hYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1jbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1kZXNjcmlwdGlvbiBjb21wbGV0ZS5wZGY=" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-description complete.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1kcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1mb3JtLTEucGRm" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1mb3JtLTIucGRm" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1mb3JtLTMucGRm" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1mb3JtLTUucGRm" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1pbnRlcm5hdGlvbmFsIHB1YmxpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-international publication.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDA0MTUta29sbnAtMjAwNi1wY3QgZm9ybXMucGRm" target="_blank" style="word-wrap:break-word;">00415-kolnp-2006-pct forms.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtKDE5LTA5LTIwMTEpLUFNQU5ERUQgQ0xBSU1TLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-(19-09-2011)-AMANDED CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtKDE5LTA5LTIwMTEpLUNPUlJFU1BPTkRFTkNFLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-(19-09-2011)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtKDE5LTA5LTIwMTEpLU9USEVSUy5wZGY=" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-(19-09-2011)-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtKDI0LTExLTIwMTEpLUNPUlJFU1BPTkRFTkNFLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-(24-11-2011)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LWtvbG5wLTIwMDYtYW1hbmRlZCBjbGFpbXMtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">415-kolnp-2006-amanded claims-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtQU1BTkRFRCBDTEFJTVMucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-AMANDED CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtQ0FOQ0VMTEVEIFBBR0VTLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-CANCELLED PAGES.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtQ09SUkVTUE9OREVOQ0UgMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-CORRESPONDENCE 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtQ09SUkVTUE9OREVOQ0UgMS4yLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-CORRESPONDENCE 1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LWtvbG5wLTIwMDYtY29ycmVzcG9uZGVuY2UgMS40LnBkZg==" target="_blank" style="word-wrap:break-word;">415-kolnp-2006-correspondence 1.4.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LWtvbG5wLTIwMDYtY29ycmVzcG9uZGVuY2UtMS4zLnBkZg==" target="_blank" style="word-wrap:break-word;">415-kolnp-2006-correspondence-1.3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtQ09SUkVTUE9OREVOQ0UucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtREVTQ1JJUFRJT04gKENPTVBMRVRFKSAxLjEucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-DESCRIPTION (COMPLETE) 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRFJBV0lOR1MgMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-DRAWINGS 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRVhBTUlOQVRJT04gUkVQT1JULnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRk9STSAxLjEucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-FORM 1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRk9STSAxOC5wZGY=" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-FORM 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRk9STSAyLjEucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-FORM 2.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRk9STSAzLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-FORM 3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtRk9STSA1LnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-FORM 5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1BBLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GPA.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1BQlNUUkFDVC5wZGY=" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1DTEFJTVMucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1ERVNDUklQVElPTiAoQ09NUExFVEUpLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1EUkFXSU5HUy5wZGY=" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-DRAWINGS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1GT1JNIDEucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1GT1JNIDIucGRm" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtR1JBTlRFRC1TUEVDSUZJQ0FUSU9OLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-GRANTED-SPECIFICATION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtT1RIRVJTLnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LWtvbG5wLTIwMDYtcGEucGRm" target="_blank" style="word-wrap:break-word;">415-kolnp-2006-pa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtUkVQTFkgVE8gRVhBTUlOQVRJT04gUkVQT1JULnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-REPLY TO EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=NDE1LUtPTE5QLTIwMDYtVFJBTlNMQVRFRCBDT1BZIE9GIFBSSU9SSVRZIERPQ1VNRU5ULnBkZg==" target="_blank" style="word-wrap:break-word;">415-KOLNP-2006-TRANSLATED COPY OF PRIORITY DOCUMENT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=YWJzdHJhY3QtMDA0MTUta29sbnAtMjAwNi5qcGc=" target="_blank" style="word-wrap:break-word;">abstract-00415-kolnp-2006.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="251742-rack-for-a-switchgear-cabinet.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="251744-a-composite-microcannula-device-with-proximal-and-distal-ends-for-access-and-advancement-within-the-suprachoroidal-space-of-the-eye.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>251743</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>415/KOLNP/2006</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>14/2012</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>06-Apr-2012</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>30-Mar-2012</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>23-Feb-2006</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>TELEFONAKTIEBOLAGET LM ERICSSON [PUBL]</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>S-164 83 STOCKHOLM, SWEDEN</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>BRCKNER, RAYMOND</td>
											<td>ULMER STRASSE 14, 89134 BLAUSTEIN, GERMANY</td>
										</tr>
										<tr>
											<td>2</td>
											<td>JUNKAWITSCH, JOCHEN</td>
											<td>SONNENSTRASSE 24, 92676 ESCHENBACH, GERMANY</td>
										</tr>
										<tr>
											<td>3</td>
											<td>REINHARD, KLAUS</td>
											<td>SCHLOSSPLATZ 5, 99310 ARNSTADT, GERMANY</td>
										</tr>
										<tr>
											<td>4</td>
											<td>DOBLER, STEFAN</td>
											<td>OBERER GRENZWEG 32 B, 91077 NEUNKIRCHEN AM BRAND, GERMANY</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 15/22</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/EP2004/007185</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2004-07-02</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>10/631,985</td>
									<td>2003-07-31</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/251743-system-and-method-enabling-acoustic-barge-in by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 13:45:09 GMT -->
</html>
