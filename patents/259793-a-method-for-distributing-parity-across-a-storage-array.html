<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/259793-a-method-for-distributing-parity-across-a-storage-array by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 00:18:40 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 259793:A METHOD FOR DISTRIBUTING PARITY ACROSS A STORAGE ARRAY</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">A METHOD FOR DISTRIBUTING PARITY ACROSS A STORAGE ARRAY</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>ABSTRACT A METHOD FOR DISTRIBUTING PARITY ACROSS A STORAGE ARRAY A method for distributing parity across a storage array, the method comprising the steps of: adding a new storage device to a number of pre-existing storage devices of the array; dividing each storage device into blocks, the blocks being organized into stripes such that each stripe contains one block from each storage device; and distributing, by a processor (122) of a storage system (200) parity among blocks of the new and pre-existing storage devices without recalculation or moving of any blocks containing data, the method characterized in that the distribution of parity is by reassigning every Nth parity block (P) from each of the pre-existing storage devices (202-206) to the new storage device (208) to arrange each storage device of the array with approximately 1/N of the parity blocks (P), where N is equal to the number of the pre- ^ existing storage devices (202-206) plus the new storage device (208). Fig: 2 26</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>The present invention relates to a method for distributing parity across a storage array.<br>
FIELD OF THE INVENTION<br>
The present inventionrelates to airays of storage systems and, more specifically,<br>
to a system tliat efficiently assigns parity blocks within storage devices of a storage<br>
array,<br>
BACKGROUND OF THE INVENTION<br>
A storage system typically comprises one or more storage devices into which<br>
information may be entered, and ftom which information may be obtained, as desired.<br>
' The storage system includes a storage operating system tliatflmctionally organizes the<br>
^ system by, Mer alia, invoking storage operations m support of a storage service implemented<br>
by the system. The storage system may be implemented in accordance with<br>
' a variety of storage architectures including, but not limited to, a network-attached storage<br>
enviromnent, a storage area network and a disk assembly directly attached to a client<br>
or hbst computer. The storage devices arc typically disk drives organized as a disk<br>
ai-ray, wherein the term "disk" conunonly describes a self-contauied rotating magnetic<br>
media storage device. The term disk hi this context is synonymous with hard disk drive . '<br>
(HDD) or direct access storage device (DASD).<br>
Storage of information on the disk aiiay is preferably hnplemented as one or '<br>
more storage "voliunes" that comprises a cluster of physical disks, defining an overall<br>
logical arrangement of disk space. The disks within a volume are typically .organized<br>
as one or more groups, wherein each gimip is operated as a Redundant Array of Inde-<br>
^ pendent (or Inexpensive) Disks (RAID), Tn this context, a RAID group is defined as a<br>
number of disks and an address/block space associated with those disks. The term<br>
"RAID" and its various hnplementattons are well-laiown and disclosed in A Case for<br>
Redundant Arrays of Inexpensive Disks (RAID), by D. A. Patterson, G. A. Gibson and<br>
R. H. Katz, Proceedmgs of the International Conference on Management of Data .<br>
(SIGMOD), June 1988.<br>
2<br>
The storage operating system of the storage system may implement a file system<br>
to logically organize the information as a hierarchical structure of directories, files<br>
and blocks on the disks. For example, each "on-disk" file may be implemented as set<br>
i of data structures, i.e., disk blocks, configured to store information, such as the actual<br>
data for the file. The storage operating system may also implement a RAID system that<br>
'• ' manages the storage and retrieval of the information to and from the disks in accordance<br>
with write and read operations. There is typically a one-to-one mapping between<br>
the mfomiation stored on the disks in, e.g., a disk block number space, and the informa-<br>
1^ tion organized by the file system in, e.g., volume block number space.<br>
A common type of file system is a "vwite in-place" file system, an example of<br>
which is the conventional Berkeley fast file system. In a write in-place file system, the<br>
locations of the data structures, such as data blocks, on disk are typically fixed.<br>
Changes to the data blocks are made "in-place"; if an update to a file extends the quan-<br>
^ tity of data for the file, an additional data block is allocated. Another type of file system<br>
is a write-anywhere file system that does not overwrite data on disks. If a data<br>
block on disk is retrieved (read) firom disk into a memory of the storage system and<br>
"dirtied" with new data, the data block is stored (written) to a new location on disk to<br>
; thereby opthnize write performance. A write-anywhere file system may initially assume<br>
an optimal layout such that the data is substantially contiguously arranged on<br>
disks. The optimal disk layout results in efficient access operations, particularly for<br>
W sequential read operations, directed to the disks. An example of a vwite-kn3nvhere file<br>
system that is configured to operate on a storage system is the Write Anywhere File<br>
Layout (WAFL™) file system availabie fiom Network Appliance, Inc., Sunnyvale,<br>
' Califbmia.<br>
: Most RAID implementations enhance the reliability/integrity of data storage<br>
through the redundant writing of data "stripes" across a given number of physical disks<br>
in the RAID group, and the appropriate storing of redundant information with respect to<br>
, the striped data. The redundant information, e.g., parity information, enables recovery<br>
: of data lost when a disk fails. A parity value may be computed by sumnung (usually<br>
modulo 2) data of a particular word size (usually one bit) across a number of similar<br>
disks holding different data and then stormg the results on an additional shnilar disk.<br>
-<br>
•<br>
- - ,<br>
That is, parity may be computed on vectors 1-bit wide, composed of bits in correspond-'<br>
ing positions on each of the disks. When computed on vectors 1 -bit wide, the parity<br>
; can be either the computed sum or its complement; these are referred to as even and<br>
odd parity respectively. Addition and subtraction on 1-bit vectors are both equivalent<br>
to exclusive-OR (XOR) logical operations. The data is then protected against the loss<br>
; of any one of the disks, or of any portion of the data on any one of the disks. If the disl^<br>
- storing the parity is lost, the parity can be regenerated from the data. If one of the data<br>
disks is lost, the data can be regenerated by adding the contents of the surviving data<br>
• ^ disks together and then subtracting the result from the stored parity.<br>
Typically, the disks are divided into parity groups, each of which comprises one<br>
or more data disks and a parity disk. A parity set is a set of blocks, including several<br>
data blocks and one parity block, where the parity block is the XOR of all the data<br>
blocks. A parity group is a set of disks from which one or more parity sets are selected.<br>
i<br>
The disk space is divided into stripes, with each stripe containing one block from each<br>
disk. Theblocksof a stripe are usually at the same locations on each disk in the parity<br>
group. Within a stripe, all but one block contains data ("data blocks"), while the one<br>
block contains parity ("parity block") computed by the XOR of all the data.<br>
As used herein, the term "encoding" means the computation of a redundancy<br>
value over a predetermined subset of data blocks, whereas the term "decoding" means<br>
^ the reconstruction of a data or parity block by the same process as the redundancy<br>
computation using a subset of data blocks and redundancy values. If one disk fails in<br>
the parity group, the contents of that disk can be decoded (reconstructed) on a spare<br>
disk or disks by adding all the contents of the remaining data blocks and subtracting the<br>
result from the parity block. Since two's complement addition and subtraction over 1-<br>
• bit fields are both equivalent to XOR operations, this reconstruction consists of the<br>
XOR of all the surviving data and parity blocks. Shnilarly, if the parity disk is lost, it<br>
can be recomputed in the same way from the surviving data.<br>
. • . •• • .<br>
If Cie parity blocks are all stored on one disk, thereby providing a single disk<br>
that contains all (and only) parity information, a RAID-4 level implementation is provided.<br>
The RAID-4 implementation is conceptually the simplest form of advanced<br>
RAID (i.6i, more than stripmg and mirroring) since it fixes the position of the parity<br>
information in each RAID group. In particular, a RAID-4 implementation provides<br>
protection from single disk errors with a single additional disk, while making it easy to<br>
incrementally add data disks to a RAID group.<br>
: If the parity blocks are contained within different disks in each stripe, in a rotat-<br>
\ ing pattern, then the implementation is RAID-5. Most commercial implementations<br>
that use advanced RAID techniques use RAID-5 level implementations, which distribute<br>
the parity mformation. A motivation for choosing a RAID-5 implementation is<br>
that, for most static file systems, using a RAID-4 implementation would limit write<br>
w throughput. Such static file systems tend to scatter write data across many stripes in the<br>
disk array, causing the parity disks to seek for each stripe written. However, a writeanywhere<br>
file system, such as the WAFL file system, does not have this issue since it<br>
concentrates write data on a few nearby stripes.<br>
Use of a RAID-4 level implementation in a write-anywhere file system is a desirable<br>
way of allowing incremental capacity increase ^vhile retaining performance;<br>
however there are some "hidden" downsides. First, where all the disks in a RAID<br>
group are available for servicing read traffic in a RAID-5 implementation, one of the<br>
i disks (the parity disk) does not participate in such traffic in the RAID-4 implementation.<br>
Although this effect is insigniBcant for large RAID group sizes, those group sizes<br>
have been decreasing because of, e.g., a limited number of available disks or increasing<br>
^ reconstruction times of larger disks. As disks continue to increase in size, smaller<br>
RAID group configurations become more attractive. But this increases the fraction of<br>
; disks unavailable to service read operations in a RAID-4 configuration. The use of a<br>
RAID-4 level implementation may therefore result in significant loss of read operations<br>
per second. Second, when a new disk is added to a fiiU volume, the write anywhere file<br>
system tends to direct most of the write data traffic to the new disk, which is where<br>
most of the free space is located.<br>
.<br>
The RAID system typically keeps track of allocated data in a RAID-5 level implementation<br>
of the disk array. To that end, the RAID system reserves parity blocks hi<br>
a fixed pattern that is simple to compute and that allows efficient identification of the<br>
non-data (parity) blocks. However, adding new hidividual disks to a RAID group of a<br>
RAID-5 level implementation typically requires repositioning of the parity mformation<br>
across the old and new disks in each stripe of the array to maintain the fixed pattern.<br>
- Repositioning of the parity information typically requires use of a complex (and costly)<br>
parity block redistribution scheme that "sweeps-through" the old and new disks, copy-<br>
: - ing both parity and data blocks to conform to the new distribution. The parity redistril<br>
bution scheme ftirther requkes a mechanism to identify which blocks contain data and<br>
• to ensure, per stripe, that there are not too many data blocks allocated so that there is<br>
sufficient space for the parity mformation. As a result of the complexity and cost of<br>
' such a scheme, most RAID-5 implementations relinquish the ability to add individual<br>
m diskstoaRAIDgroupand,mstead, use a fixed RAE) group size (usually in the 4-8<br>
disk range). Disk capacity is then increased a full RAID group at a tune. Yet, the use<br>
of small RAID groups translates to high parity overhead, whereas the use of larger<br>
RAID groups means having a high-cost for mcremental capacity.<br>
Therefore, it is desirable to provide a distribution system that enables a storage<br>
system to distribute parity evenly, or nearly evenly, among disks of the system, while<br>
retaining the capability of incremental disk addition.<br>
In addition, it is desirable to provide a distribution system that enables a write<br>
anywhere file system of a storage system to run with better performance in smaller<br>
(RAID group) configurations.<br>
^ SUMMARY OF THE INVENTION<br>
The present invention overcomes the disadvantages of the prior art by providing<br>
a semi-static distribution technique that distributes parity across disks of an array. According<br>
to an illustrative embodiment of the technique, parity is distributed (assigned)<br>
across the disks of the array in a manner that maintains a fixed pattern of parity blocks<br>
among stripes of the disks. When one or more disks are added to the array, the semistatic<br>
technique redistributes parity in a way that does not require recalculation of parity<br>
or moving of any data blocks. Notably, the parity information is not actually moved;<br>
the technique merely involves a change in the assignment (or reservation) for some of ,<br>
^ the parity blocks of each pre-existmg disk to the newly added disk. For example, a preexisting<br>
block that stored.parity on, e.g., a first pre-existing disk, may continue to store<br>
parity; alternatively, a block on tiie newly added disk can be assigned to store parity for<br>
the stripe, which "frees up" the pre-existing parity block on the first disk to store file<br>
: ' system data.<br>
• • Advantageously, semi-static distribution allows those blocks that hold parity (in<br>
; the stripe) to change when disks are added to the array. Reassignment occurs among<br>
'] blocks of a stripe to rebalance parity to avoid the case where a disk with a preponder-<br>
: anceofparitygets"hot", i.e., more heavily utilized than other disks, during write traf-<br>
-<br>
; fic. The novel distribution technique applies to single disk failure correction and can be<br>
s extended to apply to double (or greater) disk loss protection. In addition, the senii-<br>
^ static distribution technique has the potential to unprove performance in disk-bound<br>
configurations while retainmg the capability to add disks to a volume one or more disks<br>
at a time.<br>
BRIEF DESCRIPTION OF THE DRAWINGS<br>
The above and further advantages of the invention may be better understood by<br>
referring to the following description in conjunction with the accompanying drawings<br>
in which like reference niraierals indicate identical or functionally similar elements:<br>
Fig. 1 is a schematic block diagram of a storage system that may be advantageously<br>
used with the present invention;<br>
Fig. 2 is a schematic diagram of a disk array illustrating parity assignments ac-<br>
•<br>
' cording to a semi-static distribution technique of the present invention;<br>
Fig. 3 is a flowchart illustrating a sequence of steps for distributing parity<br>
•<br>
among disks of an array in accordance with an illustrative embodiment of the semistatic<br>
distribution teclinique; and<br>
Fig. 4 is a diagram of a parity assignment table illustrating a repeat interval for<br>
various group sizes in accordance with the semi-static distribution technique.<br>
DETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT<br>
Fig. 1 is a schematic block diagram of a storage system 100 that may be advantageously<br>
used witli the present invention. In the illustrative embodiment, the storage<br>
system 100 comprises a processor 122, a memory 124 and a storage adapter 128 interconnected<br>
by a system bus 125. The memory 124 comprises storage locations that are<br>
addressable by the processor and adapter for storing software program code and data<br>
; structures associated with the present invention. The processor and adapter may, in<br>
; turn, comprise processing elements and/or logic circuitry configured to execute the<br>
\ software code and manipulate the data structures. It will be apparent to those slcilled in<br>
; the art that other processing and memory means, including various computer readable<br>
; media, may be used for storing and executing program instructions pertaining to the<br>
' inventive technique described herein.<br>
i A storage operating system 150, portions of which are typically resident in<br>
^k memory and executed by the processing elements, ftmctionally organizes the system<br>
• 100 by, inif^erfl/za, invoking storage operations executed by the storage system. The<br>
storage operating system implements a high-level module to logically organize the information<br>
as a hierarchical structure of directories, files and blocks on disks of an array.<br>
The operating system 150 ftirther implements a storage module that manages the storage<br>
and retrieval of the information to and ftom the disks in accordance with write and<br>
read operations. It should be noted that the high-level and storage modules can be implemen.<br>
ted in software, hardware, fumware, or a combination thereof.<br>
' Specifically, the high-level module may comprise a file system 160 or other<br>
i module, such as a database, that allocates storage space for itselfin the disk array and .<br>
that controls the layout of data on that array. In addition, the storage module may comprise<br>
a disk array control system or RAID system 170 configured to compute redundant<br>
W (e.g., parity) information usmg a redundant storage algorithm and recover from disk<br>
failures. The disk array control system ("disk array controller") or RAID system may<br>
further compute the redundant information using algebraic and algorithmic calculations<br>
in response to the placement of fixed data on the array. It should be noted that the term<br>
•. . ' "RAID system" is synonymous with "disk array control system" or "disk array controller"<br>
and, as such, use of the term "RAID system" does not imply employment of one of<br>
i inventive semi-static parity distribution technique. As described herein, the file system<br>
J or database makes decisions about where to place data on the array and forwards tho se<br>
decisions to the RAID system.<br>
In the illustrative embodiment, the storage operating system is preferably the<br>
NetApp® Data ONTAP'^" operating system available from Network Appliance, Inc.,<br>
Sunnyvale, California that implements a Write Anywhere File Layout (WAFL™) file<br>
system having an on-disk format representation that is block-based using, e.g., 4 kilobyte<br>
(kB) WAFL blocks. However, it is expressly contemplated that any appropriate<br>
storage operating system mcluding, for example, a write in-place file system may be<br>
enhanced for use in accordance with the inventive principles described herein. As<br>
such, where the term "WAFL" is employed, it should be taken broadly to refer to any<br>
0^ storage operating system that is otherwise adaptable to the teachings of this invention.<br>
i • •<br>
As used herem, the term "storage operating system" generally refers to the, computer-<br>
executable code operable to perform a storage function in a storage system, e.g.,<br>
that manages file semantics and may, in the case of a file server, implement file system<br>
, semantics and manage data access. In this sense, the ONTAP software is an example<br>
,; of such a storage operating system implemented as a microkernel and including a<br>
WAFL layer to implement the WAFL file system semantics and manage data access.<br>
The storage operating system can also be implemented as an application program oper-•<br>
ating over a general-purpose operating system, such as UNIX® or Windows NT®, or<br>
as a general-purpose operating system with configurable fimctionality, which is configured<br>
for storage applications as described herein. ,<br>
• ^ The storage adapter 128 cooperates with the storage operating system 150 executing<br>
on the system 100 to access information requested by a user (or client). The information<br>
may be stored on any type of attached array of writeable storage device me-<br>
: dia such as video tape, optical, DVD, magnetic tape, bubble memory, electronic ran- ,<br>
: _ dom access memory, micro-electro mechanical and any other sknilar media adapted to<br>
store mformation, including data and parity information. However, as illustratively described<br>
hereui, the mformation is preferably stored on the disks, such as HDD and/or<br>
DASD, of array 200. The storage adapter includes input/output (I/O) interface chcuitry<br>
tiiat couples to the disks over an I/O mterconnect arrangement, such as a conventional<br>
high-performance. Fibre Channel serial link topology.<br>
Storage of information on array 200 is preferably unplemented as one or more<br>
storage "volumes" (e.g., VOLl-2 140) that comprise a cluster of physical storage disks,<br>
generally shown at 130 and defining an overall logical arrangement of disk space.<br>
Each volume is generally, although not necessarily, associated with its own file system.<br>
' The disks within a volume/file system are typically organized as one or more groups,<br>
wherein each group is comparable to a RAID group. Most RAID implementations enhance<br>
the rehability/integrity of data storage through the redundant writing of data<br>
"stripes" across a given number of physical disks in the RAID group, and the appropriate<br>
storing of parity information with respect to the striped data.<br>
Specifically, each volume 140 is constructed from an array of physical disks<br>
4 ^ 130 that are divided into blocks, with the blocks being organized into stripes. The disks<br>
* ' are organized as groups 132,134, and 136. Although these groups are comparable to<br>
' RAID groups, a semi-static distribution technique described herein is used within each<br>
group. Each stripe in each group has one or more parity blocks, depending on the degree<br>
of failure tolerance required of the group. The selection ofwhichdisk(s) in each<br>
stripe contains parity is not determined by the RAID configuration, as it would be in a<br>
conventional RAID-4 or RAID-5 array.<br>
•<br>
The present invention relates to the semi-static distribution technique that distributes<br>
parity across disks of an array. The inventive technique is preferably implemented<br>
by the RAID system 170 that, among other things, computes parity in stripes<br>
across the disks, distributes the parity among those stripes as described herein and reconstructs<br>
disks lost as a result of failure. The semi-static distribution technique does<br>
.^P not.require the participation of the file system 160 and, as such, is also suitable for deployment<br>
in RAID code embodied as, e.g., a RAID controller that may be internally or<br>
externally coupled to the storage system 100.<br>
According to the technique, parity is distributed (assigned) across the disks of<br>
the array in a manner that maintains a fixed pattern of parity blocks among stripes of<br>
the disks. When one or more disks are added to the array, the semi-static technique redistributes<br>
parity in a way that does not require recalculation of parity or moving of any<br>
data blocks. Notably, tlie parity information is not actually moved; the technique<br>
merely involves a change in the assignment (or reservation) for some of the parity<br>
. blocks of each pre-existing disk to the newly added disk. For example, a pre-existing<br>
block that stored parity on, e.g., a first pre-existing disk, may continue to store parity;<br>
alternatively, a block on the newly added disk can be assigned to store parity for the<br>
stripe, which "frees up" the pre-existmg parity block on the first disk to store file system<br>
data. Note that references to the file system data do not preclude data generated by<br>
other high-level modules, such as databases.<br>
Assuming data is allocated densely across the disks of array 200, the storage<br>
operating system 150 can choose to assign parity evenly across the disks in a fixed pattern.<br>
However, the fixed pattern changes when one or more disks are added to the array.<br>
In response, the semi-static distribution technique redistributes (reassigns) parity<br>
^ in a manner that maintains a fixed pattern ofparity blocks among the stripes of the<br>
disks. Note that each newly added disk is initialized to a predetermined and fixed<br>
value, e.g., zeroed, so as to not affect the fixed parity of the stripes. It should be further<br>
noted that the fixed parity may be even or odd, as long as the parity value is known<br>
(predetermined); tlie following description herein is directed to the use of even parity.<br>
In addition, initializing of the newly added disk allows reassignment ofparity blocks in<br>
•<br>
some stripes (e.g., 1/N of the stripes, where N is equal to the number of disks) to the<br>
I new disk without any calculation or writing ofparity.<br>
According to the invention, the reassigrmient algorithm only ever changes a parity<br>
block to a data block and never changes a data block to a parity block. For example,<br>
in response to adding a new Nth disk to a group 132-136, the file system 160 can<br>
reassign every Nth parity block of each existing disk to the new disk. Such reassign-<br>
W ment does not require any re-computation or data movement as the new disk only contains<br>
free blocks and parity blocks, so existing parity blocks can get reassigned for use<br>
for data, but not vice versa. This reassignment (construction) algorithm forms a pattern<br>
ofparity that is deterministic for each group size and evenly distributes parity among<br>
all the disks in the group.<br>
Fig. 2 is a schematic diagram of disk array 200 illusfrating parity assigiunents<br>
according to the semi-static distribution technique of the present invention. Assume the<br>
~ array 200 kiitially comprises one disk 202 and it is desirable to store redundant (parity)<br>
information; therefore, each block on the disk stores parity (?) information. When a<br>
second disk 204 is added to expand the array, the parity blocks may be distributed between<br>
the two disks. Likewise, when a third disk 206 and, thereafter, a fourth disk 208<br>
are added to the expanded array, the parity blocks may be distributed among those<br>
disks. As disks are added to the array 200, parity is not stored in a block that contains<br>
file system data. The semi-static distribution technique is directed to only reassigning<br>
parity blocks, which frees up blocks to use for data. In other words, the technique<br>
never reassigns a data block, which is in contrast to the expansion of conventional<br>
RAID-5 level implementations.<br>
Parity may be distributed among the disks in accordance with a construction algorithm<br>
of the inventive technique that reassigns one of N parity blocks from each pre-<br>
^ existing disk to the new disk, wherein N is equal to the number of disks in the expanded<br>
array. Overall, one of N parity blocks is reassigned to the new disk, with each preexisting<br>
disk continuing to hold exactly 1/N of the parity blocks in tlie expanded array.<br>
For a 2-disk array, every other parity block on the first disk 202 is moved to the second<br>
disk 204, When the third disk 206 is added to the expanded array 200, thereby creating<br>
a 3-disk array, every third remaining parity block on the first disk 202, as well as every<br>
third parity block on the second disk 204, is moved to the third disk 206. When the<br>
fourth disk 208 is added to the array, creating a 4-disk array, every fourth remaining<br>
parity block firom each disk (disks 1-3) is moved to the fourth disk 208. As a result of<br>
this reassignment, the amount of parity on each disk is substantially the same. The location<br>
of the parity block also changes from stripe to stripe across the disks of the array<br>
in a predictable and deterministic pattern.<br>
^ Fig. 3 is a flowchart illustrating a sequence of steps for distributing parity<br>
among disks of an array in accordance with an illustrative embodiment of the semistatic<br>
distribution technique of the present invention. Here, a new Nth disk is added to<br>
a group 132-13 6 of the array and, as described above, one out of every N parity blocks<br>
is assigned to the new disk, wherein N is equal to the number of disks in the array. As<br>
• noted, there is no need to actually move the parity information among the disks; the inventive<br>
semi-static distribution technique contemplates merely a change in the assignment<br>
(or reservation) for each parity block on the newly added disk.<br>
The sequence starts in Step 300 and proceeds to Step 302 where the new disk is<br>
added to the group of N disks in the array, hi Step 304, the new disk is initialized (e.g.,<br>
zeroed) to ensure that the parity of the blocks on each stripe is unaffected. There may<br>
•<br>
•<br>
be multiple blocks witliin a stripe that do not contain data (i.e., unallocated data blocks)<br>
and that could potentially store parity. The stripe will contain at least one unallocated<br>
block, which is the-parity block, and one or more unallocated blocks that are fireed data<br>
blocks. All blocks contribute to, e.g., even parity, so the parity block(s) and. the freed<br>
•, . . data blocks ai-e all equivalent. The file system (or high-level module, if there is no file<br>
. system) detemiines which disks contain fi:ee blocks in the stripe in response to a write :<br>
request to store write data in the stripe. In Step 306, the file system 160 reserves as<br>
many free blocks as required by the redundant storage algoritlim to store parity, arbitrarily.<br>
For example, a pre-existing block that stored parity on, e.g., a first pre-existing<br>
disk, may continue to store parity; ahematively, a block on the newly added disk can be<br>
^ • assigned to store parity for the stripe, which "frees up" the pre-existing parity block on<br>
I ' • ' the fust disk to storp the data.<br>
Note that any parity algoritlim that protects against two (or more) disk failures<br>
, • may be used with the semi-static distribution technique, as long as the algorithm.allows<br>
any two (or more) blocks in the stripe to store the parity. An example of a double failure<br>
correcting algoritlmi that may be advantageously used with the present invention is<br>
,• udfonu and symmetric row-diagonal (SRD) paritj' described in U.S. VatentAppUca.-<br>
tlonSeriallio. (112056-0141) titled Uniform, andSymmeMcDoubleFailw-e Corncting<br>
Technique for Protecting against TM'O'Disk Faihtres in a Disk Array,hyVeXe.x'V.<br>
Corbett et al. Here, the inventive teclmique is not dependent upon the unifbiinity or<br>
symmetry of the parity algoritlmi, although it can take advantage of it. Wlien using a<br>
i W double failure correcting algorithm with the semi-static distribution teclmique, the file<br>
, system reserves'two unallocated data blocks to be assigned to store parity. Anonunifonn<br>
double or higher failure correcting algoritlim can be used since the location of<br>
the parity blocks is known deterministically. However, using such an algoritlim may<br>
sacrifice the advantage that parity need not be recalculated when a disk is added to the<br>
array.<br>
Another teclmique is to employ the non-uoiform algorithm such that data blocks<br>
. are written to any of the blocks of the array, even those that typically would be used to<br>
, . store.redmidant information. Since the multiple failure correcting algorithm can restore<br>
the contents of any missing disks, the remaining blocks can.be used to store redundant<br>
information, even if they are constructed using tlie technique usually intended to reconstruct<br>
lost data blocks. Using a non-unifomi algoritlim in tliis way may result in an implementation<br>
that is much more complex tlian can be achieved by using a unifomi and<br>
symmetric algorithm, such as SRD.<br>
In Step 308, the write allocator 165 of the file system arranges the write data for<br>
storage on the disks in the stripe. In Step 310, the file system provides an indication of<br>
the reserved block(s) to the RAID system (storage module) via a write request message<br>
issued by the file system. In Step 312, the RAID system provides the parity information<br>
(and write data) to the disk driver system for storage-pn the disks, hi particular, in<br>
Step 314, the parity is distributed among the blocks of the' disks such that L'N of the<br>
^ paiity blocks is stored on each disk to thereby balance the 'data across the disks of the<br>
' array. Moreover, tlie locations of the parity blocks "move" among the stripes of the<br>
array in a predictable pattern that appears complicated, but is easy to compute. The sequence<br>
then ends at Step'316.,<br>
Additional teclmiques by which a balanced semi-static distribution of redundant<br>
or parity blocks can be acliieved in a double failure con-ecMng array that has two redimdant<br>
blocks per stripe includes a teclmique that simply replaces each single disk in a<br>
singlefailirrecorrectingsemi-staticarraywithapair of di^ks in the double failure cor- ' '<br>
rectihg array. Here, the role of each pair of disks is identical to the role of the corre-<br>
• I spending single disk in the single failure-correcting array. Balance is maintained by . .<br>
using the same number of rows used in the single failure-correcting array; however, this<br>
1 ^ teclinique-is limited to adding disks to the array in multiples of two.<br>
- Anotlrer teclinique constiucts a balanced or nearly balanced array by starting<br>
with two initial ("old") disks that are completely filled with parity blocks, then adding a<br>
third disk and moving every third parity block fioni each of the two initial disks to the<br>
new disk. This technique distributes one-third of the paiity blocks to each disk, occupying<br>
two-thh-ds of the space on each disk, Wlien reassigning parity blocks fi-om an old<br>
disk, it may be discovered that the block on the new disk has already been designated<br>
as parity. In tliis case, tlie next possible parity block is reassigned fiom the old disk to<br>
i the new disk, at the next row where the new disk does not yet contain parity and the old<br>
disk does.<br>
I<br>
This latter technique can be further exti-apolated to build a deterministic set of .<br>
parity assignments for saiy number of disks, with two redundant (e.g., parity) blocks pei:<br>
' stripe and with the redundant blocks balanced or nearly balanced across the array.<br>
Similarly, for three or greater numbers of redundant blocks per stripe, the same technique<br>
can be employed to determine a placement of redundant blocks in a larger array<br>
of any size, in such a way that the. number of redundant blocks per disk is balanced or<br>
neaidy balanced. Moreover, 'the technique allows any number of disks to be added<br>
• without ever changing a data block into a parity block, while continuing to keep the<br>
number of redimdant blocks per disk balanced or nearly balanced.<br>
Other similar tecimiques can be developed to determine the roles of blocks as<br>
I ^ data blocks or redundant blocks ui any size array, wlrile preserving the property that the<br>
' array can be expanded incrementally as the distribution of both data and redundant<br>
blocks are kept balanced or nearly balanced, and without ever changing a data block<br>
"..-•• ..-.into a redundant block. Any of these assignment tecimiques can be implemented'by •<br>
^, storing.or.generatmg.a..data.structure{e,g,,..ajable)i^<br>
mentsfor a specific number ofrows in an. airay of specific size. It is also possible to<br>
. store in a single table all possible assigmnents of redundant blocks for any array size'up<br>
to a certain limit. Here, for example, the table may store a bitmap for each row, where<br>
the one (or more) highest numbered bit sdt is selected that is less than N, wherem N is<br>
the number of disks in the array. In general, any table-based parity assignment that<br>
maintains balance of distributed data and redundant blocks, while allowing expansion<br>
I w without changing data blocks to redundant (parity) blocks, is contemplated by the present<br>
invention, regardless of the number of redundant blocks per row (i.e., the number<br>
of failures the array can tolerate).<br>
The parity assignments for tire semi-static distribution teclmique are calculated<br>
for a known size of a group 132-136 of the disk array or for a maximum group size of<br>
the array; either way, as noted, the calculated parity assignments may be stored in a ta-<br>
.. ble. A parity distribution pattern defined by the stored assigrmients and,, in particular, a<br>
repeat interval of the pattern can be used to determine the location of parity storage on<br>
. any disk in the array for a given group size and for a given stripe. That is, ^the pattern<br>
can be used to indicate which block in each stripe is used for parity or a different pattern<br>
can be used for several stripes.<br>
Fig. 4 is a diagram'of a parity assignment table 400 illustrating the repeat interval<br>
for various group sizes in accordance with the-semi-static distribution technique.<br>
The parity distribution pattem repeats at a repetition interval dependent iipon the gi-oup<br>
size of the array. If a group of sizeN repeats every K stripes tlien the group of size<br>
(N+1) will repeat in the smallest number that both K and (N+1) evenly divide. Notably,<br>
the content of the table does not repeat until it reaches a number (repeat interval)<br>
dependent on the value of N, where N equals the nimiber of disks. For example, m a 2-<br>
disk array (i.e., a group size of two), the parity distribution pattem repeats every two<br>
'\ ^ stripes. Wlien a third disk is added (for a group size of three), tlie parity pattern repeats<br>
every six stripes. Wlien a fourth disk is added (for a gi'oup size of fom-), the parity pattem<br>
repeats every twelve stripes. It can be seen from table 400 that for a group size of ' •<br>
•• • . five (and six), the parity pattem repeats every sixty stripes.<br>
The repeat interval as a function of group size is determined in accordance with<br>
the set of imique prime factors ("primes") up to N, where N equals the nmnber of disks.<br>
The repeat interval (which is equivalent to the number of entries in table 400) is less<br>
than N factorial and, in fact, is equal to the product of all primes less than ot equal to N,<br>
with each prime raised to tlie largest power possible such that the result is less thaii or<br>
equal to N. As some of the numbers between one and N.are prime numbers, it is clear<br>
•<br>
that the repeat interval may get large, making the table large. For example, for N = 10,<br>
the table size is 2^3 x ^'^2 x 5^1 x 7^1 = 8 x 9 x 5 x 7 = 2520. Similarl^^ for N = 32,<br>
tlie table size is 2^5 x 3'^3 x 5'^2 x 7^1 x 11-^1 x 13^1 x 17^1 x 19^1 x 23^1 x 29^1 x<br>
31^1 = 32 x 27 X 25 X 7 x 1 Ix 13 x 17 X 19 X 23 x 29 X 31 s 144 X 10^12.<br>
A tradeoff may then be made between the table size of tlie pattern and precision<br>
of balancing; the table can be temiinated at a reasonable point and the group size at that<br>
particular repeat inteival can be used. Thereafter, even if there are more disks tha4 the<br>
group size, the technique can. continue to repeat the pattern and still realize nearly • ^ -<br>
form balance of data across the array within, e.g., a half percent, For example, as noted<br>
above, a group size often translates into a parity distrib.ution pattem that repeats every<br>
2,520 stripes. A table of this size (i.e., 2,520 entries) is relatively compact in memory<br>
K<br>
124 and can be computed relatively quickly at start-up using appropriate sofhvai-e code.<br>
In contrast, the table for a group size of 32 (i.e., 144 x 10^^12 entries) is too large to<br>
store in memory.<br>
The.2,520 entry table works well with any reasonable nmnber of disks to pro-<br>
• vide good data balance; however, it should be noted that tliis size table is not the only<br>
choiceandother sized tables may also be used. The 2,520 entry pattern is perfectly '<br>
balanced for N disks up to ten; for N greater than 10, the pattern provides good data<br>
balance even though the pattern has not repeated. In other words, although tlie parity<br>
assignment table for a 17-disk group is rather large (7.7MB with 5 bits per pattern), if<br>
only a fraction of the table is iised, good parity balance can still be achieved. Cutting<br>
(• ^ off the pattern at 2,520, for example, yields perfect balance for all group sizes up to 10<br>
' disks, and less than 1% imbalance to larger groups while limiting the table size to 2520<br>
• X4 bits =1260 bytes for N - 1 1 and 5x2520 bits = 1,575 bytes forN= 17 to 32.<br>
• ' . The parity assignment table 400 can be encoded as a single number indicating a ' •<br>
bit position of parity for a particular value of N. The table could also he coded as a bit<br>
vector, with one or two (or more) bits set indicating the position of a single or double<br>
' (or greater) parity block providing single or double (or greater) disk failure protection. • •<br>
Moreover, the table can be encoded as a single table indicating (for all disk array sizes •<br>
up to some limit, e.g., 32 disks) what disks possibly contain parity in each stripe. The<br>
determination of which disk actually contains parity for a specific value of N is then<br>
^ made by masking off the high order 32-N bits and selecting the highest order remaining<br>
' one or two (or more) bits.<br>
(,<br>
In sum, semi-static distribution strives to keep the number of data blocks per<br>
disk roughly matched across the array to thereby "spread" the read load across all disks<br>
of the array. As a result, the technique eliminates any "bottleneck" in the array caused<br>
by throughput of any single disk in the array, while also eliminating the parity disk(s)<br>
as hot spot(s) for write operations.. The general technique can be applied using a sj'mmetric<br>
algoritlmi, such as SRD parity, or an asynmiefric double failure-correctmg algorithm,<br>
such as Row-Diagonal (RD) parity. The RD parity technique is described in<br>
U.S. Patent Apphcation Serial No. 10/035,607 titled i?ow-Z3za^o;2a/?cr//y rec/777fgwe<br>
for Enabling Efficient Recoyeiyfrom Double Failures in a Storage Array', by Peter F.<br>
Corbett et al, filed on December 28, 2001.<br>
Wlien employing a non-urdform algoritliin, such as RD parity, tlie role of the<br>
disk in storing either data or redundant blocks in any particular block might be ignored<br>
• .. .. with respect to the typical role ofthe disk in the asyinmetric parity algorithni. Since<br>
any double failure correcting algoritlim can construct missing "data" for any two missing<br>
disks of an array, the contents of all the blocks in the row that are assigned the role<br>
of storing data are fixed and the contents of the bivo redundaiat blocks are computed using<br>
the double failure correcting algorithm, which is apphed differently depending on<br>
the positions of tlie disks in the row. Having stored two redmidant blocks ineach row,<br>
j ^ the array can tolerate two disk failures, recovermg the lost data or redmidant blocks re-<br>
•: gardless ofthe roles of the lost blocks in any particular stripe.<br>
Alternatively, since the roles ofthe disks are deterministically defined, any al-<br>
' • • gorithni that allows any two or more disks in the array to contain the redmidant inforination<br>
can be employed. Using such an algorithm may require the recomputation of, '<br>
parity in stripes where the parity blocks move, but it does preserve the advantage ofthe<br>
invention that no data blocks are moved. SRD has the additional advantage that no par- •<br>
ity blocks need be recomputed when parity block(s) are assigned to tlie newly added<br>
disk(s).<br>
The distribution teclmique described herein is particularly useful for systems<br>
^ having fewer disks yet that want to utilize all read operations per second (ops) that are ;<br>
available from tiiose disks. Performance of smaller arrays is bounded by the ops that ;<br>
are achievable fi:om disks (disk-bound). Yet even in large arrays where disks get lar- ''<br>
ger, because of reconsti-ucti'on times, the tendency is to reduce the number of disks pelgroup<br>
132-136. This results in an increase in redundancy oveirhead (the percentage of<br>
disks in a group devoted to redundancy increases). Therefore, it is desirable to take advantage<br>
of the read ops available in those redundant disks. Another advantage of the<br>
distribution teclmique is that reconstruction and/or recovery occurs "blindly" (i.e., ^<br>
without knowing the roles ofthe disks).<br>
Semi-static distribution may be adv'antageously used witli arrays having low<br>
numbers of large disks, since the teclmique balances data across the array. Using larger<br>
18<br>
disks is required to get reasonable capacity, but that also means using smaller .groups to<br>
limit reconstmction time. If a 14-disk configuration uses two gi'oups and one spare,<br>
then over 20% of the disks are unavailable for use in storing or retrieving data. Configurations<br>
with eight disks are even worse. . •<br>
As noted, tlie semi-static distribution technique allows incremental addition of .<br>
disks to a distributed parity implementation of a disk array. An advantage of the inven-,<br>
tive distiibution technique over a RAID-5 level implementation is that it allows easy<br>
expansion of the an-ay, avoiding the need to add an entire group to the array or to perfomi<br>
an expensive RAID-5 reorganization. The semi-static distribution teclmique may<br>
be used in connection with single/double failure error correction. In addition, the tech-<br>
( w nique allows use of inultiple disk sizes m the same group 132-136.<br>
(<br>
While there has been shown and described illustrative embodiments of a semistatic<br>
distribution technique that distributes parity across disks, it is to be miderstood<br>
that various other adaptations and modifications may be made within the spirit and<br>
scope of the uivention. For example, the distribution teclmique described herein may •'<br>
apply to block-based RAID array's to, e.g., allow easy addition of disks to RAID<br>
• • groups. Block-based RAID arrays generally are notaware-of which blocks they are<br>
asked to store contain file system data. Instead, the arrays must assume that all blocks<br>
not previously designated as parity blocks contain file system data. Therefore, they •?<br>
usually pre-allocate which blocks will be used for parity. For a given array, these pre- i<br>
^ ^ • allocated blocks remain fixed. Nomially this is done in some predetennmed algorithm U •<br>
so that the system does not have to keep track of each parity block. •;<br>
According to the invention, the RAID system may move the parity designation<br>
of some of the blocks in the existing disks to the new disks usmg the seriii-static distribution<br>
technique. The RAID system must also ensure tliat logical unit number (lun)<br>
block offsets of non-parity blocks in the existing disks are not changed. The new space<br>
will then be distributed among all the disks. Tliis non-linear mapping is usually not desirable<br>
in block-based arrays, as file systems cannot compensate for it. However, this<br>
effect can be mitigated if the parity blocks are allocated contiguously in large chunks<br>
- (e.g. at least a ti-ack size).<br>
n<br>
It will be understood to those sldlled in the art tliat the inventive technique described<br>
herein raay apply to any type of special-purpose (e.g., file ser
storage appliance) or general-purpose computer, including a standalone com-<br>
• puter or portion thereof, embodied as or including a storage system 100. An example<br>
of a multi-protocol storage appliance that may be advantageously used, with the present • ',<br>
invention is described in U.S. Patent Apphcation Serial No. 10/215,917 titled, Multiprotocol<br>
Storage Appliaijce that provides Integi'ated Support for File and Block Access<br>
Protocols, filed on August 8,2002. Moreover, the teachings of this invention can be<br>
adapted to a variety of storage system arcliitectures including, but not limited to, a network-<br>
attached storage environment, a storage area network and disk assembly directly-<br>
^ • attached to a client or host computer. The tenn "storage system" should therefore be<br>
taken broadly to include such arrangements in addition to any subsystems configured to<br>
,perform a storage fonction and associated witli other equipment or systems.<br>
The foregoing description has been directed to specific embodiments of tills in- ,:<br>
vention. It will be apparent, however, that other variations and modifications may be<br>
made to the described embodiments, with the attainment of some or all of their advan-<br>
.tages. For instance, the semi-static distribution technique can be generalized to otlrer<br>
applications involving the distribution of data structures among persistent storage, e.g.,<br>
disks, or non-persistent storage, e.g., memory, of a system. Broadly, the teclinique may<br>
apply to the redistribution of any commodity over any set of containers as more containers<br>
are added to the system. As an example, the semi-static technique may apply to<br>
( w a sj^stem having units and containers, wherein the units are distibuted imiformly over<br>
the containers and wherein it is desirable to maintain a balanced rate of assignment of '<br>
units to containers along some numbered dimension. When a new container is added to<br>
the system, the technique may be employed to transfer some of the existing units to flie<br>
new container in such a way that overall and localized balance is maintained.<br>
More specifically, the semi-static teclinique can be applied to distribution of<br>
data structures, such as inode file blocks, among persistent storage devices, ;such as<br>
disks, of an array coupled to a plurality of storage entities, such as storage "lieads".<br>
Note that a "head" is defined as all parts of a storage system, excluding the disks. An<br>
example of such an application involves distributing existing inode file blocks over the<br>
plurality of (N) storage heads, which includes one or more newly added storage heads.<br>
Here, the inventive semi-static distribution technique may be used to move only 1/N of<br>
any existiag inode file blocks to the newly added storage head.<br>
It is expressly contemplated that the teachings of this invention can be implemented<br>
as softvvare, including ..a computer-readable mediurh havuig pro grain instruclions<br>
executing- on a computer, hardware, firmware, or a combination thereof Accordingly<br>
tliis description is to be taken only by way of example .and not to otherwise limit •<br>
the scope of the invention. Therefore, it is the object of the appended claims to cover<br>
• all such variations and modifications as come within the true spirit and scope of the in-<br>
•^ vention.<br><br><br><br><br><br><br><br>
lAVE claim:<br>
1. A method for distributing parity across a storage array, the method comprising the steps of:<br>
adding a new storage device to a number of pre-existing storage devices of the array;<br>
dividing each storage device into blocks, the blocks being organized into stripes such that each<br>
stripe contains one block from each storage device; and<br>
distributing, by a processor (122) of a storage system (200) parity among blocks of the new and<br>
pre-existing storage devices without recalculation or moving of any blocks containing data,<br>
w the method characterized in that<br>
the distribution of parity is by reassigning every Nth parity block (P) from each of the preexisting<br>
storage devices (202-206) to the new storage device (208) to arrange each storage device<br>
of the array with approximately 1/N of the parity blocks (P), where N is equal to the number of<br>
the pre-existing storage devices (202-206) plus the new storage device (208).<br>
2. The method as claimed in claim 1, wherein the step of distributing comprises the step of<br>
distributing parity among blocks of the new and pre-existing storage devices in a manner that<br>
maintains a fixed pattern of parity blocks among stripes of the storage devices.<br>
3. The method as claimed in claim 1, wherein the step of distributing comprises the step of<br>
changing an assignment for one or more blocks containing parity of each pre-existing storage<br>
device to the newly added storage device.<br>
4. The method as claimed in claim 2 wherein the step of adding comprises the step of initializing<br>
the added storage device so as to not affect parity of the stripes.<br>
22<br>
5. The method as claimed in claim 4 wherein the step of initializing comprises the step of<br>
reassigning blocks containing parity in certain stripes to the new storage device without<br>
calculation or writing of parity.<br>
6. The method as claimed in claim 5 wherein the certain stripes comprise 1/N of the stripes,<br>
where N is equal to the number of storage devices in the array.<br>
^ 7. The method as claimed in claim 5 wherein the step of reassigning comprises the step of<br>
changing a block containing parity (parity block) to a block containing data (data block) and not<br>
changing a data block to a parity block.<br>
8. The method as claimed in claim 1 wherein the step of distributing comprises the step of<br>
•<br>
reassigning one of N blocks containing parity (parity blocks) from each pre-existing storage<br>
device to the added storage device, wherein N is equal to the number of storage devices in the<br>
array.<br>
9. The method as claimed in claim 8 wherein the step of reassigning comprises the step of<br>
reassigning one of N parity blocks to the new storage device, with each pre-existing storage<br>
W device continuing to hold 1/N of the parity blocks in the array.<br>
10. A system adapted to distribute parity across storage devices of a storage system, the system<br>
comprising:<br>
a storage array comprising pre-existing storage devices and at least one new storage device; and<br>
a storage module configured to compute parity in blocks of stripes across the storage devices and<br>
reconstruct blocks of storage devices lost as a result of failure, the storage module further<br>
configured to assign the parity among the blocks of the new and pre-existing storage devices<br>
without recalculation or moving of any data blocks,<br>
23<br>
characterized in that<br>
the distribution of parity is by reassigning every Nth parity block (P) from each of the preexisting<br>
storage devices (202-206) to the new storage device (208) to arrange each storage device<br>
of the array with approximately 1/N of the parity blocks (P), where N is equal to the number of<br>
pre-existing devices (202/206) plus the new storage device (208).<br>
11. The system of claim 10 further comprising a table configured to store parity assignments<br>
calculated for one of a known group size of the storage array and a maximum group size of the<br>
w array, the stored parity assignments defining a repeat interval of a parity distribution pattern used<br>
to determine locations of parity storage on any storage device in the array.<br>
12. The system of claim 10 wherein the storage module is embodied as a RAID system of the<br>
storage system.<br>
13. The system of claim 10 wherein the storage module is embodied as an internal storage array<br>
controller of the storage system.<br>
•<br>
14. The system of claim 10 wherein the storage module is embodied as a storage array control<br>
system externally coupled to the storage system.<br>
15. The system of claim 10 wherein the disk array is a block-based RAID array.<br>
16. A method for distributing commodities over containers of a system, the method comprising<br>
the steps of:<br>
24<br>
adding a new container to pre-existing containers of the system to thereby provide N containers;<br>
and<br>
moving only 1/N of the commodities to the new container,<br>
the method characterized in that:<br>
the distribution of commodities is by reassigning every Nth commodity from each of the preexisting<br>
containers to the new container to arrange each container with approximately 1/N of the<br>
commodities.<br>
•<br>
17. The method of claim 16 wherein the system is a storage system, the commodities are data<br>
structures adapted for storage on storage devices of an array, and the containers are storage<br>
entities coupled to the array.<br>
18. The method of claim 17 wherein the storage entities are storage heads.<br>
19. The method of claim 17 wherein the data structures are inode file blocks.<br>
^ Dated: 09/05/2006 O y^^^Vt<br>
[R. MAHESH]<br>
OF REMFRY &amp; SAGAR<br>
ATTORNEY FOR THE APPLICANT[s]<br>
25</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LUFic3RyYWN0LSgwMi0wOC0yMDEzKS5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Abstract-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWFic3RyYWN0LnBkZg==" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWFzc2lnbm1lbnQucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-assignment.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LUNsYWltcy0oMDItMDgtMjAxMykucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Claims-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LUNvcnJlc3BvbmRlbmNlLU90aGVycy0oMDItMDgtMjAxMykucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Correspondence-Others-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWNvcnJlc3BvbmRlbmNlLW90aGVycy0xLnBkZg==" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-correspondence-others-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWNvcnJlc3BvbmRlbmNlLW90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LURlc2NyaXB0aW9uIChDb21wbGV0ZSktKDAyLTA4LTIwMTMpLnBkZg==" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Description (Complete)-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWRlc2NyaXB0aW9uIChjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LURyYXdpbmdzLSgwMi0wOC0yMDEzKS5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Drawings-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWRyYXdpbmdzLnBkZg==" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWZvcm0tMS5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWZvcm0tMTgucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-form-18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LUZvcm0tMi0oMDItMDgtMjAxMykucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Form-2-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWZvcm0tMi5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LUZvcm0tMy0oMDItMDgtMjAxMykucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-Form-3-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWZvcm0tMy5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWZvcm0tNS5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LUdQQS0oMDItMDgtMjAxMykucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-GPA-(02-08-2013).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LWdwYS5wZGY=" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-gpa.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0xMDEucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-101.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0yMDIucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-202.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0yMTAucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-210.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0yMjAucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-220.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0yMzcucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-237.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0zMDEucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-301.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0zMDQucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-304.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MjU5MS1kZWxucC0yMDA2LXBjdC0zNzMucGRm" target="_blank" style="word-wrap:break-word;">2591-delnp-2006-pct-373.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="259792-process-for-manufacturing-entacapone.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="259794-a-sensor-package-and-a-method-for-mounting-a-vertical-sensor-circuit-component.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>259793</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>2591/DELNP/2006</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>13/2014</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>28-Mar-2014</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>27-Mar-2014</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>09-May-2006</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>NETWORK APPLIANCE INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>495 EAST JAVA DRIVE, SUNNYVALE, CA 94089, UNITED STATES FO AMERICA,</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>PETER F. CORBETT</td>
											<td>33 SUMM ER STREET, LEXINGTON, MA 02420, UNITED STATES OF AMERICA</td>
										</tr>
										<tr>
											<td>2</td>
											<td>ROBERT M. ENGLISH,</td>
											<td>4 EAST CREEK PLACE, MENLO PARK, CA 94025 UNITED STATES OF AMERICA.</td>
										</tr>
										<tr>
											<td>3</td>
											<td>STEVEN, R.KLEIMAN,</td>
											<td>157 EL MONTE COURT, LOS ALTOS, CA 94022 UNITED STATES OF AMERICA</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>H02M 1/084</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/US2004/039618</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2004-11-24</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>10/720,364</td>
									<td>2003-11-24</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/259793-a-method-for-distributing-parity-across-a-storage-array by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 00:18:41 GMT -->
</html>
