<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/252239-system-and-method-for-performing-a-backup-operation-using-multiple-data-streams by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 13:28:44 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 252239:SYSTEM AND METHOD FOR PERFORMING A BACKUP OPERATION USING MULTIPLE DATA STREAMS</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">SYSTEM AND METHOD FOR PERFORMING A BACKUP OPERATION USING MULTIPLE DATA STREAMS</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>TITLE: SYSTEM AND METHOD FOR PERFORMING A BACKUP OPERATION USING MULTIPLE DATA STREAMS The invention relates to a method for performing a backup operation in a storage network, the method comprising receiving a first data stream having first data, the first data being obtained by a first application-specific data agent; receiving a second data stream having second data, the second data being obtained by a second application-specific data agent; multiplexing the first and second data streams to form a single stream of one or more data chunks of an archive file, including writing the first data from the first data stream and the second data from the second data stream into a first data chunk of the one or more data chunks; transmitting the one or more data chunks over a transport channel to a backup medium; and storing the one or more data chunks on the backup medium.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td> <br><br><br>
SYSTEM AND METHOD FOR COMBINING DATA STREAMS IN<br>
PIPELINED STORAGE OPERATIONS IN A STORAGE NETWORK<br>
COPYRIGHT NOTICE<br>
A portion of the disclosure of this patent document contains material<br>
which is subject to copyright protection. The copyright owner has no objection to the<br>
facsimile reproduction by anyone of the patent document or the patent disclosures, as it<br>
appears in the Patent and Trademark Office patent files or records, but otherwise reserves<br>
all copyright rights whatsoever.<br>
RELATED APPLICATIONS<br>
This application claims the benefit of U.S. provisional application no.<br>
60/519,526 titled SYSTEM AND METHOD FOR PERFORMING PIPELINED<br>
STORAGE OPERATIONS IN A STORAGE NETWORK, filed November 13,2003,<br>
which application is incorporated herein by reference in its entirety.<br>
This application is related to the following patents and pending<br>
applications, each of which is hereby incorporated herein by reference in its entirety:<br>
Application Serial No. _/	_, titled SYSTEM AND<br>
METHOD FOR PROVIDING ENCRYPTION IN PIPELINED<br>
STORAGE OPERATIONS IN A STORAGE NETWORK, filed<br>
November 15, 2004, attorney docket number 4982/45;<br>
U.S. Patent No. 6,418,478, titled PIPELINED HIGH SPEED<br>
DATA TRANSFER MECHANISM, issued July 9, 2002, attorney<br>
docket number 4982/6;<br>
Application Serial No. 09/495,751, titled HIGH SPEED<br>
TRANSFER MECHANISM, filed February 1,2000, attorney<br>
docket number 4982/7US;<br><br>
Application Serial No. 09/610,738, titled MODULAR BACKUP<br>
AND RETRIEVAL SYSTEM USED IN CONJUNCTION WITH<br>
A STORAGE AREA NETWORK, filed My 6,2000, attorney<br>
docket number 4982/8;<br>
Application Serial No. 09/744,268, titled LOGICAL VIEW AND<br>
ACCESS TO PHYSICAL STORAGE IN MODULAR DATA<br>
AND STORAGE MANAGEMENT SYSTEM, filed January 30,<br>
2001, attorney docket number 4982/10;<br>
Application Serial No. 10/65 8,095, titled DYNAMIC STORAGE<br>
DEVICE POOLING IN A COMPUTER SYSTEM, filed<br>
September 9, 2003, attorney docket number 4982/18; and<br>
Application Serial No. 60/460,234, titled SYSTEM AND<br>
METHOD FOR PERFORMING STORAGE OPERATIONS IN A<br>
COMPUTER NETWORK, filed April 3,2003, attorney docket<br>
number 4982/35PROV.<br>
BACKGROUND<br>
Backup operations for client data on a storage network are often performed<br>
on streams of data which are managed by subclients and sent to a backup drive or media<br>
device. Typically, on a given stream, only one sub client can perform a backup at any<br>
given time. The concurrency limit for the number of backups that can go to a stream at<br>
any given time is one. Indirectly this means that only one backup can be sent to a media<br>
or drive at any point.<br>
This limitation has a major drawback. With tape speeds in media<br>
increasing and the difference between disk speed and tape speed widening, the tape<br><br>
throughput is being throttled by the slower disks. This becomes a major issue in a large<br>
enterprise where there are many clients with slow, under performing disks with large<br>
amounts of data that need to be backed up in a fixed backup window. The only way the<br>
backup window can be met is by backing up these clients, each to a different piece of<br>
media in different drives. This increases the hardware requirement costs. This also can<br>
create a "shoe shining" effect in which the tape is driven back and forth since drive<br>
capacity is under-utilized at certain times.<br>
Tape capacity is also growing and data from multiple clients can actually<br>
fit on a single piece of media especially if the backup being performed is an incremental<br>
backup. Scattering data across many pieces of media is a tape-handling nightmare for<br>
backup administrators.<br>
SUMMARY OF THE INVENTION<br>
In accordance with embodiments of the invention, a method is provided<br>
for performing a backup operation on a plurality of data streams containing data to be<br>
backed up. In one embodiment, the method involves combining the data streams into a<br>
single stream of one or more data chunks, including by writing data from more than one<br>
of the data streams into at least one data chunk. The combining may be done by<br>
multiplexing the data streams. The method further involves transmitting the one or more<br>
data chunks over a transport channel to a backup medium and storing the one or more<br>
data chunks on the backup medium..<br>
Data from the data streams may be written into a data chunk until the data<br>
chunk reaches a predetermined size, or until a configurable time interval has lapsed, or<br>
otherwise in accordance with a storage policy as disclosed in some of the pending<br>
applications referenced above, and as discussed herein.<br><br>
During a restore operation or during an operation to create an auxiliary<br>
backup copy, the data chunk is retrieved from the backup medium and data from the<br>
separate data streams are separated from the data chunk. All data streams written into a<br>
data chunk may be separated from each other into separate data stream portions. When<br>
the data streams have been multiplexed, separating involves demultiplexing the data<br>
streams written into the data chunk. The separated data streams may be restored to a<br>
client or further stored as auxiliary copies of the data streams.<br>
In some embodiments, the data streams contain data from a plurality of<br>
archive files. Combining the data streams thus may involve writing data from more than<br>
one archive files into at least one data chunk, and may further involve writing data from a<br>
single archive file into more than one data chunk. In these embodiments, a plurality of<br>
tag headers are inserted into the data chunk; each tag header describing data written in the<br>
data chunk from a corresponding archive file. Data may be written into a data chunk until<br>
the end of an archive file has been reached. When the data chunk is retrieved, from the<br>
backup medium, the data from at least one of the archive files is separated from the data<br>
chunk, or all the archive files may be separated into separate archive file portions, using<br>
the tag headers when necessary to identify and describe the separate archive file portions.<br>
The archive file portions may then be restored to a client or may be stored on an auxiliary<br>
storage device which may be accessed in turn during a restore operation of a given<br>
archive file requested by a client.<br>
In accordance with some embodiments, the invention provides a system<br>
for performing a backup operation on a plurality of data streams containing data to be<br>
backed up. The system includes one or more receivers for receiving the data streams, a<br>
multiplexer for combining the data streams into a combined data stream, a data writer for<br><br>
writing data from the combined data stream portion of the combined data streams into one<br>
or more data chunks, and one or more backup media for storing the one or more data<br>
chunks. The system may further include a transport channel for transporting the data<br>
chunks from the data writer to the backup media.<br>
In accordance with further aspects of embodiments of the present<br>
invention, a data structure is provided for a data chunk stored on a memory device. The<br>
data chunk data structure is used by a computer system to backup data and includes a<br>
plurality of portions of data from different archive files written into the data chunk from<br>
multiplexed data streams containing the archive files and a plurality of tag headers each<br>
describing one of the archive file portions written into the data chunk.<br>
. BRIEF DESCRIPTION OF THE DRAWINGS<br>
Fig. 1 presents a block diagram of a network architecture for a system to<br>
perform storage operations on electronic data in a computer network according to an<br>
embodiment of the invention.<br>
Fig. 2 presents a flow diagram of a system for multiplexing pipelined data<br>
. according to an embodiment of the invention.<br>
Fig. 3 presents a flow diagram of a system for multiplexing pipelined data<br>
according to an embodiment of the invention.<br>
Fig. 4 presents an exemplary current data format used prior to multiplexing<br>
according to embodiments of the invention.<br>
Fig. 5 presents an exemplary media format to support data multiplexing<br>
according to embodiments of the invention.<br>
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS<br><br>
The present invention includes methods and systems operating in<br>
conjunction with a modular storage system to enable computers on a network to share<br>
storage devices on a physical and logical level. An exemplary modular stongs system is<br>
the GALAXY™ backup and retrieval system and QiNetix™ storage management system<br>
available from CommVault Systems of New Jersey. The modular architecture underlying<br>
this system is described in the above referenced patent applications, each of winch is<br>
incorporated herein.<br>
Preferred embodiments of the invention are now described with reference<br>
to the drawings. An embodiment of the system of the present invention is showin Fig. 1.<br>
As shown, the system includes a client 85, a data agent 95, an information store 90,a<br>
storage manager (or storage management component) 100, a jobs agent 102, a storage<br>
manager index 107, one or more media management components (or media agent) IG;,<br>
one or more media agent indexes 110, and one or more storage devices 115. Although<br>
Fig. 1 depicts a system having two media agents 105, there may be one media agent 105,<br>
or a plurality of media agents 105 providing communication between the client 85,<br>
storage manager 100 and the storage devices 115. In addition, the system can include one<br>
or a plurality of storage devices 115.<br>
A client 85 can be any networked client 85 and preferably includes at least<br>
one attached information store 90. The information store 90 may be any memory device<br>
or local data storage device known in the art, such as a hard drive, CD-ROM drive, tape<br>
drive, RAM, or other types of magnetic, optical, digital and/or analog local storage. In<br>
some embodiments of the invention, the client 85 includes at least one data agent 95,<br>
which is a software module that is generally responsible for performing storage<br>
operations on data of a client 85 stored in information store 90 or other memory location.<br><br>
Storage operations include, but are not limited to, creation, storage, retrieval, migration,<br>
deletion, and tracking of primary or production volume data, secondary volume data,<br>
primary copies, secondary copies, auxiliary copies, snapshot copies, backup copies,<br>
incremental copies, differential copies, synthetic copies, HSM copies, archive copies,<br>
Information Lifecycle Management ("ILM") copies, and other types of copies and<br>
versions of electronic data. In some embodiments of the invention, the system provides at<br>
least one, and typically a plurality of data agents 95 for each client, each data agent 95 is<br>
intended to backup, migrate, and recover data associated with a different application. For<br>
example, a client 85 may have different individual data agents 95 designed to handle<br>
Microsoft Exchange data, Lotus Notes data,'Microsoft Windows file system data,<br>
Microsoft Active Directory Objects data, and other types of data known in the art.<br>
The storage manager 100 is generally a software, module or application<br>
that coordinates and controls the system, for example, the storage manager 100 manages<br>
and controls storage operations performed by the system. The storage manager 100<br>
communicates with all components of the system including client 85, data agent 95,<br>
media agent 105, and storage devices 115 to initiate and manage storage operations. The<br>
storage manager 100 preferably has an index 107, further described herein, for storing<br>
data related to storage operations. In general, the storage manager 100 communicates<br>
with storage devices 115 via a media agent 105. In some embodiments, the storage<br>
manager 100 communicates directly with the storage devices 115.<br>
The system includes one or more media agent 105. The media agent 105<br>
is generally a software module that conducts data, as directed by the storage manager 100,<br>
between the client 85 and one or more storage devices 115, for example, a tape library, a<br>
hard drive, a magnetic media storage device, an optical media storage device, or other<br><br>
storage device. The media agent 105 is communicatively coupled with and controls the<br>
storage device 115. For example, the media agent 105 might instruct a storage device<br>
115 to perform a storage operation, e.g., archive, migrate, or restore application specific<br>
data. The media agent 105 generally communicates with the storage device 115 via a<br>
local bus such as a SCSI adaptor.<br>
Each media agent 105 maintains an index cache 110 which stores index<br>
data that the system generates during storage operations as further described herein. For<br>
example, storage operations for Microsoft Exchange data generate index data. Media<br>
management index data includes, for example, information regarding the location of the<br>
stored data on a particular media, information regarding the content of the information<br>
stored such as file names, sizes, creation dates, formats, application types, and other file-<br>
related criteria, information regarding one or more clients associated with the information<br>
stored, information regarding one or more storage policies, storage criteria, or storage<br>
preferences associated with the information stored, compression information, retention-<br>
related information, encryption-related information, stream-related information, and other<br>
types of information. Index data thus provides the system with an efficient mechanism<br>
for performing storage operations including locating user files for recovery operations<br>
and for managing and tracking stored data.<br>
The system generally maintains two copies of the media management<br>
index data regarding particular stored data. A first copy is generally stored with the data<br>
copied to a storage device 115. Thus, a tape may contain the stored data as well as index<br>
information related to the stored data. In the event of a system restore, the index<br>
information stored with the stored data can be used to rebuild a media agent index 110 or<br>
other index useful in performing storage operations. In addition, the media agent 105 that<br><br>
controls the storage operation also generally writes an additional copy of the index data to<br>
its index cache 110. The data in the media agent index cache 110 is generally stored on<br>
faster media, such as magnetic media, and is thus readily available to the system for use<br>
in storage operations and other activities without having to be first retrieved from the<br>
storage device 115.<br>
The storage manager 100 also maintains an index cache 107. Storage<br>
manager index data is used to indicate, track, and associate logical relationships and<br>
associations between components of the system, user preferences, management tasks, and<br>
other useful data. For example, the storage manager 100 might use its index cache 107 to<br>
track logical associations between media agent 105 and storage devices 115. The storage<br>
manager 100 may also use its index cache 107 to track the status of storage operations to<br>
be performed, storage patterns associated with the system components such as media use,<br>
storage growth, network bandwidth, service level agreement ("SLA") compliance levels,<br>
data protection levels, storage policy information, storage criteria associated with user<br>
preferences, retention criteria, storage operation preferences, and other storage-related<br>
information.<br>
A storage policy is generally a data structure or other information which<br>
includes a set of preferences and other storage criteria for performing a storage operation.<br>
The preferences and storage criteria may include, but are not limited to: a storage<br>
location, relationships between system components, network pathway to utilize, retention<br>
policies, data characteristics, compression or encryption requirements, preferred system<br>
components to utilize in a storage operation, and other criteria relating to a storage<br>
operation. A storage policy may be stored to a storage manager index, to archive media as<br><br>
metadata for use in restore operations or other storage operations, or to other locations or<br>
components of the system.<br>
Index caches 107 and 110 typically reside on their corresponding storage<br>
component's hard disk or other fixed storage device. For example, the jobs agent 102 of a<br>
storage manager 100 may retrieve storage manager index 107 data regarding a storage<br>
policy and storage operation to be performed or scheduled for a particular client 85. The<br>
jobs agent 102, either directly or via another system module, communicates with the data<br>
agent 95 at the client 85 regarding the storage operation. In some embodiments, the jobs<br>
agent 102 also retrieves from the index cache 107 a storage policy associated with the<br>
client 85 and uses information from the storage policy to communicate to the data agent<br>
95 one or more media agents 105 associated with performing storage operations for that<br>
particular client 85 as well as other information regarding the storage operation to be<br>
performed such as retention criteria, encryption criteria, streaming criteria, etc. The data<br>
agent 95 then packages or otherwise manipulates the client information stored in the<br>
client information store 90 in accordance with the storage policy information and/or<br>
according to a user preference, and communicates this client data to the appropriate media<br>
agent(s) 100 for processing. The media agent(s) 105 store the data according to storage<br>
preferences associated with the storage, policy including storing the generated index data<br>
with the stored data, as well as storing a copy of the generated index data in the media<br>
agent index cache 110.<br>
In some embodiments, components of the system may reside and execute<br>
on the same computer. In some embodiments, a client component such as a data agent<br>
95, a media agent 105, or a storage manager 100 coordinates and directs local archiving,<br>
migration, and retrieval application functions as further described in Application Serial<br><br>
Number 09/610,738. These client components can function independently or together<br>
with other similar client components.<br>
Data and other information is transported throughout the system via<br>
buffers and network pathways including, among others, a high-speed data transfer<br>
mechanism, such as the CommVault DataPipe™, as further described in U.S. Patent No.<br>
6,418,478 and Application No. 09/495,751, each of which is hereby incorporated herein<br>
by reference in its entirety. Self describing tag headers are disclosed in these applications<br>
wherein data is transferred between a flexible grouping of data transport modules each<br>
supporting a separate function and leveraging buffers in a shared memory space. Thus, a<br>
data transport module receives a chunk of data and decodes how the data should be<br>
processed according to information contained in the chunk's header, and in some<br>
embodiments, the chunk's trailer. U.S. Patent No. 6,418,478 and Application No.<br>
09/495,751 generally address "logical data" transported via TCP/IP, however,<br>
embodiments of the invention herein are also contemplated which are directed to<br>
transporting, multiplexing, encrypting, and generally processing block level data as<br>
disclosed, for example, in pending Application No. 10/803,542, titled Method And<br>
System For Transferring Data In A Storage Operation, attorney docket number 4982/49,<br>
which is hereby incorporated herein by reference in its entirety.<br>
As discussed, these applications generally disclose systems and methods of<br>
processing logical data. Thus, for example, contiguous blocks of data from a file might<br>
be written on a first volume as blocks 1,2, 3,4, 5, etc. The operating system of the host<br>
associated with the first volume would assist in packaging the data adding additional OS-<br>
specific information to the chunks. Thus, when transported and stored on a second<br>
volume, the blocks might be written to the second in a non-contiguous order such as<br><br>
blocks 2,1, 5,3 , 4. On a restore storage operation, the blocks could (due to the OS-<br>
specific information and other information) be restored to the first volume in contiguous<br>
order, but there was no control over how the blocks were laid out or written to the second<br>
volume. Incremental block level backups of file data was therefore extremely difficult if<br>
not impossible in such a system since there was no discernable relationship between how<br>
blocks were written on the first volume and how they were written on the second volume.<br>
Thus, in some embodiments, the system supports transport and<br>
incremental backups (and other storage operations) of block level data via a TCP/IP (and<br>
other transport protocols) over a LAN, WAN, SAN, etc. Additional data is added to the<br>
multi-tag header discussed in the applications referenced above which communicates how<br>
each block was written on the first volume. Thus, for example, a header might contain a<br>
file map of how the blocks were written on the first volume and the map could be used to<br>
write the blocks in similar order on the second volume. In other embodiments, each<br>
chunk header might contain a pointer or other similar data structure indicating the<br>
chunk's position relative to other chunks in the file. Thus, when a file block or other<br>
block changed on the first volume, the system could identify and update the<br>
corresponding copy of the block located on the second volume and effectively perform an<br>
incremental backup or other storage operation.<br>
fn the system, for example as in the CommVault Galaxy system, archives<br>
are grouped by Storage Policy. Many clients/sub clients can point to the same Storage<br>
Policy. Each Storage Policy has a Primary copy and zero or more Secondary copies. Each<br>
Copy has one or more streams related to the number of Drives in a Drive Pool.<br>
The system uses a tape media to its maximum capacity and throughput by<br>
multiplexing data from several clients onto the same media at the same time. The system<br><br>
allows for a stream to be reserved more than once by different clients and have multiple<br>
data movers write to this same piece of media.<br>
During backup or other storage operations, data from a data agent to a '<br>
media agent is transferred over a "Data pipeline" as further described herein and in U.S.<br>
Patent No. 6,418,478 and Application No. 09/495,751. One or more transport processes<br>
or modules, such as the Dsbackup in the CommVault Galaxy system, form the tail end on<br>
the Media Agent for the pipeline. For example, in the Galaxy system, the Datamover<br>
process running as part of Dsbackup is responsible for writing data to the media. For data<br>
multiplexing, many such Data movers belonging to different pipelines have to write to the<br>
same piece of media. This can be achieved by splitting the Datamover pipeline process<br>
into multiple components including a data receiver, a data writer, and other modules as<br>
necessary.<br>
Fig. 2 presents a system for multiplexing pipelined data according to an<br>
embodiment of the invention. As shown, Fig. 2 includes various data streams 125 in<br>
communication with a transmit data pipeline 130 that includes one or more data receiver<br>
modules 135, a multiplexing module 140, additional modules 145, and a data writer<br>
module 150. Fig. 2 also includes a transport channel 155 from the transmit pipeline 130<br>
to a receive pipeline 160 that includes a second receiver module 165 and other modules<br>
170.<br>
Backup streams 125 are fed into the transmit pipeline 130. For example,<br>
in some embodiments, a backup process, such as the Dsbackup process in the<br>
CommVault Galaxy system, packages file data and other data into chunks and<br>
communicates the chunks via the backup streams 125. Thus, the transmit pipeline 130 or<br>
tail end of the pipeline copies the data received in pipeline buffers from the backup<br><br>
process via the backup data streams 125. A data receiver 135 processes the data received<br>
from each backup stream 125. In some embodiments, there is one data receiver 135 per<br>
backup stream 125, thus in the case of multiple backup streams 135, the system might<br>
contain multiple data receiver modules 135.<br>
A multiplexing module 140 combines the data received by the receiver<br>
module(s) 135 into a single stream of chunks as further described herein. Thus, the<br>
multiplexing module 140 may combine data from multiple archive files into a single<br>
chunk. Additional modules 145 perform other operations on the chunks of data to be<br>
transported such as encryption, compression, etc. as further described herein, in U.S.<br>
Patent No. 6,418,478, pending Application No. 09/495,751, and pending Application No.<br>
__/	, filed November 15,2004, attorney docket no. 4982/45.<br>
The data writer module 150 communicates the chunks of data from the<br>
transmit pipeline 130 over a transport channel 155 to the receive pipeline 160. The<br>
transport channel may comprise a buffer, a bus, a fiber optic channel, a LAN, a SAN, a<br>
WAN, a wireless communication medium, or other transport methods known in the art.<br>
There is generally one data writer 150 per media (not shown) that receives data from<br>
multiple data receivers 135 and writes data to the media. The data writer process 150 is<br>
generally invoked when the first pipeline is established to use a given media and<br>
generally remains running until all the pipelines backing up to this media are finished.<br>
The data writer 150 writes the data to media or to the receive pipeline 160 and closes a<br>
chunk when the chunk size is reached, the chunk size being a design parameter set to<br>
allow only certain size chunks for transmission over the datapipe. In some embodiments,<br>
the data writer 150 also updates the Archive Manager tables with the chunk information.<br>
A multiplexed chunk thus will contain data from many archive files.<br><br>
In some embodiments, the transmit pipeline receives data directly from the<br>
system's data agents and writes multiplexed data to the media directly without an<br>
intervening receive pipeline 160. Thus, in some embodiments, a single pipeline is also<br>
contemplated. In embodiments that include both a transmit pipeline 130 and a receive<br>
pipeline 160, the receive pipeline 160 processes data received from the transmit pipeline<br>
130 for storage to media, etc. A second data receiver 165 processes data received from<br>
the data writer 150 and additional modules 170 which may include encryption,<br>
decryption, compression, decompression modules, etc. further process the data before it is<br>
written to the storage media by a final data writer module (not shown).<br>
In some embodiments, Data Multiplexing is a property of a Storage Policy.<br>
Any storage policy with Data Multiplexing enabled has the ability to start backups for<br>
multiple sub clients to run simultaneously to the same media. In some embodiments, a<br>
resource manager process on the storage manager allows for multiple volume reservation<br>
for media belonging to storage policies with data multiplexing enabled.<br>
During a restore storage operation, the process is essentially reversed.<br>
Data is retrieved from the storage media and passed back through the pipeline to the<br>
original volume. Thus, during a restore, a data reader module (e.g. — a data receiver<br>
directed to also retrieve data from storage) identifies the data by the looking into the tag<br>
header of each retrieved chunk. Any offset into the chunk is a relative offset i.e. when<br>
restoring data from a given archive file all the data buffers encountered from a different<br>
archive file should not be counted into the offset calculation and should be thrown out.<br>
Data within each volume block size of data will contain data from different Archive files.<br>
The tag header also contains the archive file id. In addition, all the offsets stored are<br><br>
relative offset within an archive file and does not depend on actual physical location on<br>
the tape or other storage media.<br>
A more detailed description of data multiplexing according to<br>
embodiments of the invention is now described:<br>
A single backup is made up of one or more archive files. An archive file is<br>
made up of the smallest restorable component called the "Chunk". The chunk always<br>
belonged to only one archive file. With data multiplexing a chunk interleaves pipeline<br>
buffers from different pipelines. A tag header written for each buffer of data will uniquely<br>
identify the data to the archive file. The tag header contains the archive file id (serial ID)<br>
from the database corresponding to the archive file being backed up.<br>
In some embodiments, for example in the CommVault Galaxy system, one<br>
or more modules in the pipeline, such as the DsBackup module, package data or<br>
otherwise retrieve data from a primary volume to be backed up and from the pipeline, and<br>
sends the data to the DataMover or receive pipeline. DsBackup also initializes indexes<br>
and updates the index cache every time it receives a file header from a client. DataMover<br>
responsibility is to organize the data received from the dsBackup into chunks, start a new<br>
chunk when the size of the chunk reaches the predetermined value, update the archive<br>
manager tables information about the chunks and their location on that tape, also handle<br>
end of media conditions and media reservations. DataMover uses the MediaFileSystem<br>
object, for example I/O system API calls of a media agent or other system component, to<br>
write data on to the tape and read data from the tape. MediaFileSystem has a Write buffer<br>
and data is written onto the tape when this write buffer is filled with data.<br>
With the new data Multiplexing model of DataMover, the previous<br>
DataMover modules and their functionalities undergo changes.<br><br>
Referring now to Fig. 3, consider clients CI, C2, C3 and C4 are backing<br>
up at the same time and their data are getting multiplexed. DsBackup instantiates an<br>
object of Data Receiver and initializes the object. Media is mounted as a part of the<br>
initialization. One Data Writer object is instantiated for every media group Id.<br>
Considering that all the four clients share the same media group id; only one Data Writer<br>
object is instantiated and all the four Data Receiver objects share the same object to write<br>
to the media.<br>
Each Data Receiver writes a tag portion immediately by calling the Data<br>
Writer's Write 0 method. Data Writer has an internal buffer, which is the same as the<br>
selected block size. When this buffer is full, the buffer is locked and emptied to the<br>
media. While this write operation is ongoing to the media, the second buffer will be ready<br>
to accept data from the Data Receiver. The thread, which calls the write on Data Writer,<br>
will return from the function call when the Media IO is complete. Meanwhile, the second<br>
buffer fills. These double buffers are guarded with appropriate semaphores to ensure<br>
proper concurrent access.<br>
The Write operation is a blocking call and returns after completing the<br>
write. The Data Writer Write API takes in the archive file id as the parameter and once<br>
the write is completed, the physical offsets are updated in a list maintained by the Data<br>
Writer object accordingly. When the size of the chunk exceeds the pre-determined size,<br>
the chunk is automatically closed by writing a file mark, updating the archive manager<br>
tables in the storage manager or at the media agent, and also updating the physical offsets<br>
in the list or index maintained by the data writer object to track multiplexed storage files.<br>
The Data Writer object is responsible for handling the end of media condition and the<br>
Data Receiver does not generally require any knowledge about it.<br><br>
As previously discussed, only the data writer object generally knows the<br>
chunk closure, but there are conditions where the close chunk operation could be needed<br>
because of a CLOSE ARCHIVE FILE message sent by the client. This means that the<br>
system may need to close the chunk though the size of the chunk may not have reached<br>
the predetermined size. When a CLOSE ARCHIVE FILE message is received from the<br>
client, DsBackup calls into Data Receiver Close that in turn calls the Data Writer Close.<br>
This close waits for a pre-determined amount of time for the chunk to get close on its own<br>
as the other clients may be still pumping in data to the chunk. If after the pre-determined<br>
time the chunk is not closed, the chunk is closed forcefully by writing a file mark and<br>
updating the appropriate index cache. The only side effect this could result in is that the<br>
chunk may not be as big as the pre-determined size as the close chunk has been force<br>
fully done. The pre-determined time for wait can be made configurable or can be made a<br>
variable parameter depending on the client type. With this new model there can be a<br>
situation that the tag header gets split and is spanned between two data buffers on the<br>
tape. This is generally addressed during the restore of data.<br>
The following cases illustrate exemplary backup scenarios and<br>
considerations according to embodiments of the invention:<br>
1. Initialization of DataWriter. During the Initialization, the active media for the<br>
media group is mounted. This method returns success only if the media is<br>
mounted correctly. If the media is already mounted, this method just returns<br>
success along with the volume Id of the mounted media. This may be required for<br>
logging information for the Data Receiver in some embodiments.<br><br>
- 2. CreateArchiveFile: In this method, an Archive file header is written on to the<br>
media. This uses the special tag header which identifies the data in the tag portion<br>
as an archive file header.<br>
3.	WriteToMedia: This method returns information to the upper layer if the write is<br>
successful or not. Method returns information such as, end of the chunk, various<br>
media errors, Media is full etc. There is no other way to indicate these conditions<br>
other than as a return value in this method.<br>
4.	CloseArchiveFile: This method closes the archive file by writing an Archive file<br>
trailer to the media. This again has a specialized tag header which identifies the<br>
data as Archive file trailer. Close Archive file trailer does not return immediately,<br>
There is a configurable time interval for which the writing to the current chunk<br>
continues. The current chunk will be closed when all the archive files in this<br>
chunk gets over or after the above time out interval from the first archive file close<br>
request which comes in, whichever is the earliest.<br>
There is generally no need of any call back methods to Data Receiver from<br>
Data Writer. All communication from Data Writer to Receiver should be through return<br>
values of the functions called in.<br>
Restores of the multiplexed data are often less complicated since restores<br>
are generally not multiplexed as the back-ups. But the aim during the restores is to seek to<br>
the offsets and restore without looking into the tag headers in all of the data. Data Reader<br>
object is instantiated during restore. The parameter for this object remains the same as the<br>
current DataMover object. The client opens the required archive file by specifying the<br>
archive file id. Then the client sends the seek offset. The Data Reader object queries the<br>
archive manager to determine the chunk number that needs to be opened and the volume<br><br>
that should be mounted to seek to the give offset. Once the media is mounted the media is<br>
positioned to the correct file marker so as to open the chunk. Once the chunk header is<br>
read and discarded, data is read block by block and the size of the block is the same as the<br>
one that was used during the write. Every time a block of data is read all tag headers are<br>
examined to determine whether it contains the data of the archive file that we are looking<br>
for. This is done by traversing the buffer read in and looking through the tag headers. If it<br>
contains any other archive file's data, that tag portion is discarded and the next header is<br>
read. If the tag portion contains the data of the archive file that is being searched, then a<br>
check is done to see if the tag portion contains the offset that is being searched. If it does<br>
not contain the offset, this tag portion is skipped but the physical offset calculations are<br>
incremented appropriately. Once the correct block that contains the offset is reached, the<br>
data buffer pointer is positioned properly and the success is returned to the caller.<br>
Once the seek is successful, a data reader/retriever module in the pipeline,<br>
such as the FsRestoreHead module in the Galaxy system, requests a read with the read<br>
size equal to the size of the tag header. The process looks into the tag header to<br>
determine the size of the data that has to be read and requests a read with the size of the<br>
data. The restore happens in this fashion. The Data reader will have a buffer more than<br>
equal the size of one pipe line header as it may need to buffer the data during a read<br>
request. The Data Reader also takes care of the case of tag headers that may have spanned<br>
between two data blocks.<br>
There is also metadata that is written on to the tape (or other media) during<br>
back up to trouble shoot problems and also enable disaster recover programs, such as<br>
CommVault's Dr-restore program which retrieves data from backups. During backup<br>
every time a chunk is closed, a file marker is written. After this a data block is<br><br>
constructed containing information, e.g., the list of archive file id's whose data is<br>
contained in the recently closed chunk and their physical offsets and the size within this<br>
chunk. A file marker follows this data and does not generally make any kind of update to<br>
the database. In order to facilitate the disaster recovery tool functionality, we also indicate<br>
which of the archive file ids were closed in the current chunk.<br>
The data format on Media changes with Data Interleaving/Multiplexing.<br>
An exemplary current data format and related data structures used prior to multiplexing<br>
according to embodiments of the invention is shown in Fig. 4, and contains the following<br>
fields and properties:<br>
ArchiveFile Header<br>
{<br>
"ARCHIVE_FILE_HEADER_VERSION 2 \n "<br>
"HeaderSize"<br>
"Fileld"<br>
"FileSeqNo""<br>
"Streamld"<br>
"ArchiveFileGroup "<br>
"Agroupld "<br>
"Copyld"<br>
'AppId"<br>
"Jobld"<br>
"AppType"<br>
"BackupLevel"<br>
"BackupTime"<br>
"FileType"<br>
"SubAppType"<br>
"ClientName "<br>
"ArchiveFileName "<br>
"GalaxyVersion"<br>
"ARCfflVE_FrLE_HEADER_END 2"<br>
}<br>
ArchiveFile Trailer<br>
{<br>
"ARCH1VE_FILE_TRATLER_VERSI0N2 "<br>
"TrailerSize "<br>
"Fileld "<br>
"FileSeqNo"<br><br>
"Streaerld "<br>
"ArchiveFileGroup "<br>
"Agroupld "<br>
"Copyld "<br>
"Appld "<br>
"Jobld "<br>
"AppType "<br>
"BackupLevel "<br>
"BackupTime "<br>
"FileType "<br>
"SubAppType "<br>
"ClientName "<br>
"ArchiveFileName "<br>
"GalaxyVersion "<br>
"PhysicalFileSize "<br>
"LogicalFileSize "<br>
"ARCHIVE_FILE_TRAILER_END 2 ",<br>
ChunkHeader<br>
{<br>
"CHUNK_HEADER_VERSION 2 "<br>
"HeaderSize"<br>
"Fileld"<br>
"FileSeqNo"<br>
"ChunkSeqNo"<br>
"ChunkVol"<br>
"ChunkVoIFM"<br>
"TagHeaderOffset"<br>
"CHUNIC_HEADER_END 2"<br>
}<br>
ChunkTrailer<br>
{<br>
"CHUNK__1TLAILER_VERSI0N 1"<br>
"TrailerSize "<br>
"Fileld"<br>
"FileSeqNo"<br>
"ChunkSeqNo"<br>
"ChunkVol"<br>
"ChunkVoIFM "<br>
"ChunkLogicalSize "<br>
"ChtmkPhysicalSize "<br>
"CHUNK__TRAILER_END"<br><br>
An exemplary media format to support data multiplexing according to<br>
embodiments of the invention is shown in Fig. 5:<br>
When data multiplexing is enabled, other elements of the previous system<br>
also change in some embodiments as further described below. For example, Auxiliary<br>
Copy currently copies data chunk by chunk within an Archive File. The assumption is<br>
that the data within a chunk belongs to the same archive file. This is no longer true with<br>
data multiplexing. In embodiments where data multiplexing is supported, Auxiliary Copy<br>
allows two forms of copy mechanism: Simple Copy (copy whole chunk for all or part of<br>
archive files) and De-Multiplexed copy (archive file by archive file; only if source is<br>
magnetic).<br>
In a simple copy, Auxiliary Copy creates a list of archive files that needs<br>
to be copied and copies then chunk by chunk and volume by volume. Data from different<br>
archive files will be copied at the same time to the secondary copy. This is faster, but the<br>
resultant copy will have the data still interleaved as the original copy.<br>
In a de-multiplexed copy, Auxiliary Copy will copy data archive file by<br>
archive file. The result being that the system may go over the same set of media for each<br>
archive file discarding data encountered from a different archive file. This approach is<br>
slow and inefficient but the secondary copy has contiguous data for each archive file.<br>
The system uses flags and other signaling mechanisms, for example flag<br>
deMultiplexDataOnCopy on the ArchGroupCopy object, to dictate the choice of copy<br>
mechanism. Archive Manager will pass down a list of Archive Files to be copied to the<br>
secondary copy, if the copy is setup for a Simple Copy. If the DeMultiplexing is<br>
supported on the Copy, AuxCopyMgr will pass down a single archive file to be copied.<br><br>
Auxiliary Copy first creates all the headers for all archive files being<br>
copied and then starts the copy. A set of messages will be sent over the pipeline for<br>
creating these headers and in turn DSBackup will call DmReceiver create which will add<br>
archive file information to the dmreceiverinfo structure maintained in Dm Writer. In<br>
some embodiments, Auxiliary Copy also supports client based copies, where archive files<br>
belonging for a set of clients will be copied. In other embodiments, a synthetic full<br>
backup combines archive files backed up from a single sub client and creates a full<br>
backup of all the incremental changes since the last full backup. The new archive file<br>
being created as part of Synthetic full can be multiplexed with other backups.<br>
Systems and modules described herein may comprise software, firmware,<br>
hardware, or any combination(s) of software, firmware, or hardware suitable for the<br>
purposes described herein. Software and other modules may reside on servers,<br>
workstations, personal computers, computerized tablets, PDAs, and other devices suitable<br>
for the purposes described herein. Software and other modules may be accessible via<br>
local memory, via a network, via a browser or other application in an ASP context, or via<br>
other means suitable for the purposes described herein. Data structures described herein<br>
may comprise computer files, variables, programming arrays, programming structures, or<br>
any electronic information storage schemes or methods, or any combinations thereof,<br>
suitable for the purposes described herein. User interface elements described herein may<br>
comprise elements from graphical user interfaces, command line interfaces, and other<br>
interfaces suitable for the purposes described herein. Screenshots presented and<br>
described herein can be displayed differently as known in the art to input, access, change,<br>
manipulate, modify, alter, and work with information.<br><br>
While the invention has been described and illustrated in connection with<br>
preferred embodiments, many variations and modifications as will be evident to those<br>
skilled in this art may be made without departing from the spirit and scope of the<br>
invention, and the invention is thus not to be limited to the precise details of methodology<br>
or construction set forth above as such variations and modification are intended to be<br>
included within the scope of the invention.<br><br>
APPENDIX A<br>
Appendix A describes data structures, software modules, and other<br>
elements of the system according to embodiments of the invention, such as in the<br>
CommVault Galaxy system.<br>
ArchChunkTable<br>
table arch Chunk<br>
(<br>
id	bigint,	// 64 bit integer for unique chunk identification<br>
commCellld integer,<br>
volumeld integer,<br>
fileMarkerNo integer,<br>
createTime integer,<br>
chunkVersion integer,<br>
// INDEXES:<br>
primary key (archChunkld),<br>
foreign key (commCellld) references commCell(id),<br>
foreign key (volumeld) references MMS2Volume(VolumeId),<br>
);;<br>
ArchChunkMapping table<br>
Table archChunkMapping<br>
(<br>
archChunkld	bigint,<br>
archFileld	integer,<br>
commCellld	integer,<br>
archCopyld	integer,<br>
chunkerNumber	integer,<br>
physicalOffset	bigint,<br>
logicalOffset	bigint,<br>
physicalSize	bigint,<br>
logicalSize	bigint,<br>
primary key (archChunkld, archFileld, commCellld)<br>
foreign key (archChunkld, commCellld) references archChunk (id, commCellld),<br>
foreign key (archFileld, commCellld, archCopyld) references archFileCopy(archFileId,<br>
commCellld, copy),<br><br>
create table archGroupCopy<br>
id	serial,<br>
archGroupId	integer,<br>
commCellld	integer,<br>
copy	integer,<br>
name	varchar(MAX_DEFAULT_REC_SIZE,<br>
MINJ3EFAULT_REC_SIZE),<br>
compressionType	integer,<br>
flags	integer,<br>
maxMultiplex	integer,<br>
isActive	integer,<br>
type	integer,<br>
startTime	integer,<br>
waitForliDffline	integer,<br>
waitForlfBusy	integer,<br>
primary key (id, commCellld),<br>
foreign key (archGroupId) references archGroup(id),<br>
foreign key (commCellld) references commCell(id)<br>
):<br>
•	Every Chunk has a Unique 64 bit id used as a counter to track chunks and perform<br>
other functions<br>
•	Multiple archive file may be part of a single chunk<br>
•	The ArchChunkMapping table determines archive files contained in each chunk<br>
•	DmReceiver during restores queries archive manager and get the information for<br>
the chunk required by providing the archive file id and the physical offsets.<br>
•	All data contained in a chunk belongs to the same copy.<br>
•	An integer value in the ArchGroupCopy table defines the multiplexing factor, and<br>
determines how many clients can backup to this copy at the same time. This factor<br>
is applicable for all streams within the copy.<br>
•	deMultiplexDataOnCopy flag indicates whether Auxiliary Copy should de-<br>
multiplex data when creating a secondary copy. The flag on the secondary copy is<br>
what is taken into consideration.<br>
1.1.1 Creation of a Chunk<br>
•	DmWriter maintains a list of archive files that were written as part of the chunk<br>
•	When the chunk is closed, DmWriter makes a AMDS calls to close a chunk and<br>
will pass along a list of archive files that made up the chunk<br>
● Archive Manager creates the necessary entries in the ArchChunkMapping table<br>
and the archChunk table entries<br><br>
1.1.2 API's that changes in Archive Manager and AMDS<br>
(1) ArchiveManagerDS: :getAfilelnfo(GetAf ileInfoArgs_t *<br>
args, GetAfileInfoRet_t * ret)<br>
Following structures defined in source/indude/ArMgr/AmdsArgs.h and CVABasic.h<br>
change.<br>
typedef struct {<br>
unsigned long afileNumber;<br>
unsigned long storagePolicyNumber;<br>
unsigned long commCellld;<br>
intt needArchChunkld;<br>
} GetAfileInfoArgs_t;<br>
typedef struct {<br>
unsigned long afileNumber;<br>
unsigned long agroupNumber;<br>
unsigned long commCellld;<br>
string name;<br>
string objNamel;<br>
string objName2;<br>
unsigned long objVersion;<br>
unsigned long numAppId;<br>
unsigned long fileType;<br>
unsigned long backupTime;<br>
unsigned long seq;<br>
unsigned long flags;<br>
longlong__t createJobld;<br>
unsigned long objType;<br>
unsigned long backupLevel ,-<br>
unsigned long isValid;<br>
unsigned long streamNum;<br>
ULONGrLONG firstChunkld;<br>
} GetAfileInfoRet_t;<br>
(2) int ArchiveManagerDS: :closeChunk(CloseChunkArgsDS t *<br>
args, CloseChunkRetDS_t * ret)	<br>
Following structures defined in source/include/ArMgr/AmdsArgs.h and CVABasich<br>
change:<br>
struct CloseChunkArgsDS__t {<br>
unsigned long commCellld;<br>
unsigned long archCopyld;<br>
ULONGLONG archChunkld;<br><br>
int needNextArchChunkld;<br>
unsigned long volumeId;<br>
unsigned long fileMarkerNo;<br>
unsigned long numberOfAfiles;<br>
ChunkArchFileMsg_t *afileArray;<br>
CloseChunkArgsDS_t() : needNextArchChunkld(0) {}<br>
};<br>
typedef struct {<br>
ULONGLONG newChunkld;<br>
} CloseChunkRetDS_t;<br>
struct ChunkArchFileMsg_t {<br>
unsigned long archFileld;<br>
unsigned long chunkNumber;<br>
u_longlong_t physicalOffset;<br>
u__longlong_t logicalOf fset ;<br>
u_longlong_t physicalSize;<br>
u_longlong__t logicalSize;<br>
};<br>
typedef struct ArchiveFileId_t<br>
{<br>
unsigned long commCellld;<br>
unsigned long agroupNumber;<br>
unsigned long afileNumber;<br>
} ArcMveFileld t;<br>
1.2 Resource Manager<br>
In previous versions of the system which did not support multiplexing, a<br>
single Volume (defined by MMS2Volume table) could be reserved only once - the same<br>
held true for drive reservations. This behavior changes to support data multiplexing. A<br>
given volume can be reserved multiple times by different jobs for writes. The number of<br>
times a volume can be reserved for writes is determined by the an index value, such as the<br>
howManyCanMultiplex value set in the ArchGroupCopy table. The same stream also<br>
now can be reserved multiple times due to this change.<br><br>
commCellld	integer,<br>
archGroupId	integer,<br>
archGroupCopyld	integer,<br>
stream	integer,<br>
mediaGroupId	integer,<br>
isActive	integer,<br>
PreEmptable	integer,<br>
lastAfileldCopied	integer,<br>
howManyReservations	integer,	//Currently how<br>
// many reservations<br>
primary key (stream, commCellld, archGroupId, archGroupCopyld),<br>
foreign key (commCellld) references commCell(id),<br>
foreign key (archGroupId) references archGroup(id)<br>
foreign key (mediaGroupId) references MMS2MediaGroup(MediaGroupId)<br>
);;<br>
table archStreamReserve<br>
(<br>
jobld	LONGLONG,<br>
priority	integer,<br>
reserveTime .	TJME_T,<br>
interruptJobld	LONGLONG,<br>
);<br>
•	Resource Manager allows up to "howManyCanMultiplex" number of<br>
reservations on a volume, stream and drive when reservation type is "WRITE"<br>
•	Jobs running as part of a copy that support Multiplexing cannot generally be<br>
interrupted<br>
•	These jobs can be suspended or killed<br>
■ The Mark media full option is supported once per media group.<br>
•	Streams availability is no longer based on the "inuse" flag but on<br>
"howManyReservations" field. If this value equals the "howManyCanMultiplex"<br>
value for the ArchGroupCopy then that stream cannot generally be reserved.<br>
•	Resource Manager will reserve a specific drive.<br>
•	The selection of the drive is based on the volumeid and the mediaid set on the<br>
drive table. If the requested media is already cache mounted in the drive and then<br>
that drive is reserved.<br>
•	Resource Manager disallows any reservation of a client that wants to participate in<br>
data multiplexing if the client has not been upgraded to support multiplexing.<br>
1.3 Media manager<br>
In previous versions of the system that did not support multiplexing, Media Manager<br>
mounted a particular volume into any available drive and set the drive id in the<br><br>
. reservation tables. This will change as now the reservation will be made for a specific<br>
drive. When the mount request is received, Media Manager determines that the drive that<br>
is reserved for this job and mounts the media into that drive. If the media is cache<br>
mounted in a different drive then the reservation will be switched to that drive if that<br>
drive is not reserved.<br>
1.4 DSBackup<br>
With data multiplexing data of different clients will belong to the same chunk and hence<br>
each data block in the chunk has to be identified uniquely so as to perform the restore.<br>
This will be achieved by storing the archive file id into the tag header that is associated<br>
with every data block. The arch file id will uniquely identify the data block and the<br>
database can be used to determine the client to which the data belongs. The structure of<br>
an exemplary tag_header is given below.<br>
typedef struct<br>
{<br>
char tag_magic[8];	// version of the tag. =&gt; Always use<br>
// FILLTAGMAGIC to fill the tagjnagic<br>
ULONGLONGblock_offset;	// to be filled by client<br>
// offset within archive<br>
ULONGLONG blockjmmber;	//filled by client - block no<br>
// within afid<br>
ULONGLONG functional_header_number; // sequence no of fun.header<br>
unsigned long bufjype;	// type of the buffer -<br>
// MSB bit indicates data/functional<br>
// header / PL_NAS_CMDS etc.<br>
// 0-data 1 -functional header<br>
unsigned long actual_block_size; // size of the data associated with<br>
// this header<br>
unsigned long compressed_block_size;// compressed size of the data<br>
// associated with this header<br>
unsigned long validity_bits;	// flags<br>
unsigned long buffer_validity;	// invalid buffer<br>
unsigned long data_offset;	// offset of data within the block<br>
unsigned long next_header_offset; // offset of next header. 0 means none<br>
unsigned long compression_scheme; // scheme of compression to be applied<br><br>
// 0 gzip<br>
//111 (binary) no compression<br>
unsigned char scatter_gather; // wont be used in vldb 98<br>
// There are 3 extra bytes here for alignment<br>
unsigned long compressed_data; // data is compressed y/n ? This filed is<br>
// nolonger reliable because of a difference<br>
on UNIX<br>
unsigned char restore_backup;	// data stream of backup=0,restore==l<br>
unsigned char signature__dependency; // 0 not dependent on signature<br>
//I dependent on signature<br>
char spare[16 ];<br>
}tag_header_t;<br>
The field validity_bits has been renamed to archive_file_id to store the archive file as<br>
shown below.<br>
typedef struct<br>
{<br>
char tag_magic[8];	// version of the tag. -&gt; Always use<br>
// FILLTAGMAGIC to fill the tagjnagic<br>
ULONGLONG block_offset;	// to be filled by client.<br>
// offset within archive<br>
ULONGLONG block_number;	// filled by client - block no<br>
// within afid<br>
ULONGLONG functional_header_number; // sequence no of fun.header<br>
unsigned long bufjype;	//type of the buffer -<br>
// MSB bit indicates data/functional<br>
// header / PL_NAS_CMDS etc.<br>
// 0-data 1-functional header<br>
unsigned long actual_block_size; // size of the data associated with<br>
// this header<br>
unsigned long compressed_bl°ck_size;// compressed size of the data<br>
// associated with this header<br>
unsigned long archive_file_id;<br>
unsigned long buffer_yalidity;	// invalid buffer<br>
unsigned long data_offset;	// offset of data within the block<br>
unsigned long next_header_offset; // offset of next header. 0 means none<br>
unsigned long compression_scheme; // scheme of compression to be applied<br>
// 0 gzip<br>
//111 (binary) no compression<br>
unsigned char scatter_gather; // wont be used in vldb 98<br>
// There are 3 extra bytes here for alignment<br>
unsigned long compressed_data; // data is compressed y/n ? This filed is<br>
// nolonger reliable because of a difference<br>
on UNIX<br>
unsigned char restore_backup;	// data stream of backup=0,restore===l<br>
unsigned char signature_dependency; // 0 not dependent on signature<br><br>
// 1 dependent on signature<br>
char spare[16];<br>
}tag_headerj:;<br>
The archive file id is filled "by the pipe layer of the client during backup. The tag header is<br>
written on to the media in the same format without any modification. During restore Data<br>
Reader reads the tag header to get find the archive file and in turn determine whether the<br>
data associated with that restore is required for the current restore or not.<br>
1.5 DataMover (Windows Implementation )<br>
Datamover is responsible of writing the data transferred over the pipeline<br>
to the media. With data multiplexing Datamover gets split into two components, Data<br>
Receiver (DrnReceiver) and Data Writer (DmWriter). DsBackup invokes an instance of<br>
DmReceiver object. The DrnReceiver object internally checks for the DmWriter's<br>
existence for the requested MediaGroupId. If the DmWriter is not present then a new<br>
instance of the DmWriter is created and cached in a DmWriter map. This map is<br>
maintained in CVD's context of the media gent and is accessible to all DmReceiver.<br>
DmWriter maintains an internal buffer corresponding to the volume block of the data per<br>
DmReceiver. The volume block size is determined from the media type being used.<br>
Write on the DmReceiver will call DmWriter write. DmWriter will copy the pipeline<br>
buffers internally for aligning it to the volume block in the Receiverlnfo structure.<br>
DataMoverBase class is the class that implements the functionality of DataWriter. Since<br>
this class will be used for both backup and restore it was given a generic name<br>
"DataMoverBase"<br><br>
In the above classes, Data Receiver is a thin layer which in many cases calls the<br>
DataWriter methods directly.<br><br>
We Claim:<br>
1.	A method for performing a backup operation in a storage network, the<br>
method comprising:<br>
receiving a first data stream having first data, the first data being<br>
obtained by a first application-specific data agent;<br>
receiving a second data stream having second data, the second<br>
data being obtained by a second application-specific data agent;<br>
multiplexing the first and second data streams to form a single<br>
stream of one or more data chunks of an archive file, including writing the<br>
first data from the first data stream and the second data from the second<br>
data stream into a first data chunk of the one or more data chunks;<br>
transmitting the one or more data chunks over a transport channel<br>
to a backup medium; and<br>
storing the one or more data chunks on the backup medium.<br>
2.	The method as claimed in claim 1, wherein writing the first data and second<br>
data comprises writing data from a single archive file into more than one data<br>
chunk.<br><br>
3.	The method as claimed in claim 1, comprising inserting a plurality of tag<br>
headers into the first data chunk, each tag header describing data written in the<br>
data chunk from a corresponding data stream .<br>
4.	The method as claimed in claim 3, comprising writing data into a first data<br>
chunk until the end of an archive file has been reached.<br>
5.	The method as claimed in claim 1, wherein when the first and second data<br>
streams obtain data from a plurality of archive files, the method comprising<br>
retrieving the first data chunk from the backup medium and separating data<br>
from at least one of the archive files from one of the first and second data<br>
streams of the first data chunk, wherein separating comprises demultiplexing the<br>
data streams written into the first data chunk.<br>
6.	The method as claimed in claim 5, wherein the data from the archive files<br>
written into the first data chunk is separated from each other into separate<br>
archive file portions.<br>
7.	The method as claimed in claim 1, wherein forming a single stream of one or<br>
more data chunks comprises writing data into the first data chunk until the first<br>
data chunk reaches a predetermined size.<br><br>
8.	The method as claimed in claim 1, wherein forming a single stream of one or<br>
more data chunks comprises writing data into the first data chunk until a<br>
configurable time interval has lapsed.<br>
9.	The method as claimed in claim 1, wherein forming a single stream of one or<br>
more data chunks comprises writing data into the first data chunk in accordance<br>
with a storage policy.<br>
10.	The method as claimed in claim 1, wherein the data from all data streams<br>
written into the first data chunk is separated from each other into separate data<br>
stream portions.<br>
11.	The method as claimed in claim 1, wherein the first data and the second data<br>
comprise different types of data, and wherein each of the first and second data<br>
comprises at least one of the following types of data: migration data, snapshot<br>
data, backup data and archive data.<br>
12.	The method as claimed in claim 1, wherein said receiving the first data<br>
stream and receiving the second data stream comprise receiving the first and<br>
second data in the same buffer.<br><br>
13.	The method as claimed in claim 1, wherein the first data is obtained from a<br>
tape medium.<br>
14.	A system for performing a backup operation on a plurality of data streams<br>
containing data to be backed up in a storage network, the system comprising:<br>
a first agent module for obtaining first data, from a first application,<br>
for transmission in a first data stream;<br>
a second agent module for obtaining second data, from a second<br>
application, for transmission in a second data stream, the second<br>
application being different than the first application;<br>
one or more receivers for receiving the first data stream and the<br>
second data stream;<br>
a multiplexer for combining the first and second data streams into<br>
a combined data stream;<br>
a data writer for writing data from the combined data stream<br>
portion of the combined first and second data streams into one data<br>
chunk, the data chunk having both first data from the first application and<br>
second data from the second application; and<br>
one or more backup media for storing the data chunk .<br><br>
15.	The system as claimed in claim 14, comprising a transport channel for<br>
transporting the data chunk from the data writer to the backup media.<br>
16.	The system as claimed in claim 14, wherein the first agent module and the<br>
data writer are part of the same local computing system such that the first data<br>
stream is not transmitted over an area network.<br>
17.	The system as claimed in claim 16, wherein the first data stream is<br>
transmitted via a bus.<br>
18.	The system as claimed in claim 14, wherein the first and second data<br>
streams are from the same client computer.<br>
TITLE: SYSTEM AND METHOD FOR PERFORMING A BACKUP OPERATION USING<br>
MULTIPLE DATA STREAMS<br>
The invention relates to a method for performing a backup operation in a storage<br>
network, the method comprising receiving a first data stream having first data,<br>
the first data being obtained by a first application-specific data agent; receiving a<br>
second data stream having second data, the second data being obtained by a<br>
second application-specific data agent; multiplexing the first and second data<br>
streams to form a single stream of one or more data chunks of an archive file,<br>
including writing the first data from the first data stream and the second data<br>
from the second data stream into a first data chunk of the one or more data<br>
chunks; transmitting the one or more data chunks over a transport channel to a<br>
backup medium; and storing the one or more data chunks on the backup<br>
medium.</td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBhYnN0cmFjdC5wZGY=" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBjbGFpbXMucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBjb3JyZXNwb25kZW5jZSBvdGhlcnMucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 correspondence others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBkZXNjcmlwdGlvbihjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 description(complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBkcmF3aW5ncy5wZGY=" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBmb3JtLTEucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBmb3JtLTIucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBmb3JtLTMucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBmb3JtLTUucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBpbnRlcm5hdGlvbmFsIHB1YmxpY2F0aW9uLnBkZg==" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 international publication.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBvdGhlciBwY3QgZG9jdW1lbnQucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 other pct document.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBwY3QgZm9ybS5wZGY=" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 pct form.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNiBwcmlvcml0eSBkb2N1bWVudC5wZGY=" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006 priority document.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1jb3JyZXNwb25kZW5jZSBvdGhlcnMtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-correspondence others-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1jb3JyZXNwb25kZW5jZS0xLjEucGRm" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-correspondence-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1mb3JtLTEtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-form-1-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1mb3JtLTE4LnBkZg==" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-form-18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1mb3JtLTI2LnBkZg==" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-form-26.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1pbnRlcm5hdGlvbmFsIHB1YmxpY2F0aW9uLTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-international publication-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MDE2MjUta29sbnAtMjAwNi1pbnRlcm5hdGlvbmFsIHNlYXJjaCByZXBvcnQtMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">01625-kolnp-2006-international search report-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LSgyMC0wMS0yMDEyKS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-(20-01-2012)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LSgyOC0xMi0yMDExKS1BQlNUUkFDVC5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-(28-12-2011)-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LSgyOC0xMi0yMDExKS1BTUFOREVEIENMQUlNUy5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-(28-12-2011)-AMANDED CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LSgyOC0xMi0yMDExKS1DT1JSRVNQT05ERU5DRS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-(28-12-2011)-CORRESPONDENCE.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LTEtUEVUSVRJT04gVU5ERVIgUlVMRSAxMzcucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-1-PETITION UNDER RULE 137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUFCU1RSQUNULTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-ABSTRACT-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUFNQU5ERUQgQ0xBSU1TLnBkZg==" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-AMANDED CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUNPUlJFU1BPTkRFTkNFLTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-CORRESPONDENCE-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUNPUlJFU1BPTkRFTkNFLTEuMi5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-CORRESPONDENCE-1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LURFU0NSSVBUSU9OIChDT01QTEVURSktMS4xLnBkZg==" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-DESCRIPTION (COMPLETE)-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LURSQVdJTkdTLTEuMS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-DRAWINGS-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUVYQU1JTkFUSU9OIFJFUE9SVC5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMS0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 1-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMTgucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 18.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMi0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 2-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMjYucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 26.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMy0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 3-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMy0xLjIucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 3-1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gMy0xLjMucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 3-1.3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gNS0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 5-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUZPUk0gNS0xLjIucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-FORM 5-1.2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtQUJTVFJBQ1QucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-ABSTRACT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtQ0xBSU1TLnBkZg==" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-CLAIMS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtREVTQ1JJUFRJT04gKENPTVBMRVRFKS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-DESCRIPTION (COMPLETE).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtRFJBV0lOR1MucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-DRAWINGS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtRk9STSAxLnBkZg==" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-FORM 1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtRk9STSAyLnBkZg==" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-FORM 2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LUdSQU5URUQtU1BFQ0lGSUNBVElPTi5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-GRANTED-SPECIFICATION.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LU9USEVSUyBQQ1QgRk9STS5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-OTHERS PCT FORM.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LU9USEVSUy0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-OTHERS-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LU9USEVSUy5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-OTHERS.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LVBFVElUSU9OIFVOREVSIFJVTEUgMTM3LnBkZg==" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-PETITION UNDER RULE 137.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LVJFUExZIFRPIEVYQU1JTkFUSU9OIFJFUE9SVC0xLjEucGRm" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-REPLY TO EXAMINATION REPORT-1.1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MTYyNS1LT0xOUC0yMDA2LVJFUExZIFRPIEVYQU1JTkFUSU9OIFJFUE9SVC5wZGY=" target="_blank" style="word-wrap:break-word;">1625-KOLNP-2006-REPLY TO EXAMINATION REPORT.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=YWJzdHJhY3QtMDE2MjUta29sbnAtMjAwNi5qcGc=" target="_blank" style="word-wrap:break-word;">abstract-01625-kolnp-2006.jpg</a></p>
		<br>
		<div class="pull-left">
			<a href="252238-an-improved-gas-pressure-superplastic-forming-process-for-superplastics-forming-of-hemisphere-shapes.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="252240-a-sulphur-sorptive-membrane-and-a-method-for-separation-of-sulphur-compounds-from-liquid-hydrocarbons.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>252239</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>1625/KOLNP/2006</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>18/2012</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>04-May-2012</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>02-May-2012</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>13-Jun-2006</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>COMMVAULT SYSTEMS, INC.</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>TWO CRESCENT PLACE, OCEANPORT, NJ 07757-0090 USA</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>VIJAYAN RETNAMMA, MANOJ KUMAR</td>
											<td>2140 ALDRIN ROAD OCEAN, NJ 07712</td>
										</tr>
										<tr>
											<td>2</td>
											<td>KOTTOMTHARAYIL, RAJIV</td>
											<td>1508 GARDEN DRIVE OCEAN, NJ 07712</td>
										</tr>
										<tr>
											<td>3</td>
											<td>AMARENDRAN, ARUN PRASAD</td>
											<td>2406 E BLOCK, JOTHY, Illam SAHAKARA, NAGAR, BELLARY ROAD BANGALORE 650092</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G06F; G06F</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>PCT/US04/038280</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td>2004-11-15</td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>60/519,526</td>
									<td>2003-11-13</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/252239-system-and-method-for-performing-a-backup-operation-using-multiple-data-streams by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 13:28:45 GMT -->
</html>
