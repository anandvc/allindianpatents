<!DOCTYPE html>
<html lang="en">
  
<!-- Mirrored from www.allindianpatents.com/patents/217123-methods-and-apparatus-for-identifying-a-non-target-language-utterance-in-an-audio-stream by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:24:24 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Indian Patents. 217123:&quot;METHODS AND APPARATUS FOR IDENTIFYING A NON-TARGET LANGUAGE UTTERANCE IN AN AUDIO STREAM.&quot;</title>
    <meta content="authenticity_token" name="csrf-param" />
<meta content="cYcP52B8zyTWKbLwby2YPh9z/gvY/RLjWOwY4YXkiXg=" name="csrf-token" />

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.1/html5shiv.js" type="text/javascript"></script>
    <![endif]-->

    <link href="../assets/application-e80cf34975c5b1730c80b2f7170e7d26.css" media="all" rel="stylesheet" type="text/css" />

  </head>
  <body>

    <div class="navbar navbar-fluid-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="../index.html">Indian Patents</a>
          <div class="container-fluid nav-collapse">
            <ul class="nav">
              <li><a href="../recently-granted.html">Recently Granted Patents</a></li>
              <li><a href="../recently-published.html">Recently Published Patents</a></li>
            </ul>
            <form id="gform" class="navbar-search pull-right" action="https://www.google.com/search" method="get" target="_blank" onsubmit="document.getElementById('gform').q.value='site:http://www.allindianpatents.com '+document.getElementById('gform').q.value">
                <input type="text" name="q" id="q" class="search-query" placeholder="Search" onclick="this.value=''" autocomplete="off">
            </form>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row-fluid">
        <div class="span12">

          <style>
          .allindianpatents-top { width: 320px; height: 50px; }
          @media(min-width: 500px) { .allindianpatents-top { width: 468px; height: 60px; } }
          @media(min-width: 800px) { .allindianpatents-top { width: 728px; height: 90px; } }
          </style>
          <center>
          </center>
          
          <div class="row-fluid">
	<div class="span8">

		<table class="table">
			<tr>
				<th>Title of Invention</th>
				<td><h1 style="font-size:large;">&quot;METHODS AND APPARATUS FOR IDENTIFYING A NON-TARGET LANGUAGE UTTERANCE IN AN AUDIO STREAM.&quot;</h1></td>
			</tr>
			<tr>
				<th>Abstract</th>
				<td>Methods and apparatus are disclosed for detecting non-target language references in an audio transcription or speech recognition system using a confidence score. The confidence score may be based on (i) a probabilistic engine score provided by a speech recognition system, (ii) additional scores based on background models, or (iii) a combination of the foregoing. The engine score provided by the speech recognition system for a given input speech utterance reflects the degree of acoustic and linguistic match of the utterance with the trained target language. The background models are created or trained based on speech data in other languages, which may or may not include the target language itself. A number of types of background language models may be employed for each modeled language, including one or more of (i) prosodic models; (ii) acoustic models; (iii) phonotactic models; and (iv) keyword spotting models. The engine score can be combined with the background model scores to normalize the engine score for non-target languages. The present invention identifies a non-target language utterance within an audio stream when the confidence score falls below a predefined criteria. A language rejection mechanism can interrupt or modify the transcription process when speech in the non-target language is detected.</td>
			</tr>
		</table>

					<style>
					.allindianpatents-post-abstract { width: 320px; height: 50px; }
					@media(min-width: 880px) { .allindianpatents-post-abstract { width: 468px; height: 60px; } }
					@media(min-width: 1267px) { .allindianpatents-post-abstract { width: 728px; height: 90px; } }
					</style>
					<center>
					<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
					<!-- AllIndianPatents-post-abstract -->
					<ins class="adsbygoogle allindianpatents-post-abstract"
					     style="display:inline-block"
					     data-ad-client="ca-pub-7914358224572760"
					     data-ad-slot="9152759240"></ins>
					<script>
					(adsbygoogle = window.adsbygoogle || []).push({});
					</script>					
					</center>

		<table class="table">
			<tr>
				<th>Full Text</th>
				<td>METHODS AND APPARATUS FOR IDENTIFYING ANON-TARGET LANGUAGE IN A SPEECH RECOGNITION SYSTEM<br><br>
Field of the Invention	<br>
The present invention relates       <br>
 to methods and apparatus for detecting non-target languages utteranceis an audio stream in a monolingual       <br>
speech recognition system.<br>
Background of the Invention<br>
Speech recognition and audio indexing systems are generally developed for a specific target language. The lexica, grammar and acoustic models of such monolingual systems reflect the typical properties of the target language, In practice, however, these monolingual systems may be exposed to other non-target languages, leading to poor performance, including improper transcription or indexing, potential misinterpretations or false system reaction.<br>
For example, many organizations, such as broadcast news organizations and information retrieval services, must process large amounts of audio information, for storage and retrieval purposes. Frequently, the audio information must be classified by subject or speaker name, or both. In order to classify audio information by subject, a speech recognition system initially transcribes the audio information into text for automated classification or indexing. Thereafter, the index can be used to perform query-document matching to return relevant documents to the user.<br>
If the source audio information includes non-target language references, however, the speech recognition system may improperly transcribe the non-target language references, potentially leading to improper classification or indexing of the source information. A need therefore exists for a method and apparatus for detecting non-target language references in an audio transcription or speech recognition system.<br>
With the trend in globalizing communication technologies and providing<br><br>
services to a wide, multilingual public, the ability to distinguish between languages has<br>
become increasingly important. The language-rejection problem is closely related to this<br>
ability and thus to the problem of automatic language identification (ALT). For a detailed<br>
discussion of automatic language identification techniques, see, for example, Y.K.<br>
Muthusamy et al., "Reviewing Automatic Language Identification," IEEE Signal<br>
Processing Magazine, 11(4):33--41 (Oct. 1994); J. Navratil and W. Zuhlke,<br>
"Phonetic-Context Mapping in Language Identification," Proc. of the EUROSPEECH-97,<br>
Vol. 1, 71-74 (1997); and J. Navratil and W. Zuhlke, "An Efficient Phoriotactic-Acoustic<br>
System for Language Identification," Proc. of the Int'l Conf. on Acoustics, Speech and<br>
Signal Processing (ICASSP), Vol. 2, 781-84, Seattle, WA, IEEE (May, 1998), each<br>
incorporated by reference herein.	<br>
A number of automatic language identification techniques have been proposed or suggested for distinguishing languages based on various features contained in the speech signal. Several sources of language-discriminative information have been identified as relevant for the task of language identification including, for example, the prosody, the acoustics, and the grammatical and lexical structure. Automatic language identification techniques based on the prosody or acoustics of speech attempt to identify a given language based on typical melodic and pronunciation patterns, respectively.<br>
Due to the complexity of automatic language identification techniques based on the grammatical and lexical structure, however, most proposals have advanced techniques based on acoustic-prosodic information or derived lexical features in order to represent the phonetic structure in a less complex manner. ALI techniques have been developed that model statistical dependencies inherent in phonetic chains, referred to as the phonotactics. In the statistical sense, phonotactics can be viewed as a subset of grammatical and lexical rules of a language. Since these rules differ among languages, the ability to discriminate among languages is naturally reflected in the phonotactic properties.<br><br>
Summary of the Invention<br>
Generally, methods and apparatus are disclosed for detecting non-target language references in an audio transcription or speech recognition system using confidence scores. The confidence score may be based on (i) a probabilistic engine score provided by a speech recognition system, (ii) additional scores based on background models, or (iii) a combination of the foregoing. The engine score provided by the speech recognition system for a given input speech utterance reflects the degree of acoustic and linguistic match of the utterance with the trained target language, hi one illustrative implementation, the probabilistic engine score provided by the speech recognition system is combined with the background model scores to normalize the engine score as well as to account for the potential presence of a non-target language. The normalization narrows the variability range of the scores across speakers and channels.<br>
The present invention identifies a non-target language utterance within an audio stream when the confidence score falls below a predefined criteria. According to one aspect of the invention, a language rejection mechanism interrupts or modifies the transcription process when speech in the non-target language is detected. In this manner, the present invention prevents improper transcription and indexing and false interpretations of the speech recognition output.<br>
hi the presence of non-target language utterances, the transcription system is not able to find a good match based on its native vocabulary, language models and acoustic models. The resulting recognized text will have associated lower engine score values. Thus, the engine score alone may be used to identify a non-target language when the engine score is below a predefined threshold.<br>
The background models are created or trained based on speech data in several languages, which may or may'not include the target language itself. A number of types of background language models may be employed for each modeled language,<br><br>
including one or more of (i) prosodic models; (ii) acoustic models; (iii) phonotactic models; and (iv) keyword spotting models.<br>
Accordingly the instant invention provides for a method for identifying a non-target language utterance in an audio stream, comprising the steps of<br>
transcribing each utterance in said audio stream using a transcription system trained on a target language;<br>
generating a confidence score associated with each of said transcribed utterances; and<br>
identifying a transcribed utterance as being in a non-target language if said confidence score fails to meet predefined criteria.<br>
The instant invention also provides for a method for identifying a non-target language utterance in an audio stream, comprising the steps of:<br>
transcribing each utterance in said audio stream using a transcription system trained on a target language;<br>
generating a confidence score associated with each of said transcribed utterances based on an engine score provided by said transcription system and at least one background model score; and<br>
identifying a transcribed utterance as being in a non-target language if said confidence score fails to meet predefined criteria.<br>
Further the instant invention provides for a system for identifying a non-target language utterance in an audio stream, comprising:<br>
a memory that stores computer-readable code; and<br>
a processor operatively coupled to said memory, said processor configured to implement said computer-readable code, said computer-readable code configured to:<br>
transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
generate a confidence score associated with each of said transcribed utterances; and<br>
identify a transcribed utterance as being in a non-target language if said confidence score fails to meet predefined criteria.<br><br>
The invention also provides for a system for identifying a non-target language utterance in an audio stream, comprising:<br>
a memory that stores computer-readable code; and<br>
a processor operatively coupled to said memory, said processor configured to implement said computer-readable code, said computer-readable code configured to:<br>
transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
generate a confidence score associated with each of said transcribed utterances based on an engine score provided by said transcription system and at least one background model score; and<br>
identify a transcribed utterance as being in a non-target language if said confidence score fails to meet predefined criteria.<br>
An article of manufacture for identifying a non-target language utterance in an audio stream, comprising;<br>
a computer readable medium having computer readable code means embodied thereon, said computer readable program code means comprising:<br>
a step to transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
a step to generate a confidence score associated with each of said transcribed utterances; and<br>
a step to identify a transcribed utterance as being in a non-target language if said confidence score fails to meet predefined criteria.<br>
An article of manufacture for identifying a non-target language utterance in an audio stream, comprising:<br>
a computer readable medium having computer readable code means embodied thereon, said computer readable program code means comprising:<br>
a step to transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
a step to generate a confidence score associated with each of said transcribed utterances based on an engine score provided by said transcription system and at least one background model score; and<br><br>
a step to identify a transcribed utterance as being in a non-target language if said confidence score fails to meet predefined criteria.<br>
A more complete understanding of the present invention, as well as further features and advantages of the present invention, will be obtained by reference to the following detailed description and drawings.<br>
Brief Description of the Drawings<br>
FIG. 1 illustrates a non-target language identification system in accordance with the present invention;<br>
FIG. 2 is a schematic block diagram showing the architecture of an illustrative background language modeling module of FIG. 1; and<br>
FIG. 3 is a flow chart describing an exemplary background model score calculation process employed by the background language modeling module of FIG. 2.<br>
Detailed Description of Preferred Embodiments<br>
FIG. 1 illustrates a non-target language identification system 100 in accordance with the present invention. According to one feature of the present invention, a language rejection mechanism interrupts or modifies an otherwise conventional speech recognition process when speech in the non-target language is detected. In this manner, the present invention prevents improper transcription and indexing and false interpretations of the speech recognition output. The present invention employs probabilistic engine scores provided by a speech recognition system combined with additional scores based on background models to normalize the engine score for non-target languages.<br>
As shown in FIG. 1, the non-target language identification system 100 includes a transcription system 110, a background language modeling module 200, discussed further below in conjunction with FIG. 2, a normalization module 150 and a<br><br>
threshold decision module 160. As discussed further below, the transcription system 110 transcribes a speech signal and provides an engine score indicating the degree of confidence in a given transcription. In addition, the background language modeling module 200 generates a background (BG) model score indicating the probabilities for the hypotheses that the given transcription is associated with (i) the target and (ii) with a non-target language. As discussed further below, the normalization module 150 integrates one or both of the engine and BG model scores and the threshold decision module 160 compares the integrated score to predefined criteria to determine if a given transcription is likely associated with a non-target language utterance.<br>
ENGINE SCORE<br>
The transcription system 110 may be embodied as any speech recognition or transcription system that provides a confidence score, such as the Via Voice™ speech recognition system, commercially available from IBM Corporation of Armonk, NY. The transcription system 110 typically calculates a probabilistic engine score for the decoded audio stream given some set of acoustic model(s), pronunciation vocabulary and language model(s). In the monolingual environment of the present invention, these models are trained on one specific target language.<br>
During speech recognition based on speech in the target language, the value of the engine score depends on the type of speech and the channel quality. Nonetheless, there is a strong correspondence between the recognized text and the acoustic evidence. In the presence of non-target language utterances, however, the transcription system 110 is not able to find a good match based on its native vocabulary, language models and acoustic models. Thus, the resulting recognized text will have associated lower engine score values. In this manner, the engine score alone may be used to identify a non-target language when the engine score is below a predefined threshold.<br>
BACKGROUND MODELS SCORES<br>
As previously indicated, the present invention supplements the engine<br><br>
scores provided by the transcription system 110 with additional scores based on background models. In this manner, the present invention improves the accuracy of identifying target and non-target language utterances using background models. The background models are created or trained based on speech data in other languages, which may or may not include the target language itself. For identification purposes, scores based on all of these background models are calculated and are then used to normalize the engine score. As discussed further below in a section entitled "NORMALIZATION," the normalization helps to narrow the variability range of the scores across speakers and channels.<br>
Generally, the present invention utilizes a number of types of background language models for each non-target language to be modeled. The type of the background models should be diverse and should capture the properties of languages on the acoustic and linguistic level. The features used for training may range from amplitude and fundamental frequency measurements (prosodic models) to higher phonetic features, such as phone-level statistics (phonotactic models), partial or whole word keywords (keyword spotting models) up to full-fledged large-vocabulary recognizers.<br>
Thus, the background language models may include one or more of (i) prosodic models; (ii) acoustic models; (iii) phonotactic models; and (iv) keyword spotting models. For a more detailed discussion of various types of models, see, for example, Y.K. Muthusamy et al., "Reviewing Automatic Language Identification," IEEE Signal Processing Magazine, 11(4):33-41 (Oct. 1994); J. Navratil and W. Zilhlke, "Phonetic-Context Mapping in Language Identification," Proc. of the EUROSPEECH-97, Vol. 1, 71-74 (1997); and J. Navratil and W. Zuhlke, "An Efficient Phonotactic-Acoustic System for Language Identification," Proc. of the Int'l Conf. on Acoustics, Speech and Signal Processing (ICASSP), Vol. 2, 781-84, Seattle, WA, IEEE (May, 1998), each incorporated by reference herein.<br>
FIG. 2 is a schematic block diagram showing the architecture of an<br><br>
illustrative background language modeling module 200 in accordance with the present invention. The background language modeling module 200 may be embodied as a general purpose computing system, such as the general purpose computing system shown in FIG. 2. The background language modeling module 200 includes a processor 210 and related memory, such as a data storage device 220, which may be distributed or local. The processor 210 may be embodied as a single processor, or a number of local or distributed processors operating in parallel. The data storage device 220 and/or a read only memory (ROM) are operable to store one or more instructions, which the processor 210 is operable to retrieve, interpret and execute. It is noted that the background language modeling module 200 may be integrated with the transcription system 110 shown in FIG. 1, or the background language modeling module 200 may be a stand-alone device, as shown in FIG. 2, as would be apparent to a person of ordinary skill in the art.<br>
The data storage device 220 preferably includes a set of background models 250-1 for the target language and a set of background models 250-2 through 250-K for each modeled non-target language. As previously indicated, each set of background language models 250-K can include one or more of a (i) prosodic model; (ii) acoustic model; (iii) phonotactic model; and (iv) keyword spotting model. In one preferred embodiment, shown in FIG. 2, each set of background models includes acoustic and phonotactic models due to a favorable performance and cost ratio, hi addition, as discussed further below in conjunction with FIG. 3, the data storage device 220 includes a background model score calculation process 300. The exemplary background model score calculation process 300 calculates a phonotactic-acoustic score for each background model.<br>
It is noted that while the background models are trained on a certain set of languages, the normalization method of the present invention may contribute to improving the identification of a non-target language ever for non-target languages that were not previously seen in the training data set.<br><br>
CALCULATION OF BACKGROUND MODEL SCORE<br>
As previously indicated, the background language modeling module 200 executes a background model score calculation process 300 to calculate a score for each background model. FIG. 3 is a flow chart describing an exemplary background model score calculation process 300. It is again noted that the exemplary background model score calculation process 300 generates background model scores based on phonotactics (phone statistics) and acoustics.<br>
As shown in FIG. 3, the background model score calculation process 300 initially retrieves the utterance (speech sample) during step 310. ST-BG denotes the background score for the target language and SN-BG(I) denotes the background score for the i-th non-target model. Thereafter, the phonotactic-acoustic score is calculated for each background model, i, during step 320 as follows<br>
(Equation Removed)<br><br>
where a1 ... aT, denote a phone sequence obtained from a phone recognizer, such as a ballistic labeler, described for example, in United States Patent Application Serial Number 09/015,150, or Ramabhadan et al., "Acoustics Only Based Automatic Phonetic Baseform Generation," Proc. of the Int'l Conf. on Acoustics, Speech and Signal Processing (ICASSP), Seattle, WA, IEEE (May, 1998), each incorporated by reference herein. In addition, vt stands for the acoustic evidence (observation) within the speech segment of the phone at and P(at\ at-1,...,at-k, i) for a phonotactic model of the k-th order modeling (k+l)-tuples of phones in a sequence. It is noted that<br>
Equation (1) is one of many possible ways to obtain the phonotactic score, as would be apparent to a person of ordinary skill in the art. Alternative language modeling and language identification techniques may consist, for example, of calculating the phonotactic and acoustic scores separately and combining them in the log domain in a weighted manner. Furthermore, the phone duration information may be included, for example, using a Hidden Markov Model (FIMM). The background score for the target<br><br>
language, ST.BG, is also obtained using equation (1).<br>
NORMALIZATION<br>
Mathematically, the normalization performed by the normalization module 150 (FIG. 1) can be formulated in several ways. For example, if the engine and background scores are probabilistic, the normalization can be expressed as a ratio of the probability values of the target and non-target scores. Likewise, if the engine and background scores are expressed as log likelihoods, the normalization can be expressed as a difference between logarithmic scores of the target and non-target scores.<br>
In the illustrative embodiment, ST-E denotes the engine target score, ST-BG denotes the background score for the target language and SN-BG(i) denotes the background score for the i-th non-target model. Thus, the normalized score, S, may be obtained as follows:<br>
(Equation Removed)<br>
where N is the number of background models, and ai, bj are weights for the target and non-target scores, respectively. It is noted mat the robustness of the model of the background languages increases in proportion to the number, N, of background models. Thus, the language repertoire should be chosen as large and wide-covering as possible. It is again noted that while the background models are trained on a certain set of languages, the normalization method of the present invention may contribute to improving the identification of a non-target language ever for non-target languages that were not previously seen in the training data set.<br>
REJECTION MECHANISM<br>
As previously indicated, a non-target language utterance is identified based on the total normalized score, calculated in accordance with equation (2) and applying a threshold, T, as follows:<br>
S - T &gt; 0         Accept (Target Language)	<br>
S - T 
 <br>
Equation (3) leads to a positive or negative left side of the equation, resulting in acceptance or rejection of the utterance, respectively. The threshold value, T, may be obtained from a training stage and/or be derived in an adaptive manner from the current audio stream, as would be apparent to a person of ordinary skill in the art.<br>
The normalized score measure, S, at a certain time during transcription may be calculated within a window taking into account a history of the likelihood values from a predetermined time period. For example, a mean value of the word-based likelihoods within a predefined period of time may be utilized.<br>
In one application, the present invention may be employed as a language rejection mechanism to interrupt or modify the transcription system 110 when speech in the non-target language is detected. In other words, if the non-target language is detected in real-time using the present invention, then the speech recognition process can be suspended until the audio stream switches back to the target language. Performance of speech-based text retrieval systems depend heavily on the accuracy of the transcription. Generally, the higher the speech recognition accuracy, the better the performance of the information retrieval. In this manner, the present invention prevents improper transcription and indexing and false interpretations of the speech recognition output.<br>
In a further variation, the non-target language identification system 100 may use a different threshold value, TRESUME, for switching back to the target-language transcription after a previous rejection. The threshold value, TRESUME, may be adaptive or predetermined, as discussed above for the primary threshold value, T. In yet another variation, the present invention uses precomputed likelihoods (e.g., by-products) of the recognition process and low-computation background models.<br>
It is to be understood that the embodiments and variations shown and described herein are merely illustrative of the principles of this invention and that various modifications may be implemented by those skilled in the art without departing from the scope and spirit of the invention.<br><br><br><br><br>
We claim:<br>
1.	A method for identifying a non-target language utterance in an audio<br>
stream, comprising the steps of:<br>
transcribing each utterance in said audio stream using a transcription system trained on a target language;<br>
generating a confidence score associated with each of said transcribed utterances; and<br>
identifying a transcribed utterance as being in a non-target language if said confidence score generated by said transcription system trained on a target language fails to meet predefined criteria.<br>
2.	The method as claimed in claim 1, wherein said confidence score is an<br>
engine score generated by said transcription system.<br>
3.	The method as claimed in claim 1, comprising the step of interrupting said<br>
transcription system when said non-target language is detected.<br>
4.	The method as claimed in claim 1, comprising the step of modifying said<br>
transcription system when said non-target language is detected.<br>
5.	The method as claimed in claim 1, wherein said confidence score is based<br>
on one or more background models trained on at least one non-target<br>
language.<br>
6.	The method as claimed in claim 5, wherein said background models<br>
comprise one or more of (i) prosodic models;  (ii)  acoustic models;  (iii)<br>
phonotactic models; and (iv) keyword spotting models for each modeled<br>
language.<br>
7.	The method as claimed in claim 1, wherein said confidence score is based<br>
on an engine score provided by said transcription system combined with a<br>
background model score to normalize said engine score for said non-target<br>
language.<br><br>
8.	A method for identifying a non-target language utterance in an audio<br>
stream, comprising the steps of:<br>
transcribing each utterance in said audio stream using a transcription system trained on a target language;<br>
generating a confidence score associated with each of said transcribed utterances based on an engine score provided by said transcription system trained on a target language and at least one background model score; and identifying a transcribed utterance as being in a non-target language if said confidence score generated by said transcription system trained on a target language fails to meet predefined criteria.<br>
9.	The method as claimed in claim 8, comprising the step of interrupting said<br>
transcription system when said non-target language is detected.<br>
10.	The method as claimed in claim 8, comprising the step of modifying said<br>
transcription system when said non-target language is detected.<br>
11.	The method as claimed in claim 8, wherein said at least one background<br>
model is trained on at least one non-target language.<br>
12.	The method of claim 11, wherein said at least one background model<br>
comprises one or more of (i) prosodic models; (ii) acoustic models; (iii)<br>
phonotactic models; and (iv) keyword spotting models for each modeled<br>
language.<br>
13.	The method of claim 8, wherein said confidence score normalizes said<br>
engine score for said non-target language.<br>
14.	An apparatus for identifying a non-target language utterance in an audio<br>
stream as claimed in claim 1, comprising:<br>
a memory that stores computer-readable code; and<br><br>
a processor operatively coupled to said memory, said processor configured to implement said computer-readable code, said computer-readable code configured to:<br>
transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
generate a confidence score associated with each of said transcribed utterances; and<br>
identify a transcribed utterance as being in a non-target language if said confidence score generated by said transcription system trained on a target language fails to meet predefined criteria.<br>
15.	An apparatus for identifying a non-target language utterance in an audio<br>
stream as claimed in claim 8, comprising:<br>
a memory that stores computer-readable code; and<br>
a processor operatively coupled to said memory, said processor configured to implement said computer-readable code, said computer-readable code configured to:<br>
transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
generate a confidence score associated with each of said transcribed utterances based on an engine score provided by said transcription system trained on a target language and at least one background model score; and identify a transcribed utterance as being in a non-target language if said confidence score generated by said transcription system trained on a target language fails to meet predefined criteria.<br>
16.	An article of manufacture for identifying a non-target language utterance<br>
in an audio stream as claimed in claim 1, comprising:<br>
a computer readable medium having computer readable code means embodied thereon, said computer readable program code means comprising:<br>
a step to transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
a step to generate a confidence score associated. with each of said transcribed utterances; and<br><br>
a step to identify a transcribed utterance as being in a non-target language if said confidence score generated by said transcription system trained on a target language fails to meet predefined criteria.<br>
17.	An article of manufacture for identifying a non-target language utterance<br>
in an audio stream as claimed in claim 8, comprising:<br>
a computer readable medium having computer readable code means embodied thereon, said computer readable program code means comprising:<br>
a step to transcribe each utterance in said audio stream using a transcription system trained on a target language;<br>
a step to generate a confidence score associated with each of said transcribed utterances based on an engine score provided by said transcription system trained on a target language and at least one background model score; and<br>
a step to identify a transcribed utterance as being in a non-target language if said confidence score generated by said transcription system trained on a target language fails to meet predefined criteria.<br>
18.	A method for identifying a non-target language utterance in an audio<br>
stream substantially as herein described with reference to accompanying<br>
drawings.<br>
19.	An apparatus for identifying a non-target language utterance in an<br>
audio    stream    substantially    as    herein    described    with    reference    to<br>
accompanying drawings.<br>
20.	An  article   of  manufacture  for  identifying  a   non-target   language<br>
utterance in an audio stream substantially as herein described with reference<br>
to accompanying drawings.<br><br><br></td>
			</tr>
		</table>	
		<br>
		<h3>Documents:</h3>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWFic3RyYWN0LnBkZg==" target="_blank" style="word-wrap:break-word;">320-del-2001-abstract.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWFzc2lnbm1lbnQucGRm" target="_blank" style="word-wrap:break-word;">320-del-2001-assignment.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWNsYWltcy5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-claims.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWNvcnJlc3BvbmRlbmNlLW90aGVycy5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-correspondence-others.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWNvcnJlc3BvbmRlbmNlLXBvLnBkZg==" target="_blank" style="word-wrap:break-word;">320-del-2001-correspondence-po.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWRlc2NyaXB0aW9uIChjb21wbGV0ZSkucGRm" target="_blank" style="word-wrap:break-word;">320-del-2001-description (complete).pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWRyYXdpbmdzLnBkZg==" target="_blank" style="word-wrap:break-word;">320-del-2001-drawings.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWZvcm0tMS5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-form-1.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWZvcm0tMTkucGRm" target="_blank" style="word-wrap:break-word;">320-del-2001-form-19.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWZvcm0tMi5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-form-2.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWZvcm0tMy5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-form-3.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWZvcm0tNS5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-form-5.pdf</a></p>
				<p><a href="http://ipindiaonline.gov.in/patentsearch/GrantedSearch/pdfviewer.aspx?AppNo=MzIwLWRlbC0yMDAxLWdwYS5wZGY=" target="_blank" style="word-wrap:break-word;">320-del-2001-gpa.pdf</a></p>
		<br>
		<div class="pull-left">
			<a href="217122-a-noval-process-of-aqueous-finishing-for-waterproof-leathers.html">&laquo; Previous Patent</a>
		</div>
		<div class="pull-right">
			<a href="217124-support-for-single-node-quorum-in-a-two-node-nodeset-for-a-shared-disk-parallel-file-system.html">Next Patent &raquo;</a>
		</div>			
	</div><!-- /span8 -->
	<div class="span4">
		<div class="well infobox">
			<table class="table table-condensed">
				<tr>
					<th>Patent Number</th>
					<td>217123</td>
				</tr>
				<tr>
					<th>Indian Patent Application Number</th>
					<td>320/DEL/2001</td>
				</tr>
				<tr>
					<th>PG Journal Number</th>
					<td>13/2008</td>
				</tr>
				<tr>
					<th>Publication Date</th>
					<td>31-Mar-2008</td>
				</tr>
				<tr>
					<th>Grant Date</th>
					<td>25-Mar-2008</td>
				</tr>
				<tr>
					<th>Date of Filing</th>
					<td>21-Mar-2001</td>
				</tr>
				<tr>
					<th>Name of Patentee</th>
					<td>INTERNATIONAL BUSINESS MACHINE CORPORATION</td>
				</tr>
				<tr>
					<th>Applicant Address</th>
					<td>ARMONK, NEW YORK 10504, U.S.A.</td>
				</tr>
				<tr>
					<td colspan=2>
								<h5>Inventors:</h5>
								<table class="table">
									<tr>
										<th>#</th>
										<th>Inventor's Name</th>
										<th>Inventor's Address</th>
									</tr>

										<tr>
											<td>1</td>
											<td>NAVRATIL JIRI</td>
											<td>154A NORTH BROADWAY, APT#2D, WHITE PLAINS, NY 10603, U.S.A.</td>
										</tr>
										<tr>
											<td>2</td>
											<td>VISWANATHAN MAHESH</td>
											<td>3024 DOUGLAS DRIVE, YORKTOWN, NY 10598, U.S.A.</td>
										</tr>
								</table>
					</td>
				</tr>
				<tr>
					<th>PCT International Classification Number</th>
					<td>G10L 15/00</td>
				</tr>
				<tr>
					<th>PCT International Application Number</th>
					<td>N/A</td>
				</tr>
				<tr>
					<th>PCT International Filing date</th>
					<td></td>
				</tr>
				<tr>
					<td colspan=2>
						<h5>PCT Conventions:</h5>
						<table class="table">
							<tr>
								<th>#</th>
								<th>PCT Application Number</th>
								<th>Date of Convention</th>
								<th>Priority Country</th>
							</tr>

								<tr>
									<td>1</td>
									<td>09/544,678</td>
									<td>2000-04-07</td>
								    <td>U.S.A.</td>
								</tr>

						</table>
					</td>
				</tr>
			</table>
		</div><!-- /well -->
	</div><!-- /span4 -->
</div><!-- /row-fluid -->

        </div>

      </div><!--/row-->

      <footer class="footer">

        <style>
        .allindianpatents-footer { width: 320px; height: 50px; }
        @media(min-width: 500px) { .allindianpatents-footer { width: 468px; height: 60px; } }
        @media(min-width: 800px) { .allindianpatents-footer { width: 728px; height: 90px; } }
        </style>
        <center>
        </center>

        <p>&copy; All Indian Patents, 2013-2021.</p>
        <p>Patent data available in the public domain from Indian Patents Office, Department of Industrial Policy and Promotions, Ministry of Commerce and Industry, Government of India.</p>
      </footer>

    </div> <!-- /container -->

    <!-- Javascripts
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../assets/application-95f297ff0d8d2015987f04b30593c800.js" type="text/javascript"></script>

    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=8902313; 
    var sc_invisible=1; 
    var sc_security="3c1f8147"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/8902313/0/3c1f8147/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-244143-31', 'allindianpatents.com');
      ga('send', 'pageview');

    </script>

  </body>

<!-- Mirrored from www.allindianpatents.com/patents/217123-methods-and-apparatus-for-identifying-a-non-target-language-utterance-in-an-audio-stream by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 05 Apr 2024 11:24:25 GMT -->
</html>
